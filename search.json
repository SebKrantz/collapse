[{"path":"https://sebkrantz.github.io/collapse/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to collapse","title":"Contributing to collapse","text":"Please file issue feature request ideally using templates. broader proposals start discussion. contribute directly, fork entire repo (including ‘development’ branch), make changes ‘development’ branch, send PR ‘development’ branch. ’ll mention contributors DESCRIPTION file \"ctb\" contribution substantial improvement new functionality.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_data.table.html","id":"overview-of-both-packages","dir":"Articles","previous_headings":"","what":"Overview of Both Packages","title":"collapse and data.table","text":"data.table collapse high-performance packages work well together. effective co-use helpful understand strengths, one can , overlap. Therefore small comparison: data.table offers enhanced data frame based class contain data (including list columns). class provides concise data manipulation syntax also includes fast aggregation / slit-apply-combine computing, (rolling, non-equi) joins, keying, reshaping, time-series functionality like lagging rolling statistics, set operations tables number useful functions like fast csv reader, fast switches, list-transpose etc.. data.table makes data management, computations data easy scalable, supporting huge datasets memory efficient way. package caters well end user compressing enormous amount functionality two square brackets []. exported functions great programming also support classes, lot functionality optimization data.table happens hood can accessed non-standard evaluation table [, j, ] syntax. syntax cost 1-3 milliseconds call. Memory efficiency thread-parallelization make data.table star performer huge data. collapse class-agnostic nature, supporting vectors, matrices, data frames non-destructively handling R classes objects. focuses advanced statistical computing, proving fast column-wise grouped weighted statistical functions, fast complex data aggregation transformations, linear fitting, time series panel data computations, advanced summary statistics, recursive processing lists data objects. also includes powerful functions data manipulation, grouping / factor generation, recoding, handling outliers missing values. package default missing values na.rm = TRUE, implemented efficiently C/C++ functions. collapse supports tidyverse (piped) base R / standard evaluation programming. makes accessible ’s internal C/C++ based functionality (like grouping objects). collapse’s R functions simple strongly optimized, .e. access serial C/C++ code quickly, resulting baseline execution speeds 10-50 microseconds. makes collapse ideal advanced statistical computing matrices larger datasets, tasks requiring fast programs repeated function executions.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_data.table.html","id":"interoperating-and-some-dos-and-donts","dir":"Articles","previous_headings":"","what":"Interoperating and some Do’s and Dont’s","title":"collapse and data.table","text":"Applying collapse functions data.table always gives data.table back e.g.  default, collapse orders groups aggregations, equivalent using keyby data.table. gby / fgroup_by argument sort = FALSE yield unordered grouping equivalent data.table’s character data1. data size collapse outperforms data.table (might reverse data size grows, depending computer, number data.table threads used, function question): critical never something like : reason collapse functions S3 generic methods vectors, matrices data frames among others. incur method-dispatch every column every group function applied . may now contend base::mean also S3 generic, DT[, lapply(.SD, mean, na.rm = TRUE), = country, .SDcols = 9:13] code data.table use base::mean, data.table:::gmean, internal optimized mean function efficiently applied groups (see ?data.table::GForce). fmean works similar, includes functionality explicitly. can see x argument data, g argument grouping vectors, weight vector w, different options TRA transform original data using computed means, functionality regarding missing values (default: removed / skipped), group names (added row-names data frame, data.table) etc. can also give us result obtained high-level functions gby / fgroup_by collap. however data.table DT[, lapply(.SD, fmean), = country, .SDcols = 9:13]. Since fmean function recognizes able optimize, something like , applies fmean every group every column data. generally, important understand collapse based around applying functions data groups using universal mechanism: dplyr data %>% group_by(...) %>% summarize(...) / mutate(...) data.table [, j, ] syntax essentially universal mechanisms apply function data groups. data.table additionally internally optimizes functions (min, max, mean, median, var, sd, sum, prod, first, last, head, tail) called GForce, ?data.table::GForce. collapse instead provides grouped statistical transformation functions grouped computation done efficiently C++, supporting mechanisms (fgroup_by, collap) operate . data.table words, everything2 collapse, Fast Statistical Functions, data transformations, time series etc. GForce optimized. full set optimized grouped statistical transformation functions collapse : Additional optimized grouped functions include TRA, qsu, varying, fFtest, psmat, psacf, pspacf, psccf. nice thing GForce (fast) functions provided collapse can accessed explicitly programmatically without overhead incurred data.table, cover broader range statistical operations (mode, distinct values, order statistics), support sampling weights, operate class-agnostic way vectors, matrices, data.frame’s many related classes, cover transformations (replacing sweeping, scaling, (higher order) centering, linear fitting) time series functionality (lags, differences growth rates, including irregular time series unbalanced panels). want use fmean inside data.table, something like : Needless say kind programming seems bit arcane, actually great scope use collapse’s Fast Statistical Functions aggregations inside data.table. drive point home benchmark: evident data.table overhead, absolutely need kind syntax manipulation. scope use collapse transformation functions inside data.table. basic examples: TRA argument available Fast Statistical Functions (see macro .FAST_STAT_FUN) offers 10 different replacing sweeping operations. Note TRA() can also called directly replace sweep previously aggregated data.table. set operators %rr%, %r+%, %r-%, %r*%, %r/%, %cr%, %c+%, %c-%, %c*%, %c/% additionally facilitate row- column-wise replacing sweeping vectors statistics data.table’s. Similarly, can use following vector valued functions efficient data transformations: Since transformations (:= operations) highly optimized data.table, collapse faster circumstances.  Also time series functionality collapse significantly faster require data ordered balanced compute. example flag computes ordered lag without sorting entire data first.","code":"library(collapse) library(magrittr) library(data.table)  DT <- qDT(wlddev) # collapse::qDT converts objects to data.table using a shallow copy   DT %>% gby(country) %>% gv(9:13) %>% fmean #                    country      PCGDP   LIFEEX     GINI        ODA         POP #                     <char>      <num>    <num>    <num>      <num>       <num> #   1:           Afghanistan   483.8351 49.19717       NA 1487548499 18362258.22 #   2:               Albania  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:               Algeria  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4:        American Samoa 10071.0659       NA       NA         NA    43115.10 #   5:               Andorra 40083.0911       NA       NA         NA    51547.35 #  ---                                                                           # 212: Virgin Islands (U.S.) 35629.7336 73.71292       NA         NA    92238.53 # 213:    West Bank and Gaza  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:           Yemen, Rep.  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:                Zambia  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:              Zimbabwe  1219.4360 54.53360 45.93333  397104997  9402160.33  # Same thing, but notice that fmean give's NA's for missing countries DT[, lapply(.SD, mean, na.rm = TRUE), keyby = country, .SDcols = 9:13] # Key: <country> #                    country      PCGDP   LIFEEX     GINI        ODA         POP #                     <char>      <num>    <num>    <num>      <num>       <num> #   1:           Afghanistan   483.8351 49.19717      NaN 1487548499 18362258.22 #   2:               Albania  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:               Algeria  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4:        American Samoa 10071.0659      NaN      NaN        NaN    43115.10 #   5:               Andorra 40083.0911      NaN      NaN        NaN    51547.35 #  ---                                                                           # 212: Virgin Islands (U.S.) 35629.7336 73.71292      NaN        NaN    92238.53 # 213:    West Bank and Gaza  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:           Yemen, Rep.  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:                Zambia  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:              Zimbabwe  1219.4360 54.53360 45.93333  397104997  9402160.33  # This also works without magrittr pipes with the collap() function collap(DT, ~ country, fmean, cols = 9:13) #                    country      PCGDP   LIFEEX     GINI        ODA         POP #                     <char>      <num>    <num>    <num>      <num>       <num> #   1:           Afghanistan   483.8351 49.19717       NA 1487548499 18362258.22 #   2:               Albania  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:               Algeria  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4:        American Samoa 10071.0659       NA       NA         NA    43115.10 #   5:               Andorra 40083.0911       NA       NA         NA    51547.35 #  ---                                                                           # 212: Virgin Islands (U.S.) 35629.7336 73.71292       NA         NA    92238.53 # 213:    West Bank and Gaza  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:           Yemen, Rep.  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:                Zambia  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:              Zimbabwe  1219.4360 54.53360 45.93333  397104997  9402160.33 library(microbenchmark)  microbenchmark(collapse = DT %>% gby(country) %>% get_vars(9:13) %>% fmean,                data.table = DT[, lapply(.SD, mean, na.rm = TRUE), keyby = country, .SDcols = 9:13]) # Unit: microseconds #        expr     min       lq     mean   median       uq      max neval #    collapse 203.073 212.7285 223.4156 217.1565 225.6230  475.559   100 #  data.table 758.623 777.4010 929.5450 793.1655 854.4605 2292.515   100 DT[, lapply(.SD, fmean), keyby = country, .SDcols = 9:13] # Key: <country> #                    country      PCGDP   LIFEEX     GINI        ODA         POP #                     <char>      <num>    <num>    <num>      <num>       <num> #   1:           Afghanistan   483.8351 49.19717       NA 1487548499 18362258.22 #   2:               Albania  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:               Algeria  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4:        American Samoa 10071.0659       NA       NA         NA    43115.10 #   5:               Andorra 40083.0911       NA       NA         NA    51547.35 #  ---                                                                           # 212: Virgin Islands (U.S.) 35629.7336 73.71292       NA         NA    92238.53 # 213:    West Bank and Gaza  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:           Yemen, Rep.  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:                Zambia  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:              Zimbabwe  1219.4360 54.53360 45.93333  397104997  9402160.33 fmean # function (x, ...)  # UseMethod(\"fmean\") # <bytecode: 0x10fb58540> # <environment: namespace:collapse> methods(fmean) # [1] fmean.data.frame* fmean.default*    fmean.grouped_df* fmean.list*       fmean.matrix*     # [6] fmean.units*      fmean.zoo*        # see '?methods' for accessing help and source code args(fmean.data.frame) # function (x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],  #     use.g.names = TRUE, drop = TRUE, nthreads = .op[[\"nthreads\"]],  #     ...)  # NULL fmean(gv(DT, 9:13), DT$country) #           PCGDP   LIFEEX     GINI        ODA         POP #           <num>    <num>    <num>      <num>       <num> #   1:   483.8351 49.19717       NA 1487548499 18362258.22 #   2:  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4: 10071.0659       NA       NA         NA    43115.10 #   5: 40083.0911       NA       NA         NA    51547.35 #  ---                                                     # 212: 35629.7336 73.71292       NA         NA    92238.53 # 213:  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:  1219.4360 54.53360 45.93333  397104997  9402160.33  # Or g <- GRP(DT, \"country\") add_vars(g[[\"groups\"]], fmean(gv(DT, 9:13), g)) #                    country      PCGDP   LIFEEX     GINI        ODA         POP #                     <char>      <num>    <num>    <num>      <num>       <num> #   1:           Afghanistan   483.8351 49.19717       NA 1487548499 18362258.22 #   2:               Albania  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:               Algeria  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4:        American Samoa 10071.0659       NA       NA         NA    43115.10 #   5:               Andorra 40083.0911       NA       NA         NA    51547.35 #  ---                                                                           # 212: Virgin Islands (U.S.) 35629.7336 73.71292       NA         NA    92238.53 # 213:    West Bank and Gaza  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:           Yemen, Rep.  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:                Zambia  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:              Zimbabwe  1219.4360 54.53360 45.93333  397104997  9402160.33 BY(gv(DT, 9:13), g, fmean) # using collapse::BY #           PCGDP   LIFEEX     GINI        ODA         POP #           <num>    <num>    <num>      <num>       <num> #   1:   483.8351 49.19717       NA 1487548499 18362258.22 #   2:  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4: 10071.0659       NA       NA         NA    43115.10 #   5: 40083.0911       NA       NA         NA    51547.35 #  ---                                                     # 212: 35629.7336 73.71292       NA         NA    92238.53 # 213:  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:  1219.4360 54.53360 45.93333  397104997  9402160.33 .FAST_FUN #  [1] \"fmean\"      \"fmedian\"    \"fmode\"      \"fsum\"       \"fprod\"      \"fsd\"        \"fvar\"       #  [8] \"fmin\"       \"fmax\"       \"fnth\"       \"ffirst\"     \"flast\"      \"fnobs\"      \"fndistinct\" # [15] \"fcumsum\"    \"fscale\"     \"fbetween\"   \"fwithin\"    \"fhdbetween\" \"fhdwithin\"  \"flag\"       # [22] \"fdiff\"      \"fgrowth\" # This does not save the grouping columns, we are simply passing a grouping vector to g # and aggregating the subset of the data table (.SD). DT[, fmean(.SD, country), .SDcols = 9:13] #           PCGDP   LIFEEX     GINI        ODA         POP #           <num>    <num>    <num>      <num>       <num> #   1:   483.8351 49.19717       NA 1487548499 18362258.22 #   2:  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4: 10071.0659       NA       NA         NA    43115.10 #   5: 40083.0911       NA       NA         NA    51547.35 #  ---                                                     # 212: 35629.7336 73.71292       NA         NA    92238.53 # 213:  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:  1219.4360 54.53360 45.93333  397104997  9402160.33  # If we want to keep the grouping columns, we need to group .SD first. DT[, fmean(gby(.SD, country)), .SDcols = c(1L, 9:13)] #                    country      PCGDP   LIFEEX     GINI        ODA         POP #                     <char>      <num>    <num>    <num>      <num>       <num> #   1:           Afghanistan   483.8351 49.19717       NA 1487548499 18362258.22 #   2:               Albania  2819.2400 71.68027 31.41111  312928126  2708297.17 #   3:               Algeria  3532.2714 63.56290 34.36667  612238500 25305290.68 #   4:        American Samoa 10071.0659       NA       NA         NA    43115.10 #   5:               Andorra 40083.0911       NA       NA         NA    51547.35 #  ---                                                                           # 212: Virgin Islands (U.S.) 35629.7336 73.71292       NA         NA    92238.53 # 213:    West Bank and Gaza  2388.4348 71.60780 34.52500 1638581462  3312289.13 # 214:           Yemen, Rep.  1069.6596 52.53707 35.46667  859950996 13741375.82 # 215:                Zambia  1318.8627 51.09263 52.68889  734624330  8614972.38 # 216:              Zimbabwe  1219.4360 54.53360 45.93333  397104997  9402160.33 microbenchmark(collapse = DT %>% gby(country) %>% get_vars(9:13) %>% fmean,                data.table = DT[, lapply(.SD, mean, na.rm = TRUE), keyby = country, .SDcols = 9:13],                data.table_base = DT[, lapply(.SD, base::mean, na.rm = TRUE), keyby = country, .SDcols = 9:13],                hybrid_bad = DT[, lapply(.SD, fmean), keyby = country, .SDcols = 9:13],                hybrid_ok = DT[, fmean(gby(.SD, country)), .SDcols = c(1L, 9:13)]) # Unit: microseconds #             expr      min        lq      mean    median        uq       max neval #         collapse  207.419  234.9915  322.3994  255.6760  283.6790  1685.305   100 #       data.table  755.630  845.7685 1029.9024  904.6650  962.1060  2409.529   100 #  data.table_base 2795.257 3148.4310 4034.2081 3349.8025 3561.9570 37919.916   100 #       hybrid_bad 2198.994 2481.3815 3737.1102 2650.5680 2909.4215 62158.747   100 #        hybrid_ok  374.699  451.1025  564.6873  484.9275  542.8605  2082.554   100 # Computing a column containing the sum of ODA received by country DT[, sum_ODA := sum(ODA, na.rm = TRUE), by = country] # Same using fsum; \"replace_fill\" overwrites missing values, \"replace\" keeps the DT[, sum_ODA := fsum(ODA, country, TRA = \"replace_fill\")]   # Same: A native collapse solution using settransform (or its shortcut form) settfm(DT, sum_ODA = fsum(ODA, country, TRA = \"replace_fill\"))    # settfm may be more convenient than `:=` for multiple column modifications, # each involving a different grouping:   # This computes the percentage of total ODA distributed received by    # each country both over time and within a given year settfm(DT, perc_c_ODA = fsum(ODA, country, TRA = \"%\"),            perc_y_ODA = fsum(ODA, year, TRA = \"%\")) setdiff(.FAST_FUN, .FAST_STAT_FUN) # [1] \"fcumsum\"    \"fscale\"     \"fbetween\"   \"fwithin\"    \"fhdbetween\" \"fhdwithin\"  \"flag\"       # [8] \"fdiff\"      \"fgrowth\" # Centering GDP DT[, demean_PCGDP := PCGDP - mean(PCGDP, na.rm = TRUE), by = country] DT[, demean_PCGDP := fwithin(PCGDP, country)]  # Lagging GDP DT[order(year), lag_PCGDP := shift(PCGDP, 1L), by = country] DT[, lag_PCGDP := flag(PCGDP, 1L, country, year)]  # Computing a growth rate DT[order(year), growth_PCGDP := (PCGDP / shift(PCGDP, 1L) - 1) * 100, by = country] DT[, lag_PCGDP := fgrowth(PCGDP, 1L, 1L, country, year)] # 1 lag, 1 iteration  # Several Growth rates DT[order(year), paste0(\"growth_\", .c(PCGDP, LIFEEX, GINI, ODA)) := (.SD / shift(.SD, 1L) - 1) * 100,     by = country, .SDcols = 9:13]  # Same thing using collapse DT %<>% tfm(gv(., 9:13) %>% fgrowth(1L, 1L, country, year) %>% add_stub(\"growth_\"))  # Or even simpler using settransform and the Growth operator settfmv(DT, 9:13, G, 1L, 1L, country, year, apply = FALSE)  head(DT) #        country  iso3c       date  year decade     region     income   OECD PCGDP LIFEEX  GINI #         <char> <fctr>     <Date> <int>  <int>     <fctr>     <fctr> <lgcl> <num>  <num> <num> # 1: Afghanistan    AFG 1961-01-01  1960   1960 South Asia Low income  FALSE    NA 32.446    NA # 2: Afghanistan    AFG 1962-01-01  1961   1960 South Asia Low income  FALSE    NA 32.962    NA # 3: Afghanistan    AFG 1963-01-01  1962   1960 South Asia Low income  FALSE    NA 33.471    NA # 4: Afghanistan    AFG 1964-01-01  1963   1960 South Asia Low income  FALSE    NA 33.971    NA # 5: Afghanistan    AFG 1965-01-01  1964   1960 South Asia Low income  FALSE    NA 34.463    NA # 6: Afghanistan    AFG 1966-01-01  1965   1960 South Asia Low income  FALSE    NA 34.948    NA #          ODA     POP     sum_ODA perc_c_ODA perc_y_ODA demean_PCGDP lag_PCGDP growth_PCGDP #        <num>   <num>       <num>      <num>      <num>        <num>     <num>        <num> # 1: 116769997 8996973 89252909923  0.1308305  0.4441407           NA        NA           NA # 2: 232080002 9169410 89252909923  0.2600251  0.7356654           NA        NA           NA # 3: 112839996 9351441 89252909923  0.1264272  0.3494956           NA        NA           NA # 4: 237720001 9543205 89252909923  0.2663443  0.7003399           NA        NA           NA # 5: 295920013 9744781 89252909923  0.3315522  0.8570540           NA        NA           NA # 6: 341839996 9956320 89252909923  0.3830015  0.8992630           NA        NA           NA #    growth_LIFEEX growth_GINI growth_ODA growth_POP G1.PCGDP G1.LIFEEX G1.GINI    G1.ODA   G1.POP #            <num>       <num>      <num>      <num>    <num>     <num>   <num>     <num>    <num> # 1:            NA          NA         NA         NA       NA        NA      NA        NA       NA # 2:      1.590335          NA   98.74969   1.916611       NA  1.590335      NA  98.74969 1.916611 # 3:      1.544202          NA  -51.37884   1.985199       NA  1.544202      NA -51.37884 1.985199 # 4:      1.493830          NA  110.66998   2.050636       NA  1.493830      NA 110.66998 2.050636 # 5:      1.448294          NA   24.48259   2.112246       NA  1.448294      NA  24.48259 2.112246 # 6:      1.407306          NA   15.51770   2.170793       NA  1.407306      NA  15.51770 2.170793 # Lets generate a large dataset and benchmark this stuff DT_large <- replicate(1000, qDT(wlddev), simplify = FALSE) %>%      lapply(tfm, country = paste(country, rnorm(1))) %>%     rbindlist  # 12.7 million Obs fdim(DT_large) # [1] 13176000       13  microbenchmark(   S1 = DT_large[, sum_ODA := sum(ODA, na.rm = TRUE), by = country],   S2 = DT_large[, sum_ODA := fsum(ODA, country, TRA = \"replace_fill\")],   S3 = settfm(DT_large, sum_ODA = fsum(ODA, country, TRA = \"replace_fill\")),   W1 = DT_large[, demean_PCGDP := PCGDP - mean(PCGDP, na.rm = TRUE), by = country],   W2 = DT_large[, demean_PCGDP := fwithin(PCGDP, country)],   L1 = DT_large[order(year), lag_PCGDP := shift(PCGDP, 1L), by = country],   L2 = DT_large[, lag_PCGDP := flag(PCGDP, 1L, country, year)],   L3 = DT_large[, lag_PCGDP := shift(PCGDP, 1L), by = country], # Not ordered   L4 = DT_large[, lag_PCGDP := flag(PCGDP, 1L, country)], # Not ordered   times = 5 ) # Unit: milliseconds #  expr        min         lq      mean     median        uq       max neval #    S1  343.03396  347.18443  391.7494  364.51431  379.7866  524.2279     5 #    S2  100.52544  101.72645  165.8369  128.76042  153.6818  344.4906     5 #    S3   98.48249  104.80830  120.3499  114.20591  127.0192  157.2335     5 #    W1  913.00883 1009.29930 1071.0633 1035.74446 1104.7680 1292.4957     5 #    W2   99.48199   99.69654  110.0907  113.95884  118.5229  118.7931     5 #    L1 1812.59987 1822.58026 1896.8809 1905.67377 1942.9434 2000.6074     5 #    L2  110.36056  128.45845  135.0995  133.80219  139.1405  163.7357     5 #    L3  611.28392  665.22123  768.0616  718.38679  803.7170 1041.6991     5 #    L4   64.26369   66.99006  105.7952   84.26537  106.1809  207.2758     5  rm(DT_large) gc() #           used  (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb) # Ncells 3113072 166.3    8413097  449.4         NA   8413097  449.4 # Vcells 7897587  60.3  324289364 2474.2      16384 405361681 3092.7"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_data.table.html","id":"further-collapse-features-supporting-data-tables","dir":"Articles","previous_headings":"","what":"Further collapse features supporting data.table’s","title":"collapse and data.table","text":"mentioned, qDT flexible fast function create / column-wise convert R objects data.table’s. can also row-wise convert matrix data.table using mrtl: computational efficiency functions makes useful use data.table based workflows. example regress growth rate GDP per capita Growth rate life expectancy country save results data.table: need coefficients, standard errors, can also use collapse::flm together mrtl: … provides significant speed gain : Another feature highlight point collapse’s list processing functions, particular rsplit, rapply2d, get_elem unlist2d. rsplit efficient recursive generalization split: can use rapply2d apply function data frame / data.table arbitrary nested structure: can turn list data.table calling first get_elem recursively extract coefficient matrices unlist2d recursively bind new data.table: fact nested list matrices, can save names lists level nesting row- column- names matrices make unlist2d significant generalization rbindlist3. fuzz simply done:? Well might want things list linear models first tidying , general workflow. might also interested additional statistics like R-squared F-statistic: final example kind, lets suppose interested within-country correlations variables region income group: summary: list processing features, statistical capabilities efficient converters collapse flexibility data.table work well together, facilitating complex workflows.","code":"# Creating a matrix from mtcars m <- qM(mtcars)  str(m) #  num [1:32, 1:11] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #  - attr(*, \"dimnames\")=List of 2 #   ..$ : chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ... #   ..$ : chr [1:11] \"mpg\" \"cyl\" \"disp\" \"hp\" ...  # Demonstrating another nice feature of qDT qDT(m, row.names.col = \"car\") %>% head #                  car   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #               <char> <num> <num> <num> <num> <num> <num> <num> <num> <num> <num> <num> # 1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1     4     4 # 2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1     4     4 # 3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1     4     1 # 4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0     3     1 # 5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0     3     2 # 6:           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0     3     1  # Row-wise conversion to data.table mrtl(m, names = TRUE, return = \"data.table\") %>% head(2) #    Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive Hornet Sportabout Valiant Duster 360 Merc 240D #        <num>         <num>      <num>          <num>             <num>   <num>      <num>     <num> # 1:        21            21       22.8           21.4              18.7    18.1       14.3      24.4 # 2:         6             6        4.0            6.0               8.0     6.0        8.0       4.0 #    Merc 230 Merc 280 Merc 280C Merc 450SE Merc 450SL Merc 450SLC Cadillac Fleetwood #       <num>    <num>     <num>      <num>      <num>       <num>              <num> # 1:     22.8     19.2      17.8       16.4       17.3        15.2               10.4 # 2:      4.0      6.0       6.0        8.0        8.0         8.0                8.0 #    Lincoln Continental Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla Toyota Corona #                  <num>             <num>    <num>       <num>          <num>         <num> # 1:                10.4              14.7     32.4        30.4           33.9          21.5 # 2:                 8.0               8.0      4.0         4.0            4.0           4.0 #    Dodge Challenger AMC Javelin Camaro Z28 Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa #               <num>       <num>      <num>            <num>     <num>         <num>        <num> # 1:             15.5        15.2       13.3             19.2      27.3            26         30.4 # 2:              8.0         8.0        8.0              8.0       4.0             4          4.0 #    Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E #             <num>        <num>         <num>      <num> # 1:           15.8         19.7            15       21.4 # 2:            8.0          6.0             8        4.0 # Benchmark microbenchmark(qDT(m, \"car\"), mrtl(m, TRUE, \"data.table\")) # Unit: microseconds #                         expr   min    lq    mean median    uq    max neval #                qDT(m, \"car\") 4.838 5.043 6.16230 5.3300 6.437 20.049   100 #  mrtl(m, TRUE, \"data.table\") 3.608 3.854 4.23981 3.9975 4.182 15.908   100 library(lmtest)  wlddev %>% fselect(country, PCGDP, LIFEEX) %>%    # This counts missing values on PCGDP and LIFEEX only   na_omit(cols = -1L) %>%    # This removes countries with less than 20 observations   fsubset(fnobs(PCGDP, country, \"replace_fill\") > 20L) %>%    qDT %>%    # Run estimations by country using data.table   .[, qDT(coeftest(lm(G(PCGDP) ~ G(LIFEEX))), \"Coef\"), keyby = country] %>% head # Key: <country> #    country        Coef   Estimate Std. Error    t value    Pr(>|t|) #     <char>      <char>      <num>      <num>      <num>       <num> # 1: Albania (Intercept) -3.6146411   2.371885 -1.5239527 0.136023086 # 2: Albania   G(LIFEEX) 22.1596308   7.288971  3.0401591 0.004325856 # 3: Algeria (Intercept)  0.5973329   1.740619  0.3431726 0.732731107 # 4: Algeria   G(LIFEEX)  0.8412547   1.689221  0.4980134 0.620390703 # 5:  Angola (Intercept) -3.3793976   1.540330 -2.1939445 0.034597175 # 6:  Angola   G(LIFEEX)  4.2362895   1.402380  3.0207852 0.004553260 wlddev %>% fselect(country, PCGDP, LIFEEX) %>%    na_omit(cols = -1L) %>%    fsubset(fnobs(PCGDP, country, \"replace_fill\") > 20L) %>%    qDT %>%    .[, mrtl(flm(fgrowth(PCGDP)[-1L],                 cbind(Intercept = 1,                       LIFEEX = fgrowth(LIFEEX)[-1L])), TRUE),      keyby = country] %>% head # Key: <country> #                country   Intercept     LIFEEX #                 <char>       <num>      <num> # 1:             Albania -3.61464113 22.1596308 # 2:             Algeria  0.59733291  0.8412547 # 3:              Angola -3.37939760  4.2362895 # 4: Antigua and Barbuda -3.11880717 18.8700870 # 5:           Argentina  1.14613567 -0.2896305 # 6:             Armenia  0.08178344 11.5523992 microbenchmark(    A = wlddev %>% fselect(country, PCGDP, LIFEEX) %>%    na_omit(cols = -1L) %>%    fsubset(fnobs(PCGDP, country, \"replace_fill\") > 20L) %>%    qDT %>%    .[, qDT(coeftest(lm(G(PCGDP) ~ G(LIFEEX))), \"Coef\"), keyby = country],  B = wlddev %>% fselect(country, PCGDP, LIFEEX) %>%    na_omit(cols = -1L) %>%    fsubset(fnobs(PCGDP, country, \"replace_fill\") > 20L) %>%    qDT %>%    .[, mrtl(flm(fgrowth(PCGDP)[-1L],                 cbind(Intercept = 1,                       LIFEEX = fgrowth(LIFEEX)[-1L])), TRUE),      keyby = country] ) # Unit: milliseconds #  expr       min        lq      mean    median        uq        max neval #     A 58.914253 60.063381 68.770933 60.865217 73.507813 241.594509   100 #     B  3.145766  3.293715  3.463643  3.377006  3.503983   5.378995   100 DT_list <- rsplit(DT, country + year + PCGDP + LIFEEX ~ region + income)   # Note: rsplit(DT, year + PCGDP + LIFEEX ~ region + income, flatten = TRUE)  # would yield a simple list with interacted categories (like split)   str(DT_list, give.attr = FALSE) # List of 7 #  $ East Asia & Pacific       :List of 3 #   ..$ High income        :Classes 'data.table' and 'data.frame':  793 obs. of  4 variables: #   .. ..$ country: chr [1:793] \"Australia\" \"Australia\" \"Australia\" \"Australia\" ... #   .. ..$ year   : int [1:793] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:793] 19378 19469 19246 20053 21036 ... #   .. ..$ LIFEEX : num [1:793] 70.8 71 70.9 70.9 70.9 ... #   ..$ Lower middle income:Classes 'data.table' and 'data.frame':  793 obs. of  4 variables: #   .. ..$ country: chr [1:793] \"Cambodia\" \"Cambodia\" \"Cambodia\" \"Cambodia\" ... #   .. ..$ year   : int [1:793] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:793] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:793] 41.2 41.4 41.5 41.7 41.9 ... #   ..$ Upper middle income:Classes 'data.table' and 'data.frame':  610 obs. of  4 variables: #   .. ..$ country: chr [1:610] \"American Samoa\" \"American Samoa\" \"American Samoa\" \"American Samoa\" ... #   .. ..$ year   : int [1:610] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:610] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:610] NA NA NA NA NA NA NA NA NA NA ... #  $ Europe & Central Asia     :List of 4 #   ..$ High income        :Classes 'data.table' and 'data.frame':  2257 obs. of  4 variables: #   .. ..$ country: chr [1:2257] \"Andorra\" \"Andorra\" \"Andorra\" \"Andorra\" ... #   .. ..$ year   : int [1:2257] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:2257] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:2257] NA NA NA NA NA NA NA NA NA NA ... #   ..$ Low income         :Classes 'data.table' and 'data.frame':  61 obs. of  4 variables: #   .. ..$ country: chr [1:61] \"Tajikistan\" \"Tajikistan\" \"Tajikistan\" \"Tajikistan\" ... #   .. ..$ year   : int [1:61] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:61] 50.6 50.9 51.2 51.5 51.9 ... #   ..$ Lower middle income:Classes 'data.table' and 'data.frame':  244 obs. of  4 variables: #   .. ..$ country: chr [1:244] \"Kyrgyz Republic\" \"Kyrgyz Republic\" \"Kyrgyz Republic\" \"Kyrgyz Republic\" ... #   .. ..$ year   : int [1:244] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:244] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:244] 56.1 56.6 57 57.4 57.9 ... #   ..$ Upper middle income:Classes 'data.table' and 'data.frame':  976 obs. of  4 variables: #   .. ..$ country: chr [1:976] \"Albania\" \"Albania\" \"Albania\" \"Albania\" ... #   .. ..$ year   : int [1:976] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:976] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:976] 62.3 63.3 64.2 64.9 65.5 ... #  $ Latin America & Caribbean :List of 4 #   ..$ High income        :Classes 'data.table' and 'data.frame':  1037 obs. of  4 variables: #   .. ..$ country: chr [1:1037] \"Antigua and Barbuda\" \"Antigua and Barbuda\" \"Antigua and Barbuda\" \"Antigua and Barbuda\" ... #   .. ..$ year   : int [1:1037] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:1037] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:1037] 62 62.5 63 63.5 64 ... #   ..$ Low income         :Classes 'data.table' and 'data.frame':  61 obs. of  4 variables: #   .. ..$ country: chr [1:61] \"Haiti\" \"Haiti\" \"Haiti\" \"Haiti\" ... #   .. ..$ year   : int [1:61] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:61] 1512 1439 1523 1466 1414 ... #   .. ..$ LIFEEX : num [1:61] 41.8 42.2 42.6 43 43.4 ... #   ..$ Lower middle income:Classes 'data.table' and 'data.frame':  244 obs. of  4 variables: #   .. ..$ country: chr [1:244] \"Bolivia\" \"Bolivia\" \"Bolivia\" \"Bolivia\" ... #   .. ..$ year   : int [1:244] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:244] 1005 1007 1042 1091 1112 ... #   .. ..$ LIFEEX : num [1:244] 41.8 42.1 42.5 42.8 43.2 ... #   ..$ Upper middle income:Classes 'data.table' and 'data.frame':  1220 obs. of  4 variables: #   .. ..$ country: chr [1:1220] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ... #   .. ..$ year   : int [1:1220] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:1220] 5643 5853 5711 5323 5773 ... #   .. ..$ LIFEEX : num [1:1220] 65.1 65.2 65.3 65.3 65.4 ... #  $ Middle East & North Africa:List of 4 #   ..$ High income        :Classes 'data.table' and 'data.frame':  488 obs. of  4 variables: #   .. ..$ country: chr [1:488] \"Bahrain\" \"Bahrain\" \"Bahrain\" \"Bahrain\" ... #   .. ..$ year   : int [1:488] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:488] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:488] 51.9 53.2 54.6 55.9 57.2 ... #   ..$ Low income         :Classes 'data.table' and 'data.frame':  122 obs. of  4 variables: #   .. ..$ country: chr [1:122] \"Syrian Arab Republic\" \"Syrian Arab Republic\" \"Syrian Arab Republic\" \"Syrian Arab Republic\" ... #   .. ..$ year   : int [1:122] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:122] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:122] 52 52.6 53.2 53.8 54.4 ... #   ..$ Lower middle income:Classes 'data.table' and 'data.frame':  305 obs. of  4 variables: #   .. ..$ country: chr [1:305] \"Djibouti\" \"Djibouti\" \"Djibouti\" \"Djibouti\" ... #   .. ..$ year   : int [1:305] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:305] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:305] 44 44.5 44.9 45.3 45.7 ... #   ..$ Upper middle income:Classes 'data.table' and 'data.frame':  366 obs. of  4 variables: #   .. ..$ country: chr [1:366] \"Algeria\" \"Algeria\" \"Algeria\" \"Algeria\" ... #   .. ..$ year   : int [1:366] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:366] 2481 2091 1638 2146 2214 ... #   .. ..$ LIFEEX : num [1:366] 46.1 46.6 47.1 47.5 48 ... #  $ North America             :List of 1 #   ..$ High income:Classes 'data.table' and 'data.frame':  183 obs. of  4 variables: #   .. ..$ country: chr [1:183] \"Bermuda\" \"Bermuda\" \"Bermuda\" \"Bermuda\" ... #   .. ..$ year   : int [1:183] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:183] 33363 34080 34763 34324 37202 ... #   .. ..$ LIFEEX : num [1:183] NA NA NA NA NA ... #  $ South Asia                :List of 3 #   ..$ Low income         :Classes 'data.table' and 'data.frame':  122 obs. of  4 variables: #   .. ..$ country: chr [1:122] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ... #   .. ..$ year   : int [1:122] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:122] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:122] 32.4 33 33.5 34 34.5 ... #   ..$ Lower middle income:Classes 'data.table' and 'data.frame':  244 obs. of  4 variables: #   .. ..$ country: chr [1:244] \"Bangladesh\" \"Bangladesh\" \"Bangladesh\" \"Bangladesh\" ... #   .. ..$ year   : int [1:244] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:244] 372 384 394 381 411 ... #   .. ..$ LIFEEX : num [1:244] 45.4 46 46.6 47.1 47.6 ... #   ..$ Upper middle income:Classes 'data.table' and 'data.frame':  122 obs. of  4 variables: #   .. ..$ country: chr [1:122] \"Maldives\" \"Maldives\" \"Maldives\" \"Maldives\" ... #   .. ..$ year   : int [1:122] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:122] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:122] 37.3 37.9 38.6 39.2 39.9 ... #  $ Sub-Saharan Africa        :List of 4 #   ..$ High income        :Classes 'data.table' and 'data.frame':  61 obs. of  4 variables: #   .. ..$ country: chr [1:61] \"Seychelles\" \"Seychelles\" \"Seychelles\" \"Seychelles\" ... #   .. ..$ year   : int [1:61] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:61] 2830 2617 2763 2966 3064 ... #   .. ..$ LIFEEX : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #   ..$ Low income         :Classes 'data.table' and 'data.frame':  1464 obs. of  4 variables: #   .. ..$ country: chr [1:1464] \"Benin\" \"Benin\" \"Benin\" \"Benin\" ... #   .. ..$ year   : int [1:1464] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:1464] 712 724 689 710 745 ... #   .. ..$ LIFEEX : num [1:1464] 37.3 37.7 38.2 38.7 39.1 ... #   ..$ Lower middle income:Classes 'data.table' and 'data.frame':  1037 obs. of  4 variables: #   .. ..$ country: chr [1:1037] \"Angola\" \"Angola\" \"Angola\" \"Angola\" ... #   .. ..$ year   : int [1:1037] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:1037] NA NA NA NA NA NA NA NA NA NA ... #   .. ..$ LIFEEX : num [1:1037] 37.5 37.8 38.1 38.4 38.8 ... #   ..$ Upper middle income:Classes 'data.table' and 'data.frame':  366 obs. of  4 variables: #   .. ..$ country: chr [1:366] \"Botswana\" \"Botswana\" \"Botswana\" \"Botswana\" ... #   .. ..$ year   : int [1:366] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 ... #   .. ..$ PCGDP  : num [1:366] 408 425 444 460 480 ... #   .. ..$ LIFEEX : num [1:366] 49.2 49.7 50.2 50.6 51.1 ... # This runs region-income level regressions, with country fixed effects # following Mundlak (1978) lm_summary_list <- DT_list %>%    rapply2d(lm, formula = G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country)) %>%    # Summarizing the results   rapply2d(summary, classes = \"lm\")  # This is a nested list of linear model summaries str(lm_summary_list, give.attr = FALSE) # List of 7 #  $ East Asia & Pacific       :List of 3 #   ..$ High income        :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:441] -1.64 -2.59 2.75 3.45 2.48 ... #   .. ..$ coefficients : num [1:3, 1:4] 0.531 2.494 3.83 0.706 0.759 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 4.59 #   .. ..$ df           : int [1:3] 3 438 3 #   .. ..$ r.squared    : num 0.0525 #   .. ..$ adj.r.squared: num 0.0481 #   .. ..$ fstatistic   : Named num [1:3] 12.1 2 438 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.02361 -0.00158 -0.04895 -0.00158 0.02728 ... #   .. ..$ na.action    : 'omit' Named int [1:352] 1 61 62 63 64 65 66 67 68 69 ... #   ..$ Lower middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:549] -39.6968 3.6618 -0.0944 -1.8261 -1.0491 ... #   .. ..$ coefficients : num [1:3, 1:4] 1.348 0.524 0.949 0.701 0.757 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 5.4 #   .. ..$ df           : int [1:3] 3 546 3 #   .. ..$ r.squared    : num 0.00471 #   .. ..$ adj.r.squared: num 0.00106 #   .. ..$ fstatistic   : Named num [1:3] 1.29 2 546 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.016821 0.000511 -0.022767 0.000511 0.01965 ... #   .. ..$ na.action    : 'omit' Named int [1:244] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Upper middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:312] -32.29 -11.61 2.91 11.23 10.28 ... #   .. ..$ coefficients : num [1:3, 1:4] 1.507 -0.547 4.816 0.428 0.478 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 4.39 #   .. ..$ df           : int [1:3] 3 309 3 #   .. ..$ r.squared    : num 0.103 #   .. ..$ adj.r.squared: num 0.0976 #   .. ..$ fstatistic   : Named num [1:3] 17.8 2 309 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.009471 0.000492 -0.013551 0.000492 0.011842 ... #   .. ..$ na.action    : 'omit' Named int [1:298] 1 2 3 4 5 6 7 8 9 10 ... #  $ Europe & Central Asia     :List of 4 #   ..$ High income        :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:1355] 2.706 -0.548 1.001 3.034 0.257 ... #   .. ..$ coefficients : num [1:3, 1:4] 3.254 -0.172 -2.506 0.407 0.227 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 3.3 #   .. ..$ df           : int [1:3] 3 1352 3 #   .. ..$ r.squared    : num 0.00257 #   .. ..$ adj.r.squared: num 0.00109 #   .. ..$ fstatistic   : Named num [1:3] 1.74 2 1352 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.015254 -0.000863 -0.05461 -0.000863 0.004722 ... #   .. ..$ na.action    : 'omit' Named int [1:902] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Low income         :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:34] 0.166 -1.804 15.949 -0.778 7.165 ... #   .. ..$ coefficients : num [1:2, 1:4] -5.31 9.36 2.03 2.56 -2.61 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE TRUE #   .. ..$ sigma        : num 8.43 #   .. ..$ df           : int [1:3] 2 32 3 #   .. ..$ r.squared    : num 0.295 #   .. ..$ adj.r.squared: num 0.273 #   .. ..$ fstatistic   : Named num [1:3] 13.4 1 32 #   .. ..$ cov.unscaled : num [1:2, 1:2] 0.0582 -0.0514 -0.0514 0.092 #   .. ..$ na.action    : 'omit' Named int [1:27] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Lower middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:121] -1.626 8.745 -14.47 0.298 -11.886 ... #   .. ..$ coefficients : num [1:3, 1:4] 0.106 4.631 1.499 1.315 0.938 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 6.02 #   .. ..$ df           : int [1:3] 3 118 3 #   .. ..$ r.squared    : num 0.178 #   .. ..$ adj.r.squared: num 0.164 #   .. ..$ fstatistic   : Named num [1:3] 12.7 2 118 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.047775 -0.000927 -0.142782 -0.000927 0.024298 ... #   .. ..$ na.action    : 'omit' Named int [1:123] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Upper middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:511] 0.761 -2.153 -4.091 -6.476 -3.43 ... #   .. ..$ coefficients : num [1:3, 1:4] 2.983 4.147 -3.351 0.698 0.779 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 8.28 #   .. ..$ df           : int [1:3] 3 508 3 #   .. ..$ r.squared    : num 0.0531 #   .. ..$ adj.r.squared: num 0.0493 #   .. ..$ fstatistic   : Named num [1:3] 14.2 2 508 #   .. ..$ cov.unscaled : num [1:3, 1:3] 7.11e-03 4.52e-05 -1.45e-02 4.52e-05 8.85e-03 ... #   .. ..$ na.action    : 'omit' Named int [1:465] 1 2 3 4 5 6 7 8 9 10 ... #  $ Latin America & Caribbean :List of 4 #   ..$ High income        :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:487] 2.39 6.02 6.1 1.71 -2.27 ... #   .. ..$ coefficients : num [1:3, 1:4] 1.015 0.483 2.613 0.677 0.952 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 4.71 #   .. ..$ df           : int [1:3] 3 484 3 #   .. ..$ r.squared    : num 0.00592 #   .. ..$ adj.r.squared: num 0.00181 #   .. ..$ fstatistic   : Named num [1:3] 1.44 2 484 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.02062 0.00155 -0.05714 0.00155 0.04082 ... #   .. ..$ na.action    : 'omit' Named int [1:550] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Low income         :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:59] -5.667 5.091 -4.46 -4.224 -0.526 ... #   .. ..$ coefficients : num [1:2, 1:4] -3.18 4.02 1.73 2.28 -1.83 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE TRUE #   .. ..$ sigma        : num 3.79 #   .. ..$ df           : int [1:3] 2 57 3 #   .. ..$ r.squared    : num 0.0516 #   .. ..$ adj.r.squared: num 0.0349 #   .. ..$ fstatistic   : Named num [1:3] 3.1 1 57 #   .. ..$ cov.unscaled : num [1:2, 1:2] 0.209 -0.265 -0.265 0.364 #   .. ..$ na.action    : 'omit' Named int [1:2] 1 61 #   ..$ Lower middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:231] -1.386 2.029 3.213 0.413 1.334 ... #   .. ..$ coefficients : num [1:3, 1:4] -1.678 -0.479 3.896 2.26 0.709 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 3.96 #   .. ..$ df           : int [1:3] 3 228 3 #   .. ..$ r.squared    : num 0.0081 #   .. ..$ adj.r.squared: num -0.000602 #   .. ..$ fstatistic   : Named num [1:3] 0.931 2 228 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.3264 0.005 -0.4084 0.005 0.0321 ... #   .. ..$ na.action    : 'omit' Named int [1:13] 1 61 62 63 64 65 66 67 122 123 ... #   ..$ Upper middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:1065] 1.97 -4.16 -8.5 6.72 7.17 ... #   .. ..$ coefficients : num [1:3, 1:4] 1.681 0.583 -0.124 0.353 0.512 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 4.22 #   .. ..$ df           : int [1:3] 3 1062 3 #   .. ..$ r.squared    : num 0.0016 #   .. ..$ adj.r.squared: num -0.000283 #   .. ..$ fstatistic   : Named num [1:3] 0.85 2 1062 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.006982 0.000348 -0.013936 0.000348 0.014734 ... #   .. ..$ na.action    : 'omit' Named int [1:155] 1 61 62 122 123 183 184 244 245 305 ... #  $ Middle East & North Africa:List of 4 #   ..$ High income        :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:334] -10.728 -11.988 2.151 0.985 -8.618 ... #   .. ..$ coefficients : num [1:3, 1:4] 1.929 3.963 -3.533 1.102 0.996 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 8.36 #   .. ..$ df           : int [1:3] 3 331 3 #   .. ..$ r.squared    : num 0.0456 #   .. ..$ adj.r.squared: num 0.0399 #   .. ..$ fstatistic   : Named num [1:3] 7.91 2 331 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.01738 0.00101 -0.02441 0.00101 0.01419 ... #   .. ..$ na.action    : 'omit' Named int [1:154] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Low income         :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:29] 0.468 3.424 0.415 3.842 3.342 ... #   .. ..$ coefficients : num [1:2, 1:4] -6.91 11.38 2.11 3.64 -3.27 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE TRUE #   .. ..$ sigma        : num 6.05 #   .. ..$ df           : int [1:3] 2 27 3 #   .. ..$ r.squared    : num 0.266 #   .. ..$ adj.r.squared: num 0.239 #   .. ..$ fstatistic   : Named num [1:3] 9.81 1 27 #   .. ..$ cov.unscaled : num [1:2, 1:2] 0.122 -0.178 -0.178 0.361 #   .. ..$ na.action    : 'omit' Named int [1:93] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Lower middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:191] -0.95 -2.047 4.541 5.594 -0.723 ... #   .. ..$ coefficients : num [1:3, 1:4] 2.238 1.271 -0.647 1.002 0.599 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 3.94 #   .. ..$ df           : int [1:3] 3 188 3 #   .. ..$ r.squared    : num 0.0244 #   .. ..$ adj.r.squared: num 0.014 #   .. ..$ fstatistic   : Named num [1:3] 2.35 2 188 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.06471 -0.00043 -0.07801 -0.00043 0.02309 ... #   .. ..$ na.action    : 'omit' Named int [1:114] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Upper middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:263] -18.068 -23.976 28.692 0.858 1.141 ... #   .. ..$ coefficients : num [1:3, 1:4] 2.663 0.718 -1.19 3.538 1.318 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 13.8 #   .. ..$ df           : int [1:3] 3 260 3 #   .. ..$ r.squared    : num 0.00119 #   .. ..$ adj.r.squared: num -0.00649 #   .. ..$ fstatistic   : Named num [1:3] 0.155 2 260 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.065741 0.000795 -0.084456 0.000795 0.009122 ... #   .. ..$ na.action    : 'omit' Named int [1:103] 1 61 62 122 123 124 125 126 127 128 ... #  $ North America             :List of 1 #   ..$ High income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:137] 4.6986 -3.1098 1.8243 0.5643 0.0176 ... #   .. ..$ coefficients : num [1:3, 1:4] 6.542 -1.461 -19.53 2.272 0.662 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 2.49 #   .. ..$ df           : int [1:3] 3 134 3 #   .. ..$ r.squared    : num 0.0657 #   .. ..$ adj.r.squared: num 0.0518 #   .. ..$ fstatistic   : Named num [1:3] 4.71 2 134 #   .. ..$ cov.unscaled : num [1:3, 1:3] 8.36e-01 1.59e-17 -3.60 1.59e-17 7.10e-02 ... #   .. ..$ na.action    : 'omit' Named int [1:46] 1 2 3 4 5 6 7 8 9 10 ... #  $ South Asia                :List of 3 #   ..$ Low income         :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:76] 0.544 -6.17 3.951 -0.964 7.829 ... #   .. ..$ coefficients : num [1:3, 1:4] -108.62 -1.72 96.06 174.19 1.25 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 3.7 #   .. ..$ df           : int [1:3] 3 73 3 #   .. ..$ r.squared    : num 0.0494 #   .. ..$ adj.r.squared: num 0.0233 #   .. ..$ fstatistic   : Named num [1:3] 1.9 2 73 #   .. ..$ cov.unscaled : num [1:3, 1:3] 2210.639 -6.979 -1875.261 -6.979 0.114 ... #   .. ..$ na.action    : 'omit' Named int [1:46] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Lower middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:216] 0.294 -0.293 -6.067 4.954 -4.164 ... #   .. ..$ coefficients : num [1:3, 1:4] -2.232 0.238 5.972 1.074 0.493 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 3.44 #   .. ..$ df           : int [1:3] 3 213 3 #   .. ..$ r.squared    : num 0.111 #   .. ..$ adj.r.squared: num 0.103 #   .. ..$ fstatistic   : Named num [1:3] 13.3 2 213 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.09757 -0.00201 -0.10483 -0.00201 0.02054 ... #   .. ..$ na.action    : 'omit' Named int [1:28] 1 61 62 63 64 65 66 67 68 69 ... #   ..$ Upper middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:82] 3.262 3.976 3.128 1.67 -0.901 ... #   .. ..$ coefficients : num [1:3, 1:4] 3.859 -0.577 -0.476 1.036 1.365 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 4.25 #   .. ..$ df           : int [1:3] 3 79 3 #   .. ..$ r.squared    : num 0.00622 #   .. ..$ adj.r.squared: num -0.0189 #   .. ..$ fstatistic   : Named num [1:3] 0.247 2 79 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.0595 -0.028 -0.0473 -0.028 0.1034 ... #   .. ..$ na.action    : 'omit' Named int [1:40] 1 2 3 4 5 6 7 8 9 10 ... #  $ Sub-Saharan Africa        :List of 4 #   ..$ High income        :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:39] -11.33 -5.041 -3.158 0.585 7.81 ... #   .. ..$ coefficients : num [1:2, 1:4] 2.551 -0.644 0.775 0.55 3.293 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE TRUE #   .. ..$ sigma        : num 4.8 #   .. ..$ df           : int [1:3] 2 37 3 #   .. ..$ r.squared    : num 0.0357 #   .. ..$ adj.r.squared: num 0.00959 #   .. ..$ fstatistic   : Named num [1:3] 1.37 1 37 #   .. ..$ cov.unscaled : num [1:2, 1:2] 0.026 -0.00217 -0.00217 0.01312 #   .. ..$ na.action    : 'omit' Named int [1:22] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Low income         :List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:1085] 0.694 -5.869 2.069 3.855 2.415 ... #   .. ..$ coefficients : num [1:3, 1:4] -0.0756 0.5308 0.5124 0.8887 0.137 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 5.88 #   .. ..$ df           : int [1:3] 3 1082 3 #   .. ..$ r.squared    : num 0.0146 #   .. ..$ adj.r.squared: num 0.0128 #   .. ..$ fstatistic   : Named num [1:3] 8.01 2 1082 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.022858 -0.000025 -0.025534 -0.000025 0.000543 ... #   .. ..$ na.action    : 'omit' Named int [1:379] 1 61 62 122 123 183 184 244 245 305 ... #   ..$ Lower middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:891] -8.2839 -4.0289 0.0449 1.8231 -0.5267 ... #   .. ..$ coefficients : num [1:3, 1:4] 2.352 0.782 -2.616 0.608 0.169 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 5.27 #   .. ..$ df           : int [1:3] 3 888 3 #   .. ..$ r.squared    : num 0.0277 #   .. ..$ adj.r.squared: num 0.0255 #   .. ..$ fstatistic   : Named num [1:3] 12.7 2 888 #   .. ..$ cov.unscaled : num [1:3, 1:3] 1.33e-02 -1.13e-05 -2.00e-02 -1.13e-05 1.02e-03 ... #   .. ..$ na.action    : 'omit' Named int [1:146] 1 2 3 4 5 6 7 8 9 10 ... #   ..$ Upper middle income:List of 12 #   .. ..$ call         : language FUN(formula = ..1, data = y) #   .. ..$ terms        :Classes 'terms', 'formula'  language G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country) #   .. ..$ residuals    : Named num [1:298] 0.7659 0.9133 0.0921 0.996 0.0765 ... #   .. ..$ coefficients : num [1:3, 1:4] 0.584 0.456 4.112 2.472 0.652 ... #   .. ..$ aliased      : Named logi [1:3] FALSE FALSE FALSE #   .. ..$ sigma        : num 11.4 #   .. ..$ df           : int [1:3] 3 295 3 #   .. ..$ r.squared    : num 0.00658 #   .. ..$ adj.r.squared: num -0.000152 #   .. ..$ fstatistic   : Named num [1:3] 0.977 2 295 #   .. ..$ cov.unscaled : num [1:3, 1:3] 0.047213 0.000438 -0.070778 0.000438 0.003285 ... #   .. ..$ na.action    : 'omit' Named int [1:68] 1 61 62 63 64 65 66 67 68 69 ... lm_summary_list %>%   get_elem(\"coefficients\") %>%    unlist2d(idcols = .c(Region, Income),             row.names = \"Coef\",             DT = TRUE) %>% head #                 Region              Income                  Coef  Estimate Std. Error   t value #                 <char>              <char>                <char>     <num>      <num>     <num> # 1: East Asia & Pacific         High income           (Intercept) 0.5313479  0.7058550 0.7527720 # 2: East Asia & Pacific         High income             G(LIFEEX) 2.4935584  0.7586943 3.2866443 # 3: East Asia & Pacific         High income B(G(LIFEEX), country) 3.8297123  1.6916770 2.2638554 # 4: East Asia & Pacific Lower middle income           (Intercept) 1.3476602  0.7008556 1.9228785 # 5: East Asia & Pacific Lower middle income             G(LIFEEX) 0.5238856  0.7574904 0.6916069 # 6: East Asia & Pacific Lower middle income B(G(LIFEEX), country) 0.9494439  1.2031228 0.7891496 #       Pr(>|t|) #          <num> # 1: 0.451991327 # 2: 0.001095466 # 3: 0.024071386 # 4: 0.055015131 # 5: 0.489478164 # 6: 0.430367103 DT[, qDT(coeftest(lm(G(PCGDP) ~ G(LIFEEX) + B(G(LIFEEX), country))), \"Coef\"),     keyby = .(region, income)] %>% head # Key: <region, income> #                 region              income                  Coef  Estimate Std. Error   t value #                 <fctr>              <fctr>                <char>     <num>      <num>     <num> # 1: East Asia & Pacific         High income           (Intercept) 0.5313479  0.7058550 0.7527720 # 2: East Asia & Pacific         High income             G(LIFEEX) 2.4935584  0.7586943 3.2866443 # 3: East Asia & Pacific         High income B(G(LIFEEX), country) 3.8297123  1.6916770 2.2638554 # 4: East Asia & Pacific Lower middle income           (Intercept) 1.3476602  0.7008556 1.9228785 # 5: East Asia & Pacific Lower middle income             G(LIFEEX) 0.5238856  0.7574904 0.6916069 # 6: East Asia & Pacific Lower middle income B(G(LIFEEX), country) 0.9494439  1.2031228 0.7891496 #       Pr(>|t|) #          <num> # 1: 0.451991327 # 2: 0.001095466 # 3: 0.024071386 # 4: 0.055015131 # 5: 0.489478164 # 6: 0.430367103 DT_sum <- lm_summary_list %>% get_elem(\"coef|r.sq|fstat\", regex = TRUE) %>%    unlist2d(idcols = .c(Region, Income, Statistic),             row.names = \"Coef\",             DT = TRUE)  head(DT_sum) #                 Region      Income     Statistic                  Coef  Estimate Std. Error #                 <char>      <char>        <char>                <char>     <num>      <num> # 1: East Asia & Pacific High income  coefficients           (Intercept) 0.5313479  0.7058550 # 2: East Asia & Pacific High income  coefficients             G(LIFEEX) 2.4935584  0.7586943 # 3: East Asia & Pacific High income  coefficients B(G(LIFEEX), country) 3.8297123  1.6916770 # 4: East Asia & Pacific High income     r.squared                  <NA>        NA         NA # 5: East Asia & Pacific High income adj.r.squared                  <NA>        NA         NA # 6: East Asia & Pacific High income    fstatistic                  <NA>        NA         NA #     t value    Pr(>|t|)         V1    value numdf dendf #       <num>       <num>      <num>    <num> <num> <num> # 1: 0.752772 0.451991327         NA       NA    NA    NA # 2: 3.286644 0.001095466         NA       NA    NA    NA # 3: 2.263855 0.024071386         NA       NA    NA    NA # 4:       NA          NA 0.05245359       NA    NA    NA # 5:       NA          NA 0.04812690       NA    NA    NA # 6:       NA          NA         NA 12.12325     2   438  # Reshaping to long form:  DT_sum %>%   melt(1:4, na.rm = TRUE) %>%   roworderv(1:2) %>% head(20) #                  Region              Income     Statistic                  Coef   variable #                  <char>              <char>        <char>                <char>     <fctr> #  1: East Asia & Pacific         High income  coefficients           (Intercept)   Estimate #  2: East Asia & Pacific         High income  coefficients             G(LIFEEX)   Estimate #  3: East Asia & Pacific         High income  coefficients B(G(LIFEEX), country)   Estimate #  4: East Asia & Pacific         High income  coefficients           (Intercept) Std. Error #  5: East Asia & Pacific         High income  coefficients             G(LIFEEX) Std. Error #  6: East Asia & Pacific         High income  coefficients B(G(LIFEEX), country) Std. Error #  7: East Asia & Pacific         High income  coefficients           (Intercept)    t value #  8: East Asia & Pacific         High income  coefficients             G(LIFEEX)    t value #  9: East Asia & Pacific         High income  coefficients B(G(LIFEEX), country)    t value # 10: East Asia & Pacific         High income  coefficients           (Intercept)   Pr(>|t|) # 11: East Asia & Pacific         High income  coefficients             G(LIFEEX)   Pr(>|t|) # 12: East Asia & Pacific         High income  coefficients B(G(LIFEEX), country)   Pr(>|t|) # 13: East Asia & Pacific         High income     r.squared                  <NA>         V1 # 14: East Asia & Pacific         High income adj.r.squared                  <NA>         V1 # 15: East Asia & Pacific         High income    fstatistic                  <NA>      value # 16: East Asia & Pacific         High income    fstatistic                  <NA>      numdf # 17: East Asia & Pacific         High income    fstatistic                  <NA>      dendf # 18: East Asia & Pacific Lower middle income  coefficients           (Intercept)   Estimate # 19: East Asia & Pacific Lower middle income  coefficients             G(LIFEEX)   Estimate # 20: East Asia & Pacific Lower middle income  coefficients B(G(LIFEEX), country)   Estimate #                  Region              Income     Statistic                  Coef   variable #            value #            <num> #  1: 5.313479e-01 #  2: 2.493558e+00 #  3: 3.829712e+00 #  4: 7.058550e-01 #  5: 7.586943e-01 #  6: 1.691677e+00 #  7: 7.527720e-01 #  8: 3.286644e+00 #  9: 2.263855e+00 # 10: 4.519913e-01 # 11: 1.095466e-03 # 12: 2.407139e-02 # 13: 5.245359e-02 # 14: 4.812690e-02 # 15: 1.212325e+01 # 16: 2.000000e+00 # 17: 4.380000e+02 # 18: 1.347660e+00 # 19: 5.238856e-01 # 20: 9.494439e-01 #            value DT[, qDT(pwcor(W(.SD, country)), \"Variable\"),     keyby = .(region, income), .SDcols = PCGDP:ODA] %>% head # Key: <region, income> #                 region              income Variable    W.PCGDP   W.LIFEEX    W.GINI       W.ODA #                 <fctr>              <fctr>   <char>      <num>      <num>     <num>       <num> # 1: East Asia & Pacific         High income  W.PCGDP  1.0000000  0.7562668 0.6253844 -0.25258496 # 2: East Asia & Pacific         High income W.LIFEEX  0.7562668  1.0000000 0.3191255 -0.33611662 # 3: East Asia & Pacific         High income   W.GINI  0.6253844  0.3191255 1.0000000          NA # 4: East Asia & Pacific         High income    W.ODA -0.2525850 -0.3361166        NA  1.00000000 # 5: East Asia & Pacific Lower middle income  W.PCGDP  1.0000000  0.4685618 0.4428879 -0.02508852 # 6: East Asia & Pacific Lower middle income W.LIFEEX  0.4685618  1.0000000 0.3231520  0.09356733"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_data.table.html","id":"additional-benchmarks","dir":"Articles","previous_headings":"","what":"Additional Benchmarks","title":"collapse and data.table","text":"See . run 2 core laptop, honestly don’t know collapse scales powerful multi-core machines. limited computational resources part reason opt thread-parallel package start. multi-core version collapse eventually released, maybe end 2021.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_data.table.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"collapse and data.table","text":"Mundlak, Yair. 1978. “Pooling Time Series Cross Section Data.” Econometrica 46 (1): 69–85.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"fast-aggregations","dir":"Articles","previous_headings":"","what":"1. Fast Aggregations","title":"collapse and dplyr","text":"key feature collapse ’s broad set Fast Statistical Functions (fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs, fndistinct) able substantially speed-column-wise, grouped weighted computations vectors, matrices data frames. functions S3 generic, default (vector), matrix data frame method, well grouped_df method grouped tibbles used dplyr. grouped tibble method following arguments: w weight variable, TRA can used transform x using computed statistics one 10 available transformations (\"replace_fill\", \"replace\", \"-\", \"-+\", \"/\", \"%\", \"+\", \"*\", \"%%\", \"-%%\", discussed section 2). na.rm efficiently removes missing values TRUE default. use.g.names generates new row-names unique combinations groups (default: disabled), whereas keep.group_vars (default: enabled) keep grouping columns custom native data %>% group_by(...) %>% summarize(...) workflow dplyr. Finally, keep.w regulates whether weighting variable used also aggregated saved column. fsum, fmean, fmedian, fnth, fvar, fsd fmode compute sum weights group, whereas fprod returns product weights. mind, let’s consider straightforward applications.","code":"FUN.grouped_df(x, [w = NULL,] TRA = NULL, [na.rm = TRUE,]                use.g.names = FALSE, keep.group_vars = TRUE, [keep.w = TRUE,] ...)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"simple-aggregations","dir":"Articles","previous_headings":"1. Fast Aggregations","what":"1.1 Simple Aggregations","title":"collapse and dplyr","text":"Consider Groningen Growth Development Center 10-Sector Database included collapse introduced main vignette: Simple column-wise computations using fast functions pipe operators performed follows: Moving grouped statistics, can compute average value added employment sector country using: Similarly can aggregate using functions. important use dplyr’s summarize together functions since eliminate speed gain. functions fast executed carry grouped computations C++, whereas summarize apply function group grouped tibble.","code":"library(collapse) head(GGDC10S) #   Country Regioncode             Region Variable Year      AGR      MIN       MAN        PU # 1     BWA        SSA Sub-saharan Africa       VA 1960       NA       NA        NA        NA # 2     BWA        SSA Sub-saharan Africa       VA 1961       NA       NA        NA        NA # 3     BWA        SSA Sub-saharan Africa       VA 1962       NA       NA        NA        NA # 4     BWA        SSA Sub-saharan Africa       VA 1963       NA       NA        NA        NA # 5     BWA        SSA Sub-saharan Africa       VA 1964 16.30154 3.494075 0.7365696 0.1043936 # 6     BWA        SSA Sub-saharan Africa       VA 1965 15.72700 2.495768 1.0181992 0.1350976 #         CON      WRT      TRA     FIRE      GOV      OTH      SUM # 1        NA       NA       NA       NA       NA       NA       NA # 2        NA       NA       NA       NA       NA       NA       NA # 3        NA       NA       NA       NA       NA       NA       NA # 4        NA       NA       NA       NA       NA       NA       NA # 5 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229 # 6 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710  # Summarize the Data:  # descr(GGDC10S, cols = is_categorical) # aperm(qsu(GGDC10S, ~Variable, cols = is.numeric))  # Efficiently converting to tibble (no deep copy) GGDC10S <- qTBL(GGDC10S) library(dplyr)  GGDC10S %>% fnobs                       # Number of Observations #    Country Regioncode     Region   Variable       Year        AGR        MIN        MAN         PU  #       5027       5027       5027       5027       5027       4364       4355       4355       4354  #        CON        WRT        TRA       FIRE        GOV        OTH        SUM  #       4355       4355       4355       4355       3482       4248       4364 GGDC10S %>% fndistinct                  # Number of distinct values #    Country Regioncode     Region   Variable       Year        AGR        MIN        MAN         PU  #         43          6          6          2         67       4353       4224       4353       4237  #        CON        WRT        TRA       FIRE        GOV        OTH        SUM  #       4339       4344       4334       4349       3470       4238       4364 GGDC10S %>% select_at(6:16) %>% fmedian # Median #        AGR        MIN        MAN         PU        CON        WRT        TRA       FIRE        GOV  #  4394.5194   173.2234  3718.0981   167.9500  1473.4470  3773.6430  1174.8000   960.1251  3928.5127  #        OTH        SUM  #  1433.1722 23186.1936 GGDC10S %>% select_at(6:16) %>% fmean   # Mean #        AGR        MIN        MAN         PU        CON        WRT        TRA       FIRE        GOV  #  2526696.5  1867908.9  5538491.4   335679.5  1801597.6  3392909.5  1473269.7  1657114.8  1712300.3  #        OTH        SUM  #  1684527.3 21566436.8 GGDC10S %>% fmode                       # Mode #            Country         Regioncode             Region           Variable               Year  #              \"USA\"              \"ASI\"             \"Asia\"              \"EMP\"             \"2010\"  #                AGR                MIN                MAN                 PU                CON  # \"171.315882316326\"                \"0\" \"4645.12507642586\"                \"0\" \"1.34623115930777\"  #                WRT                TRA               FIRE                GOV                OTH  # \"21.8380052682527\" \"8.97743416914571\" \"40.0701608636442\"                \"0\" \"3626.84423577048\"  #                SUM  # \"37.4822945751317\" GGDC10S %>% fmode(drop = FALSE)         # Keep data structure intact # # A tibble: 1 × 16 #   Country Regioncode Region Variable  Year   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV # * <chr>   <chr>      <chr>  <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 USA     ASI        Asia   EMP       2010  171.     0 4645.     0  1.35  21.8  8.98  40.1     0 # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> GGDC10S %>%    group_by(Variable, Country) %>%   select_at(6:16) %>% fmean # # A tibble: 85 × 13 #    Variable Country     AGR     MIN     MAN     PU    CON    WRT    TRA   FIRE     GOV    OTH    SUM #    <chr>    <chr>     <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> #  1 EMP      ARG       1420.   52.1   1932.  1.02e2 7.42e2 1.98e3 6.49e2  628.   2043.  9.92e2 1.05e4 #  2 EMP      BOL        964.   56.0    235.  5.35e0 1.23e2 2.82e2 1.15e2   44.6    NA   3.96e2 2.22e3 #  3 EMP      BRA      17191.  206.    6991.  3.65e2 3.52e3 8.51e3 2.05e3 4414.   5307.  5.71e3 5.43e4 #  4 EMP      BWA        188.   10.5     18.1 3.09e0 2.53e1 3.63e1 8.36e0   15.3    61.1 2.76e1 3.94e2 #  5 EMP      CHL        702.  101.     625.  2.94e1 2.96e2 6.95e2 2.58e2  272.     NA   1.00e3 3.98e3 #  6 EMP      CHN     287744. 7050.   67144.  1.61e3 2.09e4 2.89e4 1.39e4 4929.  22669.  3.10e4 4.86e5 #  7 EMP      COL       3091.  145.    1175.  3.39e1 5.24e2 2.07e3 4.70e2  649.     NA   1.73e3 9.89e3 #  8 EMP      CRI        231.    1.70   136.  1.43e1 5.76e1 1.57e2 4.24e1   54.9   128.  6.51e1 8.87e2 #  9 EMP      DEW       2490.  407.    8473.  2.26e2 2.09e3 4.44e3 1.48e3 1689.   3945.  9.99e2 2.62e4 # 10 EMP      DNK        236.    8.03   507.  1.38e1 1.71e2 4.55e2 1.61e2  181.    549.  1.11e2 2.39e3 # # ℹ 75 more rows"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"excursus-what-is-happening-behind-the-scenes","dir":"Articles","previous_headings":"1. Fast Aggregations > 1.1 Simple Aggregations","what":"Excursus: What is Happening Behind the Scenes?","title":"collapse and dplyr","text":"better explain point perhaps good shed light happening behind scenes dplyr collapse. Fundamentally packages follow different computing paradigms: dplyr efficient implementation Split-Apply-Combine computing paradigm. Data split groups, data-chunks passed function carrying computation, finally recombined produce aggregated data.frame.  modus operandi evident grouping mechanism dplyr. data.frame passed group_by, ‘groups’ attribute attached: object data.frame giving unique groups third (last) column vectors containing indices rows belonging group. command like summarize uses information split data.frame groups passed sequentially function used later recombined. steps also done C++ makes dplyr quite efficient. Now collapse based around one-pass grouped computations C++ level using grouped statistical functions. words data split recombined entire computation performed single C++ loop running data completing computations group simultaneously. modus operandi also evident collapse grouping objects. method GRP.grouped_df takes dplyr grouping object grouped tibble efficiently converts collapse grouping object: object list first three elements give number groups, group-id row belongs vector group-sizes. function like fsum uses information (column) create result vector size ‘N.groups’ run column using ‘group.id’ vector add ’th data point ’group.id[]’th element result vector. loop finished, grouped computation also finished. obvious collapse faster dplyr since ’s method computing involves less steps, need call statistical functions multiple times. See benchmark section.","code":"GGDC10S %>% group_by(Variable, Country) %>% attr(\"groups\") # # A tibble: 85 × 3 #    Variable Country       .rows #    <chr>    <chr>   <list<int>> #  1 EMP      ARG            [62] #  2 EMP      BOL            [61] #  3 EMP      BRA            [62] #  4 EMP      BWA            [52] #  5 EMP      CHL            [63] #  6 EMP      CHN            [62] #  7 EMP      COL            [61] #  8 EMP      CRI            [62] #  9 EMP      DEW            [61] # 10 EMP      DNK            [64] # # ℹ 75 more rows GGDC10S %>% group_by(Variable, Country) %>% GRP %>% str # Class 'GRP'  hidden list of 9 #  $ N.groups    : int 85 #  $ group.id    : int [1:5027] 46 46 46 46 46 46 46 46 46 46 ... #  $ group.sizes : int [1:85] 62 61 62 52 63 62 61 62 61 64 ... #  $ groups      :List of 2 #   ..$ Variable: chr [1:85] \"EMP\" \"EMP\" \"EMP\" \"EMP\" ... #   .. ..- attr(*, \"label\")= chr \"Variable\" #   .. ..- attr(*, \"format.stata\")= chr \"%9s\" #   ..$ Country : chr [1:85] \"ARG\" \"BOL\" \"BRA\" \"BWA\" ... #   .. ..- attr(*, \"label\")= chr \"Country\" #   .. ..- attr(*, \"format.stata\")= chr \"%9s\" #  $ group.vars  : chr [1:2] \"Variable\" \"Country\" #  $ ordered     : Named logi [1:2] TRUE FALSE #   ..- attr(*, \"names\")= chr [1:2] \"ordered\" \"sorted\" #  $ order       : NULL #  $ group.starts: NULL #  $ call        : language GRP.grouped_df(X = .)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"more-speed-using-collapse-verbs","dir":"Articles","previous_headings":"1. Fast Aggregations","what":"1.2 More Speed using collapse Verbs","title":"collapse and dplyr","text":"collapse fast functions develop maximal performance grouped tibble created group_by additional conversion cost grouping object incurred GRP.grouped_df. cost already minimized use C++, can even better replacing group_by collapse::fgroup_by. fgroup_by works like group_by grouping collapse::GRP (10x faster group_by) simply attaches collapse grouping object grouped_df. Thus speed gain 2-fold: Faster grouping conversion cost calling collapse functions. Another improvement comes replacing dplyr verb select collapse::fselect, , selection using column names, indices functions use collapse::get_vars instead select_at select_if. Next get_vars, collapse also introduces predicates num_vars, cat_vars, char_vars, fact_vars, logi_vars date_vars efficiently select columns type. Benchmarks different components code larger data provided ‘Benchmarks’. Note grouped tibble created fgroup_by can longer used grouped computations dplyr verbs like mutate summarize. fgroup_by first assigns class GDP_df printing grouping information subsetting, object classes (tbl_df, data.table whatever else), followed classes grouped_df data.frame, adds grouping object ‘groups’ attribute. Since tbl_df assigned grouped_df, object treated dplyr ecosystem like normal tibble. function fungroup removes classes ‘GDP_df’ ‘grouped_df’ ‘groups’ attribute (can thus also used grouped tibbles created dplyr::group_by). Note kind data frame based class can grouped fgroup_by, still retain full responsiveness methods defined class. Functions performing aggregation grouped data frame remove grouping object classes afterwards, yielding object class attributes input. print method shown reports grouping variables, square brackets information [number groups | average group size (standard-deviation group sizes)]: Note fselect get_vars full drop-replacements select grouped_df method: Since default keep.group_vars = TRUE Fast Statistical Functions, end result nevertheless : Another useful verb introduced collapse fgroup_vars, can used efficiently obtain grouping columns grouping variables grouped tibble: Another collapse verb mention fsubset, faster alternative dplyr::filter also provides option flexibly subset columns select argument: collapse also offers roworder, frename, colorder ftransform/TRA fast replacements dplyr::arrange, dplyr::rename, dplyr::relocate dplyr::mutate.","code":"GGDC10S %>% fgroup_by(Variable, Country) %>% get_vars(6:16) %>% fmedian # # A tibble: 85 × 13 #    Variable Country     AGR     MIN     MAN     PU    CON    WRT    TRA   FIRE     GOV    OTH    SUM #    <chr>    <chr>     <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> #  1 EMP      ARG       1325.   47.4   1988.  1.05e2 7.82e2 1.85e3 5.80e2  464.   1739.   866.  9.74e3 #  2 EMP      BOL        943.   53.5    167.  4.46e0 6.60e1 1.32e2 9.70e1   15.3    NA    384.  1.84e3 #  3 EMP      BRA      17481.  225.    7208.  3.76e2 4.05e3 6.45e3 1.58e3 4355.   4450.  4479.  5.19e4 #  4 EMP      BWA        175.   12.2     13.1 3.71e0 1.90e1 2.11e1 6.75e0   10.4    53.8   31.2 3.61e2 #  5 EMP      CHL        690.   93.9    607.  2.58e1 2.30e2 4.84e2 2.05e2  106.     NA    900.  3.31e3 #  6 EMP      CHN     293915  8150.   61761.  1.14e3 1.06e4 1.70e4 9.56e3 4328.  19468.  9954.  4.45e5 #  7 EMP      COL       3006.   84.0   1033.  3.71e1 4.19e2 1.55e3 3.91e2  655.     NA   1430.  8.63e3 #  8 EMP      CRI        216.    1.49   114.  7.92e0 5.50e1 8.98e1 2.55e1   19.6   122.    60.6 7.19e2 #  9 EMP      DEW       2178   320.    8459.  2.47e2 2.10e3 4.45e3 1.53e3 1656    3700    900   2.65e4 # 10 EMP      DNK        187.    3.75   508.  1.36e1 1.65e2 4.61e2 1.61e2  169.    642.   104.  2.42e3 # # ℹ 75 more rows  microbenchmark(collapse = GGDC10S %>% fgroup_by(Variable, Country) %>% get_vars(6:16) %>% fmedian,                hybrid = GGDC10S %>% group_by(Variable, Country) %>% select_at(6:16) %>% fmedian,                dplyr = GGDC10S %>% group_by(Variable, Country) %>% select_at(6:16) %>% summarise_all(median, na.rm = TRUE)) # Unit: microseconds #      expr       min         lq      mean     median        uq       max neval #  collapse   236.406   263.6095   303.309   295.9175   337.061   419.635   100 #    hybrid  2699.317  2894.9690  3573.611  2998.3505  3119.772 56249.212   100 #     dplyr 15923.908 16297.8280 18810.943 16742.5140 18578.105 71125.939   100 class(group_by(GGDC10S, Variable, Country)) # [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"  class(fgroup_by(GGDC10S, Variable, Country)) # [1] \"GRP_df\"     \"tbl_df\"     \"tbl\"        \"grouped_df\" \"data.frame\" fgroup_by(GGDC10S, Variable, Country) # # A tibble: 5,027 × 16 #    Country Regioncode Region Variable  Year   AGR   MIN    MAN     PU    CON   WRT   TRA  FIRE   GOV #    <chr>   <chr>      <chr>  <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #  1 BWA     SSA        Sub-s… VA        1960  NA   NA    NA     NA     NA     NA    NA    NA    NA    #  2 BWA     SSA        Sub-s… VA        1961  NA   NA    NA     NA     NA     NA    NA    NA    NA    #  3 BWA     SSA        Sub-s… VA        1962  NA   NA    NA     NA     NA     NA    NA    NA    NA    #  4 BWA     SSA        Sub-s… VA        1963  NA   NA    NA     NA     NA     NA    NA    NA    NA    #  5 BWA     SSA        Sub-s… VA        1964  16.3  3.49  0.737  0.104  0.660  6.24  1.66  1.12  4.82 #  6 BWA     SSA        Sub-s… VA        1965  15.7  2.50  1.02   0.135  1.35   7.06  1.94  1.25  5.70 #  7 BWA     SSA        Sub-s… VA        1966  17.7  1.97  0.804  0.203  1.35   8.27  2.15  1.36  6.37 #  8 BWA     SSA        Sub-s… VA        1967  19.1  2.30  0.938  0.203  0.897  4.31  1.72  1.54  7.04 #  9 BWA     SSA        Sub-s… VA        1968  21.1  1.84  0.750  0.203  1.22   5.17  2.44  1.03  5.03 # 10 BWA     SSA        Sub-s… VA        1969  21.9  5.24  2.14   0.578  3.47   5.75  2.72  1.23  5.59 # # ℹ 5,017 more rows # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] GGDC10S %>% group_by(Variable, Country) %>% select_at(6:16) %>% tail(3) # # A tibble: 3 × 13 # # Groups:   Variable, Country [1] #   Variable Country   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH    SUM #   <chr>    <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> # 1 EMP      EGY     5206.  29.0 2436.  307. 2733. 2977. 1992.  801. 5539.    NA 22020. # 2 EMP      EGY     5186.  27.6 2374.  318. 2795. 3020. 2048.  815. 5636.    NA 22219. # 3 EMP      EGY     5161.  24.8 2348.  325. 2931. 3110. 2065.  832. 5736.    NA 22533. GGDC10S %>% group_by(Variable, Country) %>% get_vars(6:16) %>% tail(3) # # A tibble: 3 × 11 #     AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH    SUM #   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> # 1 5206.  29.0 2436.  307. 2733. 2977. 1992.  801. 5539.    NA 22020. # 2 5186.  27.6 2374.  318. 2795. 3020. 2048.  815. 5636.    NA 22219. # 3 5161.  24.8 2348.  325. 2931. 3110. 2065.  832. 5736.    NA 22533. GGDC10S %>% group_by(Variable, Country) %>% select_at(6:16) %>% fmean %>% tail(3) # # A tibble: 3 × 13 #   Variable Country      AGR      MIN    MAN     PU    CON    WRT    TRA   FIRE     GOV    OTH    SUM #   <chr>    <chr>      <dbl>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> # 1 VA       VEN        6860.   35478. 1.96e4 1.06e3 1.17e4 1.93e4 8.03e3 5.60e3 NA      19986. 1.28e5 # 2 VA       ZAF       16419.   42928. 8.76e4 1.38e4 1.64e4 6.83e4 4.53e4 6.64e4  7.58e4 30167. 4.63e5 # 3 VA       ZMB     1268849. 1006099. 9.00e5 2.19e5 8.66e5 2.10e6 7.05e5 9.10e5  1.10e6 81871. 9.16e6 GGDC10S %>% group_by(Variable, Country) %>% get_vars(6:16) %>% fmean %>% tail(3) # # A tibble: 3 × 13 #   Variable Country      AGR      MIN    MAN     PU    CON    WRT    TRA   FIRE     GOV    OTH    SUM #   <chr>    <chr>      <dbl>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> # 1 VA       VEN        6860.   35478. 1.96e4 1.06e3 1.17e4 1.93e4 8.03e3 5.60e3 NA      19986. 1.28e5 # 2 VA       ZAF       16419.   42928. 8.76e4 1.38e4 1.64e4 6.83e4 4.53e4 6.64e4  7.58e4 30167. 4.63e5 # 3 VA       ZMB     1268849. 1006099. 9.00e5 2.19e5 8.66e5 2.10e6 7.05e5 9.10e5  1.10e6 81871. 9.16e6 # fgroup_by fully supports grouped tibbles created with group_by or fgroup_by:  GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars %>% head(3) # # A tibble: 3 × 2 #   Variable Country #   <chr>    <chr>   # 1 VA       BWA     # 2 VA       BWA     # 3 VA       BWA GGDC10S %>% fgroup_by(Variable, Country) %>% fgroup_vars %>% head(3) # # A tibble: 3 × 2 #   Variable Country #   <chr>    <chr>   # 1 VA       BWA     # 2 VA       BWA     # 3 VA       BWA  # The other possibilities: GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars(\"unique\") %>% head(3) # # A tibble: 3 × 2 #   Variable Country #   <chr>    <chr>   # 1 EMP      ARG     # 2 EMP      BOL     # 3 EMP      BRA GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars(\"names\") # [1] \"Variable\" \"Country\" GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars(\"indices\") # [1] 4 1 GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars(\"named_indices\") # Variable  Country  #        4        1 GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars(\"logical\") #  [1]  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE GGDC10S %>% group_by(Variable, Country) %>% fgroup_vars(\"named_logical\") #    Country Regioncode     Region   Variable       Year        AGR        MIN        MAN         PU  #       TRUE      FALSE      FALSE       TRUE      FALSE      FALSE      FALSE      FALSE      FALSE  #        CON        WRT        TRA       FIRE        GOV        OTH        SUM  #      FALSE      FALSE      FALSE      FALSE      FALSE      FALSE      FALSE # Two equivalent calls, the first is substantially faster GGDC10S %>% fsubset(Variable == \"VA\" & Year > 1990, Country, Year, AGR:GOV) %>% head(3) # # A tibble: 3 × 11 #   Country  Year   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV #   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 BWA      1991  303. 2647.  473.  161.  580.  807.  233.  433. 1073. # 2 BWA      1992  333. 2691.  537.  178.  679.  725.  285.  517. 1234. # 3 BWA      1993  405. 2625.  567.  219.  634.  772.  350.  673. 1487.  GGDC10S %>% filter(Variable == \"VA\" & Year > 1990) %>% select(Country, Year, AGR:GOV) %>% head(3) # # A tibble: 3 × 11 #   Country  Year   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV #   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 BWA      1991  303. 2647.  473.  161.  580.  807.  233.  433. 1073. # 2 BWA      1992  333. 2691.  537.  178.  679.  725.  285.  517. 1234. # 3 BWA      1993  405. 2625.  567.  219.  634.  772.  350.  673. 1487."},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"multi-function-aggregations","dir":"Articles","previous_headings":"1. Fast Aggregations","what":"1.3 Multi-Function Aggregations","title":"collapse and dplyr","text":"One can also aggregate multiple functions time. operations often necessary use curly braces { prevent first argument injection %>% cbind(FUN1(.), FUN2(.)) evaluate %>% cbind(., FUN1(.), FUN2(.)): function add_stub used collapse function adding prefix (default) suffix variables names. collapse predicate add_vars provides efficient alternative cbind.data.frame. idea ‘adding’ variables data.frame first argument .e. attributes first argument preserved, expression still gives tibble instead data.frame: Another nice feature add_vars can also efficiently reorder columns .e. bind columns different order passed. can done simply specifying positions added columns final data frame, add_vars shifts first argument columns right fill gaps. much compact solution multi-function multi-type aggregation offered function collapg: default aggregates numeric columns using fmean categorical columns using fmode, preserves order columns. Changing defaults easy: One can apply multiple functions numeric /categorical data: Applying multiple functions numeric (categorical) data allows return long format: Finally, collapg also makes easy apply aggregator functions certain columns : understand collapg, look documentation (?collapg).","code":"GGDC10S %>%   fgroup_by(Variable, Country) %>%   get_vars(6:16) %>% {     cbind(fmedian(.),           add_stub(fmean(., keep.group_vars = FALSE), \"mean_\"))     } %>% head(3) #   Variable Country        AGR       MIN       MAN         PU        CON      WRT        TRA # 1      EMP     ARG  1324.5255  47.35255 1987.5912 104.738825  782.40283 1854.612  579.93982 # 2      EMP     BOL   943.1612  53.53538  167.1502   4.457895   65.97904  132.225   96.96828 # 3      EMP     BRA 17480.9810 225.43693 7207.7915 375.851832 4054.66103 6454.523 1580.81120 #         FIRE      GOV       OTH       SUM   mean_AGR  mean_MIN  mean_MAN    mean_PU  mean_CON # 1  464.39920 1738.836  866.1119  9743.223  1419.8013  52.08903 1931.7602 101.720936  742.4044 # 2   15.34259       NA  384.0678  1842.055   964.2103  56.03295  235.0332   5.346433  122.7827 # 3 4354.86210 4449.942 4478.6927 51881.110 17191.3529 206.02389 6991.3710 364.573404 3524.7384 #    mean_WRT  mean_TRA  mean_FIRE mean_GOV  mean_OTH  mean_SUM # 1 1982.1775  648.5119  627.79291 2043.471  992.4475 10542.177 # 2  281.5164  115.4728   44.56442       NA  395.5650  2220.524 # 3 8509.4612 2054.3731 4413.54448 5307.280 5710.2665 54272.985 GGDC10S %>%   fgroup_by(Variable, Country) %>% {    add_vars(get_vars(., \"Reg\", regex = TRUE) %>% ffirst, # Regular expression matching column names             num_vars(.) %>% fmean(keep.group_vars = FALSE) %>% add_stub(\"mean_\"), # num_vars selects all numeric variables             fselect(., PU:TRA) %>% fmedian(keep.group_vars = FALSE) %>% add_stub(\"median_\"),              fselect(., PU:CON) %>% fmin(keep.group_vars = FALSE) %>% add_stub(\"min_\"))         } %>% head(3) # # A tibble: 3 × 22 #   Variable Country Regioncode Region  mean_Year mean_AGR mean_MIN mean_MAN mean_PU mean_CON mean_WRT #   <chr>    <chr>   <chr>      <chr>       <dbl>    <dbl>    <dbl>    <dbl>   <dbl>    <dbl>    <dbl> # 1 EMP      ARG     LAM        Latin …     1980.    1420.     52.1    1932.  102.       742.    1982. # 2 EMP      BOL     LAM        Latin …     1980      964.     56.0     235.    5.35     123.     282. # 3 EMP      BRA     LAM        Latin …     1980.   17191.    206.     6991.  365.      3525.    8509. # # ℹ 11 more variables: mean_TRA <dbl>, mean_FIRE <dbl>, mean_GOV <dbl>, mean_OTH <dbl>, # #   mean_SUM <dbl>, median_PU <dbl>, median_CON <dbl>, median_WRT <dbl>, median_TRA <dbl>, # #   min_PU <dbl>, min_CON <dbl> GGDC10S %>%   fsubset(Variable == \"VA\", Country, AGR, SUM) %>%    fgroup_by(Country) %>% {    add_vars(fgroup_vars(.,\"unique\"),             fmean(., keep.group_vars = FALSE) %>% add_stub(\"mean_\"),             fsd(., keep.group_vars = FALSE) %>% add_stub(\"sd_\"),              pos = c(2,4,3,5))   } %>% head(3) # # A tibble: 3 × 5 #   Country mean_AGR sd_AGR mean_SUM  sd_SUM #   <chr>      <dbl>  <dbl>    <dbl>   <dbl> # 1 ARG       14951. 33061.  152534. 301316. # 2 BOL        3300.  4456.   22619.  33173. # 3 BRA       76870. 59442. 1200563. 976963. # This aggregates numeric colums using the mean (fmean) and categorical columns with the mode (fmode) GGDC10S %>% fgroup_by(Variable, Country) %>% collapg %>% head(3) # # A tibble: 3 × 16 #   Variable Country Regioncode Region   Year    AGR   MIN   MAN     PU   CON   WRT   TRA   FIRE   GOV #   <chr>    <chr>   <chr>      <chr>   <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> # 1 EMP      ARG     LAM        Latin … 1980.  1420.  52.1 1932. 102.    742. 1982.  649.  628.  2043. # 2 EMP      BOL     LAM        Latin … 1980    964.  56.0  235.   5.35  123.  282.  115.   44.6   NA  # 3 EMP      BRA     LAM        Latin … 1980. 17191. 206.  6991. 365.   3525. 8509. 2054. 4414.  5307. # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> # This aggregates numeric colums using the median and categorical columns using the first value GGDC10S %>% fgroup_by(Variable, Country) %>% collapg(fmedian, flast) %>% head(3) # # A tibble: 3 × 16 #   Variable Country Regioncode Region       Year    AGR   MIN   MAN     PU    CON   WRT    TRA   FIRE #   <chr>    <chr>   <chr>      <chr>       <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl> # 1 EMP      ARG     LAM        Latin Amer… 1980.  1325.  47.4 1988. 105.    782.  1855.  580.   464.  # 2 EMP      BOL     LAM        Latin Amer… 1980    943.  53.5  167.   4.46   66.0  132.   97.0   15.3 # 3 EMP      BRA     LAM        Latin Amer… 1980. 17481. 225.  7208. 376.   4055.  6455. 1581.  4355.  # # ℹ 3 more variables: GOV <dbl>, OTH <dbl>, SUM <dbl> GGDC10S %>% fgroup_by(Variable, Country) %>%   collapg(list(fmean, fmedian), list(first, fmode, flast)) %>% head(3) # # A tibble: 3 × 32 #   Variable Country first.Regioncode fmode.Regioncode flast.Regioncode first.Region  fmode.Region  #   <chr>    <chr>   <chr>            <chr>            <chr>            <chr>         <chr>         # 1 EMP      ARG     LAM              LAM              LAM              Latin America Latin America # 2 EMP      BOL     LAM              LAM              LAM              Latin America Latin America # 3 EMP      BRA     LAM              LAM              LAM              Latin America Latin America # # ℹ 25 more variables: flast.Region <chr>, fmean.Year <dbl>, fmedian.Year <dbl>, fmean.AGR <dbl>, # #   fmedian.AGR <dbl>, fmean.MIN <dbl>, fmedian.MIN <dbl>, fmean.MAN <dbl>, fmedian.MAN <dbl>, # #   fmean.PU <dbl>, fmedian.PU <dbl>, fmean.CON <dbl>, fmedian.CON <dbl>, fmean.WRT <dbl>, # #   fmedian.WRT <dbl>, fmean.TRA <dbl>, fmedian.TRA <dbl>, fmean.FIRE <dbl>, fmedian.FIRE <dbl>, # #   fmean.GOV <dbl>, fmedian.GOV <dbl>, fmean.OTH <dbl>, fmedian.OTH <dbl>, fmean.SUM <dbl>, # #   fmedian.SUM <dbl> GGDC10S %>% fgroup_by(Variable, Country) %>%   collapg(list(fmean, fmedian), cols = is.numeric, return = \"long\") %>% head(3) # # A tibble: 3 × 15 #   Function Variable Country  Year    AGR   MIN   MAN     PU   CON   WRT   TRA   FIRE   GOV   OTH #   <chr>    <chr>    <chr>   <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> # 1 fmean    EMP      ARG     1980.  1420.  52.1 1932. 102.    742. 1982.  649.  628.  2043.  992. # 2 fmean    EMP      BOL     1980    964.  56.0  235.   5.35  123.  282.  115.   44.6   NA   396. # 3 fmean    EMP      BRA     1980. 17191. 206.  6991. 365.   3525. 8509. 2054. 4414.  5307. 5710. # # ℹ 1 more variable: SUM <dbl> GGDC10S %>% fgroup_by(Variable, Country) %>%   collapg(custom = list(fmean = 6:8, fmedian = 10:12)) %>% head(3) # # A tibble: 3 × 8 #   Variable Country    AGR   MIN   MAN    CON   WRT    TRA #   <chr>    <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> # 1 EMP      ARG      1420.  52.1 1932.  782.  1855.  580.  # 2 EMP      BOL       964.  56.0  235.   66.0  132.   97.0 # 3 EMP      BRA     17191. 206.  6991. 4055.  6455. 1581."},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"weighted-aggregations","dir":"Articles","previous_headings":"1. Fast Aggregations","what":"1.4 Weighted Aggregations","title":"collapse and dplyr","text":"Weighted aggregations possible functions fsum, fprod, fmean, fmedian, fnth, fmode, fvar fsd. implementation default (option keep.w = TRUE) functions also aggregate weights, weighted computations can performed aggregated data. fprod saves product weights, whereas functions save sum weights column next grouping variables. na.rm = TRUE (default), rows missing weights omitted computation. weighted variance / standard deviation currently implemented frequency weights. Weighted aggregations may also performed collapg. default fsum used compute sum weights, also possible aggregate weights functions:","code":"# This computes a frequency-weighted grouped standard-deviation, taking the total EMP / VA as weight GGDC10S %>%   fgroup_by(Variable, Country) %>%   fselect(AGR:SUM) %>% fsd(SUM) %>% head(3) # # A tibble: 3 × 13 #   Variable Country  sum.SUM    AGR   MIN   MAN    PU   CON   WRT    TRA   FIRE   GOV   OTH #   <chr>    <chr>      <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl> <dbl> # 1 EMP      ARG      653615.  225.   22.2  176. 20.5   285.  856.  195.   493.  1123.  506. # 2 EMP      BOL      135452.   99.7  17.1  168.  4.87  123.  324.   98.1   69.8   NA   258. # 3 EMP      BRA     3364925. 1587.   73.8 2952. 93.8  1861. 6285. 1306.  3003.  3621. 4257.  # This computes a weighted grouped mode, taking the total EMP / VA as weight GGDC10S %>%   fgroup_by(Variable, Country) %>%   fselect(AGR:SUM) %>% fmode(SUM) %>% head(3) # # A tibble: 3 × 13 #   Variable Country  sum.SUM    AGR   MIN    MAN    PU   CON    WRT   TRA   FIRE    GOV    OTH #   <chr>    <chr>      <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> # 1 EMP      ARG      653615.  1162. 127.   2164. 152.  1415.  3768. 1060.  1748.  4336.  1999. # 2 EMP      BOL      135452.   819.  37.6   604.  10.8  433.   893.  333.   321.    NA   1057. # 3 EMP      BRA     3364925. 16451. 313.  11841. 388.  8154. 21860. 5169. 12011. 12149. 14235. # This aggregates numeric colums using the weighted mean (the default) and categorical columns using the weighted mode (the default). # Weights (column SUM) are aggregated using both the sum and the maximum.  GGDC10S %>% group_by(Variable, Country) %>%    collapg(w = SUM, wFUN = list(fsum, fmax)) %>% head(3) # # A tibble: 3 × 17 #   Variable Country fsum.SUM fmax.SUM Regioncode Region   Year    AGR   MIN   MAN     PU   CON    WRT #   <chr>    <chr>      <dbl>    <dbl> <chr>      <chr>   <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> # 1 EMP      ARG      653615.   17929. LAM        Latin … 1985.  1361.  56.5 1935. 105.    811.  2217. # 2 EMP      BOL      135452.    4508. LAM        Latin … 1987.   977.  57.9  296.   7.07  167.   400. # 3 EMP      BRA     3364925.  102572. LAM        Latin … 1989. 17746. 238.  8466. 389.   4436. 11376. # # ℹ 4 more variables: TRA <dbl>, FIRE <dbl>, GOV <dbl>, OTH <dbl>"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"fast-transformations","dir":"Articles","previous_headings":"","what":"2. Fast Transformations","title":"collapse and dplyr","text":"collapse also provides fast transformations significantly extend scope speed manipulations can performed dplyr::mutate.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"fast-transform-and-compute-variables","dir":"Articles","previous_headings":"2. Fast Transformations","what":"2.1 Fast Transform and Compute Variables","title":"collapse and dplyr","text":"function ftransform can used manipulate columns ways mutate: modification brought ftransformv enables transformations groups columns like dplyr::mutate_at dplyr::mutate_if: Instead column = value type arguments, also possible pass single list transformed variables ftransform, regarded way evaluated list column = value arguments. can used complex transformations: computed columns need returned, fcompute provides efficient alternative: ftransform fcompute order magnitude faster mutate, support grouped computations using arbitrary functions. see hardly limitation collapse provides efficient elegant alternative programming mechanisms…","code":"GGDC10S %>% fsubset(Variable == \"VA\", Country, Year, AGR, SUM) %>%   ftransform(AGR_perc = AGR / SUM * 100,  # Computing % of VA in Agriculture              AGR_mean = fmean(AGR),       # Average Agricultural VA              AGR = NULL, SUM = NULL) %>%  # Deleting columns AGR and SUM              head # # A tibble: 6 × 4 #   Country  Year AGR_perc AGR_mean #   <chr>   <dbl>    <dbl>    <dbl> # 1 BWA      1960     NA   5137561. # 2 BWA      1961     NA   5137561. # 3 BWA      1962     NA   5137561. # 4 BWA      1963     NA   5137561. # 5 BWA      1964     43.5 5137561. # 6 BWA      1965     40.0 5137561. # This replaces variables mpg, carb and wt by their log (.c turns expressions into character vectors) mtcars %>% ftransformv(.c(mpg, carb, wt), log) %>% head #                        mpg cyl disp  hp drat        wt  qsec vs am gear      carb # Mazda RX4         3.044522   6  160 110 3.90 0.9631743 16.46  0  1    4 1.3862944 # Mazda RX4 Wag     3.044522   6  160 110 3.90 1.0560527 17.02  0  1    4 1.3862944 # Datsun 710        3.126761   4  108  93 3.85 0.8415672 18.61  1  1    4 0.0000000 # Hornet 4 Drive    3.063391   6  258 110 3.08 1.1678274 19.44  1  0    3 0.0000000 # Hornet Sportabout 2.928524   8  360 175 3.15 1.2354715 17.02  0  0    3 0.6931472 # Valiant           2.895912   6  225 105 2.76 1.2412686 20.22  1  0    3 0.0000000  # Logging numeric variables iris %>% ftransformv(is.numeric, log) %>% head #   Sepal.Length Sepal.Width Petal.Length Petal.Width Species # 1     1.629241    1.252763    0.3364722  -1.6094379  setosa # 2     1.589235    1.098612    0.3364722  -1.6094379  setosa # 3     1.547563    1.163151    0.2623643  -1.6094379  setosa # 4     1.526056    1.131402    0.4054651  -1.6094379  setosa # 5     1.609438    1.280934    0.3364722  -1.6094379  setosa # 6     1.686399    1.360977    0.5306283  -0.9162907  setosa # Logging values and replacing generated Inf values mtcars %>% ftransform(fselect(., mpg, cyl, vs:gear) %>% lapply(log) %>% replace_Inf) %>% head #                        mpg      cyl disp  hp drat    wt  qsec vs am     gear carb # Mazda RX4         3.044522 1.791759  160 110 3.90 2.620 16.46 NA  0 1.386294    4 # Mazda RX4 Wag     3.044522 1.791759  160 110 3.90 2.875 17.02 NA  0 1.386294    4 # Datsun 710        3.126761 1.386294  108  93 3.85 2.320 18.61  0  0 1.386294    1 # Hornet 4 Drive    3.063391 1.791759  258 110 3.08 3.215 19.44  0 NA 1.098612    1 # Hornet Sportabout 2.928524 2.079442  360 175 3.15 3.440 17.02 NA NA 1.098612    2 # Valiant           2.895912 1.791759  225 105 2.76 3.460 20.22  0 NA 1.098612    1 GGDC10S %>% fsubset(Variable == \"VA\", Country, Year, AGR, SUM) %>%   fcompute(AGR_perc = AGR / SUM * 100,            AGR_mean = fmean(AGR)) %>% head # # A tibble: 6 × 2 #   AGR_perc AGR_mean #      <dbl>    <dbl> # 1     NA   5137561. # 2     NA   5137561. # 3     NA   5137561. # 4     NA   5137561. # 5     43.5 5137561. # 6     40.0 5137561."},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"replacing-and-sweeping-out-statistics","dir":"Articles","previous_headings":"2. Fast Transformations","what":"2.2 Replacing and Sweeping out Statistics","title":"collapse and dplyr","text":"statistical (scalar-valued) functions collapse package (fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs, fndistinct) TRA argument can used efficiently transform data either (column-wise) replacing data values computed statistics sweeping statistics data. Operations can specified using either integer quoted operator / string. 10 operations supported TRA : 1 - “replace_fill” : replace overwrite missing values (mutate) 2 - “replace” : replace preserve missing values 3 - “-” : subtract (center) 4 - “-+” : subtract group-statistics add average group statistics 5 - “/” : divide (scale) 6 - “%” : compute percentages (divide multiply 100) 7 - “+” : add 8 - “*” : multiply 9 - “%%” : modulus 10 - “-%%” : subtract modulus Simple transformations straightforward specify: Similarly grouped transformations: benchmarks demonstrate internal sweeping replacement operations fully performed C++ compute significantly faster using dplyr::mutate, especially number groups grows large. S3 generic nature Fast Statistical Functions allows us perform grouped mutations fly (together ftransform fcompute), without need first creating grouped tibble: Weights easily added grouped transformation: Sequential operations also easily performed: course also possible combine multiple functions aggregation section, add variables existing data: lots examples one construct using 10 operations 14 functions listed , examples provided just outline suggested programming basics. Performance considerations make much worthwhile spend time think complex operations can implemented programming framework, defining function R applying data using dplyr::mutate.","code":"# This subtracts the median value from all data points i.e. centers on the median GGDC10S %>% num_vars %>% fmedian(TRA = \"-\") %>% head # # A tibble: 6 × 12 #    Year    AGR   MIN    MAN    PU    CON    WRT    TRA  FIRE    GOV    OTH     SUM #   <dbl>  <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>   <dbl> # 1   -22    NA    NA     NA    NA     NA     NA     NA    NA     NA     NA      NA  # 2   -21    NA    NA     NA    NA     NA     NA     NA    NA     NA     NA      NA  # 3   -20    NA    NA     NA    NA     NA     NA     NA    NA     NA     NA      NA  # 4   -19    NA    NA     NA    NA     NA     NA     NA    NA     NA     NA      NA  # 5   -18 -4378. -170. -3717. -168. -1473. -3767. -1173. -959. -3924. -1431. -23149. # 6   -17 -4379. -171. -3717. -168. -1472. -3767. -1173. -959. -3923. -1430. -23147.  # This replaces all data points with the mode GGDC10S %>% char_vars %>% fmode(TRA = \"replace\") %>% head # # A tibble: 6 × 4 #   Country Regioncode Region Variable #   <chr>   <chr>      <chr>  <chr>    # 1 USA     ASI        Asia   EMP      # 2 USA     ASI        Asia   EMP      # 3 USA     ASI        Asia   EMP      # 4 USA     ASI        Asia   EMP      # 5 USA     ASI        Asia   EMP      # 6 USA     ASI        Asia   EMP # Replacing data with the 2nd quartile (25%) GGDC10S %>%   fselect(Variable, Country, AGR:SUM) %>%     fgroup_by(Variable, Country) %>% fnth(0.25, TRA = \"replace_fill\") %>% head(3) # # A tibble: 3 × 13 #   Variable Country   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH   SUM #   <chr>    <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 VA       BWA      63.5  33.1  27.3  7.36  26.8  31.1  13.2  12.0  33.6  11.5  262. # 2 VA       BWA      63.5  33.1  27.3  7.36  26.8  31.1  13.2  12.0  33.6  11.5  262. # 3 VA       BWA      63.5  33.1  27.3  7.36  26.8  31.1  13.2  12.0  33.6  11.5  262.  # Scaling sectoral data by Variable and Country GGDC10S %>%   fselect(Variable, Country, AGR:SUM) %>%     fgroup_by(Variable, Country) %>% fsd(TRA = \"/\") %>% head # # A tibble: 6 × 13 #   Variable Country     AGR       MIN       MAN       PU      CON      WRT      TRA     FIRE      GOV #   <chr>    <chr>     <dbl>     <dbl>     <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl> # 1 VA       BWA     NA      NA        NA        NA       NA       NA       NA       NA       NA       # 2 VA       BWA     NA      NA        NA        NA       NA       NA       NA       NA       NA       # 3 VA       BWA     NA      NA        NA        NA       NA       NA       NA       NA       NA       # 4 VA       BWA     NA      NA        NA        NA       NA       NA       NA       NA       NA       # 5 VA       BWA      0.0270  0.000556  0.000523  3.88e-4  5.11e-4  0.00194  0.00154  5.23e-4  0.00134 # 6 VA       BWA      0.0260  0.000397  0.000723  5.03e-4  1.04e-3  0.00220  0.00180  5.83e-4  0.00158 # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> # AGR_gmed = TRUE if AGR is greater than it's median value, grouped by Variable and Country # Note: This calls fmedian.default settransform(GGDC10S, AGR_gmed = AGR > fmedian(AGR, list(Variable, Country), TRA = \"replace\")) tail(GGDC10S, 3) # # A tibble: 3 × 17 #   Country Regioncode Region     Variable  Year   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV #   <chr>   <chr>      <chr>      <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 EGY     MENA       Middle Ea… EMP       2010 5206.  29.0 2436.  307. 2733. 2977. 1992.  801. 5539. # 2 EGY     MENA       Middle Ea… EMP       2011 5186.  27.6 2374.  318. 2795. 3020. 2048.  815. 5636. # 3 EGY     MENA       Middle Ea… EMP       2012 5161.  24.8 2348.  325. 2931. 3110. 2065.  832. 5736. # # ℹ 3 more variables: OTH <dbl>, SUM <dbl>, AGR_gmed <lgl>  # Dividing (scaling) the sectoral data (columns 6 through 16) by their grouped standard deviation settransformv(GGDC10S, 6:16, fsd, list(Variable, Country), TRA = \"/\", apply = FALSE) tail(GGDC10S, 3) # # A tibble: 3 × 17 #   Country Regioncode Region     Variable  Year   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV #   <chr>   <chr>      <chr>      <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 EGY     MENA       Middle Ea… EMP       2010  8.41  2.28  4.32  3.56  3.62  3.75  3.75  3.14  3.80 # 2 EGY     MENA       Middle Ea… EMP       2011  8.38  2.17  4.21  3.68  3.70  3.81  3.86  3.19  3.86 # 3 EGY     MENA       Middle Ea… EMP       2012  8.34  1.95  4.17  3.76  3.88  3.92  3.89  3.26  3.93 # # ℹ 3 more variables: OTH <dbl>, SUM <dbl>, AGR_gmed <lgl> rm(GGDC10S) # This subtracts weighted group means from the data, using SUM column as weights..  GGDC10S %>%   fselect(Variable, Country, AGR:SUM) %>%     fgroup_by(Variable, Country) %>% fmean(SUM, \"-\") %>% head # # A tibble: 6 × 13 #   Variable Country   SUM    AGR     MIN    MAN    PU    CON    WRT    TRA   FIRE    GOV    OTH #   <chr>    <chr>   <dbl>  <dbl>   <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> # 1 VA       BWA      NA      NA      NA     NA    NA     NA     NA     NA     NA     NA     NA  # 2 VA       BWA      NA      NA      NA     NA    NA     NA     NA     NA     NA     NA     NA  # 3 VA       BWA      NA      NA      NA     NA    NA     NA     NA     NA     NA     NA     NA  # 4 VA       BWA      NA      NA      NA     NA    NA     NA     NA     NA     NA     NA     NA  # 5 VA       BWA      37.5 -1301. -13317. -2965. -529. -2746. -6540. -2157. -4431. -7551. -2613. # 6 VA       BWA      39.3 -1302. -13318. -2964. -529. -2745. -6540. -2156. -4431. -7550. -2613. # This scales and then subtracts the median GGDC10S %>%   fselect(Variable, Country, AGR:SUM) %>%     fgroup_by(Variable, Country) %>% fsd(TRA = \"/\") %>% fmedian(TRA = \"-\") # # A tibble: 5,027 × 13 #    Variable Country    AGR    MIN    MAN     PU    CON     WRT     TRA    FIRE    GOV     OTH    SUM #  * <chr>    <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>  <dbl> #  1 VA       BWA     NA     NA     NA     NA     NA     NA      NA      NA      NA     NA      NA     #  2 VA       BWA     NA     NA     NA     NA     NA     NA      NA      NA      NA     NA      NA     #  3 VA       BWA     NA     NA     NA     NA     NA     NA      NA      NA      NA     NA      NA     #  4 VA       BWA     NA     NA     NA     NA     NA     NA      NA      NA      NA     NA      NA     #  5 VA       BWA     -0.182 -0.235 -0.183 -0.245 -0.118 -0.0820 -0.0724 -0.0661 -0.108 -0.0848 -0.146 #  6 VA       BWA     -0.183 -0.235 -0.183 -0.245 -0.117 -0.0817 -0.0722 -0.0660 -0.108 -0.0846 -0.146 #  7 VA       BWA     -0.180 -0.235 -0.183 -0.245 -0.117 -0.0813 -0.0720 -0.0659 -0.107 -0.0843 -0.145 #  8 VA       BWA     -0.177 -0.235 -0.183 -0.245 -0.117 -0.0826 -0.0724 -0.0659 -0.107 -0.0841 -0.146 #  9 VA       BWA     -0.174 -0.235 -0.183 -0.245 -0.117 -0.0823 -0.0717 -0.0661 -0.108 -0.0848 -0.146 # 10 VA       BWA     -0.173 -0.234 -0.182 -0.243 -0.115 -0.0821 -0.0715 -0.0660 -0.108 -0.0846 -0.145 # # ℹ 5,017 more rows #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] # This adds a groupwise observation count next to each column add_vars(GGDC10S, seq(7,27,2)) <- GGDC10S %>%     fgroup_by(Variable, Country) %>% fselect(AGR:SUM) %>%     fnobs(\"replace_fill\") %>% add_stub(\"N_\")  head(GGDC10S) # # A tibble: 6 × 27 #   Country Regioncode Region  Variable  Year   AGR N_AGR   MIN N_MIN    MAN N_MAN     PU  N_PU    CON #   <chr>   <chr>      <chr>   <chr>    <dbl> <dbl> <int> <dbl> <int>  <dbl> <int>  <dbl> <int>  <dbl> # 1 BWA     SSA        Sub-sa… VA        1960  NA      47 NA       47 NA        47 NA        47 NA     # 2 BWA     SSA        Sub-sa… VA        1961  NA      47 NA       47 NA        47 NA        47 NA     # 3 BWA     SSA        Sub-sa… VA        1962  NA      47 NA       47 NA        47 NA        47 NA     # 4 BWA     SSA        Sub-sa… VA        1963  NA      47 NA       47 NA        47 NA        47 NA     # 5 BWA     SSA        Sub-sa… VA        1964  16.3    47  3.49    47  0.737    47  0.104    47  0.660 # 6 BWA     SSA        Sub-sa… VA        1965  15.7    47  2.50    47  1.02     47  0.135    47  1.35  # # ℹ 13 more variables: N_CON <int>, WRT <dbl>, N_WRT <int>, TRA <dbl>, N_TRA <int>, FIRE <dbl>, # #   N_FIRE <int>, GOV <dbl>, N_GOV <int>, OTH <dbl>, N_OTH <int>, SUM <dbl>, N_SUM <int> rm(GGDC10S)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"more-control-using-the-tra-function","dir":"Articles","previous_headings":"2. Fast Transformations","what":"2.3 More Control using the TRA Function","title":"collapse and dplyr","text":"Towards end, calling TRA() directly also facilitates complex customized operations. Behind scenes TRA = ... argument, Fast Statistical Functions first compute grouped statistics columns data, statistics directly fed C++ function uses replace sweep data points one 10 ways described . function can also called directly name TRA. Fundamentally, TRA generalization base::sweep column-wise grouped operations1. Direct calls TRA enable control inputs outputs. two operations equivalent, although first slightly efficient requires one method dispatch one check inputs: TRA.grouped_df designed matches columns statistics (aggregated columns) original data, transforms matching columns returning whole data frame. Thus easily possible apply transformation first two sectors: Since TRA already built Fast Statistical Functions argument, best used computations grouped statistics computed using function. Another potential use TRA computations two- steps, example aggregated transformed data needed, computations complex involve manipulations -aggregating sweeping part: discussed, whether using argument fast statistical functions TRA directly, data transformations essentially two-step process: Statistics first computed used transform original data. Although steps efficiently done C++, even efficient single step without materializing statistics transforming data. slightly efficient functions provided commonly applied tasks centering averaging data groups (widely known ‘’-group ‘within’-group transformations), scaling centering data groups (also known ‘standardizing’ data).","code":"# This divides by the product GGDC10S %>%   fgroup_by(Variable, Country) %>%     get_vars(6:16) %>% fprod(TRA = \"/\") %>% head # # A tibble: 6 × 11 #          AGR        MIN        MAN        PU        CON        WRT       TRA      FIRE        GOV #        <dbl>      <dbl>      <dbl>     <dbl>      <dbl>      <dbl>     <dbl>     <dbl>      <dbl> # 1 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 2 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 3 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 4 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 5  1.29e-105  2.81e-127  1.40e-101  4.44e-74  4.19e-102  3.97e-113  6.91e-92  1.01e-97  2.51e-117 # 6  1.24e-105  2.00e-127  1.94e-101  5.75e-74  8.55e-102  4.49e-113  8.08e-92  1.13e-97  2.96e-117 # # ℹ 2 more variables: OTH <dbl>, SUM <dbl>  # Same thing GGDC10S %>%   fgroup_by(Variable, Country) %>%     get_vars(6:16) %>%       TRA(fprod(., keep.group_vars = FALSE), \"/\") %>% head # [same as TRA(.,fprod(., keep.group_vars = FALSE),\"/\")] # # A tibble: 6 × 11 #          AGR        MIN        MAN        PU        CON        WRT       TRA      FIRE        GOV #        <dbl>      <dbl>      <dbl>     <dbl>      <dbl>      <dbl>     <dbl>     <dbl>      <dbl> # 1 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 2 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 3 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 4 NA         NA         NA         NA        NA         NA         NA        NA        NA         # 5  1.29e-105  2.81e-127  1.40e-101  4.44e-74  4.19e-102  3.97e-113  6.91e-92  1.01e-97  2.51e-117 # 6  1.24e-105  2.00e-127  1.94e-101  5.75e-74  8.55e-102  4.49e-113  8.08e-92  1.13e-97  2.96e-117 # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> # This only demeans Agriculture (AGR) and Mining (MIN) GGDC10S %>%   fgroup_by(Variable, Country) %>%     TRA(fselect(., AGR, MIN) %>% fmean(keep.group_vars = FALSE), \"-\") %>% head # # A tibble: 6 × 16 #   Country Regioncode Region Variable  Year   AGR    MIN    MAN     PU    CON   WRT   TRA  FIRE   GOV #   <chr>   <chr>      <chr>  <chr>    <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> # 1 BWA     SSA        Sub-s… VA        1960   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 2 BWA     SSA        Sub-s… VA        1961   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 3 BWA     SSA        Sub-s… VA        1962   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 4 BWA     SSA        Sub-s… VA        1963   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 5 BWA     SSA        Sub-s… VA        1964 -446. -4505.  0.737  0.104  0.660  6.24  1.66  1.12  4.82 # 6 BWA     SSA        Sub-s… VA        1965 -446. -4506.  1.02   0.135  1.35   7.06  1.94  1.25  5.70 # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> # Same as above, with one line of code using fmean.data.frame and ftransform... GGDC10S %>% ftransform(fmean(list(AGR = AGR, MIN = MIN), list(Variable, Country), TRA = \"-\")) %>% head # # A tibble: 6 × 16 #   Country Regioncode Region Variable  Year   AGR    MIN    MAN     PU    CON   WRT   TRA  FIRE   GOV #   <chr>   <chr>      <chr>  <chr>    <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> # 1 BWA     SSA        Sub-s… VA        1960   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 2 BWA     SSA        Sub-s… VA        1961   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 3 BWA     SSA        Sub-s… VA        1962   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 4 BWA     SSA        Sub-s… VA        1963   NA     NA  NA     NA     NA     NA    NA    NA    NA    # 5 BWA     SSA        Sub-s… VA        1964 -446. -4505.  0.737  0.104  0.660  6.24  1.66  1.12  4.82 # 6 BWA     SSA        Sub-s… VA        1965 -446. -4506.  1.02   0.135  1.35   7.06  1.94  1.25  5.70 # # ℹ 2 more variables: OTH <dbl>, SUM <dbl> # Get grouped tibble gGGDC <- GGDC10S %>% fgroup_by(Variable, Country)  # Get aggregated data gsumGGDC <- gGGDC %>% fselect(AGR:SUM) %>% fsum head(gsumGGDC) # # A tibble: 6 × 13 #   Variable Country       AGR     MIN    MAN     PU    CON    WRT    TRA   FIRE     GOV    OTH    SUM #   <chr>    <chr>       <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> # 1 EMP      ARG        88028.   3230. 1.20e5  6307. 4.60e4 1.23e5 4.02e4 3.89e4  1.27e5 6.15e4 6.54e5 # 2 EMP      BOL        58817.   3418. 1.43e4   326. 7.49e3 1.72e4 7.04e3 2.72e3 NA      2.41e4 1.35e5 # 3 EMP      BRA      1065864.  12773. 4.33e5 22604. 2.19e5 5.28e5 1.27e5 2.74e5  3.29e5 3.54e5 3.36e6 # 4 EMP      BWA         8839.    493. 8.49e2   145. 1.19e3 1.71e3 3.93e2 7.21e2  2.87e3 1.30e3 1.85e4 # 5 EMP      CHL        44220.   6389. 3.94e4  1850. 1.86e4 4.38e4 1.63e4 1.72e4 NA      6.32e4 2.51e5 # 6 EMP      CHN     17264654. 422972. 4.03e6 96364. 1.25e6 1.73e6 8.36e5 2.96e5  1.36e6 1.86e6 2.91e7  # Get transformed (scaled) data head(TRA(gGGDC, gsumGGDC, \"/\")) # # A tibble: 6 × 16 #   Country Regioncode Region     Variable  Year      AGR      MIN      MAN       PU      CON      WRT #   <chr>   <chr>      <chr>      <chr>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl> # 1 BWA     SSA        Sub-sahar… VA        1960 NA       NA       NA       NA       NA       NA       # 2 BWA     SSA        Sub-sahar… VA        1961 NA       NA       NA       NA       NA       NA       # 3 BWA     SSA        Sub-sahar… VA        1962 NA       NA       NA       NA       NA       NA       # 4 BWA     SSA        Sub-sahar… VA        1963 NA       NA       NA       NA       NA       NA       # 5 BWA     SSA        Sub-sahar… VA        1964  7.50e-4  1.65e-5  1.66e-5  1.03e-5  1.57e-5  6.82e-5 # 6 BWA     SSA        Sub-sahar… VA        1965  7.24e-4  1.18e-5  2.30e-5  1.33e-5  3.20e-5  7.72e-5 # # ℹ 5 more variables: TRA <dbl>, FIRE <dbl>, GOV <dbl>, OTH <dbl>, SUM <dbl>"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"faster-centering-averaging-and-standardizing","dir":"Articles","previous_headings":"2. Fast Transformations","what":"2.4 Faster Centering, Averaging and Standardizing","title":"collapse and dplyr","text":"functions fbetween fwithin slightly memory efficient implementations fmean invoked different TRA options: Apart higher speed, fwithin mean argument assign arbitrary mean centered data, default mean = 0. common choice added mean just overall mean data, can added invoking mean = \"overall.mean\": can also done using weights. code uses SUM column weights, variable group subtracts weighted mean, adds overall weighted column mean back centered columns. SUM column just kept added grouping columns. Another argument fwithin theta parameter, allowing partial- quasi-demeaning operations, e.g. fwithin(gdata, theta = theta) equal gdata - theta * fbetween(gdata). particularly useful prepare data variance components (also known ‘random-effects’) estimation. Apart fbetween fwithin, function fscale exists efficiently scale center data, avoid sequential calls ... %>% fsd(TRA = \"/\") %>% fmean(TRA = \"-\"). fscale also additional mean sd arguments allowing user (group-) scale data arbitrary mean standard deviation. Setting mean = FALSE just scales data preserves means, thus different fsd(..., TRA = \"/\") simply divides values standard deviation: One can also set mean = \"overall.mean\", group-centers columns overall mean illustrated fwithin. Another interesting option setting sd = \"within.sd\". group-scales data every group standard deviation equal within-standard deviation data: grouped scaling operation mean = \"overall.mean\" sd = \"within.sd\" thus efficiently achieves harmonization groups first two moments without changing fundamental properties (terms level scale) data.","code":"GGDC10S %>% # Same as ... %>% fmean(TRA = \"replace\")   fgroup_by(Variable, Country) %>% get_vars(6:16) %>% fbetween %>% tail(2) # # A tibble: 2 × 11 #     AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH    SUM #   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> # 1 4444.  34.9 1614.  131.  997. 1307.  799.  320. 2958.    NA 12605. # 2 4444.  34.9 1614.  131.  997. 1307.  799.  320. 2958.    NA 12605.  GGDC10S %>% # Same as ... %>% fmean(TRA = \"replace_fill\")   fgroup_by(Variable, Country) %>% get_vars(6:16) %>% fbetween(fill = TRUE) %>% tail(2) # # A tibble: 2 × 11 #     AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH    SUM #   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> # 1 4444.  34.9 1614.  131.  997. 1307.  799.  320. 2958.    NA 12605. # 2 4444.  34.9 1614.  131.  997. 1307.  799.  320. 2958.    NA 12605.  GGDC10S %>% # Same as ... %>% fmean(TRA = \"-\")   fgroup_by(Variable, Country) %>% get_vars(6:16) %>% fwithin %>% tail(2) # # A tibble: 2 × 11 #     AGR    MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH   SUM #   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1  742.  -7.35  760.  187. 1798. 1713. 1249.  495. 2678.    NA 9614. # 2  717. -10.1   734.  194. 1934. 1803. 1266.  512. 2778.    NA 9928. GGDC10S %>%    fgroup_by(Variable, Country) %>%      fselect(Country, Variable, AGR:SUM) %>% fwithin(mean = \"overall.mean\") %>% tail(3) # # A tibble: 3 × 13 #   Country Variable      AGR      MIN      MAN     PU    CON    WRT    TRA   FIRE    GOV   OTH    SUM #   <chr>   <chr>       <dbl>    <dbl>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl>  <dbl> # 1 EGY     EMP      2527458. 1867903. 5539313. 3.36e5 1.80e6 3.39e6 1.47e6 1.66e6 1.71e6    NA 2.16e7 # 2 EGY     EMP      2527439. 1867902. 5539251. 3.36e5 1.80e6 3.39e6 1.47e6 1.66e6 1.71e6    NA 2.16e7 # 3 EGY     EMP      2527413. 1867899. 5539226. 3.36e5 1.80e6 3.39e6 1.47e6 1.66e6 1.72e6    NA 2.16e7 GGDC10S %>%    fgroup_by(Variable, Country) %>%      fselect(Country, Variable, AGR:SUM) %>% fwithin(SUM, mean = \"overall.mean\") %>% tail(3) # # A tibble: 3 × 13 #   Country Variable    SUM        AGR      MIN    MAN     PU    CON    WRT    TRA   FIRE    GOV   OTH #   <chr>   <chr>     <dbl>      <dbl>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> # 1 EGY     EMP      22020. 429066006.   3.70e8 7.38e8 2.73e7 2.83e8 4.33e8 1.97e8 1.55e8 2.10e8    NA # 2 EGY     EMP      22219. 429065986.   3.70e8 7.38e8 2.73e7 2.83e8 4.33e8 1.97e8 1.55e8 2.10e8    NA # 3 EGY     EMP      22533. 429065961.   3.70e8 7.38e8 2.73e7 2.83e8 4.33e8 1.97e8 1.55e8 2.10e8    NA # This efficiently scales and centers (i.e. standardizes) the data GGDC10S %>%   fgroup_by(Variable, Country) %>%     fselect(Country, Variable, AGR:SUM) %>% fscale # # A tibble: 5,027 × 13 #    Country Variable    AGR    MIN    MAN     PU    CON    WRT    TRA   FIRE    GOV    OTH    SUM #  * <chr>   <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> #  1 BWA     VA       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     #  2 BWA     VA       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     #  3 BWA     VA       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     #  4 BWA     VA       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     #  5 BWA     VA       -0.738 -0.717 -0.668 -0.805 -0.692 -0.603 -0.589 -0.635 -0.656 -0.596 -0.676 #  6 BWA     VA       -0.739 -0.717 -0.668 -0.805 -0.692 -0.603 -0.589 -0.635 -0.656 -0.596 -0.676 #  7 BWA     VA       -0.736 -0.717 -0.668 -0.805 -0.692 -0.603 -0.589 -0.635 -0.656 -0.595 -0.676 #  8 BWA     VA       -0.734 -0.717 -0.668 -0.805 -0.692 -0.604 -0.589 -0.635 -0.655 -0.595 -0.676 #  9 BWA     VA       -0.730 -0.717 -0.668 -0.805 -0.692 -0.604 -0.588 -0.635 -0.656 -0.596 -0.676 # 10 BWA     VA       -0.729 -0.716 -0.667 -0.803 -0.690 -0.603 -0.588 -0.635 -0.656 -0.596 -0.675 # # ℹ 5,017 more rows #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] # Saving grouped tibble gGGDC <- GGDC10S %>%   fgroup_by(Variable, Country) %>%     fselect(Country, Variable, AGR:SUM)  # Original means head(fmean(gGGDC))  # # A tibble: 6 × 13 #   Variable Country     AGR    MIN     MAN      PU     CON    WRT    TRA   FIRE     GOV    OTH    SUM #   <chr>    <chr>     <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> # 1 EMP      ARG       1420.   52.1  1932.   102.     742.  1.98e3 6.49e2  628.   2043.  9.92e2 1.05e4 # 2 EMP      BOL        964.   56.0   235.     5.35   123.  2.82e2 1.15e2   44.6    NA   3.96e2 2.22e3 # 3 EMP      BRA      17191.  206.   6991.   365.    3525.  8.51e3 2.05e3 4414.   5307.  5.71e3 5.43e4 # 4 EMP      BWA        188.   10.5    18.1    3.09    25.3 3.63e1 8.36e0   15.3    61.1 2.76e1 3.94e2 # 5 EMP      CHL        702.  101.    625.    29.4    296.  6.95e2 2.58e2  272.     NA   1.00e3 3.98e3 # 6 EMP      CHN     287744. 7050.  67144.  1606.   20852.  2.89e4 1.39e4 4929.  22669.  3.10e4 4.86e5  # Mean Preserving Scaling head(fmean(fscale(gGGDC, mean = FALSE))) # # A tibble: 6 × 13 #   Variable Country     AGR    MIN     MAN      PU     CON    WRT    TRA   FIRE     GOV    OTH    SUM #   <chr>    <chr>     <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> # 1 EMP      ARG       1420.   52.1  1932.   102.     742.  1.98e3 6.49e2  628.   2043.  9.92e2 1.05e4 # 2 EMP      BOL        964.   56.0   235.     5.35   123.  2.82e2 1.15e2   44.6    NA   3.96e2 2.22e3 # 3 EMP      BRA      17191.  206.   6991.   365.    3525.  8.51e3 2.05e3 4414.   5307.  5.71e3 5.43e4 # 4 EMP      BWA        188.   10.5    18.1    3.09    25.3 3.63e1 8.36e0   15.3    61.1 2.76e1 3.94e2 # 5 EMP      CHL        702.  101.    625.    29.4    296.  6.95e2 2.58e2  272.     NA   1.00e3 3.98e3 # 6 EMP      CHN     287744. 7050.  67144.  1606.   20852.  2.89e4 1.39e4 4929.  22669.  3.10e4 4.86e5 head(fsd(fscale(gGGDC, mean = FALSE))) # # A tibble: 6 × 13 #   Variable Country   AGR   MIN   MAN    PU   CON   WRT   TRA  FIRE   GOV   OTH   SUM #   <chr>    <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> # 1 EMP      ARG      1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 # 2 EMP      BOL      1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 NA     1.00  1.00 # 3 EMP      BRA      1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 # 4 EMP      BWA      1.00  1.00  1.00  1     1.00  1.00  1.00  1     1.00  1.00  1.00 # 5 EMP      CHL      1.00  1     1.00  1.00  1.00  1.00  1.00  1.00 NA     1.00  1.00 # 6 EMP      CHN      1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 # Just using VA data for this example gGGDC <- GGDC10S %>%   fsubset(Variable == \"VA\", Country, AGR:SUM) %>%        fgroup_by(Country)  # This calculates the within- standard deviation for all columns fsd(num_vars(ungroup(fwithin(gGGDC)))) #       AGR       MIN       MAN        PU       CON       WRT       TRA      FIRE       GOV       OTH  #  45046972  40122220  75608708   3062688  30811572  44125207  20676901  16030868  20358973  18780869  #       SUM  # 306429102  # This scales all groups to take on the within- standard deviation while preserving group means  fsd(fscale(gGGDC, mean = FALSE, sd = \"within.sd\")) # # A tibble: 43 × 12 #    Country       AGR       MIN       MAN       PU     CON    WRT    TRA   FIRE     GOV    OTH    SUM #    <chr>       <dbl>     <dbl>     <dbl>    <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> #  1 ARG     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 #  2 BOL     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7 NA      1.88e7 3.06e8 #  3 BRA     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 #  4 BWA     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 #  5 CHL     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7 NA      1.88e7 3.06e8 #  6 CHN     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 #  7 COL     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7 NA      1.88e7 3.06e8 #  8 CRI     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 #  9 DEW     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 # 10 DNK     45046972. 40122220. 75608708. 3062688.  3.08e7 4.41e7 2.07e7 1.60e7  2.04e7 1.88e7 3.06e8 # # ℹ 33 more rows"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"lags-leads-differences-and-growth-rates","dir":"Articles","previous_headings":"2. Fast Transformations","what":"2.5 Lags / Leads, Differences and Growth Rates","title":"collapse and dplyr","text":"section introduces 3 powerful collapse functions: flag, fdiff fgrowth. first function, flag, efficiently computes sequences fully identified lags leads time series panel data. following code computes 1 fully-identified panel-lag 1 fully identified panel-lead variable data: time-variable passed exactly identify data (.e. repeated values group), 3 functions issue appropriate error messages. flag, fdiff fgrowth support irregular time series unbalanced panels. also possible omit time-variable one certain data sorted: fdiff computes sequences lagged-leaded iterated differences well quasi-differences log-differences time series panel data. code computes 1 10 year first second differences variable data: Log-differences form log(x_t) - log(x_{t-s}) also easily computed. Finally, also possible compute quasi-differences quasi-log-differences form x_t - \\rho x_{t-s} log(x_t) - \\rho log(x_{t-s}): quasi-differencing feature added fdiff facilitate preparation time series panel data least-squares estimations suffering serial correlation following Cochrane & Orcutt (1949). Finally, fgrowth computes growth rates way. default exact growth rates computed percentage terms using (x_t-x_{t-s}) / x_{t-s} \\times 100 (default argument scale = 100). user can also request growth rates obtained log-differencing using log(x_t/ x_{t-s}) \\times 100. fdiff fgrowth can also perform leaded (forward) differences growth rates (.e. ... %>% fgrowth(-c(1, 10), 1:2, Year) compute one 10-year leaded first second differences). possible perform sequential operations:","code":"GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% flag(-1:1, Year) # # A tibble: 5,027 × 36 #    Country Variable  Year F1.AGR   AGR L1.AGR F1.MIN   MIN L1.MIN F1.MAN    MAN L1.MAN  F1.PU     PU #  * <chr>   <chr>    <dbl>  <dbl> <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> #  1 BWA     VA        1960   NA    NA     NA    NA    NA     NA    NA     NA     NA     NA     NA     #  2 BWA     VA        1961   NA    NA     NA    NA    NA     NA    NA     NA     NA     NA     NA     #  3 BWA     VA        1962   NA    NA     NA    NA    NA     NA    NA     NA     NA     NA     NA     #  4 BWA     VA        1963   16.3  NA     NA     3.49 NA     NA     0.737 NA     NA      0.104 NA     #  5 BWA     VA        1964   15.7  16.3   NA     2.50  3.49  NA     1.02   0.737 NA      0.135  0.104 #  6 BWA     VA        1965   17.7  15.7   16.3   1.97  2.50   3.49  0.804  1.02   0.737  0.203  0.135 #  7 BWA     VA        1966   19.1  17.7   15.7   2.30  1.97   2.50  0.938  0.804  1.02   0.203  0.203 #  8 BWA     VA        1967   21.1  19.1   17.7   1.84  2.30   1.97  0.750  0.938  0.804  0.203  0.203 #  9 BWA     VA        1968   21.9  21.1   19.1   5.24  1.84   2.30  2.14   0.750  0.938  0.578  0.203 # 10 BWA     VA        1969   23.1  21.9   21.1  10.2   5.24   1.84  4.15   2.14   0.750  1.12   0.578 # # ℹ 5,017 more rows # # ℹ 22 more variables: L1.PU <dbl>, F1.CON <dbl>, CON <dbl>, L1.CON <dbl>, F1.WRT <dbl>, WRT <dbl>, # #   L1.WRT <dbl>, F1.TRA <dbl>, TRA <dbl>, L1.TRA <dbl>, F1.FIRE <dbl>, FIRE <dbl>, L1.FIRE <dbl>, # #   F1.GOV <dbl>, GOV <dbl>, L1.GOV <dbl>, F1.OTH <dbl>, OTH <dbl>, L1.OTH <dbl>, F1.SUM <dbl>, # #   SUM <dbl>, L1.SUM <dbl> #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] GGDC10S %>%   fselect(Variable, Country,AGR:SUM) %>%      fgroup_by(Variable, Country) %>% flag # # A tibble: 5,027 × 13 #    Variable Country   AGR   MIN    MAN     PU    CON   WRT   TRA  FIRE   GOV   OTH   SUM #  * <chr>    <chr>   <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #  1 VA       BWA      NA   NA    NA     NA     NA     NA    NA    NA    NA    NA     NA   #  2 VA       BWA      NA   NA    NA     NA     NA     NA    NA    NA    NA    NA     NA   #  3 VA       BWA      NA   NA    NA     NA     NA     NA    NA    NA    NA    NA     NA   #  4 VA       BWA      NA   NA    NA     NA     NA     NA    NA    NA    NA    NA     NA   #  5 VA       BWA      NA   NA    NA     NA     NA     NA    NA    NA    NA    NA     NA   #  6 VA       BWA      16.3  3.49  0.737  0.104  0.660  6.24  1.66  1.12  4.82  2.34  37.5 #  7 VA       BWA      15.7  2.50  1.02   0.135  1.35   7.06  1.94  1.25  5.70  2.68  39.3 #  8 VA       BWA      17.7  1.97  0.804  0.203  1.35   8.27  2.15  1.36  6.37  2.99  43.1 #  9 VA       BWA      19.1  2.30  0.938  0.203  0.897  4.31  1.72  1.54  7.04  3.31  41.4 # 10 VA       BWA      21.1  1.84  0.750  0.203  1.22   5.17  2.44  1.03  5.03  2.36  41.1 # # ℹ 5,017 more rows #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% fdiff(c(1, 10), 1:2, Year) # # A tibble: 5,027 × 47 #    Country Variable  Year D1.AGR D2.AGR L10D1.AGR L10D2.AGR D1.MIN D2.MIN L10D1.MIN L10D2.MIN D1.MAN #  * <chr>   <chr>    <dbl>  <dbl>  <dbl>     <dbl>     <dbl>  <dbl>  <dbl>     <dbl>     <dbl>  <dbl> #  1 BWA     VA        1960 NA     NA            NA        NA NA     NA            NA        NA NA     #  2 BWA     VA        1961 NA     NA            NA        NA NA     NA            NA        NA NA     #  3 BWA     VA        1962 NA     NA            NA        NA NA     NA            NA        NA NA     #  4 BWA     VA        1963 NA     NA            NA        NA NA     NA            NA        NA NA     #  5 BWA     VA        1964 NA     NA            NA        NA NA     NA            NA        NA NA     #  6 BWA     VA        1965 -0.575 NA            NA        NA -0.998 NA            NA        NA  0.282 #  7 BWA     VA        1966  1.95   2.53         NA        NA -0.525  0.473        NA        NA -0.214 #  8 BWA     VA        1967  1.47  -0.488        NA        NA  0.328  0.854        NA        NA  0.134 #  9 BWA     VA        1968  1.95   0.488        NA        NA -0.460 -0.788        NA        NA -0.188 # 10 BWA     VA        1969  0.763 -1.19         NA        NA  3.41   3.87         NA        NA  1.39  # # ℹ 5,017 more rows # # ℹ 35 more variables: D2.MAN <dbl>, L10D1.MAN <dbl>, L10D2.MAN <dbl>, D1.PU <dbl>, D2.PU <dbl>, # #   L10D1.PU <dbl>, L10D2.PU <dbl>, D1.CON <dbl>, D2.CON <dbl>, L10D1.CON <dbl>, L10D2.CON <dbl>, # #   D1.WRT <dbl>, D2.WRT <dbl>, L10D1.WRT <dbl>, L10D2.WRT <dbl>, D1.TRA <dbl>, D2.TRA <dbl>, # #   L10D1.TRA <dbl>, L10D2.TRA <dbl>, D1.FIRE <dbl>, D2.FIRE <dbl>, L10D1.FIRE <dbl>, # #   L10D2.FIRE <dbl>, D1.GOV <dbl>, D2.GOV <dbl>, L10D1.GOV <dbl>, L10D2.GOV <dbl>, D1.OTH <dbl>, # #   D2.OTH <dbl>, L10D1.OTH <dbl>, L10D2.OTH <dbl>, D1.SUM <dbl>, D2.SUM <dbl>, L10D1.SUM <dbl>, … #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% fdiff(c(1, 10), 1, Year, log = TRUE) # # A tibble: 5,027 × 25 #    Country Variable  Year Dlog1.AGR L10Dlog1.AGR Dlog1.MIN L10Dlog1.MIN Dlog1.MAN L10Dlog1.MAN #  * <chr>   <chr>    <dbl>     <dbl>        <dbl>     <dbl>        <dbl>     <dbl>        <dbl> #  1 BWA     VA        1960   NA                NA    NA               NA    NA               NA #  2 BWA     VA        1961   NA                NA    NA               NA    NA               NA #  3 BWA     VA        1962   NA                NA    NA               NA    NA               NA #  4 BWA     VA        1963   NA                NA    NA               NA    NA               NA #  5 BWA     VA        1964   NA                NA    NA               NA    NA               NA #  6 BWA     VA        1965   -0.0359           NA    -0.336           NA     0.324           NA #  7 BWA     VA        1966    0.117            NA    -0.236           NA    -0.236           NA #  8 BWA     VA        1967    0.0796           NA     0.154           NA     0.154           NA #  9 BWA     VA        1968    0.0972           NA    -0.223           NA    -0.223           NA # 10 BWA     VA        1969    0.0355           NA     1.05            NA     1.05            NA # # ℹ 5,017 more rows # # ℹ 16 more variables: Dlog1.PU <dbl>, L10Dlog1.PU <dbl>, Dlog1.CON <dbl>, L10Dlog1.CON <dbl>, # #   Dlog1.WRT <dbl>, L10Dlog1.WRT <dbl>, Dlog1.TRA <dbl>, L10Dlog1.TRA <dbl>, Dlog1.FIRE <dbl>, # #   L10Dlog1.FIRE <dbl>, Dlog1.GOV <dbl>, L10Dlog1.GOV <dbl>, Dlog1.OTH <dbl>, L10Dlog1.OTH <dbl>, # #   Dlog1.SUM <dbl>, L10Dlog1.SUM <dbl> #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% fdiff(t = Year, rho = 0.95) # # A tibble: 5,027 × 14 #    Country Variable  Year    AGR    MIN    MAN      PU     CON    WRT    TRA   FIRE    GOV    OTH #  * <chr>   <chr>    <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> #  1 BWA     VA        1960 NA     NA     NA     NA      NA      NA     NA     NA     NA     NA     #  2 BWA     VA        1961 NA     NA     NA     NA      NA      NA     NA     NA     NA     NA     #  3 BWA     VA        1962 NA     NA     NA     NA      NA      NA     NA     NA     NA     NA     #  4 BWA     VA        1963 NA     NA     NA     NA      NA      NA     NA     NA     NA     NA     #  5 BWA     VA        1964 NA     NA     NA     NA      NA      NA     NA     NA     NA     NA     #  6 BWA     VA        1965  0.241 -0.824  0.318  0.0359  0.719   1.13   0.363  0.184  1.11   0.454 #  7 BWA     VA        1966  2.74  -0.401 -0.163  0.0743  0.0673  1.56   0.312  0.174  0.955  0.449 #  8 BWA     VA        1967  2.35   0.427  0.174  0.0101 -0.381  -3.55  -0.323  0.246  0.988  0.465 #  9 BWA     VA        1968  2.91  -0.345 -0.141  0.0101  0.365   1.08   0.804 -0.427 -1.66  -0.780 # 10 BWA     VA        1969  1.82   3.50   1.43   0.385   2.32    0.841  0.397  0.252  0.818  0.385 # # ℹ 5,017 more rows # # ℹ 1 more variable: SUM <dbl> #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] # Exact growth rates, computed as: (x/lag(x) - 1) * 100 GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% fgrowth(c(1, 10), 1, Year) # # A tibble: 5,027 × 25 #    Country Variable  Year G1.AGR L10G1.AGR G1.MIN L10G1.MIN G1.MAN L10G1.MAN G1.PU L10G1.PU G1.CON #  * <chr>   <chr>    <dbl>  <dbl>     <dbl>  <dbl>     <dbl>  <dbl>     <dbl> <dbl>    <dbl>  <dbl> #  1 BWA     VA        1960  NA           NA   NA          NA   NA          NA  NA         NA   NA   #  2 BWA     VA        1961  NA           NA   NA          NA   NA          NA  NA         NA   NA   #  3 BWA     VA        1962  NA           NA   NA          NA   NA          NA  NA         NA   NA   #  4 BWA     VA        1963  NA           NA   NA          NA   NA          NA  NA         NA   NA   #  5 BWA     VA        1964  NA           NA   NA          NA   NA          NA  NA         NA   NA   #  6 BWA     VA        1965  -3.52        NA  -28.6        NA   38.2        NA  29.4       NA  104.  #  7 BWA     VA        1966  12.4         NA  -21.1        NA  -21.1        NA  50         NA    0   #  8 BWA     VA        1967   8.29        NA   16.7        NA   16.7        NA   0         NA  -33.3 #  9 BWA     VA        1968  10.2         NA  -20          NA  -20          NA   0         NA   35.7 # 10 BWA     VA        1969   3.61        NA  185.         NA  185.         NA 185.        NA  185.  # # ℹ 5,017 more rows # # ℹ 13 more variables: L10G1.CON <dbl>, G1.WRT <dbl>, L10G1.WRT <dbl>, G1.TRA <dbl>, # #   L10G1.TRA <dbl>, G1.FIRE <dbl>, L10G1.FIRE <dbl>, G1.GOV <dbl>, L10G1.GOV <dbl>, G1.OTH <dbl>, # #   L10G1.OTH <dbl>, G1.SUM <dbl>, L10G1.SUM <dbl> #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65]  # Log-difference growth rates, computed as: log(x / lag(x)) * 100 GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% fgrowth(c(1, 10), 1, Year, logdiff = TRUE) # # A tibble: 5,027 × 25 #    Country Variable  Year Dlog1.AGR L10Dlog1.AGR Dlog1.MIN L10Dlog1.MIN Dlog1.MAN L10Dlog1.MAN #  * <chr>   <chr>    <dbl>     <dbl>        <dbl>     <dbl>        <dbl>     <dbl>        <dbl> #  1 BWA     VA        1960     NA              NA      NA             NA      NA             NA #  2 BWA     VA        1961     NA              NA      NA             NA      NA             NA #  3 BWA     VA        1962     NA              NA      NA             NA      NA             NA #  4 BWA     VA        1963     NA              NA      NA             NA      NA             NA #  5 BWA     VA        1964     NA              NA      NA             NA      NA             NA #  6 BWA     VA        1965     -3.59           NA     -33.6           NA      32.4           NA #  7 BWA     VA        1966     11.7            NA     -23.6           NA     -23.6           NA #  8 BWA     VA        1967      7.96           NA      15.4           NA      15.4           NA #  9 BWA     VA        1968      9.72           NA     -22.3           NA     -22.3           NA # 10 BWA     VA        1969      3.55           NA     105.            NA     105.            NA # # ℹ 5,017 more rows # # ℹ 16 more variables: Dlog1.PU <dbl>, L10Dlog1.PU <dbl>, Dlog1.CON <dbl>, L10Dlog1.CON <dbl>, # #   Dlog1.WRT <dbl>, L10Dlog1.WRT <dbl>, Dlog1.TRA <dbl>, L10Dlog1.TRA <dbl>, Dlog1.FIRE <dbl>, # #   L10Dlog1.FIRE <dbl>, Dlog1.GOV <dbl>, L10Dlog1.GOV <dbl>, Dlog1.OTH <dbl>, L10Dlog1.OTH <dbl>, # #   Dlog1.SUM <dbl>, L10Dlog1.SUM <dbl> #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65] # This computes the 1 and 10-year growth rates, for the current period and lagged by one period GGDC10S %>%   fselect(-Region, -Regioncode) %>%      fgroup_by(Variable, Country) %>% fgrowth(c(1, 10), 1, Year) %>% flag(0:1, Year) # # A tibble: 5,027 × 47 #    Country Variable  Year G1.AGR L1.G1.AGR L10G1.AGR L1.L10G1.AGR G1.MIN L1.G1.MIN L10G1.MIN #  * <chr>   <chr>    <dbl>  <dbl>     <dbl>     <dbl>        <dbl>  <dbl>     <dbl>     <dbl> #  1 BWA     VA        1960  NA        NA           NA           NA   NA        NA          NA #  2 BWA     VA        1961  NA        NA           NA           NA   NA        NA          NA #  3 BWA     VA        1962  NA        NA           NA           NA   NA        NA          NA #  4 BWA     VA        1963  NA        NA           NA           NA   NA        NA          NA #  5 BWA     VA        1964  NA        NA           NA           NA   NA        NA          NA #  6 BWA     VA        1965  -3.52     NA           NA           NA  -28.6      NA          NA #  7 BWA     VA        1966  12.4      -3.52        NA           NA  -21.1     -28.6        NA #  8 BWA     VA        1967   8.29     12.4         NA           NA   16.7     -21.1        NA #  9 BWA     VA        1968  10.2       8.29        NA           NA  -20        16.7        NA # 10 BWA     VA        1969   3.61     10.2         NA           NA  185.      -20          NA # # ℹ 5,017 more rows # # ℹ 37 more variables: L1.L10G1.MIN <dbl>, G1.MAN <dbl>, L1.G1.MAN <dbl>, L10G1.MAN <dbl>, # #   L1.L10G1.MAN <dbl>, G1.PU <dbl>, L1.G1.PU <dbl>, L10G1.PU <dbl>, L1.L10G1.PU <dbl>, # #   G1.CON <dbl>, L1.G1.CON <dbl>, L10G1.CON <dbl>, L1.L10G1.CON <dbl>, G1.WRT <dbl>, # #   L1.G1.WRT <dbl>, L10G1.WRT <dbl>, L1.L10G1.WRT <dbl>, G1.TRA <dbl>, L1.G1.TRA <dbl>, # #   L10G1.TRA <dbl>, L1.L10G1.TRA <dbl>, G1.FIRE <dbl>, L1.G1.FIRE <dbl>, L10G1.FIRE <dbl>, # #   L1.L10G1.FIRE <dbl>, G1.GOV <dbl>, L1.G1.GOV <dbl>, L10G1.GOV <dbl>, L1.L10G1.GOV <dbl>, … #  # Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65]"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"benchmarks","dir":"Articles","previous_headings":"","what":"3. Benchmarks","title":"collapse and dplyr","text":"section seeks demonstrate functionality introduced preceding 2 sections indeed produces code evaluates substantially faster native dplyr. properly, different components typical piped call (selecting / subsetting, ordering, grouping, performing computation) benchmarked separately 2 different data sizes. benchmarks run Windows 8.1 laptop 2x 2.2 GHZ Intel i5 processor, 8GB DDR3 RAM Samsung 850 EVO SSD hard drive.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"data","dir":"Articles","previous_headings":"3. Benchmarks","what":"3.1 Data","title":"collapse and dplyr","text":"Benchmarks run original GGDC10S data used throughout vignette larger dataset approx. 1 million observations, obtained replicating row-binding GGDC10S 200 times maintaining unique groups.","code":"# This shows the groups in GGDC10S GRP(GGDC10S, ~ Variable + Country) # collapse grouping object of length 5027 with 85 ordered groups #  # Call: GRP.default(X = GGDC10S, by = ~Variable + Country), X is unsorted #  # Distribution of group sizes:  #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #    4.00   53.00   62.00   59.14   63.00   65.00  #  # Groups with sizes:  # EMP.ARG EMP.BOL EMP.BRA EMP.BWA EMP.CHL EMP.CHN  #      62      61      62      52      63      62  #   --- # VA.TWN VA.TZA VA.USA VA.VEN VA.ZAF VA.ZMB  #     63     52     65     63     52     52  # This replicates the data 200 times  data <- replicate(200, GGDC10S, simplify = FALSE)  # This function adds a number i to the country and variable columns of each dataset uniquify <- function(x, i) ftransform(x, lapply(unclass(x)[c(1,4)], paste0, i)) # Making datasets unique and row-binding them data <- unlist2d(Map(uniquify, data, as.list(1:200)), idcols = FALSE) fdim(data) # [1] 1005400      16  # This shows the groups in the replicated data GRP(data, ~ Variable + Country) # collapse grouping object of length 1005400 with 17000 ordered groups #  # Call: GRP.default(X = data, by = ~Variable + Country), X is unsorted #  # Distribution of group sizes:  #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #    4.00   53.00   62.00   59.14   63.00   65.00  #  # Groups with sizes:  # EMP1.ARG1 EMP1.BOL1 EMP1.BRA1 EMP1.BWA1 EMP1.CHL1 EMP1.CHN1  #        62        61        62        52        63        62  #   --- # VA99.TWN99 VA99.TZA99 VA99.USA99 VA99.VEN99 VA99.ZAF99 VA99.ZMB99  #         63         52         65         63         52         52  gc() #            used  (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb) # Ncells  3184710 170.1    8862174  473.3         NA   8862174  473.3 # Vcells 23965820 182.9  147787078 1127.6      16384 445825141 3401.4"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"selecting-subsetting-ordering-and-grouping","dir":"Articles","previous_headings":"3. Benchmarks","what":"3.1 Selecting, Subsetting, Ordering and Grouping","title":"collapse and dplyr","text":"","code":"## Selecting columns # Small microbenchmark(dplyr = select(GGDC10S, Country, Variable, AGR:SUM),                collapse = fselect(GGDC10S, Country, Variable, AGR:SUM)) # Unit: microseconds #      expr     min       lq      mean  median      uq     max neval #     dplyr 400.775 410.7585 425.43117 416.396 424.637 820.041   100 #  collapse   2.911   3.4645   4.59856   4.469   5.412  15.293   100  # Large microbenchmark(dplyr = select(data, Country, Variable, AGR:SUM),                collapse = fselect(data, Country, Variable, AGR:SUM)) # Unit: microseconds #      expr     min      lq      mean   median       uq     max neval #     dplyr 388.926 396.429 412.67730 402.9890 411.0455 728.734   100 #  collapse   2.870   3.280   4.44686   3.8335   5.3300  12.669   100  ## Subsetting columns  # Small microbenchmark(dplyr = filter(GGDC10S, Variable == \"VA\"),                collapse = fsubset(GGDC10S, Variable == \"VA\")) # Unit: microseconds #      expr     min       lq      mean   median       uq     max neval #     dplyr 374.084 394.4405 409.23986 401.0005 414.3050 716.475   100 #  collapse  39.278  48.2775  55.85307  55.5550  60.4545 103.320   100  # Large microbenchmark(dplyr = filter(data, Variable == \"VA\"),                collapse = fsubset(data, Variable == \"VA\")) # Unit: milliseconds #      expr      min       lq     mean   median       uq       max neval #     dplyr 4.487409 5.242752 8.352270 5.653223 6.434048 159.13658   100 #  collapse 2.840808 3.082359 3.469128 3.163478 3.302714  16.56047   100  ## Ordering rows # Small microbenchmark(dplyr = arrange(GGDC10S, desc(Country), Variable, Year),                collapse = roworder(GGDC10S, -Country, Variable, Year)) # Unit: microseconds #      expr      min        lq      mean   median        uq      max neval #     dplyr 1715.112 1867.4270 1983.4726 2015.109 2080.7500 2367.791   100 #  collapse  192.495  232.4085  256.3878  247.968  258.7715 1055.381   100  # Large microbenchmark(dplyr = arrange(data, desc(Country), Variable, Year),                collapse = roworder(data, -Country, Variable, Year), times = 2) # Unit: milliseconds #      expr      min       lq      mean    median        uq       max neval #     dplyr 89.37512 89.37512 101.05180 101.05180 112.72848 112.72848     2 #  collapse 66.46703 66.46703  67.45254  67.45254  68.43806  68.43806     2   ## Grouping  # Small microbenchmark(dplyr = group_by(GGDC10S, Country, Variable),                collapse = fgroup_by(GGDC10S, Country, Variable)) # Unit: microseconds #      expr     min       lq     mean   median       uq      max neval #     dplyr 778.713 815.1825 911.3484 874.2225 960.3840 1529.874   100 #  collapse 146.534 157.6245 198.5921 165.0660 177.3455 1484.241   100  # Large microbenchmark(dplyr = group_by(data, Country, Variable),                collapse = fgroup_by(data, Country, Variable), times = 10) # Unit: milliseconds #      expr      min       lq     mean   median       uq      max neval #     dplyr 34.20294 34.62839 34.88041 34.88432 35.07821 35.48279    10 #  collapse 27.89972 28.03211 28.55175 28.36954 29.32283 29.54206    10  ## Computing a new column  # Small microbenchmark(dplyr = mutate(GGDC10S, NEW = AGR+1),                collapse = ftransform(GGDC10S, NEW = AGR+1)) # Unit: microseconds #      expr     min       lq      mean   median       uq     max neval #     dplyr 317.463 321.7270 333.38822 324.9660 333.7810 631.564   100 #  collapse   8.897  11.0495  12.95354  12.4435  14.2065  38.991   100  # Large microbenchmark(dplyr = mutate(data, NEW = AGR+1),                collapse = ftransform(data, NEW = AGR+1)) # Unit: microseconds #      expr     min       lq     mean    median        uq      max neval #     dplyr 637.878 1084.225 1330.006 1164.6665 1291.2335 15869.05   100 #  collapse 210.740  657.025 1021.434  698.3735  781.7675 16725.09   100  ## All combined with pipes  # Small microbenchmark(dplyr = filter(GGDC10S, Variable == \"VA\") %>%                         select(Country, Year, AGR:SUM) %>%                         arrange(desc(Country), Year) %>%                        mutate(NEW = AGR+1) %>%                        group_by(Country),                collapse = fsubset(GGDC10S, Variable == \"VA\", Country, Year, AGR:SUM) %>%                         roworder(-Country, Year) %>%                        ftransform(NEW = AGR+1) %>%                        fgroup_by(Country)) # Unit: microseconds #      expr      min       lq      mean   median       uq      max neval #     dplyr 2982.340 3416.325 3525.7983 3538.464 3668.516 5034.021   100 #  collapse  136.858  186.632  214.4681  211.683  243.130  314.470   100  # Large microbenchmark(dplyr = filter(data, Variable == \"VA\") %>%                         select(Country, Year, AGR:SUM) %>%                         arrange(desc(Country), Year) %>%                        mutate(NEW = AGR+1) %>%                        group_by(Country),                collapse = fsubset(data, Variable == \"VA\", Country, Year, AGR:SUM) %>%                         roworder(-Country, Year) %>%                        ftransform(NEW = AGR+1) %>%                        fgroup_by(Country), times = 10) # Unit: milliseconds #      expr      min       lq     mean   median       uq      max neval #     dplyr 7.917182 7.997378 8.142653 8.109943 8.292291 8.423163    10 #  collapse 3.080289 3.104028 3.150153 3.140969 3.188365 3.251259    10  gc() #            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb) # Ncells  3184728 170.1    8862174 473.3         NA   8862174  473.3 # Vcells 23970594 182.9   75772825 578.2      16384 445825141 3401.4"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"aggregation","dir":"Articles","previous_headings":"3. Benchmarks","what":"3.1 Aggregation","title":"collapse and dplyr","text":"additional benchmarks weighted aggregations aggregations using statistical mode, easily efficiently performed dplyr.","code":"## Grouping the data cgGGDC10S <- fgroup_by(GGDC10S, Variable, Country) %>% fselect(-Region, -Regioncode) gGGDC10S <- group_by(GGDC10S, Variable, Country) %>% fselect(-Region, -Regioncode) cgdata <- fgroup_by(data, Variable, Country) %>% fselect(-Region, -Regioncode) gdata <- group_by(data, Variable, Country) %>% fselect(-Region, -Regioncode) rm(data, GGDC10S)  gc() #            used (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb) # Ncells  3201723  171    8862174 473.3         NA   8862174  473.3 # Vcells 23589381  180   75772825 578.2      16384 445825141 3401.4  ## Conversion of Grouping object: This time would be required extra in all hybrid calls  ## i.e. when calling collapse functions on data grouped with dplyr::group_by # Small microbenchmark(GRP(gGGDC10S)) # Unit: microseconds #           expr   min     lq     mean median     uq    max neval #  GRP(gGGDC10S) 8.692 9.2455 10.16021 9.4915 10.086 39.196   100  # Large microbenchmark(GRP(gdata)) # Unit: microseconds #        expr     min       lq     mean   median       uq      max neval #  GRP(gdata) 885.641 1160.915 1248.258 1237.236 1323.234 1651.398   100   ## Sum  # Small microbenchmark(dplyr = summarise_all(gGGDC10S, sum, na.rm = TRUE),                collapse = fsum(cgGGDC10S)) # Unit: microseconds #      expr      min        lq      mean    median       uq       max neval #     dplyr 3017.723 3354.1895 3733.4739 3620.9560 3738.441 22135.736   100 #  collapse  218.120  227.3655  236.7693  235.1965  244.852   270.805   100  # Large microbenchmark(dplyr = summarise_all(gdata, sum, na.rm = TRUE),                collapse = fsum(cgdata), times = 10) # Unit: milliseconds #      expr      min        lq      mean    median        uq       max neval #     dplyr 272.9737 279.91024 305.02067 283.59737 303.57122 448.07629    10 #  collapse  41.5330  41.63214  41.88717  41.77062  41.96059  42.78662    10  ## Mean # Small microbenchmark(dplyr = summarise_all(gGGDC10S, mean.default, na.rm = TRUE),                collapse = fmean(cgGGDC10S)) # Unit: microseconds #      expr      min        lq      mean   median       uq       max neval #     dplyr 4360.104 4596.6740 5125.4194 4754.791 5005.710 37144.852   100 #  collapse  169.084  174.3935  185.4594  183.434  194.832   221.933   100  # Large microbenchmark(dplyr = summarise_all(gdata, mean.default, na.rm = TRUE),                collapse = fmean(cgdata), times = 10) # Unit: milliseconds #      expr      min        lq      mean    median        uq       max neval #     dplyr 623.5123 642.83748 704.39836 681.32260 786.82731 829.74435    10 #  collapse  31.7636  31.88037  32.00222  31.99445  32.08209  32.43875    10  ## Median # Small microbenchmark(dplyr = summarise_all(gGGDC10S, median, na.rm = TRUE),                collapse = fmedian(cgGGDC10S)) # Unit: microseconds #      expr       min        lq       mean     median        uq       max neval #     dplyr 14399.118 14849.933 16170.3500 14982.5685 15145.892 33613.235   100 #  collapse   137.596   164.902   189.2056   178.1245   214.676   248.624   100  # Large microbenchmark(dplyr = summarise_all(gdata, median, na.rm = TRUE),                collapse = fmedian(cgdata), times = 2) # Unit: milliseconds #      expr        min         lq       mean     median         uq        max neval #     dplyr 2826.83036 2826.83036 2828.12912 2828.12912 2829.42788 2829.42788     2 #  collapse   19.95564   19.95564   19.98524   19.98524   20.01485   20.01485     2  ## Standard Deviation # Small microbenchmark(dplyr = summarise_all(gGGDC10S, sd, na.rm = TRUE),                collapse = fsd(cgGGDC10S)) # Unit: microseconds #      expr      min        lq      mean   median       uq       max neval #     dplyr 8332.635 8612.5215 9365.1216 8712.766 8989.086 25087.982   100 #  collapse  242.228  251.0225  269.7849  273.552  282.326   321.891   100  # Large microbenchmark(dplyr = summarise_all(gdata, sd, na.rm = TRUE),                collapse = fsd(cgdata), times = 2) # Unit: milliseconds #      expr        min         lq       mean     median         uq        max neval #     dplyr 1375.80363 1375.80363 1409.60358 1409.60358 1443.40352 1443.40352     2 #  collapse   46.21713   46.21713   56.88205   56.88205   67.54697   67.54697     2  ## Maximum # Small microbenchmark(dplyr = summarise_all(gGGDC10S, max, na.rm = TRUE),                collapse = fmax(cgGGDC10S)) # Unit: microseconds #      expr       min         lq        mean    median         uq       max neval #     dplyr 39964.504 41008.8560 43577.92707 41448.273 44195.1095 58816.550   100 #  collapse    68.798    74.7225    87.83389    77.572   100.9215   129.519   100  # Large microbenchmark(dplyr = summarise_all(gdata, max, na.rm = TRUE),                collapse = fmax(cgdata), times = 10) # Unit: milliseconds #      expr       min       lq     mean    median        uq       max neval #     dplyr 480.83804 490.9982 540.7374 517.86136 533.85723 687.14713    10 #  collapse  11.40116  11.7745  11.9366  11.85156  11.94908  13.18318    10  ## First Value # Small microbenchmark(dplyr = summarise_all(gGGDC10S, first),                collapse = ffirst(cgGGDC10S, na.rm = FALSE)) # Unit: microseconds #      expr      min       lq       mean   median       uq       max neval #     dplyr 4147.888 4242.249 4801.88966 4383.248 4701.532 19254.215   100 #  collapse   11.685   14.227   26.25476   24.764   35.301   137.514   100  # Large microbenchmark(dplyr = summarise_all(gdata, first),                collapse = ffirst(cgdata, na.rm = FALSE), times = 10) # Unit: microseconds #      expr       min         lq       mean    median         uq        max neval #     dplyr 530327.66 558767.393 637499.226 596503.08 672801.103 969373.660    10 #  collapse    872.89    999.088   1087.845   1068.87   1204.416   1289.327    10  ## Number of Distinct Values # Small microbenchmark(dplyr = summarise_all(gGGDC10S, n_distinct, na.rm = TRUE),                collapse = fndistinct(cgGGDC10S)) # Unit: microseconds #      expr       min        lq       mean    median        uq       max neval #     dplyr 11316.574 11600.847 12573.1010 11759.435 11939.487 31659.667   100 #  collapse   189.051   205.164   226.0933   235.422   239.604   443.661   100  # Large microbenchmark(dplyr = summarise_all(gdata, n_distinct, na.rm = TRUE),                collapse = fndistinct(cgdata), times = 5) # Unit: milliseconds #      expr        min         lq       mean     median         uq        max neval #     dplyr 2044.13376 2110.16926 2133.91960 2138.07456 2154.39797 2222.82246     5 #  collapse   30.65443   30.94582   31.51081   31.17123   31.17972   33.60286     5  gc() #            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb) # Ncells  3972309 212.2    8862174 473.3         NA   8862174  473.3 # Vcells 24857303 189.7   75772825 578.2      16384 445825141 3401.4 ## Weighted Mean # Small microbenchmark(fmean(cgGGDC10S, SUM))  # Unit: microseconds #                   expr     min       lq     mean   median       uq     max neval #  fmean(cgGGDC10S, SUM) 195.488 200.4285 218.2836 211.1295 218.8375 444.276   100  # Large  microbenchmark(fmean(cgdata, SUM), times = 10)  # Unit: milliseconds #                expr      min       lq     mean   median       uq      max neval #  fmean(cgdata, SUM) 34.73516 35.28276 35.66689 35.32257 36.44802 36.80722    10  ## Weighted Standard-Deviation # Small microbenchmark(fsd(cgGGDC10S, SUM))  # Unit: microseconds #                 expr     min      lq     mean   median      uq   max neval #  fsd(cgGGDC10S, SUM) 243.048 244.606 249.2181 246.9635 249.444 323.9   100  # Large  microbenchmark(fsd(cgdata, SUM), times = 10)  # Unit: milliseconds #              expr    min       lq     mean   median       uq      max neval #  fsd(cgdata, SUM) 44.905 44.93116 45.15391 45.01095 45.22677 46.14689    10  ## Statistical Mode # Small microbenchmark(fmode(cgGGDC10S))  # Unit: microseconds #              expr     min       lq     mean   median       uq     max neval #  fmode(cgGGDC10S) 245.098 248.3575 253.4809 250.6945 253.9335 420.619   100  # Large  microbenchmark(fmode(cgdata), times = 10)  # Unit: milliseconds #           expr      min       lq     mean   median      uq     max neval #  fmode(cgdata) 40.26151 41.82082 41.63019 41.88382 42.0232 42.0587    10  ## Weighted Statistical Mode # Small microbenchmark(fmode(cgGGDC10S, SUM))  # Unit: microseconds #                   expr     min      lq     mean   median       uq     max neval #  fmode(cgGGDC10S, SUM) 330.993 333.535 337.7744 334.5395 337.3685 447.187   100  # Large  microbenchmark(fmode(cgdata, SUM), times = 10)  # Unit: milliseconds #                expr      min       lq     mean   median       uq      max neval #  fmode(cgdata, SUM) 57.69815 57.78466 57.98187 57.84567 58.09942 58.81835    10  gc() #            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb) # Ncells  3971768 212.2    8862174 473.3         NA   8862174  473.3 # Vcells 24853915 189.7   75772825 578.2      16384 445825141 3401.4"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"transformation","dir":"Articles","previous_headings":"3. Benchmarks","what":"3.2 Transformation","title":"collapse and dplyr","text":"benchmarks transformations easily efficiently performed dplyr, centering overall mean, mean-preserving scaling, weighted scaling centering, sequences lags / leads, (iterated) panel-differences growth rates.","code":"## Replacing with group sum # Small microbenchmark(dplyr = mutate_all(gGGDC10S, sum, na.rm = TRUE),                collapse = fsum(cgGGDC10S, TRA = \"replace_fill\")) # Unit: microseconds #      expr       min        lq       mean     median       uq       max neval #     dplyr 13088.102 13223.340 14388.9000 13359.7680 14380.05 29060.554   100 #  collapse   238.456   273.757   292.1693   293.9905   312.01   388.106   100  # Large microbenchmark(dplyr = mutate_all(gdata, sum, na.rm = TRUE),                collapse = fsum(cgdata, TRA = \"replace_fill\"), times = 10) # Unit: milliseconds #      expr       min        lq      mean    median       uq      max neval #     dplyr 391.63618 679.62609 662.91807 716.40975 729.7527 749.4973    10 #  collapse  49.63788  50.24189  61.77658  55.18416  63.4596 111.6039    10  ## Dividing by group sum # Small microbenchmark(dplyr = mutate_all(gGGDC10S, function(x) x/sum(x, na.rm = TRUE)),                collapse = fsum(cgGGDC10S, TRA = \"/\")) # Unit: microseconds #      expr       min         lq       mean   median        uq       max neval #     dplyr 13058.992 13203.8450 14294.3733 13321.41 13880.796 42300.028   100 #  collapse   242.884   268.5295   278.8541   274.29   294.585   330.255   100  # Large microbenchmark(dplyr = mutate_all(gdata, function(x) x/sum(x, na.rm = TRUE)),                collapse = fsum(cgdata, TRA = \"/\"), times = 10) # Unit: milliseconds #      expr      min       lq      mean    median        uq      max neval #     dplyr 474.9046 654.6199 796.14248 907.32863 942.32567 999.2501    10 #  collapse  49.3542  50.9056  84.66647  52.05635  74.51705 325.4319    10  ## Centering # Small microbenchmark(dplyr = mutate_all(gGGDC10S, function(x) x-mean.default(x, na.rm = TRUE)),                collapse = fwithin(cgGGDC10S)) # Unit: microseconds #      expr      min         lq       mean    median        uq       max neval #     dplyr 14460.04 14769.4095 15977.4942 14859.815 15013.421 37113.077   100 #  collapse   203.77   229.7845   246.5043   242.638   266.664   293.191   100  # Large microbenchmark(dplyr = mutate_all(gdata, function(x) x-mean.default(x, na.rm = TRUE)),                collapse = fwithin(cgdata), times = 10) # Unit: milliseconds #      expr       min        lq      mean     median       uq       max neval #     dplyr 893.06503 925.50231 1217.2225 1259.34620 1445.254 1545.5490    10 #  collapse  43.90731  56.97093  143.4797   73.39498  152.872  429.3341    10  ## Centering and Scaling (Standardizing) # Small microbenchmark(dplyr = mutate_all(gGGDC10S, function(x) (x-mean.default(x, na.rm = TRUE))/sd(x, na.rm = TRUE)),                collapse = fscale(cgGGDC10S)) # Unit: microseconds #      expr       min        lq       mean    median         uq       max neval #     dplyr 20275.033 21145.524 24976.1242 22214.190 25194.0285 79869.435   100 #  collapse   277.775   304.958   323.3613   314.388   338.2705   437.388   100  # Large microbenchmark(dplyr = mutate_all(gdata, function(x) (x-mean.default(x, na.rm = TRUE))/sd(x, na.rm = TRUE)),                collapse = fscale(cgdata), times = 2) # Unit: milliseconds #      expr        min         lq      mean    median         uq        max neval #     dplyr 2118.97696 2118.97696 2315.9282 2315.9282 2512.87938 2512.87938     2 #  collapse   60.17144   60.17144   60.6284   60.6284   61.08537   61.08537     2  ## Lag # Small microbenchmark(dplyr_unordered = mutate(gGGDC10S, across(everything(), dplyr::lag)),                collapse_unordered = flag(cgGGDC10S),                dplyr_ordered = mutate(gGGDC10S, across(everything(), \\(x) dplyr::lag(x, order_by = Year))),                collapse_ordered = flag(cgGGDC10S, t = Year)) # Unit: microseconds #                expr       min        lq        mean     median         uq       max neval #     dplyr_unordered 14495.386 14796.101 17579.85413 15265.3250 15889.7550 49137.721   100 #  collapse_unordered    48.544    75.071    90.29225    86.6330   109.6545   225.377   100 #       dplyr_ordered 24893.437 25327.607 27521.59809 25904.9275 27136.2190 51312.074   100 #    collapse_ordered    80.196   107.953   120.85160   117.5675   131.6715   189.051   100  # Large microbenchmark(dplyr_unordered = mutate(gdata, across(everything(), dplyr::lag)),                collapse_unordered = flag(cgdata),                dplyr_ordered = mutate(gdata, across(everything(), \\(x) dplyr::lag(x, order_by = Year))),                collapse_ordered = flag(cgdata, t = Year), times = 2) # Unit: milliseconds #                expr        min         lq       mean     median         uq        max neval #     dplyr_unordered 3461.11500 3461.11500 3471.95821 3471.95821 3482.80142 3482.80142     2 #  collapse_unordered   13.71897   13.71897  211.59809  211.59809  409.47721  409.47721     2 #       dplyr_ordered 5786.57522 5786.57522 6291.90389 6291.90389 6797.23256 6797.23256     2 #    collapse_ordered   25.14399   25.14399   35.36102   35.36102   45.57806   45.57806     2  ## First-Difference (unordered) # Small microbenchmark(dplyr_unordered = mutate_all(gGGDC10S, function(x) x - dplyr::lag(x)),                collapse_unordered = fdiff(cgGGDC10S)) # Unit: microseconds #                expr       min         lq        mean     median        uq       max neval #     dplyr_unordered 25613.274 25878.0725 27951.41954 26257.3225 27226.808 43048.893   100 #  collapse_unordered    56.539    72.3035    95.72147    91.6965   102.664   254.077   100  # Large microbenchmark(dplyr_unordered = mutate_all(gdata, function(x) x - dplyr::lag(x)),                collapse_unordered = fdiff(cgdata), times = 2) # Unit: milliseconds #                expr        min         lq       mean     median       uq      max neval #     dplyr_unordered 3287.88487 3287.88487 3425.69703 3425.69703 3563.509 3563.509     2 #  collapse_unordered   16.58971   16.58971   23.36885   23.36885   30.148   30.148     2  gc() #            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb) # Ncells  3978800 212.5    8862175 473.3         NA   8862175  473.3 # Vcells 24870572 189.8   72805912 555.5      16384 445825141 3401.4 # Centering on overall mean microbenchmark(fwithin(cgdata, mean = \"overall.mean\"), times = 10) # Unit: milliseconds #                                    expr      min       lq     mean   median       uq      max neval #  fwithin(cgdata, mean = \"overall.mean\") 44.66782 48.03445 52.04073 50.07953 53.67134 71.13221    10  # Weighted Centering microbenchmark(fwithin(cgdata, SUM), times = 10) # Unit: milliseconds #                  expr      min       lq     mean   median       uq      max neval #  fwithin(cgdata, SUM) 40.45204 42.86833 46.55326 46.18277 47.28202 57.82673    10 microbenchmark(fwithin(cgdata, SUM, mean = \"overall.mean\"), times = 10) # Unit: milliseconds #                                         expr      min       lq    mean   median       uq      max #  fwithin(cgdata, SUM, mean = \"overall.mean\") 39.99279 40.32256 43.0638 40.60269 41.34366 54.45542 #  neval #     10  # Weighted Scaling and Standardizing microbenchmark(fsd(cgdata, SUM, TRA = \"/\"), times = 10) # Unit: milliseconds #                         expr      min      lq     mean   median       uq      max neval #  fsd(cgdata, SUM, TRA = \"/\") 50.19536 50.9145 55.12553 53.23862 56.27094 67.46816    10 microbenchmark(fscale(cgdata, SUM), times = 10) # Unit: milliseconds #                 expr      min       lq     mean   median       uq      max neval #  fscale(cgdata, SUM) 54.14792 57.64584 60.83251 59.88025 61.16425 72.31928    10  # Sequence of lags and leads microbenchmark(flag(cgdata, -1:1), times = 10) # Unit: milliseconds #                expr      min       lq     mean   median       uq      max neval #  flag(cgdata, -1:1) 26.03902 48.02695 194.8518 257.0652 264.5479 276.5348    10  # Iterated difference microbenchmark(fdiff(cgdata, 1, 2), times = 10) # Unit: milliseconds #                 expr      min       lq     mean   median       uq      max neval #  fdiff(cgdata, 1, 2) 38.76001 39.83896 44.93731 41.08887 48.98348 63.42528    10  # Growth Rate microbenchmark(fgrowth(cgdata,1), times = 10) # Unit: milliseconds #                expr      min       lq     mean   median       uq      max neval #  fgrowth(cgdata, 1) 11.58627 13.81528 18.05776 14.03489 22.34279 31.15811    10"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_dplyr.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"collapse and dplyr","text":"Timmer, M. P., de Vries, G. J., & de Vries, K. (2015). “Patterns Structural Change Developing Countries.” . J. Weiss, & M. Tribe (Eds.), Routledge Handbook Industry Development. (pp. 65-83). Routledge. Cochrane, D. & Orcutt, G. H. (1949). “Application Least Squares Regression Relationships Containing Auto-Correlated Error Terms”. Journal American Statistical Association. 44 (245): 32–61. Prais, S. J. & Winsten, C. B. (1954). “Trend Estimators Serial Correlation”. Cowles Commission Discussion Paper . 383. Chicago.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"part-1-fast-transformation-of-panel-data","dir":"Articles","previous_headings":"","what":"Part 1: Fast Transformation of Panel Data","title":"collapse and plm","text":"First let us convert data plm panel data.frame (class pdata.frame): plm::pdata.frame data.frame panel identifiers attached list factors index attribute (non-factor index variables converted factor). column data.frame Panel Series (plm::pseries), also panel identifiers attached: Now explored basic data structures provided plm package, let’s compute transformations :","code":"library(plm)  # This creates a panel data frame pwlddev <- pdata.frame(wlddev, index = c(\"iso3c\", \"year\"))  str(pwlddev, give.attr = FALSE) # Classes 'pdata.frame' and 'data.frame':   13176 obs. of  13 variables: #  $ country: 'pseries' Named chr  \"Aruba\" \"Aruba\" \"Aruba\" \"Aruba\" ... #  $ iso3c  : Factor w/ 216 levels \"ABW\",\"AFG\",\"AGO\",..: 1 1 1 1 1 1 1 1 1 1 ... #  $ date   : pseries, format: \"1961-01-01\" \"1962-01-01\" \"1963-01-01\" ... #  $ year   : Factor w/ 61 levels \"1960\",\"1961\",..: 1 2 3 4 5 6 7 8 9 10 ... #  $ decade : 'pseries' Named int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... #  $ region : Factor w/ 7 levels \"East Asia & Pacific\",..: 3 3 3 3 3 3 3 3 3 3 ... #  $ income : Factor w/ 4 levels \"High income\",..: 1 1 1 1 1 1 1 1 1 1 ... #  $ OECD   : 'pseries' Named logi  FALSE FALSE FALSE FALSE FALSE FALSE ... #  $ PCGDP  : 'pseries' Named num  NA NA NA NA NA NA NA NA NA NA ... #  $ LIFEEX : 'pseries' Named num  65.7 66.1 66.4 66.8 67.1 ... #  $ GINI   : 'pseries' Named num  NA NA NA NA NA NA NA NA NA NA ... #  $ ODA    : 'pseries' Named num  NA NA NA NA NA NA NA NA NA NA ... #  $ POP    : 'pseries' Named num  54211 55438 56225 56695 57032 ...  # A pdata.frame has an index attribute attached [retrieved using index(pwlddev) or attr(pwlddev, \"index\")] str(index(pwlddev)) # Classes 'pindex' and 'data.frame':    13176 obs. of  2 variables: #  $ iso3c: Factor w/ 216 levels \"ABW\",\"AFG\",\"AGO\",..: 1 1 1 1 1 1 1 1 1 1 ... #  $ year : Factor w/ 61 levels \"1960\",\"1961\",..: 1 2 3 4 5 6 7 8 9 10 ...  # This shows the individual and time dimensions pdim(pwlddev) # Balanced Panel: n = 216, T = 61, N = 13176 # Panel Series of GDP per Capita and Life-Expectancy at Birth PCGDP <- pwlddev$PCGDP LIFEEX <- pwlddev$LIFEEX str(LIFEEX) #  'pseries' Named num [1:13176] 65.7 66.1 66.4 66.8 67.1 ... #  - attr(*, \"names\")= chr [1:13176] \"ABW-1960\" \"ABW-1961\" \"ABW-1962\" \"ABW-1963\" ... #  - attr(*, \"index\")=Classes 'pindex' and 'data.frame':    13176 obs. of  2 variables: #   ..$ iso3c: Factor w/ 216 levels \"ABW\",\"AFG\",\"AGO\",..: 1 1 1 1 1 1 1 1 1 1 ... #   ..$ year : Factor w/ 61 levels \"1960\",\"1961\",..: 1 2 3 4 5 6 7 8 9 10 ..."},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"between-and-within-transformations","dir":"Articles","previous_headings":"Part 1: Fast Transformation of Panel Data","what":"1.1 Between and Within Transformations","title":"collapse and plm","text":"functions fbetween fbetween can used compute efficient within transformations panel vectors panel data.frames: default na.rm = TRUE thus functions skip (preserve) missing values data (default collapse functions). fbetween output behavior can altered option fill: Setting fill = TRUE compute group-means complete cases group (long na.rm = TRUE), replace values group group mean (hence overwriting ‘filling ’ missing values): fwithin mean argument allows set arbitrary data mean (different 0) data centered. grouped centering task, sensible choice added mean overall mean data series, enabled option mean = \"overall.mean\". add overall mean series back data subtracting group means, thus preserve level data (change intercept employed regression): fbetween fwithin can also applied pdata.frame’s perform computations variable variable: Now next fbetween fwithin also exist short versions B W, referred transformation operators. essentially wrappers around fbetween fwithin provide functionality, parsimonious employ regression formulas also offer additional features applied panel data.frames. panel series, B W exact analogues fbetween fwithin, just shorter name: applied panel data.frames, B W offer additional utility () allowing select columns transform using cols argument (default cols = .numeric, default numeric columns selected transformation), (b) allowing add prefix transformed columns stub argument (default stub = \"B.\" B stub = \"W.\" W) (c) preserving panel-id’s keep.ids argument (default keep.ids = TRUE): fbetween / B fwithin / W also support weighted computations. course applies panel-survey settings, sake illustration suppose wanted weight within transformations population countries: shown , B W weight column can also passed formula character string, whereas fbetween fwithin require inputs passed directly terms data (.e. fbetween(get_vars(pwlddev, 9:11), w = pwlddev$POP)), weight vector id columns never preserved output. Therefore applications B W probably convenient quick use, whereas fbetween fwithin preferred programmers choice, also little less R-overhead makes tiny bit faster.","code":"# Between-Transformations head(fbetween(LIFEEX))                        # Between individual (default) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  # 72.40653 72.40653 72.40653 72.40653 72.40653 72.40653  head(fbetween(LIFEEX, effect = \"year\"))       # Between time # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  # 53.91206 54.47441 54.85718 55.20272 55.66802 56.12963  # Within-Transformations head(fwithin(LIFEEX))                         # Within individuals (default) #  ABW-1960  ABW-1961  ABW-1962  ABW-1963  ABW-1964  ABW-1965  # -6.744533 -6.332533 -5.962533 -5.619533 -5.293533 -4.971533  head(fwithin(LIFEEX, effect = \"year\"))        # Within time # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  # 11.74994 11.59959 11.58682 11.58428 11.44498 11.30537 # This preserves missing values in the output head(fbetween(PCGDP), 30) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965 ABW-1966 ABW-1967 ABW-1968 ABW-1969 ABW-1970  #       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA  # ABW-1971 ABW-1972 ABW-1973 ABW-1974 ABW-1975 ABW-1976 ABW-1977 ABW-1978 ABW-1979 ABW-1980 ABW-1981  #       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA  # ABW-1982 ABW-1983 ABW-1984 ABW-1985 ABW-1986 ABW-1987 ABW-1988 ABW-1989  #       NA       NA       NA       NA 25413.84 25413.84 25413.84 25413.84  # This replaces all individuals with the group mean head(fbetween(PCGDP, fill = TRUE), 30) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965 ABW-1966 ABW-1967 ABW-1968 ABW-1969 ABW-1970  # 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84  # ABW-1971 ABW-1972 ABW-1973 ABW-1974 ABW-1975 ABW-1976 ABW-1977 ABW-1978 ABW-1979 ABW-1980 ABW-1981  # 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84  # ABW-1982 ABW-1983 ABW-1984 ABW-1985 ABW-1986 ABW-1987 ABW-1988 ABW-1989  # 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 25413.84 # This performed standard grouped centering head(fwithin(LIFEEX)) #  ABW-1960  ABW-1961  ABW-1962  ABW-1963  ABW-1964  ABW-1965  # -6.744533 -6.332533 -5.962533 -5.619533 -5.293533 -4.971533  # This adds the overall average Life-Expectancy (across countries) to the country-demeaned series head(fwithin(LIFEEX, mean = \"overall.mean\")) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  # 57.55177 57.96377 58.33377 58.67677 59.00277 59.32477 head(fbetween(num_vars(pwlddev)), 3) #            decade PCGDP   LIFEEX GINI ODA      POP # ABW-1960 1985.574    NA 72.40653   NA  NA 76268.63 # ABW-1961 1985.574    NA 72.40653   NA  NA 76268.63 # ABW-1962 1985.574    NA 72.40653   NA  NA 76268.63  head(fbetween(num_vars(pwlddev), fill = TRUE), 3) #            decade    PCGDP   LIFEEX GINI      ODA      POP # ABW-1960 1985.574 25413.84 72.40653   NA 33245000 76268.63 # ABW-1961 1985.574 25413.84 72.40653   NA 33245000 76268.63 # ABW-1962 1985.574 25413.84 72.40653   NA 33245000 76268.63  head(fwithin(num_vars(pwlddev)), 3) #             decade PCGDP    LIFEEX GINI ODA       POP # ABW-1960 -25.57377    NA -6.744533   NA  NA -22057.63 # ABW-1961 -25.57377    NA -6.332533   NA  NA -20830.63 # ABW-1962 -25.57377    NA -5.962533   NA  NA -20043.63  head(fwithin(num_vars(pwlddev), mean = \"overall.mean\"), 3) #          decade PCGDP   LIFEEX GINI ODA      POP # ABW-1960   1960    NA 57.55177   NA  NA 24223914 # ABW-1961   1960    NA 57.96377   NA  NA 24225141 # ABW-1962   1960    NA 58.33377   NA  NA 24225928 identical(fbetween(PCGDP), B(PCGDP)) # [1] TRUE identical(fbetween(PCGDP, fill = TRUE), B(PCGDP, fill = TRUE)) # [1] TRUE identical(fwithin(PCGDP), W(PCGDP)) # [1] TRUE identical(fwithin(PCGDP, mean = \"overall.mean\"), W(PCGDP, mean = \"overall.mean\")) # [1] TRUE head(B(pwlddev), 3) #          iso3c year B.decade B.PCGDP B.LIFEEX B.GINI B.ODA    B.POP # ABW-1960   ABW 1960 1985.574      NA 72.40653     NA    NA 76268.63 # ABW-1961   ABW 1961 1985.574      NA 72.40653     NA    NA 76268.63 # ABW-1962   ABW 1962 1985.574      NA 72.40653     NA    NA 76268.63  head(W(pwlddev, cols = 9:12), 3) # Here using the cols argument #          iso3c year W.PCGDP  W.LIFEEX W.GINI W.ODA # ABW-1960   ABW 1960      NA -6.744533     NA    NA # ABW-1961   ABW 1961      NA -6.332533     NA    NA # ABW-1962   ABW 1962      NA -5.962533     NA    NA # This replaces values by the POP-weighted group mean and also preserves the weight variable (POP, argument keep.w = TRUE) head(B(pwlddev, w = ~ POP), 3) #          iso3c year   POP B.decade B.PCGDP B.LIFEEX B.GINI B.ODA # ABW-1960   ABW 1960 54211 1988.976      NA 72.96257     NA    NA # ABW-1961   ABW 1961 55438 1988.976      NA 72.96257     NA    NA # ABW-1962   ABW 1962 56225 1988.976      NA 72.96257     NA    NA  # This centers values on the POP-weighted group mean head(W(pwlddev, w = ~ POP, cols = c(\"PCGDP\",\"LIFEEX\",\"GINI\")), 3) #          iso3c year   POP W.PCGDP  W.LIFEEX W.GINI # ABW-1960   ABW 1960 54211      NA -7.300566     NA # ABW-1961   ABW 1961 55438      NA -6.888566     NA # ABW-1962   ABW 1962 56225      NA -6.518566     NA  # This centers values on the POP-weighted group mean and also adds the overall POP-weighted mean of the data head(W(pwlddev, w = ~ POP, cols = c(\"PCGDP\",\"LIFEEX\",\"GINI\"), mean = \"overall.mean\"), 3) #          iso3c year   POP W.PCGDP W.LIFEEX W.GINI # ABW-1960   ABW 1960 54211      NA 58.58012     NA # ABW-1961   ABW 1961 55438      NA 58.99212     NA # ABW-1962   ABW 1962 56225      NA 59.36212     NA"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"higher-dimensional-between-and-within-transformations","dir":"Articles","previous_headings":"Part 1: Fast Transformation of Panel Data","what":"1.2 Higher-Dimensional Between and Within Transformations","title":"collapse and plm","text":"Analogous fbetween / B fwithin / W, collapse provides duo functions operators fhdbetween / HDB fhdwithin / HDW efficiently average center data multiple groups. credit herefore goes Laurent Berge, author fixest package wrote efficient C-implementation alternating-projections algorithm perform task. fhdbetween / HDB fhdwithin / HDW enrich implementation (available function fixest::demean) providing options regarding missing values, also allowing continuous covariates (full) interactions projected alongside factors. methods pseries pdata.frame’s however rather simple, simply simultaneously center panel-vectors various panel-identifiers index (can 2, default center identifiers): architecture fhdbetween / HDB fhdwithin / HDW differs bit fbetween / B fwithin / W. essentially consequence underlying C++-implementation (accessed fixest::demean), built accommodate missing values. fhdbetween / HDB fhdwithin / HDW therefore argument fill = TRUE (default), stipulates missing values data preserved output. collapse default na.rm = TRUE ensures complete cases used computation: pdata.frame methods 3 different choices deal missing values. default plm classes variable.wise = TRUE, essentially sequentially apply fhdbetween.pseries fhdwithin.pseries (default fill = TRUE) columns. behavior fbetween / B fwithin / W, also consider column-wise complete obs: variable.wise = FALSE, fhdbetween / HDB fhdwithin / HDW consider complete cases dataset, still return dataset dimensions (long fill = TRUE), resulting rows -missing: Finally, also fill = FALSE, behavior pseries method: Missing cases removed data: Notes:  (1) different missing case options associated challenges, panel-identifiers preserved HDB HDW. (2) default variable.wise = TRUE fill = TRUE set pseries pdata.frame methods, harmonize default implementations fbetween / B fwithin / W classes. standard default, matrix data.frame methods, defaults variable.wise = FALSE fill = FALSE (.e. missing cases removed beforehand), generally efficient.","code":"# This simultaneously averages Life-Expectancy across countries and years head(HDB(LIFEEX)) # (same as running a regression on country and year dummies and taking the fitted values) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  # 62.36179 62.85981 63.24258 63.65245 64.11774 64.52503  # This simultaneously centers Life-Expectenacy on countries and years head(HDW(LIFEEX)) # (same as running a regression on country and year dummies and taking the residuals) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  # 3.300210 3.214193 3.201424 3.134554 2.995255 2.909975 # Missing values are preserved in the output when fill = TRUE (the default) head(HDB(PCGDP), 30) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965 ABW-1966 ABW-1967 ABW-1968 ABW-1969 ABW-1970  #       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA  # ABW-1971 ABW-1972 ABW-1973 ABW-1974 ABW-1975 ABW-1976 ABW-1977 ABW-1978 ABW-1979 ABW-1980 ABW-1981  #       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA  # ABW-1982 ABW-1983 ABW-1984 ABW-1985 ABW-1986 ABW-1987 ABW-1988 ABW-1989  #       NA       NA       NA       NA 21833.32 22132.25 22479.20 22772.31  # When fill = FALSE, only the complete cases are returned nofill <- HDB(PCGDP, fill = FALSE) head(nofill, 30) # ABW-1986 ABW-1987 ABW-1988 ABW-1989 ABW-1990 ABW-1991 ABW-1992 ABW-1993 ABW-1994 ABW-1995 ABW-1996  # 21833.32 22132.25 22479.20 22772.31 23064.29 23060.00 23089.75 23115.36 23343.25 23595.16 23823.11  # ABW-1997 ABW-1998 ABW-1999 ABW-2000 ABW-2001 ABW-2002 ABW-2003 ABW-2004 ABW-2005 ABW-2006 ABW-2007  # 24149.44 24424.69 24727.46 25205.98 25399.16 25603.11 25851.29 26349.64 26665.54 27224.58 27772.82  # ABW-2008 ABW-2009 ABW-2010 ABW-2011 ABW-2012 ABW-2013 ABW-2014 ABW-2015  # 27769.52 27002.95 27218.84 27424.18 27471.49 27660.92 27889.34 28107.78  # This results in a shorter panel-vector length(nofill) # [1] 9470 length(PCGDP) # [1] 13176  # The cases that were missing and removed from the output are available as an attribute head(attr(nofill, \"na.rm\"), 30) #  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 59 60 61 62 # This column-wise centers the data on countries and years tail(HDW(pwlddev), 10) #          HDW.decade HDW.PCGDP HDW.LIFEEX  HDW.GINI   HDW.ODA  HDW.POP # ZWE-2011          0 -4632.971  -8.080748 -3.663217 118306300 -4547122 # ZWE-2012          0 -4523.505  -6.271385        NA 385526419 -4749368 # ZWE-2013          0 -4710.576  -4.753056        NA 149910333 -4903132 # ZWE-2014          0 -4931.693  -3.568136        NA  93295114 -5059317 # ZWE-2015          0 -5148.895  -2.685053        NA 150833589 -5224484 # ZWE-2016          0 -5433.809  -2.203219        NA -27844184 -5404667 # ZWE-2017          0 -5645.022  -1.920365 -1.964138  10266318 -5591762 # ZWE-2018          0 -5938.794  -1.759333        NA  59646823 -5774326 # ZWE-2019          0 -5710.646  -1.669415  5.627356 223473855 -5946725 # ZWE-2020          0        NA         NA        NA        NA       NA # This centers the complete cases of the data data on countries and years and keeps missing cases tail(HDW(pwlddev, variable.wise = FALSE), 10) #          HDW.decade HDW.PCGDP HDW.LIFEEX  HDW.GINI    HDW.ODA  HDW.POP # ZWE-2011          0  517.6924  -4.379840 -3.839653 -176176494 -3042247 # ZWE-2012         NA        NA         NA        NA         NA       NA # ZWE-2013         NA        NA         NA        NA         NA       NA # ZWE-2014         NA        NA         NA        NA         NA       NA # ZWE-2015         NA        NA         NA        NA         NA       NA # ZWE-2016         NA        NA         NA        NA         NA       NA # ZWE-2017          0 -128.5240   1.971143 -1.314869  -67497466  1936716 # ZWE-2018         NA        NA         NA        NA         NA       NA # ZWE-2019          0 -389.1684   2.408697  5.154522  243673961  1105530 # ZWE-2020         NA        NA         NA        NA         NA       NA # This centers the complete cases of the data data on countries and years, and removes missing cases res <- HDW(pwlddev, fill = FALSE) tail(res, 10) #          HDW.decade   HDW.PCGDP HDW.LIFEEX   HDW.GINI    HDW.ODA    HDW.POP # ZMB-1996          0   534.39373 -3.6445256  -4.744748 -174237036  4911230.7 # ZMB-1998          0   201.58094 -4.1708951  -5.085621 -492258601   644947.7 # ZMB-2002          0   250.78234 -2.9085522 -10.912265   81848768 -1027712.3 # ZMB-2004          0   -72.94954 -1.9629513   1.494340  396830282 -3774596.6 # ZMB-2006          0  -308.55937 -0.4975872   2.407226  485998870 -2255101.6 # ZMB-2010          0  -428.16949  3.9600416   4.497547 -148714637 -4174306.2 # ZMB-2015          0 -1106.52213  8.4099983   7.553052 -335529320 -4962997.8 # ZWE-2011          0   517.69244 -4.3798401  -3.839653 -176176494 -3042246.9 # ZWE-2017          0  -128.52402  1.9711431  -1.314869  -67497466  1936716.5 # ZWE-2019          0  -389.16842  2.4086971   5.154522  243673961  1105530.5  tail(attr(res, \"na.rm\")) # [1] 13169 13170 13171 13172 13174 13176"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"scaling-and-centering","dir":"Articles","previous_headings":"Part 1: Fast Transformation of Panel Data","what":"1.3 Scaling and Centering","title":"collapse and plm","text":"Next functions grouped centering averaging, function / operator pair fscale / STD can used efficiently standardize (.e. scale center) panel data along arbitrary dimension. architecture identical fwithin / W fbetween / B. similarly pdata.frame’s: customized scaling can done help mean sd arguments fscale / STD. default mean = 0 sd = 1, assigned numeric values: Even customization (.e. setting means standard deviations group / column) can course achieved calling collapse::TRA result fscale sweep appropriate set means standard deviations. Scaling without centering can done option mean = FALSE. also preserve mean data overall within group: Finally special kind data harmonization first two moments can done setting mean = \"overall.mean\" sd = \"within.sd\" grouped scaling task. harmonize data across groups mean group equal overall data mean standard deviation equal within standard deviation (= standard deviation calculated group-centered series): seamlessly generalizes weighted scaling centering, using w argument add weight vector.","code":"# This standardizes GDP per capita in each country STD_PCGDP <- STD(PCGDP)  # Checks: head(fmean(STD_PCGDP, index(STD_PCGDP, 1))) #           ABW           AFG           AGO           ALB           AND           ARE  # -1.422473e-15  2.528841e-16 -6.189493e-16 -2.275957e-16 -9.281464e-16 -6.661338e-17 head(fsd(STD_PCGDP, index(STD_PCGDP, 1))) # ABW AFG AGO ALB AND ARE  #   1   1   1   1   1   1  # This standardizes GDP per capita in each year STD_PCGDP_T <- STD(PCGDP, effect = \"year\")  # Checks: head(fmean(STD_PCGDP_T, index(STD_PCGDP_T, 2))) #          1960          1961          1962          1963          1964          1965  #  9.882205e-17  3.496021e-16  1.889741e-17 -2.185013e-16 -1.724389e-16  2.616954e-16 head(fsd(STD_PCGDP_T, index(STD_PCGDP_T, 2))) # 1960 1961 1962 1963 1964 1965  #    1    1    1    1    1    1 head(STD(pwlddev, cols = 9:12)) #          iso3c year STD.PCGDP STD.LIFEEX STD.GINI STD.ODA # ABW-1960   ABW 1960        NA  -2.372636       NA      NA # ABW-1961   ABW 1961        NA  -2.227700       NA      NA # ABW-1962   ABW 1962        NA  -2.097539       NA      NA # ABW-1963   ABW 1963        NA  -1.976876       NA      NA # ABW-1964   ABW 1964        NA  -1.862193       NA      NA # ABW-1965   ABW 1965        NA  -1.748918       NA      NA  head(STD(pwlddev, cols = 9:12, effect = \"year\")) #          iso3c year STD.PCGDP STD.LIFEEX STD.GINI STD.ODA # ABW-1960   ABW 1960        NA  0.9609854       NA      NA # ABW-1961   ABW 1961        NA  0.9485730       NA      NA # ABW-1962   ABW 1962        NA  0.9585105       NA      NA # ABW-1963   ABW 1963        NA  0.9669638       NA      NA # ABW-1964   ABW 1964        NA  0.9579477       NA      NA # ABW-1965   ABW 1965        NA  0.9556529       NA      NA # This will scale the data such that mean mean within each country is 5 and the standard deviation is 3 qsu(fscale(pwlddev$PCGDP, mean = 5, sd = 3)) #              N/T  Mean     SD      Min     Max # Overall     9466     5  2.968  -6.1908  16.257 # Between      202     5      0        5       5 # Within   46.8614     5  2.968  -6.1908  16.257 # Scaling without centering: Mean preserving with fscale / STD qsu(fscale(pwlddev$PCGDP, mean = FALSE, sd = 3)) #              N/T        Mean          SD         Min         Max # Overall     9466  12031.4627  17803.3537    247.7598   131349.27 # Between      202  12169.2793  18055.6626    253.1886  131342.669 # Within   46.8614  12031.4627       2.968  12020.2718  12042.7196  # Scaling without centering can also be done using fsd, but this does not preserve the mean qsu(fsd(pwlddev$PCGDP, index(pwlddev, 1), TRA = \"/\")) #              N/T    Mean      SD     Min      Max # Overall     9466   4.247   3.192  0.0579   26.647 # Between      202  4.6036  3.5846  0.8207  24.8111 # Within   46.8614   4.247  0.9893  0.5167   7.9993 fmean(pwlddev$PCGDP)  # Overall mean # [1] 12048.78 fsd(W(pwlddev$PCGDP)) # Within sd # [1] 6723.681  # Scaling and centerin such that the mean of each country is the overall mean, and the sd of each country is the within sd qsu(fscale(pwlddev$PCGDP, mean = \"overall.mean\", sd = \"within.sd\")) #              N/T       Mean         SD          Min         Max # Overall     9466  12048.778  6651.9052  -13032.4333  37278.2175 # Between      202  12048.778          0    12048.778   12048.778 # Within   46.8614  12048.778  6651.9052  -13032.4333  37278.2175"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"panel-lags-leads-differences-and-growth-rates","dir":"Articles","previous_headings":"Part 1: Fast Transformation of Panel Data","what":"1.4 Panel Lags / Leads, Differences and Growth Rates","title":"collapse and plm","text":"flag / L / F, fdiff / D fgrowth / G, collapse provides fast comprehensive C++ based solution computation (sequences ) lags / leads (sequences ) lagged / leaded suitably iterated (quasi-, log-) differences growth rates panel data. pseries pdata.frame methods functions associated transformation operators use panel-identifiers ‘index’ attached objects (last variable ‘index’ taken time-variable variables taken individual identifiers) perform fast fully-identified time-dependent operations panel data, without need sorting data. flag / L / F, easy lag lead pseries: also possible compute sequence lags / leads using flag one operators: course lag orders may unevenly spaced, .e. L(x, -1:3*12) compute seasonal lags monthly data. pdata.frame’s, effects flag L / F differ insofar flag just lag entire dataset without preserving identifiers (although index attribute always preserved), whereas L / F default (cols = .numeric) select numeric variables add panel-id’s left (default keep.ids = TRUE): can also easily compute sequence lags / leads panel data.frame: Essentially functionality applies fdiff / D fgrowth / G, main differences functions also diff argument determine number iterations: default, growth rates calculated percentage terms set default argument scale = 100. also possible compute log-differences fdiff(.., log = TRUE) Dlog operator, growth rates percentage terms based log-differences using fgrowth(.., logdiff = TRUE). also possible compute sequences lagged / leaded iterated differences, log-differences growth rates: possibility compute quasi-differences quasi-log-differences form x_t - \\rho x_{t-s} log(x_t) - \\rho log(x_{t-s}). useful panel-regressions suffering serial-correlation, following Cochrane & Orcutt (1949), can specified rho argument fdiff, D Dlog. final important advantage collapse functions panel-identifiers preserved, even matrix lags / leads / differences growth rates returned. allows nested panel-computations, example can compute shifted sequences lagged / leaded iterated panel differences: naturally generalized computations pdata.frames:","code":"# A panel-lag head(flag(LIFEEX)) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  #       NA   65.662   66.074   66.444   66.787   67.113  # A panel-lead head(flag(LIFEEX, -1)) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  #   66.074   66.444   66.787   67.113   67.435   67.762  # The lag and lead operators are even more parsimonious to employ: all_identical(L(LIFEEX), flag(LIFEEX), lag(LIFEEX)) # [1] TRUE all_identical(F(LIFEEX), flag(LIFEEX, -1), lead(LIFEEX)) # [1] TRUE # sequence of panel- lags and leads head(flag(LIFEEX, -1:3)) #              F1     --     L1     L2     L3 # ABW-1960 66.074 65.662     NA     NA     NA # ABW-1961 66.444 66.074 65.662     NA     NA # ABW-1962 66.787 66.444 66.074 65.662     NA # ABW-1963 67.113 66.787 66.444 66.074 65.662 # ABW-1964 67.435 67.113 66.787 66.444 66.074 # ABW-1965 67.762 67.435 67.113 66.787 66.444  all_identical(L(LIFEEX, -1:3), F(LIFEEX, 1:-3), flag(LIFEEX, -1:3)) # [1] TRUE  # The native plm implementation also returns a matrix of lags but with different column names head(lag(LIFEEX, -1:3), 4) #              -1      0      1      2      3 # ABW-1960 66.074 65.662     NA     NA     NA # ABW-1961 66.444 66.074 65.662     NA     NA # ABW-1962 66.787 66.444 66.074 65.662     NA # ABW-1963 67.113 66.787 66.444 66.074 65.662 # This lags the entire data head(flag(pwlddev)) #          country iso3c       date year decade                    region      income  OECD PCGDP # ABW-1960    <NA>  <NA>       <NA> <NA>     NA                      <NA>        <NA>    NA    NA # ABW-1961   Aruba   ABW 1961-01-01 1960   1960 Latin America & Caribbean High income FALSE    NA # ABW-1962   Aruba   ABW 1962-01-01 1961   1960 Latin America & Caribbean High income FALSE    NA # ABW-1963   Aruba   ABW 1963-01-01 1962   1960 Latin America & Caribbean High income FALSE    NA # ABW-1964   Aruba   ABW 1964-01-01 1963   1960 Latin America & Caribbean High income FALSE    NA # ABW-1965   Aruba   ABW 1965-01-01 1964   1960 Latin America & Caribbean High income FALSE    NA #          LIFEEX GINI ODA   POP # ABW-1960     NA   NA  NA    NA # ABW-1961 65.662   NA  NA 54211 # ABW-1962 66.074   NA  NA 55438 # ABW-1963 66.444   NA  NA 56225 # ABW-1964 66.787   NA  NA 56695 # ABW-1965 67.113   NA  NA 57032  # This lags only numeric columns and preserves panel-id's head(L(pwlddev)) #          iso3c year L1.decade L1.PCGDP L1.LIFEEX L1.GINI L1.ODA L1.POP # ABW-1960   ABW 1960        NA       NA        NA      NA     NA     NA # ABW-1961   ABW 1961      1960       NA    65.662      NA     NA  54211 # ABW-1962   ABW 1962      1960       NA    66.074      NA     NA  55438 # ABW-1963   ABW 1963      1960       NA    66.444      NA     NA  56225 # ABW-1964   ABW 1964      1960       NA    66.787      NA     NA  56695 # ABW-1965   ABW 1965      1960       NA    67.113      NA     NA  57032  # This lags only columns 9 through 12 and preserves panel-id's head(L(pwlddev, cols = 9:12)) #          iso3c year L1.PCGDP L1.LIFEEX L1.GINI L1.ODA # ABW-1960   ABW 1960       NA        NA      NA     NA # ABW-1961   ABW 1961       NA    65.662      NA     NA # ABW-1962   ABW 1962       NA    66.074      NA     NA # ABW-1963   ABW 1963       NA    66.444      NA     NA # ABW-1964   ABW 1964       NA    66.787      NA     NA # ABW-1965   ABW 1965       NA    67.113      NA     NA # This lags only columns 9 through 12 and preserves panel-id's head(L(pwlddev, -1:3, cols = 9:12)) #          iso3c year F1.PCGDP PCGDP L1.PCGDP L2.PCGDP L3.PCGDP F1.LIFEEX LIFEEX L1.LIFEEX L2.LIFEEX # ABW-1960   ABW 1960       NA    NA       NA       NA       NA    66.074 65.662        NA        NA # ABW-1961   ABW 1961       NA    NA       NA       NA       NA    66.444 66.074    65.662        NA # ABW-1962   ABW 1962       NA    NA       NA       NA       NA    66.787 66.444    66.074    65.662 # ABW-1963   ABW 1963       NA    NA       NA       NA       NA    67.113 66.787    66.444    66.074 # ABW-1964   ABW 1964       NA    NA       NA       NA       NA    67.435 67.113    66.787    66.444 # ABW-1965   ABW 1965       NA    NA       NA       NA       NA    67.762 67.435    67.113    66.787 #          L3.LIFEEX F1.GINI GINI L1.GINI L2.GINI L3.GINI F1.ODA ODA L1.ODA L2.ODA L3.ODA # ABW-1960        NA      NA   NA      NA      NA      NA     NA  NA     NA     NA     NA # ABW-1961        NA      NA   NA      NA      NA      NA     NA  NA     NA     NA     NA # ABW-1962        NA      NA   NA      NA      NA      NA     NA  NA     NA     NA     NA # ABW-1963    65.662      NA   NA      NA      NA      NA     NA  NA     NA     NA     NA # ABW-1964    66.074      NA   NA      NA      NA      NA     NA  NA     NA     NA     NA # ABW-1965    66.444      NA   NA      NA      NA      NA     NA  NA     NA     NA     NA # Panel-difference of Life Expectancy head(fdiff(LIFEEX)) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  #       NA    0.412    0.370    0.343    0.326    0.322  # Second panel-difference head(fdiff(LIFEEX, diff = 2)) # ABW-1960 ABW-1961 ABW-1962 ABW-1963 ABW-1964 ABW-1965  #       NA       NA   -0.042   -0.027   -0.017   -0.004  # Panel-growth rate of Life Expectancy head(fgrowth(LIFEEX)) #  ABW-1960  ABW-1961  ABW-1962  ABW-1963  ABW-1964  ABW-1965  #        NA 0.6274558 0.5599782 0.5162242 0.4881189 0.4797878  # Growth rate of growth rate of Life Expectancy head(fgrowth(LIFEEX, diff = 2)) #   ABW-1960   ABW-1961   ABW-1962   ABW-1963   ABW-1964   ABW-1965  #         NA         NA -10.754153  -7.813521  -5.444387  -1.706782  identical(D(LIFEEX), fdiff(LIFEEX)) # [1] TRUE identical(G(LIFEEX), fgrowth(LIFEEX)) # [1] TRUE identical(fdiff(LIFEEX), diff(LIFEEX)) # Same as plm::diff.pseries (which does not compute iterated panel-differences) # [1] TRUE # Panel log-difference of Life Expectancy head(Dlog(LIFEEX)) #    ABW-1960    ABW-1961    ABW-1962    ABW-1963    ABW-1964    ABW-1965  #          NA 0.006254955 0.005584162 0.005148963 0.004869315 0.004786405  # Panel log-difference growth rate (in percentage terms) of Life Expectancy head(G(LIFEEX, logdiff = TRUE)) #  ABW-1960  ABW-1961  ABW-1962  ABW-1963  ABW-1964  ABW-1965  #        NA 0.6254955 0.5584162 0.5148963 0.4869315 0.4786405 # first and second forward-difference and first and second difference of lags 1-3 of Life-Expectancy head(D(LIFEEX, -1:3, 1:2)) #             FD1    FD2     --    D1     D2  L2D1   L2D2  L3D1 L3D2 # ABW-1960 -0.412 -0.042 65.662    NA     NA    NA     NA    NA   NA # ABW-1961 -0.370 -0.027 66.074 0.412     NA    NA     NA    NA   NA # ABW-1962 -0.343 -0.017 66.444 0.370 -0.042 0.782     NA    NA   NA # ABW-1963 -0.326 -0.004 66.787 0.343 -0.027 0.713     NA 1.125   NA # ABW-1964 -0.322  0.005 67.113 0.326 -0.017 0.669 -0.113 1.039   NA # ABW-1965 -0.327  0.006 67.435 0.322 -0.004 0.648 -0.065 0.991   NA  # Same with Log-differences head(Dlog(LIFEEX, -1:3, 1:2)) #                FDlog1        FDlog2       --       Dlog1         Dlog2    L2Dlog1      L2Dlog2 # ABW-1960 -0.006254955 -6.707929e-04 4.184520          NA            NA         NA           NA # ABW-1961 -0.005584162 -4.351984e-04 4.190775 0.006254955            NA         NA           NA # ABW-1962 -0.005148963 -2.796481e-04 4.196359 0.005584162 -0.0006707929 0.01183912           NA # ABW-1963 -0.004869315 -8.291000e-05 4.201508 0.005148963 -0.0004351984 0.01073312           NA # ABW-1964 -0.004786405  5.098981e-05 4.206378 0.004869315 -0.0002796481 0.01001828 -0.001820838 # ABW-1965 -0.004837395  6.482830e-05 4.211164 0.004786405 -0.0000829100 0.00965572 -0.001077405 #             L3Dlog1 L3Dlog2 # ABW-1960         NA      NA # ABW-1961         NA      NA # ABW-1962         NA      NA # ABW-1963 0.01698808      NA # ABW-1964 0.01560244      NA # ABW-1965 0.01480468      NA  # Same with (exact) growth rates head(G(LIFEEX, -1:3, 1:2)) #                 FG1       FG2     --        G1         G2      L2G1      L2G2     L3G1 L3G2 # ABW-1960 -0.6235433 11.974895 65.662        NA         NA        NA        NA       NA   NA # ABW-1961 -0.5568599  8.428580 66.074 0.6274558         NA        NA        NA       NA   NA # ABW-1962 -0.5135730  5.728297 66.444 0.5599782 -10.754153 1.1909476        NA       NA   NA # ABW-1963 -0.4857479  1.727984 66.787 0.5162242  -7.813521 1.0790931        NA 1.713320   NA # ABW-1964 -0.4774968 -1.051555 67.113 0.4881189  -5.444387 1.0068629 -15.45699 1.572479   NA # ABW-1965 -0.4825714 -1.319230 67.435 0.4797878  -1.706782 0.9702487 -10.08666 1.491482   NA # Regression of GDP on Life Expectance with country and time FE mod <- lm(PCGDP ~ LIFEEX, data = fhdwithin(fselect(pwlddev, PCGDP, LIFEEX), fill = FALSE)) mod #  # Call: # lm(formula = PCGDP ~ LIFEEX, data = fhdwithin(fselect(pwlddev,  #     PCGDP, LIFEEX), fill = FALSE)) #  # Coefficients: # (Intercept)       LIFEEX   #  -2.442e-12   -3.330e+02  # Computing autocorrelation of residuals r <- residuals(mod) r <- pwcor(r, L(r, 1, substr(names(r), 1, 3)))  # Need this to compute a panel-lag r # [1] .98  # Running the regression again quasi-differencing the transformed data modCO <- lm(PCGDP ~ LIFEEX, data = fdiff(fhdwithin(fselect(pwlddev, PCGDP, LIFEEX), variable.wise = FALSE), rho = r, stubs = FALSE)) modCO #  # Call: # lm(formula = PCGDP ~ LIFEEX, data = fdiff(fhdwithin(fselect(pwlddev,  #     PCGDP, LIFEEX), variable.wise = FALSE), rho = r, stubs = FALSE)) #  # Coefficients: # (Intercept)       LIFEEX   #      -12.93       -91.97  # In this case rho is almost 1, so we might as well just difference the untransformed data and go with that # We also need to bootstrap this for proper standard errors. # Sequence of differneces (same as above), adding one extra lag of the whole sequence head(L(D(LIFEEX, -1:3, 1:2), 0:1)) #             FD1 L1.FD1    FD2 L1.FD2     --  L1.--    D1 L1.D1     D2  L1.D2  L2D1 L1.L2D1   L2D2 # ABW-1960 -0.412     NA -0.042     NA 65.662     NA    NA    NA     NA     NA    NA      NA     NA # ABW-1961 -0.370 -0.412 -0.027 -0.042 66.074 65.662 0.412    NA     NA     NA    NA      NA     NA # ABW-1962 -0.343 -0.370 -0.017 -0.027 66.444 66.074 0.370 0.412 -0.042     NA 0.782      NA     NA # ABW-1963 -0.326 -0.343 -0.004 -0.017 66.787 66.444 0.343 0.370 -0.027 -0.042 0.713   0.782     NA # ABW-1964 -0.322 -0.326  0.005 -0.004 67.113 66.787 0.326 0.343 -0.017 -0.027 0.669   0.713 -0.113 # ABW-1965 -0.327 -0.322  0.006  0.005 67.435 67.113 0.322 0.326 -0.004 -0.017 0.648   0.669 -0.065 #          L1.L2D2  L3D1 L1.L3D1 L3D2 L1.L3D2 # ABW-1960      NA    NA      NA   NA      NA # ABW-1961      NA    NA      NA   NA      NA # ABW-1962      NA    NA      NA   NA      NA # ABW-1963      NA 1.125      NA   NA      NA # ABW-1964      NA 1.039   1.125   NA      NA # ABW-1965  -0.113 0.991   1.039   NA      NA head(D(pwlddev, -1:3, 1:2, cols = 9:10), 3) #          iso3c year FD1.PCGDP FD2.PCGDP PCGDP D1.PCGDP D2.PCGDP L2D1.PCGDP L2D2.PCGDP L3D1.PCGDP # ABW-1960   ABW 1960        NA        NA    NA       NA       NA         NA         NA         NA # ABW-1961   ABW 1961        NA        NA    NA       NA       NA         NA         NA         NA # ABW-1962   ABW 1962        NA        NA    NA       NA       NA         NA         NA         NA #          L3D2.PCGDP FD1.LIFEEX FD2.LIFEEX LIFEEX D1.LIFEEX D2.LIFEEX L2D1.LIFEEX L2D2.LIFEEX # ABW-1960         NA     -0.412     -0.042 65.662        NA        NA          NA          NA # ABW-1961         NA     -0.370     -0.027 66.074     0.412        NA          NA          NA # ABW-1962         NA     -0.343     -0.017 66.444     0.370    -0.042       0.782          NA #          L3D1.LIFEEX L3D2.LIFEEX # ABW-1960          NA          NA # ABW-1961          NA          NA # ABW-1962          NA          NA  head(L(D(pwlddev, -1:3, 1:2, cols = 9:10), 0:1), 3) #          iso3c year FD1.PCGDP L1.FD1.PCGDP FD2.PCGDP L1.FD2.PCGDP PCGDP L1.PCGDP D1.PCGDP # ABW-1960   ABW 1960        NA           NA        NA           NA    NA       NA       NA # ABW-1961   ABW 1961        NA           NA        NA           NA    NA       NA       NA # ABW-1962   ABW 1962        NA           NA        NA           NA    NA       NA       NA #          L1.D1.PCGDP D2.PCGDP L1.D2.PCGDP L2D1.PCGDP L1.L2D1.PCGDP L2D2.PCGDP L1.L2D2.PCGDP # ABW-1960          NA       NA          NA         NA            NA         NA            NA # ABW-1961          NA       NA          NA         NA            NA         NA            NA # ABW-1962          NA       NA          NA         NA            NA         NA            NA #          L3D1.PCGDP L1.L3D1.PCGDP L3D2.PCGDP L1.L3D2.PCGDP FD1.LIFEEX L1.FD1.LIFEEX FD2.LIFEEX # ABW-1960         NA            NA         NA            NA     -0.412            NA     -0.042 # ABW-1961         NA            NA         NA            NA     -0.370        -0.412     -0.027 # ABW-1962         NA            NA         NA            NA     -0.343        -0.370     -0.017 #          L1.FD2.LIFEEX LIFEEX L1.LIFEEX D1.LIFEEX L1.D1.LIFEEX D2.LIFEEX L1.D2.LIFEEX L2D1.LIFEEX # ABW-1960            NA 65.662        NA        NA           NA        NA           NA          NA # ABW-1961        -0.042 66.074    65.662     0.412           NA        NA           NA          NA # ABW-1962        -0.027 66.444    66.074     0.370        0.412    -0.042           NA       0.782 #          L1.L2D1.LIFEEX L2D2.LIFEEX L1.L2D2.LIFEEX L3D1.LIFEEX L1.L3D1.LIFEEX L3D2.LIFEEX # ABW-1960             NA          NA             NA          NA             NA          NA # ABW-1961             NA          NA             NA          NA             NA          NA # ABW-1962             NA          NA             NA          NA             NA          NA #          L1.L3D2.LIFEEX # ABW-1960             NA # ABW-1961             NA # ABW-1962             NA"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"panel-data-to-array-conversions","dir":"Articles","previous_headings":"Part 1: Fast Transformation of Panel Data","what":"1.5 Panel Data to Array Conversions","title":"collapse and plm","text":"Viewing transforming panel data stored array can powerful strategy, especially provides much direct access different dimensions data. function psmat can used efficiently transform pseries 2D matrix, pdata.frame’s 3D array: Applying psmat pdata.frame yields 3D array: format can convenient quickly freely access data different countries, variables time-periods: psmat can also return output list panel series matrices: list can unlisted using function unlist2d (unlisting 2-dimensions), yield reshaped data.frame: course also applied transformation (like computing pairwise correlations) matrix unlisting. case kind programming provides lots possibilities explore manipulate panel data (see Part 2).","code":"# Converting the panel series to array, individual rows (default) str(psmat(LIFEEX)) #  'psmat' num [1:216, 1:61] 65.7 32.4 37.5 62.3 NA ... #  - attr(*, \"dimnames\")=List of 2 #   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #  - attr(*, \"transpose\")= logi FALSE  # Converting the panel series to array, individual columns str(psmat(LIFEEX, transpose = TRUE)) #  'psmat' num [1:61, 1:216] 65.7 66.1 66.4 66.8 67.1 ... #  - attr(*, \"dimnames\")=List of 2 #   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #  - attr(*, \"transpose\")= logi TRUE  # Same as plm::as.matrix.pseries, apart from attributes identical(unattrib(psmat(LIFEEX)),           unattrib(as.matrix(LIFEEX))) # [1] TRUE identical(unattrib(psmat(LIFEEX, transpose = TRUE)),           unattrib(as.matrix(LIFEEX, idbyrow = FALSE))) # [1] TRUE psar <- psmat(pwlddev, cols = 9:12) str(psar) #  'psmat' num [1:216, 1:61, 1:4] NA NA NA NA NA ... #  - attr(*, \"dimnames\")=List of 3 #   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..$ : chr [1:4] \"PCGDP\" \"LIFEEX\" \"GINI\" \"ODA\" #  - attr(*, \"transpose\")= logi FALSE  str(psmat(pwlddev, cols = 9:12, transpose = TRUE)) #  'psmat' num [1:61, 1:216, 1:4] NA NA NA NA NA NA NA NA NA NA ... #  - attr(*, \"dimnames\")=List of 3 #   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   ..$ : chr [1:4] \"PCGDP\" \"LIFEEX\" \"GINI\" \"ODA\" #  - attr(*, \"transpose\")= logi TRUE # Looking at wealth, health and inequality in Brazil and Argentinia, 1990-1999 aperm(psar[c(\"BRA\",\"ARG\"), as.character(1990:1999), c(\"PCGDP\", \"LIFEEX\", \"GINI\")]) # , , BRA #  #          1990   1991   1992   1993   1994   1995   1996   1997   1998   1999 # PCGDP  7983.7 7963.1 7791.8 8020.6 8311.6 8540.1 8591.0 8744.8 8641.3 8554.1 # LIFEEX   66.3   66.7   67.1   67.5   67.9   68.3   68.7   69.1   69.4   69.8 # GINI     60.5     NA   53.2   60.1     NA   59.6   59.9   59.8   59.6   59.0 #  # , , ARG #  #          1990   1991   1992   1993   1994   1995   1996   1997   1998   1999 # PCGDP  6245.7 6721.3 7157.3 7644.2 7988.6 7666.5 7994.2 8543.0 8772.1 8381.3 # LIFEEX   71.6   71.8   72.0   72.2   72.5   72.7   72.8   73.0   73.2   73.4 # GINI       NA   46.8   45.5   44.9   45.9   48.9   49.5   49.1   50.7   49.8 pslist <- psmat(pwlddev, cols = 9:12, array = FALSE) str(pslist) # List of 4 #  $ PCGDP : 'psmat' num [1:216, 1:61] NA NA NA NA NA ... #   ..- attr(*, \"dimnames\")=List of 2 #   .. ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   .. ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..- attr(*, \"transpose\")= logi FALSE #  $ LIFEEX: 'psmat' num [1:216, 1:61] 65.7 32.4 37.5 62.3 NA ... #   ..- attr(*, \"dimnames\")=List of 2 #   .. ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   .. ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..- attr(*, \"transpose\")= logi FALSE #  $ GINI  : 'psmat' num [1:216, 1:61] NA NA NA NA NA NA NA NA NA NA ... #   ..- attr(*, \"dimnames\")=List of 2 #   .. ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   .. ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..- attr(*, \"transpose\")= logi FALSE #  $ ODA   : 'psmat' num [1:216, 1:61] NA 116769997 -390000 NA NA ... #   ..- attr(*, \"dimnames\")=List of 2 #   .. ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   .. ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..- attr(*, \"transpose\")= logi FALSE head(unlist2d(pslist, idcols = \"Variable\", row.names = \"Country Code\"), 3) #   Variable Country Code 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 # 1    PCGDP          ABW   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA # 2    PCGDP          AFG   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA # 3    PCGDP          AGO   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA #   1975 1976 1977 1978 1979     1980     1981     1982     1983     1984     1985      1986 # 1   NA   NA   NA   NA   NA       NA       NA       NA       NA       NA       NA 15669.616 # 2   NA   NA   NA   NA   NA       NA       NA       NA       NA       NA       NA        NA # 3   NA   NA   NA   NA   NA 3193.404 2947.194 2844.322 2859.919 2925.367 2922.217  2902.618 #        1987      1988      1989      1990      1991      1992      1993      1994     1995 # 1 18427.612 22134.017 24837.951 25357.787 26329.313 26401.969 26663.208 27272.310 26705.18 # 2        NA        NA        NA        NA        NA        NA        NA        NA       NA # 3  2916.794  2989.617  2889.886  2697.491  2635.156  2401.234  1767.025  1733.844  1930.80 #        1996      1997     1998     1999      2000      2001       2002       2003       2004 # 1 26087.776 27190.501 27151.92 26954.40 28417.384 26966.055 25508.3027 25469.2876 27005.5294 # 2        NA        NA       NA       NA        NA        NA   330.3036   343.0809   333.2167 # 3  2122.968  2205.294  2235.39  2211.13  2205.205  2223.335  2444.4178  2433.8616  2608.7840 #         2005       2006      2007       2008       2009      2010       2011       2012       2013 # 1 26979.8854 27046.2242 27427.579 27365.9312 24463.6922 23512.603 24233.0011 23781.2573 24635.7649 # 2   357.2347   365.2845   405.549   412.0143   488.3003   543.303   528.7366   576.1901   587.5651 # 3  2896.5547  3116.1810  3424.372  3668.0799  3565.0569  3587.884  3579.9599  3748.4507  3796.8822 #         2014       2015       2016       2017     2018      2019 2020 # 1 24563.2343 25822.2514 26231.0267 26630.2053       NA        NA   NA # 2   583.6562   574.1841   571.0738   571.4407  564.610  573.2876   NA # 3  3843.1979  3748.3201  3530.3107  3409.9303 3233.906 3111.1577   NA"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"benchmarks","dir":"Articles","previous_headings":"Part 1: Fast Transformation of Panel Data","what":"Benchmarks","title":"collapse and plm","text":"benchmarks provided collapse implementation native plm. dataset used far extended approx 1 million observations: data 21600 individuals (countries) observed 61 years (1960-2020), total number rows 1317600. can pull series life expectancy run benchmarks. Windows laptop benchmarks run 2x 2.2 GHZ Intel i5 processor, 8GB DDR3 RAM Samsung SSD hard drive. shows comparison flag data.table’s shift: dataset 1 million obs 20 thousand groups, 10 million obs 1 million groups? collapse functions scale efficiently data number groups grows large? simple benchmark: results show collapse functions perform well even number groups grows large. conclusion benchmark analysis collapse’s fast functions, without help plm classes, allow fast transformations panel data, enable R programmers econometricians implement high-performance panel data estimators without dive C/C++ resorting data.table metaprogramming.","code":"wlddevsmall <- get_vars(wlddev, c(\"iso3c\",\"year\",\"OECD\",\"PCGDP\",\"LIFEEX\",\"GINI\",\"ODA\")) wlddevsmall$iso3c <- as.character(wlddevsmall$iso3c) data <- replicate(100, wlddevsmall, simplify = FALSE) rm(wlddevsmall) uniquify <- function(x, i) {   x$iso3c <- paste0(x$iso3c, i)   x } data <- unlist2d(Map(uniquify, data, as.list(1:100)), idcols = FALSE) data <- pdata.frame(data, index = c(\"iso3c\", \"year\")) pdim(data) # Balanced Panel: n = 21600, T = 61, N = 1317600 library(microbenchmark) # Creating the extended panel series for Life Expectancy (l for large) LIFEEX_l <- data$LIFEEX str(LIFEEX_l) #  'pseries' Named num [1:1317600] 65.7 66.1 66.4 66.8 67.1 ... #  - attr(*, \"names\")= chr [1:1317600] \"ABW1-1960\" \"ABW1-1961\" \"ABW1-1962\" \"ABW1-1963\" ... #  - attr(*, \"index\")=Classes 'pindex' and 'data.frame':    1317600 obs. of  2 variables: #   ..$ iso3c: Factor w/ 21600 levels \"ABW1\",\"ABW10\",..: 1 1 1 1 1 1 1 1 1 1 ... #   ..$ year : Factor w/ 61 levels \"1960\",\"1961\",..: 1 2 3 4 5 6 7 8 9 10 ...  # Between Transformations microbenchmark(Between(LIFEEX_l, na.rm = TRUE), times = 10) # Unit: milliseconds #                             expr      min       lq     mean   median       uq      max neval #  Between(LIFEEX_l, na.rm = TRUE) 17.73594 18.71248 21.84342 20.13574 22.35853 37.94689    10 microbenchmark(fbetween(LIFEEX_l), times = 10) # Unit: milliseconds #                expr      min       lq     mean   median       uq      max neval #  fbetween(LIFEEX_l) 4.408771 4.639519 4.705529 4.718424 4.771498 4.908684    10  # Within Transformations microbenchmark(Within(LIFEEX_l, na.rm = TRUE), times = 10) # Unit: milliseconds #                            expr      min       lq     mean  median       uq      max neval #  Within(LIFEEX_l, na.rm = TRUE) 10.17887 10.74663 10.91092 10.8766 11.24224 11.37664    10 microbenchmark(fwithin(LIFEEX_l), times = 10) # Unit: milliseconds #               expr      min       lq     mean   median       uq      max neval #  fwithin(LIFEEX_l) 4.522218 4.550303 4.735344 4.644296 4.696017 5.297036    10  # Higher-Dimenional Between and Within Transformations microbenchmark(fhdbetween(LIFEEX_l), times = 10) # Unit: milliseconds #                  expr    min       lq    mean   median       uq      max neval #  fhdbetween(LIFEEX_l) 56.916 57.29971 66.0179 58.13864 76.50108 84.10625    10 microbenchmark(fhdwithin(LIFEEX_l), times = 10) # Unit: milliseconds #                 expr      min      lq     mean   median       uq      max neval #  fhdwithin(LIFEEX_l) 55.55906 56.2372 62.31852 56.56555 75.78784 77.20657    10  # Single Lag microbenchmark(lag(LIFEEX_l), times = 10) # Unit: milliseconds #           expr      min       lq     mean   median       uq      max neval #  lag(LIFEEX_l) 7.967776 8.144896 8.542879 8.632468 8.840092 8.949357    10 microbenchmark(flag(LIFEEX_l), times = 10) # Unit: milliseconds #            expr      min       lq     mean   median       uq     max neval #  flag(LIFEEX_l) 7.994057 8.038747 8.337862 8.180484 8.603481 9.12086    10  # Sequence of Lags / Leads microbenchmark(lag(LIFEEX_l, -1:3), times = 10) # Unit: milliseconds #                 expr     min       lq     mean   median       uq     max neval #  lag(LIFEEX_l, -1:3) 18.7525 19.29476 28.61876 27.95813 38.11081 39.5329    10 microbenchmark(flag(LIFEEX_l, -1:3), times = 10) # Unit: milliseconds #                  expr     min       lq     mean   median       uq      max neval #  flag(LIFEEX_l, -1:3) 15.5415 15.64335 21.10042 15.83998 33.37699 34.10265    10  # Single difference microbenchmark(diff(LIFEEX_l), times = 10) # Unit: milliseconds #            expr     min      lq     mean   median       uq      max neval #  diff(LIFEEX_l) 8.00525 8.16884 8.370421 8.368776 8.554404 8.733697    10 microbenchmark(fdiff(LIFEEX_l), times = 10) # Unit: milliseconds #             expr      min       lq   mean median       uq     max neval #  fdiff(LIFEEX_l) 7.937805 8.020502 8.3458 8.2451 8.426238 9.34923    10  # Iterated Difference microbenchmark(fdiff(LIFEEX_l, diff = 2), times = 10) # Unit: milliseconds #                       expr      min       lq     mean   median       uq      max neval #  fdiff(LIFEEX_l, diff = 2) 10.20129 10.62786 10.72184 10.77488 10.82326 11.21805    10  # Sequence of Lagged / Leaded and iterated differences microbenchmark(fdiff(LIFEEX_l, -1:3, 1:2), times = 10) # Unit: milliseconds #                        expr      min       lq     mean   median       uq      max neval #  fdiff(LIFEEX_l, -1:3, 1:2) 45.90159 52.22494 66.83236 53.21347 57.53222 187.8582    10  # Single Growth Rate microbenchmark(fgrowth(LIFEEX_l), times = 10) # Unit: milliseconds #               expr      min       lq    mean   median       uq      max neval #  fgrowth(LIFEEX_l) 8.222304 8.357153 8.69059 8.727158 8.884167 9.436683    10  # Single Log-Difference microbenchmark(fdiff(LIFEEX_l, log = TRUE), times = 10) # Unit: milliseconds #                         expr      min      lq     mean   median       uq      max neval #  fdiff(LIFEEX_l, log = TRUE) 12.41394 12.8583 15.06961 13.17156 13.61659 32.51989    10  # Panel Series to Matrix Conversion # system.time(as.matrix(LIFEEX_l))  This takes about 3 minutes to compute microbenchmark(psmat(LIFEEX_l), times = 10) # Unit: milliseconds #             expr      min       lq     mean   median       uq      max neval #  psmat(LIFEEX_l) 1.482478 1.500149 1.628028 1.520813 1.553941 2.438639    10 microbenchmark(L(data, cols = 3:6), times = 10) # Unit: milliseconds #                 expr      min       lq     mean median       uq      max neval #  L(data, cols = 3:6) 14.13692 14.43877 20.88276 18.865 19.73141 37.06244    10 library(data.table) setDT(data) # 'Improper' panel-lag microbenchmark(data[, shift(.SD), by = iso3c, .SDcols = 3:6], times = 10) # Unit: milliseconds #                                           expr      min       lq     mean   median      uq      max #  data[, shift(.SD), by = iso3c, .SDcols = 3:6] 176.5308 199.9415 215.6897 204.0719 230.089 268.9992 #  neval #     10  # This does what L is actually doing (without sorting the data) microbenchmark(data[order(year), shift(.SD), by = iso3c, .SDcols = 3:6], times = 10) # Unit: milliseconds #                                                      expr      min       lq     mean   median #  data[order(year), shift(.SD), by = iso3c, .SDcols = 3:6] 193.9684 210.7025 213.7664 213.0727 #        uq      max neval #  221.9783 226.3685    10 x <- rnorm(1e7)                                     # 10 million obs g <- qF(rep(1:1e6, each = 10), na.exclude = FALSE)  # 1 million individuals t <- qF(rep(1:10, 1e6), na.exclude = FALSE)         # 10 time-periods per individual  microbenchmark(fbetween(x, g), times = 10) # Unit: milliseconds #            expr      min       lq     mean   median       uq      max neval #  fbetween(x, g) 51.66189 53.60693 91.00168 62.54655 73.87835 233.3696    10 microbenchmark(fwithin(x, g), times = 10) # Unit: milliseconds #           expr      min       lq    mean   median       uq      max neval #  fwithin(x, g) 43.46291 44.03954 77.0216 45.33919 58.65132 196.7248    10 microbenchmark(flag(x, 1, g, t), times = 10) # Unit: milliseconds #              expr      min       lq     mean   median       uq      max neval #  flag(x, 1, g, t) 42.65382 55.05332 87.72527 59.55935 80.86143 210.8074    10 microbenchmark(flag(x, -1:1, g, t), times = 10) # Unit: milliseconds #                 expr      min      lq     mean  median       uq      max neval #  flag(x, -1:1, g, t) 92.19842 92.5559 162.8994 166.736 228.6354 239.6953    10 microbenchmark(fdiff(x, 1, 1, g, t), times = 10) # Unit: milliseconds #                  expr      min       lq     mean   median       uq      max neval #  fdiff(x, 1, 1, g, t) 42.51778 46.29306 82.27838 53.85735 67.54295 205.0114    10 microbenchmark(fdiff(x, 1, 2, g, t), times = 10) # Unit: milliseconds #                  expr     min       lq     mean   median       uq      max neval #  fdiff(x, 1, 2, g, t) 59.9363 62.11689 84.42818 69.85072 75.38506 217.1431    10 microbenchmark(fdiff(x, -1:1, 1:2, g, t), times = 10) # Unit: milliseconds #                       expr      min       lq     mean  median       uq      max neval #  fdiff(x, -1:1, 1:2, g, t) 163.5046 182.9127 246.2855 250.664 301.4046 339.1415    10"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"part-2-fast-exploration-of-panel-data","dir":"Articles","previous_headings":"","what":"Part 2: Fast Exploration of Panel Data","title":"collapse and plm","text":"collapse also provides essential functions summarize explore panel data, fast check variation different dimensions, fast summary-statistics panel data, panel-auto, partial-auto cross-correlation functions, fast F-test test fixed effects exclusion restrictions (large) panel data models. Panel data matrix conversion allows application correlational unsupervised learning tools PCA, clustering dynamic factor analysis.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"variation-check-for-panel-data","dir":"Articles","previous_headings":"Part 2: Fast Exploration of Panel Data","what":"2.1 Variation Check for Panel Data","title":"collapse and plm","text":"function varying can used check panel-dimensions different variable variation. passed pdata.frame, varying default takes first identifier checks variation within dimension. Alternatively index variable combination index variables can specified: Another possibility checking variation within group: varying also pseries method. code checks time-variation GINI index within country. NA returned observations within particular country. like gave information variation, also invoke functions fndistinct fsd, pseries methods:","code":"# This checks for any variation within \"iso3c\", the first index variable: TRUE means data vary within country i.e. over time. varying(pwlddev) # country    date    year  decade  region  income    OECD   PCGDP  LIFEEX    GINI     ODA     POP  #   FALSE    TRUE    TRUE    TRUE   FALSE   FALSE   FALSE    TRUE    TRUE    TRUE    TRUE    TRUE # This checks any variation within time variable, i.e. cross-sectional variation. varying(pwlddev, effect = \"year\") # country   iso3c    date  decade  region  income    OECD   PCGDP  LIFEEX    GINI     ODA     POP  #    TRUE    TRUE   FALSE   FALSE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE # This checks cross-sectional variation within each year for 4 indicators. head(varying(pwlddev, effect = \"year\", cols = 9:12, any_group = FALSE)) #      PCGDP LIFEEX GINI  ODA # 1960  TRUE   TRUE   NA TRUE # 1961  TRUE   TRUE   NA TRUE # 1962  TRUE   TRUE   NA TRUE # 1963  TRUE   TRUE   NA TRUE # 1964  TRUE   TRUE   NA TRUE # 1965  TRUE   TRUE   NA TRUE head(varying(pwlddev$GINI, any_group = FALSE), 20) #  ABW  AFG  AGO  ALB  AND  ARE  ARG  ARM  ASM  ATG  AUS  AUT  AZE  BDI  BEL  BEN  BFA  BGD  BGR  BHR  #   NA   NA TRUE TRUE   NA TRUE TRUE TRUE   NA   NA TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE   NA head(fndistinct(pwlddev$GINI, index(pwlddev, \"iso3c\")), 20) # ABW AFG AGO ALB AND ARE ARG ARM ASM ATG AUS AUT AZE BDI BEL BEN BFA BGD BGR BHR  #   0   0   3   9   0   2  29  20   0   0   9  16   5   4  16   3   5   9  12   0  head(round(fsd(pwlddev$GINI, index(pwlddev, \"iso3c\")), 2), 20) #  ABW  AFG  AGO  ALB  AND  ARE  ARG  ARM  ASM  ATG  AUS  AUT  AZE  BDI  BEL  BEN  BFA  BGD  BGR  BHR  #   NA   NA 5.18 2.47   NA 4.60 3.84 2.76   NA   NA 1.19 1.76 4.85 4.37 1.71 4.60 5.98 3.02 2.58   NA"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"summary-statistics-for-panel-data","dir":"Articles","previous_headings":"Part 2: Fast Exploration of Panel Data","what":"2.2 Summary Statistics for Panel Data","title":"collapse and plm","text":"Efficient summary statistics panel data long implemented statistical softwares. command qsu, shorthand ‘quick-summary’, efficient summary statistics command inspired xtsummarize command Stata statistical software. computes default set 5 statistics (N, mean, sd, min max) can also computed higher moments (skewness kurtosis) single pass data (using numerically stable online algorithm generalized Welford’s Algorithm variance computations). panel data, qsu computes statistics just raw data, also -transformed within-transformed data: Key statistics look summary sample size standard-deviation decomposed -individuals within-individuals standard-deviation: GDP per Capita 8995 observations panel series 203 countries, average 44.31 observations (time-periods T) per country. -country standard deviation 19600 USD, around 3-times larger within-country (-time) standard deviation 6300 USD. Regarding mean, -mean, computed cross-sectional average country averages, usually differs slightly overall average taken across data points. within-transformed data computed summarized overall mean added back (.e. fwithin(PCGDP, mean = \"overall.mean\")). can also groupwise panel-statistics qsu also supports weights (shown): noted grouping applied independently data-transformation, .e. data first transformed, grouped statistics calculated transformed data. computation statistics efficient: Using transformation functions functions pwcor pwcov, can also easily explore correlation structure data: correlations show (cross-country) relationships macro-variables quite strong, within countries relationships much weaker, example seems significant relationship GDP per Capita either inequality ODA received within countries time.","code":"qsu(pwlddev, cols = 9:12, higher = TRUE) # , , PCGDP #  #              N/T        Mean          SD          Min         Max    Skew     Kurt # Overall     9470   12048.778  19077.6416     132.0776  196061.417  3.1276  17.1154 # Between      206  12962.6054  20189.9007     253.1886   141200.38  3.1263  16.2299 # Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254  0.6576  17.2003 #  # , , LIFEEX #  #              N/T     Mean       SD      Min      Max     Skew    Kurt # Overall    11670  64.2963  11.4764   18.907  85.4171  -0.6748  2.6718 # Between      207  64.9537   9.8936  40.9663  85.4171  -0.5012  2.1693 # Within   56.3768  64.2963   6.0842  32.9068  84.4198  -0.2643  3.7027 #  # , , GINI #  #              N/T     Mean      SD      Min      Max    Skew    Kurt # Overall     1744  38.5341  9.2006     20.7     65.8   0.596  2.5329 # Between      167  39.4233  8.1356  24.8667  61.7143  0.5832  2.8256 # Within   10.4431  38.5341  2.9277  25.3917  55.3591  0.3263  5.3389 #  # , , ODA #  #              N/T        Mean          SD              Min             Max    Skew      Kurt # Overall     8608  454'720131  868'712654      -997'679993  2.56715605e+10  6.9832   114.889 # Between      178  439'168412  569'049959       468717.916  3.62337432e+09   2.355    9.9487 # Within   48.3596  454'720131  650'709624  -2.44379420e+09  2.45610972e+10  9.6047  263.3716 qsu(pwlddev, ~ income, cols = 9:12, higher = TRUE) # , , Overall, PCGDP #  #                       N/T        Mean          SD       Min         Max    Skew     Kurt # High income          3179  30280.7283  23847.0483  932.0417  196061.417  2.1702  10.3425 # Low income           1311    597.4053    288.4392  164.3366   1864.7925  1.2385   4.7115 # Lower middle income  2246   1574.2535    858.7183  144.9863   4818.1922  0.9093   3.7153 # Upper middle income  2734   4945.3258   2979.5609  132.0776  20532.9523  1.2286   4.9391 #  # , , Between, PCGDP #  #                      N/T        Mean          SD        Min         Max    Skew    Kurt # High income           71  30280.7283  20908.5323  5413.4495   141200.38  2.1347  9.9673 # Low income            28    597.4053    243.8219   253.1886   1357.3326  1.4171  5.3137 # Lower middle income   47   1574.2535    676.3157   444.2899   2896.8682  0.3562  2.2358 # Upper middle income   60   4945.3258   2327.3834   1604.595  13344.5423    1.24  4.7803 #  # , , Within, PCGDP #  #                          N/T       Mean          SD          Min         Max    Skew    Kurt # High income          44.7746  12048.778  11467.9987  -33504.8721  76767.5254  0.3924  6.0523 # Low income           46.8214  12048.778    154.1039   11606.2382   12698.296  0.5098  4.0676 # Lower middle income  47.7872  12048.778    529.1449   10377.7234  14603.1055  0.7658  5.4272 # Upper middle income  45.5667  12048.778    1860.395    4846.3834  24883.1246  0.6858  7.8469 #  # , , Overall, LIFEEX #  #                       N/T     Mean      SD     Min      Max     Skew    Kurt # High income          3831  73.6246  5.6693  42.672  85.4171  -1.0067  5.5553 # Low income           1800  49.7301  9.0944  26.172    74.43   0.2748  2.6721 # Lower middle income  2790  58.1481  9.3115  18.907   76.699  -0.3406  2.6845 # Upper middle income  3249  66.6466   7.537  36.535   80.279  -1.0988  4.2262 #  # , , Between, LIFEEX #  #                      N/T     Mean      SD      Min      Max     Skew    Kurt # High income           73  73.6246  3.3499  64.0302  85.4171  -0.6537  2.9946 # Low income            30  49.7301  4.8321  40.9663   66.945   1.5195  6.6802 # Lower middle income   47  58.1481  5.9945  45.7687  71.6078   0.0352  2.2126 # Upper middle income   57  66.6466  4.9955   48.057  74.0504  -1.3647   5.303 #  # , , Within, LIFEEX #  #                          N/T     Mean      SD      Min      Max     Skew    Kurt # High income          52.4795  64.2963  4.5738  42.9381  78.1271  -0.4838  3.8923 # Low income                60  64.2963  7.7045  41.5678  84.4198   0.0402  2.6086 # Lower middle income  59.3617  64.2963  7.1253  32.9068  83.9918  -0.2522   3.181 # Upper middle income       57  64.2963  5.6437  41.4342  83.0122   -0.507  4.0355 #  # , , Overall, GINI #  #                      N/T     Mean      SD   Min   Max    Skew    Kurt # High income          680  33.3037  6.7885  20.7  58.9  1.4864  5.6772 # Low income           107  41.1327  6.5767  29.5  65.8  0.7523   4.236 # Lower middle income  369  40.0504  9.3032    24  63.2  0.4388  2.2218 # Upper middle income  588  43.1585  8.9549  25.2  64.8  0.0814  2.3517 #  # , , Between, GINI #  #                      N/T     Mean      SD      Min      Max    Skew    Kurt # High income           41  33.3037  6.5238  24.8667  53.6296  1.5091  5.3913 # Low income            28  41.1327  5.1706  32.1333    58.75  0.6042  4.0473 # Lower middle income   46  40.0504  8.4622  27.6955   54.925   0.334   1.797 # Upper middle income   52  43.1585  8.4359  27.9545  61.7143  0.0336  2.2441 #  # , , Within, GINI #  #                          N/T     Mean      SD      Min      Max     Skew    Kurt # High income          16.5854  38.5341  1.8771  31.1841  45.8841  -0.0818   4.902 # Low income            3.8214  38.5341  4.0643  29.4591  55.3591   0.6766  5.1025 # Lower middle income   8.0217  38.5341  3.8654  27.9452  55.1008   0.4093  4.0058 # Upper middle income  11.3077  38.5341  3.0043  25.3917  48.0131   0.0728  3.5115 #  # , , Overall, ODA #  #                       N/T        Mean              SD          Min             Max     Skew # High income          1575  153'663194      425'918409  -464'709991  4.34612988e+09   5.2505 # Low income           1692  631'660165      941'498380      -500000  1.04032100e+10   4.4628 # Lower middle income  2544  692'072692  1.02452490e+09  -605'969971  1.18790801e+10   3.7913 # Upper middle income  2797  301'326218      765'116131  -997'679993  2.56715605e+10  16.3123 #                          Kurt # High income           36.2748 # Low income            32.1305 # Lower middle income   25.2442 # Upper middle income  464.8625 #  # , , Between, ODA #  #                      N/T        Mean          SD          Min             Max    Skew     Kurt # High income           42  153'663194  339'972909   468717.916  2.05456932e+09  3.9522  19.0792 # Low income            30  631'660165  466'265486    91'536334  1.67220583e+09  0.9769   2.6602 # Lower middle income   47  692'072692  765'003585  28'919000.2  3.62337432e+09  2.0429   7.2664 # Upper middle income   59  301'326218  382'148153    13'160000  1.91297800e+09  2.1072   7.0291 #  # , , Within, ODA #  #                          N/T        Mean          SD              Min             Max     Skew # High income             37.5  454'720131  256'563661      -920'977647  2.87632242e+09   2.2074 # Low income              56.4  454'720131  817'933797  -1.19519570e+09  9.18572426e+09   3.8872 # Lower middle income  54.1277  454'720131  681'484247  -2.44379420e+09  1.12814455e+10   3.8965 # Upper middle income  47.4068  454'720131  662'846500  -2.04042108e+09  2.45610972e+10  19.6351 #                          Kurt # High income           28.8682 # Low income            33.5194 # Lower middle income   47.7246 # Upper middle income  657.3041 qsu(LIFEEX_l) #               N/T     Mean       SD      Min      Max # Overall  1'167000  64.2963  11.4759   18.907  85.4171 # Between     20700  64.9537     9.87  40.9663  85.4171 # Within    56.3768  64.2963   6.0839  32.9068  84.4198  microbenchmark(qsu(LIFEEX_l)) # Unit: milliseconds #           expr     min       lq     mean   median       uq      max neval #  qsu(LIFEEX_l) 9.49355 10.25679 11.07317 10.37214 10.78839 50.22574   100 # Overall pairwise correlations with pairwise observation count and significance testing (* = significant at 5% level) pwcor(get_vars(pwlddev, 9:12), N = TRUE, P = TRUE) #               PCGDP        LIFEEX         GINI          ODA # PCGDP    1   (9470)   .57* (9022) -.44* (1735) -.16* (7128) # LIFEEX  .57* (9022)   1   (11670) -.35* (1742) -.02  (8142) # GINI   -.44* (1735)  -.35* (1742)   1   (1744) -.20* (1109) # ODA    -.16* (7128)  -.02  (8142) -.20* (1109)   1   (8608)  # Between correlations pwcor(fmean(get_vars(pwlddev, 9:12), pwlddev$iso3c), N = TRUE, P = TRUE) #              PCGDP      LIFEEX        GINI         ODA # PCGDP    1   (206)  .60* (199) -.42* (165) -.25* (172) # LIFEEX  .60* (199)   1   (207) -.40* (165) -.21* (172) # GINI   -.42* (165) -.40* (165)   1   (167) -.19* (145) # ODA    -.25* (172) -.21* (172) -.19* (145)   1   (178)  # Within correlations pwcor(W(pwlddev, cols = 9:12, keep.ids = FALSE), N = TRUE, P = TRUE) #               W.PCGDP      W.LIFEEX       W.GINI        W.ODA # W.PCGDP    1   (9470)   .31* (9022) -.01  (1735) -.01  (7128) # W.LIFEEX  .31* (9022)   1   (11670) -.16* (1742)  .17* (8142) # W.GINI   -.01  (1735)  -.16* (1742)   1   (1744) -.08* (1109) # W.ODA    -.01  (7128)   .17* (8142) -.08* (1109)   1   (8608)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"exploring-panel-data-in-matrix-array-form","dir":"Articles","previous_headings":"Part 2: Fast Exploration of Panel Data","what":"2.3 Exploring Panel Data in Matrix / Array Form","title":"collapse and plm","text":"can take single panel series GDP per Capita explore : plot chunk PLMGDPmat plot chunk PLMGDPmat plot chunk PLMGDPmat also nice plot-method applied panel series arrays returned psmat applied panel data.frame: plot chunk pwlddev_plot explored cross-sectional relationship different national GDP series. Now explore time-dependence panel-vectors whole:","code":"# Generating a (transposed) matrix of country GDPs per capita tGDPmat <- psmat(PCGDP, transpose = TRUE) tGDPmat[1:10, 1:10] #      ABW AFG AGO ALB AND ARE  ARG ARM ASM ATG # 1960  NA  NA  NA  NA  NA  NA 5643  NA  NA  NA # 1961  NA  NA  NA  NA  NA  NA 5853  NA  NA  NA # 1962  NA  NA  NA  NA  NA  NA 5711  NA  NA  NA # 1963  NA  NA  NA  NA  NA  NA 5323  NA  NA  NA # 1964  NA  NA  NA  NA  NA  NA 5773  NA  NA  NA # 1965  NA  NA  NA  NA  NA  NA 6286  NA  NA  NA # 1966  NA  NA  NA  NA  NA  NA 6152  NA  NA  NA # 1967  NA  NA  NA  NA  NA  NA 6255  NA  NA  NA # 1968  NA  NA  NA  NA  NA  NA 6461  NA  NA  NA # 1969  NA  NA  NA  NA  NA  NA 6981  NA  NA  NA  # plot the matrix (it will plot correctly no matter how the matrix is transposed) plot(tGDPmat, main = \"GDP per Capita\") # Taking series with more than 20 observation suffsamp <- tGDPmat[, fnobs(tGDPmat) > 20]  # Minimum pairwise observations between any two series: min(pwnobs(suffsamp)) # [1] 16  # We can use the pairwise-correlations of the annual growth rates to hierarchically cluster the economies: plot(hclust(as.dist(1-pwcor(G(suffsamp))))) # Finally we could do PCA on the growth rates: eig <- eigen(pwcor(G(suffsamp))) plot(seq_col(suffsamp), eig$values/sum(eig$values)*100, xlab = \"Number of Principal Components\", ylab = \"% Variance Explained\", main = \"Screeplot\") plot(psmat(pwlddev, cols = 9:12), legend = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"panel--auto--partial-auto-and-cross-correlation-functions","dir":"Articles","previous_headings":"Part 2: Fast Exploration of Panel Data","what":"2.4 Panel- Auto-, Partial-Auto and Cross-Correlation Functions","title":"collapse and plm","text":"functions psacf, pspacf psccf mimic stats::acf, stats::pacf stats::ccf panel-vectors panel data.frames. compute panel series autocorrelation function data: plot chunk plm_psacf computation conducted first scaling centering (.e. standardizing) panel-vectors groups (using fscale, default argument gscale = TRUE), taking covariance series matrix properly computed panel-lags (using flag), dividing variance overall series (using fvar). similar way can compute Partial-ACF (using multivariate Yule-Walker decomposition ACF, stats::pacf), plot chunk plm_pspacf panel-cross-correlation function GDP per capita life expectancy (already contained ACF plot ): plot chunk plm_psccf","code":"psacf(pwlddev, cols = 9:12) pspacf(pwlddev, cols = 9:12) psccf(PCGDP, LIFEEX)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"testing-for-individual-specific-and-time-effects","dir":"Articles","previous_headings":"Part 2: Fast Exploration of Panel Data","what":"2.5 Testing for Individual Specific and Time-Effects","title":"collapse and plm","text":"final step exploration, analyze series simple models significance explanatory power individual time-fixed effects, without going way running Hausman Test fixed vs. random effects fully specified model. main function fFtest efficiently computes fast R-Squared based F-test exclusion restrictions models potentially involving many factors. default (argument full.df = TRUE) degrees freedom test adjusted make identical F-statistic regressing series set country time dummies1. test correlation country time-means GDP Life-Expectancy: can also test significance individual time-fixed effects () regression GDP life expectancy ODA received: can expected cross-country data, individual time-fixed effects play large role explaining data, effects correlated across series, suggesting fixed-effects model types fixed-effects appropriate. round things , compute Hausman test Fixed vs. Random effects, confirms conclusion:","code":"# Testing GDP per Capita fFtest(PCGDP, index(PCGDP))    # Testing individual and time-fixed effects #    R-Sq.      DF1      DF2  F-Stat.  P-value  #    0.905      264     9205  330.349    0.000 fFtest(PCGDP, index(PCGDP, 1)) # Testing individual effects #    R-Sq.      DF1      DF2  F-Stat.  P-value  #    0.876      215     9254  303.476    0.000 fFtest(PCGDP, index(PCGDP, 2)) # Testing time effects #    R-Sq.      DF1      DF2  F-Stat.  P-value  #    0.027       60     9409    4.276    0.000  # Same for Life-Expectancy fFtest(LIFEEX, index(LIFEEX))    # Testing individual and time-fixed effects #     R-Sq.       DF1       DF2   F-Stat.   P-value  #     0.924       265     11404   519.762     0.000 fFtest(LIFEEX, index(LIFEEX, 1)) # Testing individual effects #     R-Sq.       DF1       DF2   F-Stat.   P-value  #     0.719       215     11454   136.276     0.000 fFtest(LIFEEX, index(LIFEEX, 2)) # Testing time effects #     R-Sq.       DF1       DF2   F-Stat.   P-value  #     0.218        60     11609    54.075     0.000 cor.test(B(PCGDP), B(LIFEEX)) # Testing correlation of country means #  #   Pearson's product-moment correlation #  # data:  B(PCGDP) and B(LIFEEX) # t = 78.752, df = 9020, p-value < 2.2e-16 # alternative hypothesis: true correlation is not equal to 0 # 95 percent confidence interval: #  0.6259141 0.6503737 # sample estimates: #      cor  # 0.638305  cor.test(B(PCGDP, effect = 2), B(LIFEEX, effect = 2)) # Same for time-means #  #   Pearson's product-moment correlation #  # data:  B(PCGDP, effect = 2) and B(LIFEEX, effect = 2) # t = 325.6, df = 9020, p-value < 2.2e-16 # alternative hypothesis: true correlation is not equal to 0 # 95 percent confidence interval: #  0.9583431 0.9615804 # sample estimates: #       cor  # 0.9599938 fFtest(PCGDP, index(PCGDP), get_vars(pwlddev, c(\"LIFEEX\",\"ODA\")))    # Testing individual and time-fixed effects #                     R-Sq.  DF1  DF2  F-Stat.  P-Value # Full Model          0.915  227 6682  316.551    0.000 # Restricted Model    0.162    2 6907  668.816    0.000 # Exclusion Rest.     0.753  225 6682  262.732    0.000 fFtest(PCGDP, index(PCGDP, 2), get_vars(pwlddev, c(\"iso3c\",\"LIFEEX\",\"ODA\")))    # Testing time-fixed effects #                     R-Sq.  DF1  DF2  F-Stat.  P-Value # Full Model          0.915  227 6682  316.551    0.000 # Restricted Model    0.909  168 6741  403.168    0.000 # Exclusion Rest.     0.005   59 6682    7.238    0.000 phtest(PCGDP ~ LIFEEX, data = pwlddev) #  #   Hausman Test #  # data:  PCGDP ~ LIFEEX # chisq = 397.04, df = 1, p-value < 2.2e-16 # alternative hypothesis: one model is inconsistent"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"part-3-programming-panel-data-estimators","dir":"Articles","previous_headings":"","what":"Part 3: Programming Panel Data Estimators","title":"collapse and plm","text":"central goal collapse package facilitate advanced fast programming data. primary field application fast functions introduced program efficient panel data estimators. section walk short example can done. application implementation Hausman Taylor (1981) estimator, considering general case currently implemented plm package. Hausman Taylor (1981), general scenario, linear panel-model form y_{} = \\beta_1X_{1it} + \\beta_2X_{2it} + \\beta_3Z_{1i} + \\beta_4Z_{2i} + \\alpha_i + \\gamma_t + \\epsilon \\alpha_i denotes unobserved individual specific effects \\gamma_t denotes unobserved global events. model 4 kinds covariates: Time-Varying covariates X_{1it} uncorrelated individual specific effect \\alpha_i, E[X_{1it}\\alpha_i] = 0. may case E[X_{1it}\\gamma_t] \\neq 0 Time-Varying covariates X_{2it} E[X_{2it}\\alpha_i] \\neq 0 possibly E[X_{2it}\\gamma_t] \\neq 0 Time-Invariant covariates Z_{1i} E[Z_{1i}\\alpha_i] = 0 Time-Invariant covariates Z_{2i} E[Z_{2i}\\alpha_i] \\neq 0 main estimation problem arises E[Z_{2i}\\alpha_i] \\neq 0, usually prevent us estimating \\beta_4 since taking within-transformation (fixed effects) remove Z_{2i} equation. Hausman Taylor (1981) stipulated since E[X_{1it}\\alpha_i] = 0, use X_{1i.} .e. -transformed X_{1it} instrument Z_{2i}. propose IV/2SLS estimation whole equation within-transformed covariates \\tilde{X}_{1it} \\tilde{X}_{2it} used instrument X_{1it} X_{2it}, X_{1i.} instruments Z_{2i}. Assuming missing values removed beforehand, also taking account possibility E[X_{1it}\\gamma_t] \\neq 0 E[X_{2it}\\gamma_t] \\neq 0 (.e. accounting time fixed-effects), estimator can coded follows:    estimator written way variables type X_{2it} Z_{1i} optional, also includes option also project time-FE . expected inputs X_{1it} (X1), X_{2it} (X2) column-subsets pdata.frame. coded estimator, good example run . tried squeeze example wlddev data used far vignette. quite crappy suffers weak-IV problem, sake illustration lets : want estimate panel-regression life-expectancy GDP per Capita, ODA received, GINI index time-invariant dummy indicating whether country OECD member. variables except dummy enter logs, elasticity regression.     < Using GINI index cost lot observations brought sample size 918, GINI index key variable follows. Clearly OECD dummy time-invariant. run Hausman-tests fixed vs. random effects determine covariates might correlated unobserved individual effects, model appropriate. tests suggest GDP per Capita ODA correlated country-specific unobservables affecting life-expectancy, overall fixed-effects model appropriate. However, Hausman test GINI index fails reject: Country specific unobservables affecting average life-expectancy necessarily correlated level inequality across countries. Now want include OECD dummy regression, use fixed-effects wipe-dummy well. dummy uncorrelated country-specific unobservables affecting life-expectancy (\\alpha_i), use solution suggested Mundlak (1978) simply add -transformed versions PCGDP ODA regression (addition PCGDP ODA levels), ‘control’ part PCGDP ODA correlated \\alpha_i (IV literature known control-function approach). however OECD dummy correlated \\alpha_i, need use Hausman Taylor (1981) estimator. suggest 2 methods testing correlation: interpret test results rejecting hypothesis dummy uncorrelated \\alpha_i, thus case Hausman Taylor (1981) : OECD dummy Z_{2i} E[Z_{2i}\\alpha_i]\\neq 0. Hausman tests suggested GINI index variable uncorrelated \\alpha_i, thus GINI X_{1it} E[X_{1it}\\alpha_i] = 0. Finally PCGDP ODA jointly constitute X_{2it}, Hausman tests strongly suggested E[X_{2it}\\alpha_i] \\neq 0. Z_{1i} setup, .e. time-invariant variable uncorrelated \\alpha_i. Hausman Taylor (1981) estimator stipulates instrument OECD dummy X_{1i.}, -transformed GINI index. Let us therefore test regression dummy instrument see good (.e. relevant) instrument: 0 R-Squared F-Statistic 0.21 suggest instrument weak indeed, rubbish precise, thus implementation HT estimator also rubbish example, still good illustration purposes: Now central questions course: computationally efficient estimator? Let us try re-run data generated benchmark Part 1: around 100,000 obs 13,000 groups unbalanced panel, computation involving 3 grouped centering 1 grouped averaging task well 2 list-matrix conversions IV-procedure took 10 milliseconds individual effects, 40 - 45 milliseconds individual time-fixed effects (projected iteratively). leave room running much larger data.","code":"HT_est <- function(y, X1, Z2, X2 = NULL, Z1 = NULL, time.FE = FALSE) {    # Create matrix of independent variables   X <- cbind(Intercept = 1, do.call(cbind, c(X1, X2, Z1, Z2)))    # Create instrument matrix: if time.FE, higher-order demean X1 and X2, else normal demeaning   IVS <- cbind(Intercept = 1, do.call(cbind,                c(if(time.FE) fhdwithin(X1, na.rm = FALSE) else fwithin(X1, na.rm = FALSE),                  if(is.null(X2)) X2 else if(time.FE) fhdwithin(X2, na.rm = FALSE) else fwithin(X2, na.rm = FALSE),                  Z1, fbetween(X1, na.rm = FALSE))))    if(length(IVS) == length(X)) { # The IV estimator case     return(drop(solve(crossprod(IVS, X), crossprod(IVS, y))))   } else { # The 2SLS case     Xhat <- qr.fitted(qr(IVS), X)  # First stage     return(drop(qr.coef(qr(Xhat), y)))   # Second stage   } } dat <- get_vars(wlddev, c(\"iso3c\",\"year\",\"OECD\",\"PCGDP\",\"LIFEEX\",\"GINI\",\"ODA\")) get_vars(dat, 4:7) <- lapply(get_vars(dat, 4:7), log) # Taking logs of the data dat$OECD <- as.numeric(dat$OECD)                      # Creating OECD dummy dat <- pdata.frame(fdroplevels(na_omit(dat)),         # Creating Panel data.frame, after removing missing values                    index = c(\"iso3c\", \"year\"))        # and dropping unused factor levels pdim(dat) # Unbalanced Panel: n = 134, T = 1-34, N = 1068 varying(dat) #   year   OECD  PCGDP LIFEEX   GINI    ODA  #   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE # This tests whether each of the covariates is correlated with alpha_i phtest(LIFEEX ~ PCGDP, dat)  # Likely correlated #  #   Hausman Test #  # data:  LIFEEX ~ PCGDP # chisq = 17.495, df = 1, p-value = 2.881e-05 # alternative hypothesis: one model is inconsistent phtest(LIFEEX ~ ODA, dat)    # Likely correlated #  #   Hausman Test #  # data:  LIFEEX ~ ODA # chisq = 43.925, df = 1, p-value = 3.413e-11 # alternative hypothesis: one model is inconsistent phtest(LIFEEX ~ GINI, dat)   # Likely not correlated ! #  #   Hausman Test #  # data:  LIFEEX ~ GINI # chisq = 0.56851, df = 1, p-value = 0.4509 # alternative hypothesis: one model is inconsistent phtest(LIFEEX ~ PCGDP + ODA + GINI, dat)  # Fixed Effects is the appropriate model for this regression #  #   Hausman Test #  # data:  LIFEEX ~ PCGDP + ODA + GINI # chisq = 24.198, df = 3, p-value = 2.272e-05 # alternative hypothesis: one model is inconsistent # Testing the correlation between OECD dummy and the Between-transformed Life-Expectancy (i.e. not accounting for other covariates) cor.test(dat$OECD, B(dat$LIFEEX)) # -> Significant correlation of 0.21 #  #   Pearson's product-moment correlation #  # data:  dat$OECD and B(dat$LIFEEX) # t = 6.797, df = 1066, p-value = 1.774e-11 # alternative hypothesis: true correlation is not equal to 0 # 95 percent confidence interval: #  0.1456048 0.2606109 # sample estimates: #       cor  # 0.2038109  # Getting the fixed-effects (estimates of alpha_i) from the model (i.e. accounting for the other covariates) fe <- fixef(plm(LIFEEX ~ PCGDP + ODA + GINI, dat, model = \"within\")) mODA <- fmean(dat$ODA, dat$iso3c) # Again testing the correlation cor.test(fe, mODA[match(names(fe), names(mODA))]) # -> Not Significant.. but probably due to small sample size, the correlation is still 0.13 #  #   Pearson's product-moment correlation #  # data:  fe and mODA[match(names(fe), names(mODA))] # t = 1.1218, df = 132, p-value = 0.264 # alternative hypothesis: true correlation is not equal to 0 # 95 percent confidence interval: #  -0.07362567  0.26243949 # sample estimates: #        cor  # 0.09717608 # This computes the regression of OECD on the GINI instrument: Weak IV problem !! fFtest(dat$OECD, B(dat$GINI)) #    R-Sq.      DF1      DF2  F-Stat.  P-value  #    0.000        1     1066    0.153    0.695 HT_est(y = dat$LIFEEX,        X1 = get_vars(dat, \"GINI\"),        Z2 = get_vars(dat, \"OECD\"),        X2 = get_vars(dat, c(\"PCGDP\",\"ODA\"))) #    Intercept         GINI        PCGDP          ODA         OECD  #  3.638486969 -0.035596160  0.120981946  0.005744747 -5.862368476 dat <- get_vars(data, c(\"iso3c\",\"year\",\"OECD\",\"PCGDP\",\"LIFEEX\",\"GINI\",\"ODA\")) get_vars(dat, 4:7) <- lapply(get_vars(dat, 4:7), log) # Taking logs of the data dat$OECD <- as.numeric(dat$OECD)                      # Creating OECD dummy dat <- pdata.frame(fdroplevels(na_omit(dat)),         # Creating Panel data.frame, after removing missing values                    index = c(\"iso3c\", \"year\"))        # and dropping unused factor levels pdim(dat) # Unbalanced Panel: n = 13400, T = 1-34, N = 106800 varying(dat) #   year   OECD  PCGDP LIFEEX   GINI    ODA  #   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE  library(microbenchmark) microbenchmark(HT_est = HT_est(y = dat$LIFEEX,     # The estimator as before                       X1 = get_vars(dat, \"GINI\"),                       Z2 = get_vars(dat, \"OECD\"),                       X2 = get_vars(dat, c(\"PCGDP\",\"ODA\"))),               HT_est_TFE =  HT_est(y = dat$LIFEEX, # Also Projecting out Time-FE                       X1 = get_vars(dat, \"GINI\"),                       Z2 = get_vars(dat, \"OECD\"),                       X2 = get_vars(dat, c(\"PCGDP\",\"ODA\")),                       time.FE = TRUE)) # Unit: milliseconds #        expr       min       lq      mean    median        uq      max neval #      HT_est  7.919437  8.46937  9.761301  8.869612  9.508597 45.08717   100 #  HT_est_TFE 22.501128 23.18640 25.387041 23.469835 24.490612 85.96462   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_plm.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"collapse and plm","text":"Hausman J, Taylor W (1981). “Panel Data Unobservable Individual Effects.” Econometrica, 49, 1377–1398. Mundlak, Yair. 1978. “Pooling Time Series Cross Section Data.” Econometrica 46 (1): 69–85. Cochrane, D. & Orcutt, G. H. (1949). “Application Least Squares Regression Relationships Containing Auto-Correlated Error Terms”. Journal American Statistical Association. 44 (245): 32–61. Prais, S. J. & Winsten, C. B. (1954). “Trend Estimators Serial Correlation”. Cowles Commission Discussion Paper . 383. Chicago.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"summarising-sf-data-frames","dir":"Articles","previous_headings":"","what":"Summarising sf Data Frames","title":"collapse and sf","text":"Computing summary statistics sf data frames automatically excludes ‘geometry’ column:","code":"# Which columns have at least 2 non-missing distinct values varying(nc)  #      AREA PERIMETER     CNTY_   CNTY_ID      NAME      FIPS    FIPSNO  CRESS_ID     BIR74     SID74  #      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE  #   NWBIR74     BIR79     SID79   NWBIR79  #      TRUE      TRUE      TRUE      TRUE  # Quick summary stats qsu(nc) #              N     Mean         SD    Min    Max # AREA       100   0.1263     0.0492  0.042  0.241 # PERIMETER  100    1.673     0.4823  0.999   3.64 # CNTY_      100  1985.96   106.5166   1825   2241 # CNTY_ID    100  1985.96   106.5166   1825   2241 # NAME       100        -          -      -      - # FIPS       100        -          -      -      - # FIPSNO     100    37100     58.023  37001  37199 # CRESS_ID   100     50.5    29.0115      1    100 # BIR74      100  3299.62  3848.1651    248  21588 # SID74      100     6.67     7.7812      0     44 # NWBIR74    100  1050.81  1432.9117      1   8027 # BIR79      100  4223.92  5179.4582    319  30757 # SID79      100     8.36     9.4319      0     57 # NWBIR79    100  1352.81  1975.9988      3  11631  # Detailed statistics description of each column descr(nc) # Dataset: nc, 14 Variables, N = 100 # ---------------------------------------------------------------------------------------------------- # AREA (numeric):  # Statistics #     N  Ndist  Mean    SD   Min   Max  Skew  Kurt #   100     77  0.13  0.05  0.04  0.24  0.48   2.5 # Quantiles #     1%    5%   10%   25%   50%   75%  90%   95%   99% #   0.04  0.06  0.06  0.09  0.12  0.15  0.2  0.21  0.24 # ---------------------------------------------------------------------------------------------------- # PERIMETER (numeric):  # Statistics #     N  Ndist  Mean    SD  Min   Max  Skew  Kurt #   100     96  1.67  0.48    1  3.64  1.48  5.95 # Quantiles #   1%    5%   10%   25%   50%   75%  90%   95%  99% #    1  1.09  1.19  1.32  1.61  1.86  2.2  2.72  3.2 # ---------------------------------------------------------------------------------------------------- # CNTY_ (numeric):  # Statistics #     N  Ndist     Mean      SD   Min   Max  Skew  Kurt #   100    100  1985.96  106.52  1825  2241  0.26  2.32 # Quantiles #        1%       5%     10%      25%   50%      75%   90%     95%      99% #   1826.98  1832.95  1837.9  1902.25  1982  2067.25  2110  2156.3  2238.03 # ---------------------------------------------------------------------------------------------------- # CNTY_ID (numeric):  # Statistics #     N  Ndist     Mean      SD   Min   Max  Skew  Kurt #   100    100  1985.96  106.52  1825  2241  0.26  2.32 # Quantiles #        1%       5%     10%      25%   50%      75%   90%     95%      99% #   1826.98  1832.95  1837.9  1902.25  1982  2067.25  2110  2156.3  2238.03 # ---------------------------------------------------------------------------------------------------- # NAME (character):  # Statistics #     N  Ndist #   100    100 # Table #                Freq  Perc # Ashe              1     1 # Alleghany         1     1 # Surry             1     1 # Currituck         1     1 # Northampton       1     1 # Hertford          1     1 # Camden            1     1 # Gates             1     1 # Warren            1     1 # Stokes            1     1 # Caswell           1     1 # Rockingham        1     1 # Granville         1     1 # Person            1     1 # ... 86 Others    86    86 #  # Summary of Table Frequencies #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #       1       1       1       1       1       1  # ---------------------------------------------------------------------------------------------------- # FIPS (character):  # Statistics #     N  Ndist #   100    100 # Table #                Freq  Perc # 37009             1     1 # 37005             1     1 # 37171             1     1 # 37053             1     1 # 37131             1     1 # 37091             1     1 # 37029             1     1 # 37073             1     1 # 37185             1     1 # 37169             1     1 # 37033             1     1 # 37157             1     1 # 37077             1     1 # 37145             1     1 # ... 86 Others    86    86 #  # Summary of Table Frequencies #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #       1       1       1       1       1       1  # ---------------------------------------------------------------------------------------------------- # FIPSNO (numeric):  # Statistics #     N  Ndist   Mean     SD    Min    Max  Skew  Kurt #   100    100  37100  58.02  37001  37199    -0   1.8 # Quantiles #         1%       5%      10%      25%    50%      75%      90%      95%       99% #   37002.98  37010.9  37020.8  37050.5  37100  37149.5  37179.2  37189.1  37197.02 # ---------------------------------------------------------------------------------------------------- # CRESS_ID (integer):  # Statistics #     N  Ndist  Mean     SD  Min  Max  Skew  Kurt #   100    100  50.5  29.01    1  100     0   1.8 # Quantiles #     1%    5%   10%    25%   50%    75%   90%    95%    99% #   1.99  5.95  10.9  25.75  50.5  75.25  90.1  95.05  99.01 # ---------------------------------------------------------------------------------------------------- # BIR74 (numeric):  # Statistics #     N  Ndist     Mean       SD  Min    Max  Skew   Kurt #   100    100  3299.62  3848.17  248  21588  2.79  11.79 # Quantiles #       1%      5%    10%   25%     50%   75%     90%    95%       99% #   283.64  419.75  531.8  1077  2180.5  3936  6725.7  11193  20378.22 # ---------------------------------------------------------------------------------------------------- # SID74 (numeric):  # Statistics #     N  Ndist  Mean    SD  Min  Max  Skew   Kurt #   100     23  6.67  7.78    0   44  2.44  10.28 # Quantiles #   1%  5%  10%  25%  50%   75%   90%    95%    99% #    0   0    0    2    4  8.25  15.1  18.25  38.06 # ---------------------------------------------------------------------------------------------------- # NWBIR74 (numeric):  # Statistics #     N  Ndist     Mean       SD  Min   Max  Skew   Kurt #   100     93  1050.81  1432.91    1  8027  2.83  11.84 # Quantiles #   1%    5%   10%  25%    50%     75%     90%     95%      99% #    1  9.95  39.2  190  697.5  1168.5  2231.8  3942.9  7052.84 # ---------------------------------------------------------------------------------------------------- # BIR79 (numeric):  # Statistics #     N  Ndist     Mean       SD  Min    Max  Skew  Kurt #   100    100  4223.92  5179.46  319  30757  2.99  13.1 # Quantiles #       1%     5%    10%      25%   50%   75%   90%       95%       99% #   349.69  539.3  675.7  1336.25  2636  4889  8313  14707.45  26413.87 # ---------------------------------------------------------------------------------------------------- # SID79 (numeric):  # Statistics #     N  Ndist  Mean    SD  Min  Max  Skew  Kurt #   100     28  8.36  9.43    0   57  2.28  9.88 # Quantiles #   1%  5%  10%  25%  50%    75%  90%  95%    99% #    0   0    1    2    5  10.25   21   26  38.19 # ---------------------------------------------------------------------------------------------------- # NWBIR79 (numeric):  # Statistics #     N  Ndist     Mean    SD  Min    Max  Skew   Kurt #   100     98  1352.81  1976    3  11631  3.18  14.45 # Quantiles #     1%    5%   10%    25%    50%      75%     90%     95%       99% #   3.99  11.9  44.7  250.5  874.5  1406.75  2987.9  5090.5  10624.17 # ----------------------------------------------------------------------------------------------------"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"selecting-columns-and-subsetting","dir":"Articles","previous_headings":"","what":"Selecting Columns and Subsetting","title":"collapse and sf","text":"can select columns sf data frame without worry taking along ‘geometry’: applies subsetting rows (columns): significantly faster using [, base::subset(), dplyr::select() dplyr::filter(): However, collapse functions don’t subset ‘agr’ attribute selecting columns, (specified) relates columns (attributes) geometry, also don’t modify ‘bbox’ attribute giving overall boundaries set geometries subsetting sf data frame. Keeping full ‘agr’ attribute problematic practical purposes, changing ‘bbox’ upon subsetting may lead large margins plotting geometries subset sf data frame. One way change calling st_make_valid() subset frame; st_make_valid() expensive, thus unless subset frame small, better use [, base::subset() dplyr::filter() cases bounding box size matters.","code":"# Selecting a sequence of columns fselect(nc, AREA, NAME:FIPSNO) # Simple feature collection with 100 features and 4 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA      NAME  FIPS FIPSNO                       geometry # 1 0.114      Ashe 37009  37009 MULTIPOLYGON (((-81.47276 3... # 2 0.061 Alleghany 37005  37005 MULTIPOLYGON (((-81.23989 3... # 3 0.143     Surry 37171  37171 MULTIPOLYGON (((-80.45634 3...  # Same using standard evaluation (gv is a shorthand for get_vars()) gv(nc, c(\"AREA\", \"NAME\", \"FIPS\", \"FIPSNO\")) # Simple feature collection with 100 features and 4 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA      NAME  FIPS FIPSNO                       geometry # 1 0.114      Ashe 37009  37009 MULTIPOLYGON (((-81.47276 3... # 2 0.061 Alleghany 37005  37005 MULTIPOLYGON (((-81.23989 3... # 3 0.143     Surry 37171  37171 MULTIPOLYGON (((-80.45634 3... # A fast and enhanced version of base::subset fsubset(nc, AREA > fmean(AREA), AREA, NAME:FIPSNO) # Simple feature collection with 44 features and 4 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA        NAME  FIPS FIPSNO                       geometry # 1 0.143       Surry 37171  37171 MULTIPOLYGON (((-80.45634 3... # 2 0.153 Northampton 37131  37131 MULTIPOLYGON (((-77.21767 3... # 3 0.153  Rockingham 37157  37157 MULTIPOLYGON (((-79.53051 3...  # A fast version of `[` (where i is used and optionally j) ss(nc, 1:10, c(\"AREA\", \"NAME\", \"FIPS\", \"FIPSNO\")) # Simple feature collection with 10 features and 4 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA      NAME  FIPS FIPSNO                       geometry # 1 0.114      Ashe 37009  37009 MULTIPOLYGON (((-81.47276 3... # 2 0.061 Alleghany 37005  37005 MULTIPOLYGON (((-81.23989 3... # 3 0.143     Surry 37171  37171 MULTIPOLYGON (((-80.45634 3... library(microbenchmark) library(dplyr)  # Selecting columns microbenchmark(collapse = fselect(nc, AREA, NAME:FIPSNO),                 dplyr = select(nc, AREA, NAME:FIPSNO),                collapse2 = gv(nc, c(\"AREA\", \"NAME\", \"FIPS\", \"FIPSNO\")),                 sf = nc[c(\"AREA\", \"NAME\", \"FIPS\", \"FIPSNO\")]) # Unit: microseconds #       expr     min       lq      mean   median       uq      max neval #   collapse   3.034   3.9565   5.19429   5.1865   5.6990   22.878   100 #      dplyr 431.279 452.2915 505.29015 466.3750 493.8450 3356.342   100 #  collapse2   2.665   3.4850   4.59610   4.4075   5.0635   14.391   100 #         sf 105.165 114.1235 120.39732 118.0390 124.9270  156.497   100 # Subsetting microbenchmark(collapse = fsubset(nc, AREA > fmean(AREA), AREA, NAME:FIPSNO),                 dplyr = select(nc, AREA, NAME:FIPSNO) |> filter(AREA > fmean(AREA)),                collapse2 = ss(nc, 1:10, c(\"AREA\", \"NAME\", \"FIPS\", \"FIPSNO\")),                 sf = nc[1:10, c(\"AREA\", \"NAME\", \"FIPS\", \"FIPSNO\")]) # Unit: microseconds #       expr     min       lq       mean   median        uq      max neval #   collapse   9.676  11.5825   15.01707  14.4730   16.8920   30.463   100 #      dplyr 890.643 917.6415 1055.40970 941.7085 1009.7890 5546.685   100 #  collapse2   2.829   3.5465    5.40585   4.8995    6.4165   20.541   100 #         sf 176.997 187.6160  202.72286 200.7565  210.8220  340.464   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"aggregation-and-grouping","dir":"Articles","previous_headings":"","what":"Aggregation and Grouping","title":"collapse and sf","text":"flexibility speed collap() aggregation can used sf data frames. separate method sf objects considered necessary one can simply aggregate geometry column using st_union(): sf data frames can also grouped aggregated using fsummarise(): Typically time aggregation consumed st_union() speed collapse really become visible datasets. faster alternative use geos (sf backend planar geometries) s2 (sf backend spherical geometries) directly: general, also upon aggregation collapse, functions st_as_sfc(), st_as_sf(), , worst case, st_make_valid(), may need invoked ensure valid sf object output. Functions collap() fsummarise() attribute preserving give special regard geometry columns. One exception avoids high cost spatial functions aggregation need ex-post conversion/validation aggregating spatial panel data time-dimension. panels can quickly aggregated using ffirst() flast() aggregate geometry:","code":"# Aggregating by variable SID74 using the median for numeric and the mode for categorical columns collap(nc, ~ SID74, custom = list(fmedian = is.numeric,                                    fmode = is.character,                                    st_union = \"geometry\")) # or use is.list to fetch the geometry # Simple feature collection with 23 features and 15 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #     AREA PERIMETER  CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 SID74 NWBIR74  BIR79 # 1 0.0780    1.3070 1950.0  1950.0 Alleghany 37005  37073     37.0   487     0     0    40.0  594.0 # 2 0.0810    1.2880 1887.0  1887.0      Ashe 37009  37137     69.0   751     1     1   148.0  899.0 # 3 0.1225    1.6435 1959.5  1959.5   Caswell 37033  37078     39.5  1271     2     2   382.5 1676.5 #   SID79 NWBIR79                       geometry # 1     1      45 MULTIPOLYGON (((-83.69563 3... # 2     1     176 MULTIPOLYGON (((-80.02406 3... # 3     2     452 MULTIPOLYGON (((-77.16129 3... nc |> fgroup_by(SID74) # Simple feature collection with 100 features and 14 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 # 1 0.114     1.442  1825    1825      Ashe 37009  37009        5  1091     1      10  1364     0 # 2 0.061     1.231  1827    1827 Alleghany 37005  37005        3   487     0      10   542     3 # 3 0.143     1.630  1828    1828     Surry 37171  37171       86  3188     5     208  3616     6 #   NWBIR79                       geometry # 1      19 MULTIPOLYGON (((-81.47276 3... # 2      12 MULTIPOLYGON (((-81.23989 3... # 3     260 MULTIPOLYGON (((-80.45634 3... #  # Grouped by:  SID74  [23 | 4 (4) 1-13]  nc |>    fgroup_by(SID74) |>   fsummarise(AREA_Ag = fsum(AREA),               Perimeter_Ag = fmedian(PERIMETER),              geometry = st_union(geometry)) # Simple feature collection with 23 features and 3 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #   SID74 AREA_Ag Perimeter_Ag                       geometry # 1     0   1.103       1.3070 MULTIPOLYGON (((-83.69563 3... # 2     1   0.914       1.2880 MULTIPOLYGON (((-80.02406 3... # 3     2   1.047       1.6435 MULTIPOLYGON (((-77.16129 3... # Using s2 backend: sensible for larger tasks nc |>    fmutate(geometry = s2::as_s2_geography(geometry)) |>   fgroup_by(SID74) |>   fsummarise(AREA_Ag = fsum(AREA),               Perimeter_Ag = fmedian(PERIMETER),              geometry = s2::s2_union_agg(geometry)) |>   fmutate(geometry = st_as_sfc(geometry)) # Simple feature collection with 23 features and 3 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  WGS 84 # First 3 features: #   SID74 AREA_Ag Perimeter_Ag                       geometry # 1     0   1.103       1.3070 MULTIPOLYGON (((-83.69563 3... # 2     1   0.914       1.2880 MULTIPOLYGON (((-80.02406 3... # 3     2   1.047       1.6435 MULTIPOLYGON (((-77.16129 3... # Creating a panel-dataset by simply duplicating nc for 2 different years pnc <- rowbind(`2000` = nc, `2001` = nc, idcol = \"Year\") |> as_integer_factor() pnc  # Simple feature collection with 200 features and 15 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #   Year  AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 # 1 2000 0.114     1.442  1825    1825      Ashe 37009  37009        5  1091     1      10  1364 # 2 2000 0.061     1.231  1827    1827 Alleghany 37005  37005        3   487     0      10   542 # 3 2000 0.143     1.630  1828    1828     Surry 37171  37171       86  3188     5     208  3616 #   SID79 NWBIR79                       geometry # 1     0      19 MULTIPOLYGON (((-81.47276 3... # 2     3      12 MULTIPOLYGON (((-81.23989 3... # 3     6     260 MULTIPOLYGON (((-80.45634 3...  # Aggregating by NAME, using the last value for all categorical data collap(pnc, ~ NAME, fmedian, catFUN = flast, cols = -1L) # Simple feature collection with 100 features and 15 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA PERIMETER CNTY_ CNTY_ID      NAME      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 # 1 0.111     1.392  1904    1904  Alamance  Alamance 37001  37001        1  4672    13    1243  5767 # 2 0.066     1.070  1950    1950 Alexander Alexander 37003  37003        2  1333     0     128  1683 # 3 0.061     1.231  1827    1827 Alleghany Alleghany 37005  37005        3   487     0      10   542 #   SID79 NWBIR79                       geometry # 1    11    1397 MULTIPOLYGON (((-79.24619 3... # 2     2     150 MULTIPOLYGON (((-81.10889 3... # 3     3      12 MULTIPOLYGON (((-81.23989 3...  # Using fsummarise to aggregate just two variables and the geometry pnc_ag <- pnc |>    fgroup_by(NAME) |>   fsummarise(AREA_Ag = fsum(AREA),               Perimeter_Ag = fmedian(PERIMETER),              geometry = flast(geometry))  # The geometry is still valid... (slt = shorthand for fselect) plot(slt(pnc_ag, AREA_Ag))"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"indexing","dir":"Articles","previous_headings":"","what":"Indexing","title":"collapse and sf","text":"sf data frames can also become indexed frames (spatio-temporal panels):","code":"pnc <- pnc |> findex_by(CNTY_ID, Year) pnc  # Simple feature collection with 200 features and 15 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #   Year  AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 # 1 2000 0.114     1.442  1825    1825      Ashe 37009  37009        5  1091     1      10  1364 # 2 2000 0.061     1.231  1827    1827 Alleghany 37005  37005        3   487     0      10   542 # 3 2000 0.143     1.630  1828    1828     Surry 37171  37171       86  3188     5     208  3616 #   SID79 NWBIR79                       geometry # 1     0      19 MULTIPOLYGON (((-81.47276 3... # 2     3      12 MULTIPOLYGON (((-81.23989 3... # 3     6     260 MULTIPOLYGON (((-80.45634 3... #  # Indexed by:  CNTY_ID [100] | Year [2] qsu(pnc$AREA) #          N/T    Mean      SD     Min     Max # Overall  200  0.1263  0.0491   0.042   0.241 # Between  100  0.1263  0.0492   0.042   0.241 # Within     2  0.1263       0  0.1263  0.1263 settransform(pnc, AREA_diff = fdiff(AREA))  psmat(pnc$AREA_diff) |> head() #      2000 2001 # 1825   NA    0 # 1827   NA    0 # 1828   NA    0 # 1831   NA    0 # 1832   NA    0 # 1833   NA    0 pnc <- unindex(pnc)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"unique-values-ordering-splitting-binding","dir":"Articles","previous_headings":"","what":"Unique Values, Ordering, Splitting, Binding","title":"collapse and sf","text":"Functions funique() roworder[v]() ignore ‘geometry’ column determining unique values / order rows applied sf data frames. rsplit() can used (recursively) split sf data frame multiple chunks. default rsplit() data frames simplify = TRUE, , single LHS variable, just split column-vector. apply sf data frames ‘geometry’ column always selected well. sf data frames can combined using rowbind(), , default, preserves attributes first object.","code":"# Splitting by SID74 rsplit(nc, ~ SID74) |> head(2) # $`0` # Simple feature collection with 13 features and 13 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 NWBIR74 BIR79 SID79 NWBIR79 # 1 0.061     1.231  1827    1827 Alleghany 37005  37005        3   487      10   542     3      12 # 2 0.062     1.547  1834    1834    Camden 37029  37029       15   286     115   350     2     139 # 3 0.091     1.284  1835    1835     Gates 37073  37073       37   420     254   594     2     371 #                         geometry # 1 MULTIPOLYGON (((-81.23989 3... # 2 MULTIPOLYGON (((-76.00897 3... # 3 MULTIPOLYGON (((-76.56251 3... #  # $`1` # Simple feature collection with 11 features and 13 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 NWBIR74 BIR79 SID79 NWBIR79 # 1 0.114     1.442  1825    1825      Ashe 37009  37009        5  1091      10  1364     0      19 # 2 0.070     2.968  1831    1831 Currituck 37053  37053       27   508     123   830     2     145 # 3 0.124     1.428  1837    1837    Stokes 37169  37169       85  1612     160  2038     5     176 #                         geometry # 1 MULTIPOLYGON (((-81.47276 3... # 2 MULTIPOLYGON (((-76.00897 3... # 3 MULTIPOLYGON (((-80.02567 3... # Only splitting Area rsplit(nc, AREA ~ SID74) |> head(1) # $`0` # Simple feature collection with 13 features and 1 field # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA                       geometry # 1 0.061 MULTIPOLYGON (((-81.23989 3... # 2 0.062 MULTIPOLYGON (((-76.00897 3... # 3 0.091 MULTIPOLYGON (((-76.56251 3...  # For data frames the default simplify = TRUE drops the data frame structure rsplit(qDF(nc), AREA ~ SID74) |> head(1) # $`0` #  [1] 0.061 0.062 0.091 0.064 0.059 0.080 0.066 0.099 0.094 0.078 0.131 0.167 0.051 # Splitting by each row and recombining nc_combined <- nc %>% rsplit(seq_row(.)) %>% rowbind()  identical(nc, nc_combined) # [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"transformations","dir":"Articles","previous_headings":"","what":"Transformations","title":"collapse and sf","text":"transforming computing columns, fmutate() ftransform[v]() apply data frame. Special attention sf data frames afforded fcompute(), can used compute new columns dropping existing ones - except geometry column columns selected keep argument.","code":"fmutate(nc, gsum_AREA = fsum(AREA, SID74, TRA = \"fill\")) |> head() # Simple feature collection with 6 features and 15 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 # 1 0.114     1.442  1825    1825      Ashe 37009  37009        5  1091     1      10  1364     0 # 2 0.061     1.231  1827    1827 Alleghany 37005  37005        3   487     0      10   542     3 # 3 0.143     1.630  1828    1828     Surry 37171  37171       86  3188     5     208  3616     6 #   NWBIR79                       geometry gsum_AREA # 1      19 MULTIPOLYGON (((-81.47276 3...     0.914 # 2      12 MULTIPOLYGON (((-81.23989 3...     1.103 # 3     260 MULTIPOLYGON (((-80.45634 3...     1.380  # Same thing, more expensive nc |> fgroup_by(SID74) |> fmutate(gsum_AREA = fsum(AREA)) |> fungroup() |> head() # Simple feature collection with 6 features and 15 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA PERIMETER CNTY_ CNTY_ID      NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 # 1 0.114     1.442  1825    1825      Ashe 37009  37009        5  1091     1      10  1364     0 # 2 0.061     1.231  1827    1827 Alleghany 37005  37005        3   487     0      10   542     3 # 3 0.143     1.630  1828    1828     Surry 37171  37171       86  3188     5     208  3616     6 #   NWBIR79                       geometry gsum_AREA # 1      19 MULTIPOLYGON (((-81.47276 3...     0.914 # 2      12 MULTIPOLYGON (((-81.23989 3...     1.103 # 3     260 MULTIPOLYGON (((-80.45634 3...     1.380 fcompute(nc, scaled_AREA = fscale(AREA),               gsum_AREA = fsum(AREA, SID74, TRA = \"fill\"),           keep = .c(AREA, SID74)) # Simple feature collection with 100 features and 4 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #    AREA SID74 scaled_AREA gsum_AREA                       geometry # 1 0.114     1  -0.2491860     0.914 MULTIPOLYGON (((-81.47276 3... # 2 0.061     0  -1.3264176     1.103 MULTIPOLYGON (((-81.23989 3... # 3 0.143     5   0.3402426     1.380 MULTIPOLYGON (((-80.45634 3..."},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"conversion-to-and-from-sf","dir":"Articles","previous_headings":"","what":"Conversion to and from sf","title":"collapse and sf","text":"quick converters qDF(), qDT(), qTBL() can used efficiently convert sf data frames standard data frames, data.table’s tibbles, result can converted back original sf data frame using setAttrib(), copyAttrib() copyMostAttrib(). easiest way strip geometry column sf data frame via function atomic_elem(), removes list-like columns , default, also class attribute. example, can create data.table without list column using also handy functions join() pivot(), class agnostic like collapse, built-logic deal sf column.","code":"library(data.table) # Create a data.table on the fly to do an fast grouped rolling mean and back to sf qDT(nc)[, list(roll_AREA = frollmean(AREA, 2), geometry), by = SID74] |> copyMostAttrib(nc) # Simple feature collection with 100 features and 2 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #   SID74 roll_AREA                       geometry # 1     1        NA MULTIPOLYGON (((-81.47276 3... # 2     1     0.092 MULTIPOLYGON (((-76.00897 3... # 3     1     0.097 MULTIPOLYGON (((-80.02567 3... qDT(atomic_elem(nc)) |> head() #     AREA PERIMETER CNTY_ CNTY_ID        NAME   FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 #    <num>     <num> <num>   <num>      <char> <char>  <num>    <int> <num> <num>   <num> <num> <num> # 1: 0.114     1.442  1825    1825        Ashe  37009  37009        5  1091     1      10  1364     0 # 2: 0.061     1.231  1827    1827   Alleghany  37005  37005        3   487     0      10   542     3 # 3: 0.143     1.630  1828    1828       Surry  37171  37171       86  3188     5     208  3616     6 # 4: 0.070     2.968  1831    1831   Currituck  37053  37053       27   508     1     123   830     2 # 5: 0.153     2.206  1832    1832 Northampton  37131  37131       66  1421     9    1066  1606     3 # 6: 0.097     1.670  1833    1833    Hertford  37091  37091       46  1452     7     954  1838     5 #    NWBIR79 #      <num> # 1:      19 # 2:      12 # 3:     260 # 4:     145 # 5:    1197 # 6:    1237 # Use atomic_elem() to strip geometry off y in left join identical(nc, join(nc, atomic_elem(nc), overid = 2)) # left join: nc[AREA, PERIMETER, CNTY_, CNTY_ID, NAME, FIPS, FIPSNO, CRESS_ID, BIR74, SID74, NWBIR74, BIR79, SID79, NWBIR79] 100/100 (100%) <m:m> y[AREA, PERIMETER, CNTY_, CNTY_ID, NAME, FIPS, FIPSNO, CRESS_ID, BIR74, SID74, NWBIR74, BIR79, SID79, NWBIR79] 100/100 (100%) # [1] TRUE  # In pivot: presently need to specify what to do with geometry column pivot(nc, c(\"CNTY_ID\", \"geometry\")) |> head() # Simple feature collection with 6 features and 3 fields # Geometry type: MULTIPOLYGON # Dimension:     XY # Bounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965 # Geodetic CRS:  NAD27 # First 3 features: #   CNTY_ID                       geometry variable value # 1    1825 MULTIPOLYGON (((-81.47276 3...     AREA 0.114 # 2    1827 MULTIPOLYGON (((-81.23989 3...     AREA 0.061 # 3    1828 MULTIPOLYGON (((-80.45634 3...     AREA 0.143 # Or use pivot(qDT(atomic_elem(nc)), \"CNTY_ID\") |> head() #    CNTY_ID variable  value #      <num>   <fctr> <char> # 1:    1825     AREA  0.114 # 2:    1827     AREA  0.061 # 3:    1828     AREA  0.143 # 4:    1831     AREA   0.07 # 5:    1832     AREA  0.153 # 6:    1833     AREA  0.097"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"support-for-units","dir":"Articles","previous_headings":"","what":"Support for units","title":"collapse and sf","text":"Since v2.0.13, collapse explicitly supports/preserves units objects dedicated methods preserve ‘units’ class wherever sensible.","code":"nc_dist <- st_centroid(nc) |> st_distance() nc_dist[1:3, 1:3] # Units: [m] #          [,1]     [,2]     [,3] # [1,]     0.00 34020.35 72728.02 # [2,] 34020.35     0.00 40259.55 # [3,] 72728.02 40259.55     0.00  fmean(nc_dist) |> head() # Units: [m] # [1] 250543.9 237040.0 217941.5 337016.5 250380.2 269604.6 fndistinct(nc_dist) |> head() # [1] 100 100 100 100 100 100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"collapse and sf","text":"collapse provides deep integration sf ecosystem perform spatial operations, offers sufficient features flexibility painlessly manipulate sf data frames much greater speeds dplyr. requires bit care user though ensure returned sf objects valid, especially following aggregation subsetting.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_documentation.html","id":"built-in-structured-documentation","dir":"Articles","previous_headings":"","what":"Built-In Structured Documentation","title":"collapse Documentation and Resources","text":"installing collapse, can call help(\"collapse-documentation\") produce central help page providing broad overview entire functionality package, including direct links function documentation pages links 13 topical documentation pages (names .COLLAPSE_TOPICS) describing clusters related functions work together. Thus collapse comes fully structured hierarchical documentation can browse within R - provides everything necessary fully understand package. Documentation also available online. package page help(\"collapse-package\") provides general information package design philosophy, well compact set examples covering important functionality. Reading help(\"collapse-package\") help(\"collapse-documentation\") comprehensive way get acquainted package. help(\"collapse-documentation\") always --date resource.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_documentation.html","id":"cheatsheet","dir":"Articles","previous_headings":"","what":"Cheatsheet","title":"collapse Documentation and Resources","text":"--date (v2.0) cheatsheet compactly summarizes package.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_documentation.html","id":"article-on-arxiv","dir":"Articles","previous_headings":"","what":"Article on arXiv","title":"collapse Documentation and Resources","text":"article collapse (v2.0.10) submitted Journal Statistical Software March 2024.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_documentation.html","id":"user-2022-presentation-and-slides","dir":"Articles","previous_headings":"","what":"useR 2022 Presentation and Slides","title":"collapse Documentation and Resources","text":"presented collapse (v1.8) level detail useR 2022. 2h video recording provides quite comprehensive introduction available . corresponding slides available .","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_documentation.html","id":"vignettes","dir":"Articles","previous_headings":"","what":"Vignettes","title":"collapse Documentation and Resources","text":"Updated vignettes collapse tidyverse Users: quick introduction collapse tidyverse users collapse sf: Shows collapse can used efficiently manipulate sf data frames collapse’s Handling R Objects: quick view behind scenes class-agnostic R programming Developing collapse: write efficient statistical packages using R collapse vignettes (available online) cover major features introduced versions >= 1.7, contain much useful information examples: Introduction collapse : Introduces key features structured way collapse dplyr : Demonstrates integration collapse dplyr / tidyverse workflows associated performance improvements collapse plm: Demonstrates integration collapse plm shows examples efficient programming panel data collapse data.table: Shows collapse data.table may used together harmonious way","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_documentation.html","id":"blog","dir":"Articles","previous_headings":"","what":"Blog","title":"collapse Documentation and Resources","text":"maintain blog linked Rbloggers.com introduced collapse compact posts covering central functionality. Among , post programming collapse useful developers.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html","id":"namespace-and-global-options","dir":"Articles","previous_headings":"","what":"Namespace and Global Options","title":"collapse for tidyverse Users","text":"collapse data manipulation functions familiar tidyverse users include fselect, fgroup_by, fsummarise, fmutate, across, frename, fslice, fcount. functions like fsubset, ftransform, get_vars inspired base R, functions like join, pivot, roworder, colorder, rowbind, etc. inspired data manipulation libraries data.table polars. virtue f- prefixes, collapse namespace conflicts tidyverse, functions can easily substituted tidyverse workflow. R users willing replace tidyverse additional option mask functions eliminate prefixes set_collapse. example makes available functions select, group_by, summarise, mutate, rename, count, subset, slice, transform collapse namespace detaches re-attaches package, following code executed collapse: Note correct documentation still needs called prefixes, .e., ?fsubset. See ?set_collapse options package, also includes optimization options nthreads, na.rm, sort, stable.algo. Note also use collapse’s namespace masking, can use fastverse::fastverse_conflicts() check namespace conflicts packages.","code":"library(collapse) set_collapse(mask = \"manip\") # version >= 2.0.0 mtcars |>   subset(mpg > 11) |>   group_by(cyl, vs, am) |>   summarise(across(c(mpg, carb, hp), mean),              qsec_wt = weighted.mean(qsec, wt)) #   cyl vs am      mpg     carb        hp  qsec_wt # 1   4  0  1 26.00000 2.000000  91.00000 16.70000 # 2   4  1  0 22.90000 1.666667  84.66667 21.04028 # 3   4  1  1 28.37143 1.428571  80.57143 18.75509 # 4   6  0  1 20.56667 4.666667 131.66667 16.33306 # 5   6  1  0 19.12500 2.500000 115.25000 19.21275 # 6   8  0  0 15.98000 2.900000 191.00000 17.01239 # 7   8  0  1 15.40000 6.000000 299.50000 14.55297"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html","id":"using-the-fast-statistical-functions","dir":"Articles","previous_headings":"","what":"Using the Fast Statistical Functions","title":"collapse for tidyverse Users","text":"key feature collapse provides functions data manipulation, also full set statistical functions algorithms speed statistical calculations perform complex statistical operations (e.g. involving weights time series data). Notably among , Fast Statistical Functions consistent set S3-generic statistical functions providing fully vectorized statistical operations R. Specifically, operations calculating mean via S3 generic fmean() function vectorized across columns groups may also involve weights transformations original data: data manipulation functions collapse integrated Fast Statistical Functions enable vectorized statistical operations. example, following code gives exactly result , execution much faster (especially larger data), Fast Statistical Functions, data need split groups, need call lapply() inside across() statement: fmean.data.frame() simply applied subset data containing columns mpg, carb hp. Fast Statistical Functions also method grouped data, want calculate weighted mean qsec, code simplify follows: Note functions collapse, including Fast Statistical Functions, default na.rm = TRUE, .e., missing values skipped calculations. can changed using set_collapse(na.rm = FALSE) give behavior consistent base R. Another thing aware using Fast Statistical Functions inside data manipulation functions toggle vectorized execution wherever used. E.g. calculates grouped mean mpg adds overall minimum qsec result, whereas give mean + minimum within group, calculated different ways: former equivalent fmean(mpg, g = cyl) + fmin(qsec, g = cyl), whereas latter equal sapply(gsplit(mpg, cyl), function(x) mean(x) + min(x)). See ?fsummarise ?fmutate detailed examples. eager vectorization approach intentional allows users vectorize complex expressions fall back base R desired. blog post Andrew Ghazi provides excellent example computing p-value test statistic groups. take full advantage collapse, highly recommended use Fast Statistical Functions much possible. can also set set_collapse(mask = \"\") replace statistical functions base R like sum mean collapse versions (toggling vectorized execution cases), may affect parts code2.","code":"fmean(mtcars$mpg)     # Vector # [1] 20.09062 fmean(EuStockMarkets) # Matrix #      DAX      SMI      CAC     FTSE  # 2530.657 3376.224 2227.828 3565.643 fmean(mtcars)         # Data Frame #        mpg        cyl       disp         hp       drat         wt       qsec         vs         am  #  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250  #       gear       carb  #   3.687500   2.812500  fmean(mtcars$mpg, w = mtcars$wt)  # Weighted mean # [1] 18.54993 fmean(mtcars$mpg, g = mtcars$cyl) # Grouped mean #        4        6        8  # 26.66364 19.74286 15.10000 fmean(mtcars$mpg, g = mtcars$cyl, w = mtcars$wt)   # Weighted group mean #        4        6        8  # 25.93504 19.64578 14.80643 fmean(mtcars[5:10], g = mtcars$cyl, w = mtcars$wt) # Of data frame #       drat       wt     qsec        vs        am     gear # 4 4.031264 2.414750 19.38044 0.9148868 0.6498031 4.047250 # 6 3.569170 3.152060 18.12198 0.6212191 0.3787809 3.821036 # 8 3.205658 4.133116 16.88529 0.0000000 0.1203808 3.240762 fmean(mtcars$mpg, g = mtcars$cyl, w = mtcars$wt, TRA = \"fill\") # Replace data by weighted group mean #  [1] 19.64578 19.64578 25.93504 19.64578 14.80643 19.64578 14.80643 25.93504 25.93504 19.64578 # [11] 19.64578 14.80643 14.80643 14.80643 14.80643 14.80643 14.80643 25.93504 25.93504 25.93504 # [21] 25.93504 14.80643 14.80643 14.80643 14.80643 25.93504 25.93504 25.93504 14.80643 19.64578 # [31] 14.80643 25.93504 # etc... mtcars |>   subset(mpg > 11) |>   group_by(cyl, vs, am) |>   summarise(across(c(mpg, carb, hp), fmean),              qsec_wt = fmean(qsec, wt)) #   cyl vs am      mpg     carb        hp  qsec_wt # 1   4  0  1 26.00000 2.000000  91.00000 16.70000 # 2   4  1  0 22.90000 1.666667  84.66667 21.04028 # 3   4  1  1 28.37143 1.428571  80.57143 18.75509 # 4   6  0  1 20.56667 4.666667 131.66667 16.33306 # 5   6  1  0 19.12500 2.500000 115.25000 19.21275 # 6   8  0  0 15.98000 2.900000 191.00000 17.01239 # 7   8  0  1 15.40000 6.000000 299.50000 14.55297 mtcars |>   subset(mpg > 11) |>   group_by(cyl, vs, am) |>   select(mpg, carb, hp) |>    fmean() #   cyl vs am      mpg     carb        hp # 1   4  0  1 26.00000 2.000000  91.00000 # 2   4  1  0 22.90000 1.666667  84.66667 # 3   4  1  1 28.37143 1.428571  80.57143 # 4   6  0  1 20.56667 4.666667 131.66667 # 5   6  1  0 19.12500 2.500000 115.25000 # 6   8  0  0 15.98000 2.900000 191.00000 # 7   8  0  1 15.40000 6.000000 299.50000 mtcars |> group_by(cyl) |> summarise(mpg = fmean(mpg) + min(qsec)) # Vectorized #   cyl      mpg # 1   4 41.16364 # 2   6 34.24286 # 3   8 29.60000 mtcars |> group_by(cyl) |> summarise(mpg = fmean(mpg) + fmin(qsec)) # Vectorized #   cyl      mpg # 1   4 43.36364 # 2   6 35.24286 # 3   8 29.60000 mtcars |> group_by(cyl) |> summarise(mpg = mean(mpg) + min(qsec))   # Not vectorized #   cyl      mpg # 1   4 43.36364 # 2   6 35.24286 # 3   8 29.60000"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html","id":"writing-efficient-code","dir":"Articles","previous_headings":"","what":"Writing Efficient Code","title":"collapse for tidyverse Users","text":"also performance-critical correctly sequence operations limit excess computations. tidyverse code often inefficient simply tidyverse allows everything. example, mtcars |> group_by(cyl) |> filter(mpg > 13) |> arrange(mpg) permissible inefficient code filters reorders grouped data, requiring modifications data frame attached grouping object. collapse allow calls fsubset() grouped data, messages roworder(), encouraging write efficient code. example can also optimized subsetting whole frame computations subset columns. efficient select required columns subset operation: Without weighted mean qsec, simplify Finally, set following options toggle unsorted grouping, missing value skipping, multithreading across three columns efficient execution. Setting options globally using set_collapse(sort = FALSE, nthreads = 3, na.rm = FALSE) avoids need set repeatedly.","code":"mtcars |>   subset(mpg > 11, cyl, vs, am, mpg, carb, hp, qsec, wt) |>   group_by(cyl, vs, am) |>   summarise(across(c(mpg, carb, hp), fmean),              qsec_wt = fmean(qsec, wt)) #   cyl vs am      mpg     carb        hp  qsec_wt # 1   4  0  1 26.00000 2.000000  91.00000 16.70000 # 2   4  1  0 22.90000 1.666667  84.66667 21.04028 # 3   4  1  1 28.37143 1.428571  80.57143 18.75509 # 4   6  0  1 20.56667 4.666667 131.66667 16.33306 # 5   6  1  0 19.12500 2.500000 115.25000 19.21275 # 6   8  0  0 15.98000 2.900000 191.00000 17.01239 # 7   8  0  1 15.40000 6.000000 299.50000 14.55297 mtcars |>   subset(mpg > 11, cyl, vs, am, mpg, carb, hp) |>   group_by(cyl, vs, am) |>    fmean() #   cyl vs am      mpg     carb        hp # 1   4  0  1 26.00000 2.000000  91.00000 # 2   4  1  0 22.90000 1.666667  84.66667 # 3   4  1  1 28.37143 1.428571  80.57143 # 4   6  0  1 20.56667 4.666667 131.66667 # 5   6  1  0 19.12500 2.500000 115.25000 # 6   8  0  0 15.98000 2.900000 191.00000 # 7   8  0  1 15.40000 6.000000 299.50000 mtcars |>   subset(mpg > 11, cyl, vs, am, mpg, carb, hp) |>   group_by(cyl, vs, am, sort = FALSE) |>    fmean(nthreads = 3, na.rm = FALSE) #   cyl vs am      mpg     carb        hp # 1   6  0  1 20.56667 4.666667 131.66667 # 2   4  1  1 28.37143 1.428571  80.57143 # 3   6  1  0 19.12500 2.500000 115.25000 # 4   8  0  0 15.98000 2.900000 191.00000 # 5   4  1  0 22.90000 1.666667  84.66667 # 6   4  0  1 26.00000 2.000000  91.00000 # 7   8  0  1 15.40000 6.000000 299.50000"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html","id":"using-internal-grouping","dir":"Articles","previous_headings":"Writing Efficient Code","what":"Using Internal Grouping","title":"collapse for tidyverse Users","text":"Another key writing efficient code collapse avoid fgroup_by() possible, especially mutate operations. collapse implement .arguments manipulation functions like dplyr, instead allows ad-hoc grouped transformations statistical functions. example, easiest fastest way computed median mpg cyl, vs, common case averaging centering data, collapse also provides functions fbetween() averaging fwithin() centering, .e., fbetween(mpg, list(cyl, vs, )) fmean(mpg, list(cyl, vs, ), TRA = \"fill\"). also fscale() (grouped) scaling centering. also applies multiple columns, can use fmutate(across(...)) ftransformv(), .e.  course, want apply different functions using grouping, fgroup_by() sensible, mutate operations also argument return.groups = FALSE, avoids materializing unique grouping columns, saving memory. TRA argument supports whole array operations, see ?TRA. example fsum(mtcars, TRA = \"/\") turns column vectors proportions. application , consider generated dataset sector-level exports. easy compute Balassa’s (1965) Revealed Comparative Advantage (RCA) index, share sector country exports divided share sector world exports. index 1 indicates RCA country c sector s. Note involved single expression two different grouped operations, possible incorporating grouping statistical functions . Let’s summarise dataset using pivot() aggregate RCA index across years. \"mean\" calls highly efficient internal mean function. may also wish investigate growth rate RCA. can done using fgrowth(). Since panel irregular, .e., every sector observed every year, critical also supply time variable. Lastly, since panel unbalanced, may wish create RCA index last year, balance dataset bit taking last available trade within last three years. can done using single subset call can compute RCA index data summarise, collapse provides many options ad-hoc limited grouping, faster full fgroup_by(), also syntactically efficient. efficiency gains possible using operations reference, e.g., %/=% instead / avoid intermediate copy. also possible transform reference using fast statistical functions passing set = TRUE argument, e.g., (mtcars, fmean(mpg, cyl, TRA = \"fill\", set = TRUE)) replaces mpg group-averaged version (transformed vector returned invisibly).","code":"mtcars |>   mutate(mpg_median = fmedian(mpg, list(cyl, vs, am), TRA = \"fill\")) |>    head(3) #                mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_median # Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4       21.0 # Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4       21.0 # Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1       30.4 mtcars |>   mutate(across(c(mpg, disp, qsec), fmedian, list(cyl, vs, am), TRA = \"fill\")) |>    head(2) #               mpg cyl disp  hp drat    wt  qsec vs am gear carb # Mazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4 # Mazda RX4 Wag  21   6  160 110  3.9 2.875 16.46  0  1    4    4  # Or  mtcars |>   transformv(c(mpg, disp, qsec), fmedian, list(cyl, vs, am), TRA = \"fill\") |>    head(2) #               mpg cyl disp  hp drat    wt  qsec vs am gear carb # Mazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4 # Mazda RX4 Wag  21   6  160 110  3.9 2.875 16.46  0  1    4    4 mtcars |>   group_by(cyl, vs, am, return.groups = FALSE) |>    mutate(mpg_median = fmedian(mpg),           mpg_mean = fmean(mpg), # Or fbetween(mpg)          mpg_demean = fwithin(mpg), # Or fmean(mpg, TRA = \"-\")          mpg_scale = fscale(mpg),           .keep = \"used\") |>   ungroup() |>   head(3) #                mpg cyl vs am mpg_median mpg_mean mpg_demean  mpg_scale # Mazda RX4     21.0   6  0  1       21.0 20.56667  0.4333333  0.5773503 # Mazda RX4 Wag 21.0   6  0  1       21.0 20.56667  0.4333333  0.5773503 # Datsun 710    22.8   4  1  1       30.4 28.37143 -5.5714286 -1.1710339 # c = country, s = sector, y = year, v = value exports <- expand.grid(c = paste0(\"c\", 1:8), s = paste0(\"s\", 1:8), y = 1:15) |>            mutate(v = round(abs(rnorm(length(c), mean = 5)), 2)) |>            subset(-sample.int(length(v), 360)) # Making it unbalanced and irregular head(exports) #    c  s y    v # 1 c2 s1 1 5.55 # 2 c3 s1 1 4.33 # 3 c4 s1 1 5.21 # 4 c5 s1 1 5.31 # 5 c6 s1 1 6.17 # 6 c7 s1 1 5.62 nrow(exports) # [1] 600 # Computing Balassa's (1965) RCA index: fast and memory efficient # settfm() modifies exports and assigns it back to the global environment settfm(exports, RCA = fsum(v, list(c, y), TRA = \"/\") %/=% fsum(fsum(v, y, TRA = \"/\"), list(s, y), TRA = \"fill\", set = TRUE)) pivot(exports, ids = \"c\", values = \"RCA\", names = \"s\",        how = \"wider\", FUN = \"mean\", sort = TRUE) #    c       s1       s2       s3       s4       s5       s6       s7       s8 # 1 c1 1.456983 1.674245 2.106907 1.715610 1.517669 2.058640 1.731403 1.533286 # 2 c2 2.196345 1.741839 1.925417 1.940657 1.422963 1.523795 1.385106 1.455789 # 3 c3 1.261560 1.552989 1.710201 1.420272 1.470105 1.531912 1.562338 1.307914 # 4 c4 1.455803 1.480939 1.558595 1.424664 1.213920 1.283873 1.631415 1.249383 # 5 c5 1.420965 1.616355 1.732715 1.465465 1.579685 1.252126 1.385581 1.359236 # 6 c6 1.445393 1.452775 1.872439 1.529396 1.464301 1.732497 1.331926 1.264625 # 7 c7 1.730497 1.627966 1.678039 1.710256 1.572039 1.798925 2.119763 1.451539 # 8 c8 1.763551 1.773720 1.730399 1.553112 1.419381 1.609315 1.715916 1.568516 exports |>    mutate(RCA_growth = fgrowth(RCA, g = list(c, s), t = y)) |>    pivot(ids = \"c\", values = \"RCA_growth\", names = \"s\",          how = \"wider\", FUN = fmedian, sort = TRUE) #    c         s1         s2          s3          s4         s5          s6         s7         s8 # 1 c1         NA -31.320346  33.2382015 -17.7150170 -19.521910   7.7699227 -11.166836   9.014163 # 2 c2   1.837294  60.313915   7.6639286 -36.3451812   7.657809   0.5202565 -17.252738  16.234799 # 3 c3 -17.644211  10.140848  39.3044351  -0.5140010 -27.571156 -15.3070853 -20.052042  -9.645808 # 4 c4  -3.619271  13.614077 -11.5213936 -29.1795219  12.698973  -2.8301315   9.579979   4.351506 # 5 c5 -11.267960   1.563708  49.2593990   0.6372803  12.894361 -10.7062506 -16.359597   1.331514 # 6 c6  -8.854774 -24.375237  -0.7098001  -0.6061250 -21.095221  17.3704638 -23.141631  -5.861039 # 7 c7   7.168700   9.169368 -51.7958299 -27.7699562  10.830523  23.9014624 -27.645297 -15.541500 # 8 c8  42.166200  -6.204723 114.3084929 -18.3894910 -17.674001  -3.4403949   1.342354 -38.826719 # Taking the latest observation within the last 3 years exports_latest <- subset(exports, y > 12 & y == fmax(y, list(c, s), \"fill\"), -y) # How many sectors do we observe for each country in the last 3 years? with(exports_latest, fndistinct(s, c)) # c1 c2 c3 c4 c5 c6 c7 c8  #  8  8  7  7  8  8  6  8 exports_latest |>     mutate(RCA = fsum(v, c, TRA = \"/\") %/=% fsum(proportions(v), s, TRA = \"fill\")) |>     pivot(\"c\", \"RCA\", \"s\", how = \"wider\", sort = TRUE) #    c        s1        s2        s3        s4        s5        s6        s7        s8 # 1 c1 0.9957444 1.0039325 1.2424563 0.9257392 0.8152179 1.3325429 0.7410637 1.0259104 # 2 c2 1.1416748 0.8007287 1.1660717 1.0364984 0.7154912 1.0625854 1.2649881 0.8687216 # 3 c3 1.1104473 0.9500677 1.3770016        NA 1.1941963 1.1301935 0.9773947 1.0015135 # 4 c4 0.8381306 1.2543034 1.1274679 1.3990983 1.3918678        NA 0.7364405 1.1539036 # 5 c5 0.8536024 0.8182961 0.9638389 1.6273503 1.0172714 0.8268992 1.0423516 1.0273071 # 6 c6 0.8465415 0.8878380 1.2123911 1.7417480 0.8812675 1.1393711 0.9840424 0.6626898 # 7 c7 1.0284817 1.2207153        NA        NA 1.2871187 1.4475702 1.2210074 1.3880608 # 8 c8 1.2217063 1.1452869 0.7166041 0.9448634 0.8388402 0.9760660 1.1123412 0.9686146"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"collapse for tidyverse Users","text":"collapse enhances R statistically computationally good option tidyverse users searching efficient lightweight solutions data manipulation statistical computing problems R. information, recommend starting short vignette Documentation Resources. R users willing write efficient/lightweight code completely replace tidyverse workflow also encouraged closely examine fastverse suite packages. collapse alone may always suffice, 99% tidyverse code can replaced efficient lightweight fastverse solution.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"why-collapse","dir":"Articles","previous_headings":"","what":"Why collapse?","title":"Introduction to collapse","text":"collapse high-performance package extends enhances data-manipulation capabilities R existing popular packages (dplyr, data.table, matrix packages). ’s main focus grouped weighted statistical programming, complex aggregations transformations, time series panel data operations, programming lists data objects. lead author applied economist created package mainly facilitate advanced computations varied complex data, particular surveys, (multivariate) time series, multilevel / panel data, lists / model objects. secondary aspect applied work data often imported R richer data structures (STATA, SPSS SAS files imported haven). called intelligent suite data manipulation functions can utilize aspects richer data structure (variable labels), preserve data structure / attributes computations. Sometimes specialized classes like xts, pdata.frame grouped_df can also become useful manipulate certain types data. Thus collapse built explicitly supports classes, preserving classes / data structures R. Another objective radically improve speed R code extensively relying efficient algorithms C/C++ faster components base R. collapse ranks among fastest R packages, performs many grouped /weighted computations noticeably faster dplyr data.table. final development objective channel performance stable well conceived user API providing extensive optimized programming capabilities (standard evaluation) also facilitating quick use easy integration existing data manipulation frameworks (particular dplyr / tidyverse data.table, relying non-standard evaluation).","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"data-and-summary-tools","dir":"Articles","previous_headings":"","what":"1. Data and Summary Tools","title":"Introduction to collapse","text":"begin introducing powerful summary tools along 2 panel datasets collapse provides used throughout vignette. just interested programming can skip section. Apart 2 datasets come collapse (wlddev GGDC10S), vignette uses well known datasets base R: mtcars, iris, airquality, time series Airpassengers EuStockMarkets.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"wlddev---world-bank-development-data","dir":"Articles","previous_headings":"1. Data and Summary Tools","what":"1.1 wlddev - World Bank Development Data","title":"Introduction to collapse","text":"dataset contains 5 key World Bank Development Indicators covering 216 countries 61 years (1960-2020). balanced balanced panel 216 \\times 61 = 13176 observations. –> categorical identifiers, date variable artificially generated example dataset contains common data types frequently encountered R. detailed statistical description data computed descr: output descr can converted tidy data frame using: Note descr require data labeled. Since wlddev panel data set tracking countries time, might interested checking variables time-varying, function varying: varying tells us 5 variables PCGDP, LIFEEX, GINI, ODA POP vary time. However OECD variable , data track countries entered OECD. can also detailed look letting varying check variation country: NA indicates data country. general data varying two distinct non-missing values. also take closer look observation counts distinct values using: Note varying efficient fndistinct, although functions fast. Even powerful summary methods multilevel / panel data provided qsu (shorthand quick-summary). modeled STATA’s summarize xtsummarize commands. Calling qsu data gives concise summary. can subset columns internally using cols argument: easily compute statistics region: Computing summary statistics country course also possible much information. Fortunately qsu lets us something much powerful: output reports 3 sets summary statistics variable: Statistics computed Overall (raw) data, -country (.e. country averaged) Within-country (.e. country-demeaned) data1. powerful way summarize panel data aggregating data country gives us cross-section countries variation time, whereas subtracting country specific means data eliminates cross-sectional variation. can statistics tell us data? N/T columns shows PCGDP 8995 total observations, observe GDP data 203 countries average 44.3 observations (time-periods) per country. contrast GINI Index available 161 countries 8.4 observations average. Overall Within mean data identical definition, mean also balanced panel missing observations. practice unequal amounts observations different countries, thus countries different weights Overall mean difference Overall -country mean reflects discrepancy. interesting statistic summary arguably standard deviation, particular comparison -SD reflecting variation countries Within-SD reflecting average variation time. comparison shows PCGDP, LIFEEX GINI vary countries, ODA received varies within countries time. 0 -SD year variable fact Overall Within-SD equal shows year individual invariant. Thus qsu also provides information varying, additional details relative magnitudes cross-sectional time series variation. also common pattern kurtosis increases within-transformed data, skewness decreases cases. also regions look within country variations inside across different World regions: Notice output 4D array summary statistics, also subset ([) permute (aperm) view statistics convenient way. don’t like array, can also output nested list statistics matrices: list statistics matrices , example, converted tidy data frame using unlist2d (section list-processing): yet end qsu’s functionality, can also panel-surveys utilizing weights (w argument). Finally, can look (weighted) pairwise correlations data: can course also computed averaged within-transformed data: useful function called pwcor pwnobs, handy explore joint observation structure selecting variables include statistical model: Note pwcor/pwcov pwnobs faster matrices.","code":"library(collapse) head(wlddev) #       country iso3c       date year decade     region     income  OECD PCGDP LIFEEX GINI       ODA # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 32.446   NA 116769997 # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 32.962   NA 232080002 # 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA 33.471   NA 112839996 # 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA 33.971   NA 237720001 # 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA 34.463   NA 295920013 # 6 Afghanistan   AFG 1966-01-01 1965   1960 South Asia Low income FALSE    NA 34.948   NA 341839996 #       POP # 1 8996973 # 2 9169410 # 3 9351441 # 4 9543205 # 5 9744781 # 6 9956320  # The variables have \"label\" attributes. Use vlabels() to get and set labels namlab(wlddev, class = TRUE) #    Variable     Class # 1   country character # 2     iso3c    factor # 3      date      Date # 4      year   integer # 5    decade   integer # 6    region    factor # 7    income    factor # 8      OECD   logical # 9     PCGDP   numeric # 10   LIFEEX   numeric # 11     GINI   numeric # 12      ODA   numeric # 13      POP   numeric #                                                                                Label # 1                                                                       Country Name # 2                                                                       Country Code # 3                                                         Date Recorded (Fictitious) # 4                                                                               Year # 5                                                                             Decade # 6                                                                             Region # 7                                                                       Income Level # 8                                                            Is OECD Member Country? # 9                                                 GDP per capita (constant 2010 US$) # 10                                           Life expectancy at birth, total (years) # 11                                                  Gini index (World Bank estimate) # 12 Net official development assistance and official aid received (constant 2018 US$) # 13                                                                 Population, total # A fast and detailed statistical description descr(wlddev) # Dataset: wlddev, 13 Variables, N = 13176 # ---------------------------------------------------------------------------------------------------- # country (character): Country Name # Statistics #       N  Ndist #   13176    216 # Table #                       Freq   Perc # Afghanistan             61   0.46 # Albania                 61   0.46 # Algeria                 61   0.46 # American Samoa          61   0.46 # Andorra                 61   0.46 # Angola                  61   0.46 # Antigua and Barbuda     61   0.46 # Argentina               61   0.46 # Armenia                 61   0.46 # Aruba                   61   0.46 # Australia               61   0.46 # Austria                 61   0.46 # Azerbaijan              61   0.46 # Bahamas, The            61   0.46 # ... 202 Others       12322  93.52 #  # Summary of Table Frequencies #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #      61      61      61      61      61      61  # ---------------------------------------------------------------------------------------------------- # iso3c (factor): Country Code # Statistics #       N  Ndist #   13176    216 # Table #                  Freq   Perc # ABW                61   0.46 # AFG                61   0.46 # AGO                61   0.46 # ALB                61   0.46 # AND                61   0.46 # ARE                61   0.46 # ARG                61   0.46 # ARM                61   0.46 # ASM                61   0.46 # ATG                61   0.46 # AUS                61   0.46 # AUT                61   0.46 # AZE                61   0.46 # BDI                61   0.46 # ... 202 Others  12322  93.52 #  # Summary of Table Frequencies #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #      61      61      61      61      61      61  # ---------------------------------------------------------------------------------------------------- # date (Date): Date Recorded (Fictitious) # Statistics #          N       Ndist         Min         Max   #      13176          61  1961-01-01  2021-01-01   # ---------------------------------------------------------------------------------------------------- # year (integer): Year # Statistics #       N  Ndist  Mean     SD   Min   Max  Skew  Kurt #   13176     61  1990  17.61  1960  2020    -0   1.8 # Quantiles #     1%    5%   10%   25%   50%   75%   90%   95%   99% #   1960  1963  1966  1975  1990  2005  2014  2017  2020 # ---------------------------------------------------------------------------------------------------- # decade (integer): Decade # Statistics #       N  Ndist     Mean     SD   Min   Max  Skew  Kurt #   13176      7  1985.57  17.51  1960  2020  0.03  1.79 # Quantiles #     1%    5%   10%   25%   50%   75%   90%   95%   99% #   1960  1960  1960  1970  1990  2000  2010  2010  2020 # ---------------------------------------------------------------------------------------------------- # region (factor): Region # Statistics #       N  Ndist #   13176      7 # Table #                             Freq   Perc # Europe & Central Asia       3538  26.85 # Sub-Saharan Africa          2928  22.22 # Latin America & Caribbean   2562  19.44 # East Asia & Pacific         2196  16.67 # Middle East & North Africa  1281   9.72 # South Asia                   488   3.70 # North America                183   1.39 # ---------------------------------------------------------------------------------------------------- # income (factor): Income Level # Statistics #       N  Ndist #   13176      4 # Table #                      Freq   Perc # High income          4819  36.57 # Upper middle income  3660  27.78 # Lower middle income  2867  21.76 # Low income           1830  13.89 # ---------------------------------------------------------------------------------------------------- # OECD (logical): Is OECD Member Country? # Statistics #       N  Ndist #   13176      2 # Table #         Freq   Perc # FALSE  10980  83.33 # TRUE    2196  16.67 # ---------------------------------------------------------------------------------------------------- # PCGDP (numeric): GDP per capita (constant 2010 US$) # Statistics (28.13% NAs) #      N  Ndist      Mean        SD     Min        Max  Skew   Kurt #   9470   9470  12048.78  19077.64  132.08  196061.42  3.13  17.12 # Quantiles #       1%      5%     10%      25%      50%       75%       90%       95%       99% #   227.71  399.62  555.55  1303.19  3767.16  14787.03  35646.02  48507.84  92340.28 # ---------------------------------------------------------------------------------------------------- # LIFEEX (numeric): Life expectancy at birth, total (years) # Statistics (11.43% NAs) #       N  Ndist  Mean     SD    Min    Max   Skew  Kurt #   11670  10548  64.3  11.48  18.91  85.42  -0.67  2.67 # Quantiles #      1%     5%    10%    25%    50%    75%    90%    95%    99% #   35.83  42.77  46.83  56.36  67.44  72.95  77.08  79.34  82.36 # ---------------------------------------------------------------------------------------------------- # GINI (numeric): Gini index (World Bank estimate) # Statistics (86.76% NAs) #      N  Ndist   Mean   SD   Min   Max  Skew  Kurt #   1744    368  38.53  9.2  20.7  65.8   0.6  2.53 # Quantiles #     1%    5%   10%   25%   50%  75%   90%    95%   99% #   24.6  26.3  27.6  31.5  36.4   45  52.6  55.98  60.5 # ---------------------------------------------------------------------------------------------------- # ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) # Statistics (34.67% NAs) #      N  Ndist        Mean          SD          Min             Max  Skew    Kurt #   8608   7832  454'720131  868'712654  -997'679993  2.56715605e+10  6.98  114.89 # Quantiles #             1%           5%          10%          25%         50%         75%             90% #   -12'593999.7  1'363500.01  8'347000.31  44'887499.8  165'970001  495'042503  1.18400697e+09 #              95%             99% #   1.93281696e+09  3.73380782e+09 # ---------------------------------------------------------------------------------------------------- # POP (numeric): Population, total # Statistics (1.95% NAs) #       N  Ndist         Mean          SD   Min             Max  Skew    Kurt #   12919  12877  24'245971.6  102'120674  2833  1.39771500e+09  9.75  108.91 # Quantiles #        1%       5%      10%     25%       50%        75%          90%          95%         99% #   8698.84  31083.3  62268.4  443791  4'072517  12'816178  46'637331.4  81'177252.5  308'862641 # ---------------------------------------------------------------------------------------------------- head(as.data.frame(descr(wlddev))) #   Variable     Class                      Label     N Ndist   Min   Max     Mean       SD # 1  country character               Country Name 13176   216    NA    NA       NA       NA # 2    iso3c    factor               Country Code 13176   216    NA    NA       NA       NA # 3     date      Date Date Recorded (Fictitious) 13176    61 -3287 18628       NA       NA # 4     year   integer                       Year 13176    61  1960  2020 1990.000 17.60749 # 5   decade   integer                     Decade 13176     7  1960  2020 1985.574 17.51175 # 6   region    factor                     Region 13176     7    NA    NA       NA       NA #            Skew     Kurt   1%   5%  10%  25%  50%  75%  90%  95%  99% # 1            NA       NA   NA   NA   NA   NA   NA   NA   NA   NA   NA # 2            NA       NA   NA   NA   NA   NA   NA   NA   NA   NA   NA # 3            NA       NA   NA   NA   NA   NA   NA   NA   NA   NA   NA # 4 -5.812381e-16 1.799355 1960 1963 1966 1975 1990 2005 2014 2017 2020 # 5  3.256512e-02 1.791726 1960 1960 1960 1970 1990 2000 2010 2010 2020 # 6            NA       NA   NA   NA   NA   NA   NA   NA   NA   NA   NA varying(wlddev, wlddev$iso3c) # country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX    GINI     ODA  #   FALSE   FALSE    TRUE    TRUE    TRUE   FALSE   FALSE   FALSE    TRUE    TRUE    TRUE    TRUE  #     POP  #    TRUE head(varying(wlddev, wlddev$iso3c, any_group = FALSE)) #     country iso3c date year decade region income  OECD PCGDP LIFEEX GINI  ODA  POP # ABW   FALSE FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE   NA TRUE TRUE # AFG   FALSE FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE   NA TRUE TRUE # AGO   FALSE FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE TRUE TRUE TRUE # ALB   FALSE FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE TRUE TRUE TRUE # AND   FALSE FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE     NA   NA   NA TRUE # ARE   FALSE FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE TRUE TRUE TRUE head(fnobs(wlddev, wlddev$iso3c)) #     country iso3c date year decade region income OECD PCGDP LIFEEX GINI ODA POP # ABW      61    61   61   61     61     61     61   61    32     60    0  20  60 # AFG      61    61   61   61     61     61     61   61    18     60    0  60  60 # AGO      61    61   61   61     61     61     61   61    40     60    3  58  60 # ALB      61    61   61   61     61     61     61   61    40     60    9  32  60 # AND      61    61   61   61     61     61     61   61    50      0    0   0  60 # ARE      61    61   61   61     61     61     61   61    45     60    2  45  60  head(fndistinct(wlddev, wlddev$iso3c)) #     country iso3c date year decade region income OECD PCGDP LIFEEX GINI ODA POP # ABW       1     1   61   61      7      1      1    1    32     60    0  20  60 # AFG       1     1   61   61      7      1      1    1    18     60    0  60  60 # AGO       1     1   61   61      7      1      1    1    40     59    3  58  60 # ALB       1     1   61   61      7      1      1    1    40     59    9  32  60 # AND       1     1   61   61      7      1      1    1    50      0    0   0  60 # ARE       1     1   61   61      7      1      1    1    45     60    2  45  60 qsu(wlddev, cols = 9:12, higher = TRUE) # higher adds skewness and kurtosis #             N        Mean          SD          Min             Max     Skew     Kurt # PCGDP    9470   12048.778  19077.6416     132.0776      196061.417   3.1276  17.1154 # LIFEEX  11670     64.2963     11.4764       18.907         85.4171  -0.6748   2.6718 # GINI     1744     38.5341      9.2006         20.7            65.8    0.596   2.5329 # ODA      8608  454'720131  868'712654  -997'679993  2.56715605e+10   6.9832  114.889 qsu(wlddev, by = ~region, cols = 9:12, vlabels = TRUE, higher = TRUE) # , , PCGDP: GDP per capita (constant 2010 US$) #  #                                N        Mean          SD         Min         Max    Skew     Kurt # East Asia & Pacific         1467  10513.2441  14383.5507    132.0776  71992.1517  1.6392   4.7419 # Europe & Central Asia       2243  25992.9618  26435.1316    366.9354  196061.417  2.2022  10.1977 # Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085  88391.3331  4.1702  29.3739 # Middle East & North Africa   842  13878.4213  18419.7912    578.5996  116232.753  2.4178   9.7669 # North America                180    48699.76  24196.2855  16405.9053  113236.091   0.938   2.9688 # South Asia                   382   1235.9256   1611.2232    265.9625    8476.564  2.7874  10.3402 # Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366  20532.9523  3.1161  14.4175 #  # , , LIFEEX: Life expectancy at birth, total (years) #  #                                N     Mean       SD      Min      Max     Skew    Kurt # East Asia & Pacific         1807  65.9445  10.1633   18.907   85.078   -0.856  4.3125 # Europe & Central Asia       3046  72.1625   5.7602   45.369  85.4171  -0.5594  4.0434 # Latin America & Caribbean   2107  68.3486   7.3768   41.762  82.1902  -1.0357  3.9379 # Middle East & North Africa  1226  66.2508   9.8306   29.919  82.8049  -0.8782  3.3054 # North America                144  76.2867   3.5734  68.8978  82.0488  -0.1963   1.976 # South Asia                   480  57.5585  11.3004   32.446   78.921  -0.2623  2.1147 # Sub-Saharan Africa          2860   51.581   8.6876   26.172  74.5146   0.1452  2.7245 #  # , , GINI: Gini index (World Bank estimate) #  #                               N     Mean      SD   Min   Max     Skew    Kurt # East Asia & Pacific         154  37.7571  5.0318  27.8  49.1   0.3631  2.3047 # Europe & Central Asia       798  31.9114  4.5809  20.7  48.4   0.2989  2.5254 # Latin America & Caribbean   413  49.9557  5.4821  34.4  63.3  -0.0386  2.3631 # Middle East & North Africa   91  36.0143  5.2073    26  47.4   0.0241  1.9209 # North America                49  37.4816  3.6972    31  41.5  -0.4282  1.4577 # South Asia                   46  33.8804  3.9898  25.9  43.8   0.4205  2.7748 # Sub-Saharan Africa          193  44.6606  8.2003  29.8  65.8   0.6598  2.8451 #  # , , ODA: Net official development assistance and official aid received (constant 2018 US$) #  #                                N            Mean              SD           Min             Max # East Asia & Pacific         1537      352'017964      622'847624   -997'679993  4.04487988e+09 # Europe & Central Asia        787      402'455286      568'237036   -322'070007  4.34612988e+09 # Latin America & Caribbean   1972      172'880081      260'781049   -444'040009  2.99568994e+09 # Middle East & North Africa  1105      732'380009  1.52108993e+09   -141'789993  2.56715605e+10 # North America                 39      468717.916     10'653560.8  -15'869999.9     61'509998.3 # South Asia                   466  1.27049955e+09  1.61492889e+09   -247'369995  8.75425977e+09 # Sub-Saharan Africa          2702      486'371750      656'336230  -18'409999.8  1.18790801e+10 #                               Skew     Kurt # East Asia & Pacific          2.722  11.5221 # Europe & Central Asia       3.1305  15.2525 # Latin America & Caribbean   3.3259  22.4569 # Middle East & North Africa  6.6304  79.2238 # North America               4.8602  29.3092 # South Asia                  1.7923    6.501 # Sub-Saharan Africa          4.5456  48.8447 qsu(wlddev, pid = ~ iso3c, cols = c(1,4,9:12), vlabels = TRUE, higher = TRUE) # , , country: Country Name #  #            N/T  Mean  SD  Min  Max  Skew  Kurt # Overall  13176     -   -    -    -     -     - # Between    216     -   -    -    -     -     - # Within      61     -   -    -    -     -     - #  # , , year: Year #  #            N/T  Mean       SD   Min   Max  Skew    Kurt # Overall  13176  1990  17.6075  1960  2020    -0  1.7994 # Between    216  1990        0  1990  1990     -       - # Within      61  1990  17.6075  1960  2020    -0  1.7994 #  # , , PCGDP: GDP per capita (constant 2010 US$) #  #              N/T        Mean          SD          Min         Max    Skew     Kurt # Overall     9470   12048.778  19077.6416     132.0776  196061.417  3.1276  17.1154 # Between      206  12962.6054  20189.9007     253.1886   141200.38  3.1263  16.2299 # Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254  0.6576  17.2003 #  # , , LIFEEX: Life expectancy at birth, total (years) #  #              N/T     Mean       SD      Min      Max     Skew    Kurt # Overall    11670  64.2963  11.4764   18.907  85.4171  -0.6748  2.6718 # Between      207  64.9537   9.8936  40.9663  85.4171  -0.5012  2.1693 # Within   56.3768  64.2963   6.0842  32.9068  84.4198  -0.2643  3.7027 #  # , , GINI: Gini index (World Bank estimate) #  #              N/T     Mean      SD      Min      Max    Skew    Kurt # Overall     1744  38.5341  9.2006     20.7     65.8   0.596  2.5329 # Between      167  39.4233  8.1356  24.8667  61.7143  0.5832  2.8256 # Within   10.4431  38.5341  2.9277  25.3917  55.3591  0.3263  5.3389 #  # , , ODA: Net official development assistance and official aid received (constant 2018 US$) #  #              N/T        Mean          SD              Min             Max    Skew      Kurt # Overall     8608  454'720131  868'712654      -997'679993  2.56715605e+10  6.9832   114.889 # Between      178  439'168412  569'049959       468717.916  3.62337432e+09   2.355    9.9487 # Within   48.3596  454'720131  650'709624  -2.44379420e+09  2.45610972e+10  9.6047  263.3716 qsu(wlddev, by = ~ region, pid = ~ iso3c, cols = 9:12, vlabels = TRUE, higher = TRUE) # , , Overall, PCGDP: GDP per capita (constant 2010 US$) #  #                              N/T        Mean          SD         Min         Max    Skew     Kurt # East Asia & Pacific         1467  10513.2441  14383.5507    132.0776  71992.1517  1.6392   4.7419 # Europe & Central Asia       2243  25992.9618  26435.1316    366.9354  196061.417  2.2022  10.1977 # Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085  88391.3331  4.1702  29.3739 # Middle East & North Africa   842  13878.4213  18419.7912    578.5996  116232.753  2.4178   9.7669 # North America                180    48699.76  24196.2855  16405.9053  113236.091   0.938   2.9688 # South Asia                   382   1235.9256   1611.2232    265.9625    8476.564  2.7874  10.3402 # Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366  20532.9523  3.1161  14.4175 #  # , , Between, PCGDP: GDP per capita (constant 2010 US$) #  #                             N/T        Mean          SD         Min         Max    Skew     Kurt # East Asia & Pacific          34  10513.2441   12771.742    444.2899  39722.0077  1.1488   2.7089 # Europe & Central Asia        56  25992.9618   24051.035    809.4753   141200.38  2.0026   9.0733 # Latin America & Caribbean    38   7628.4477   8470.9708   1357.3326  77403.7443  4.4548  32.4956 # Middle East & North Africa   20  13878.4213  17251.6962   1069.6596  64878.4021  1.9508   6.0796 # North America                 3    48699.76  18604.4369  35260.4708  74934.5874  0.7065      1.5 # South Asia                    8   1235.9256   1488.3669      413.68   6621.5002  3.0546  11.3083 # Sub-Saharan Africa           47   1840.0259   2234.3254    253.1886   9922.0052  2.1442   6.8259 #  # , , Within, PCGDP: GDP per capita (constant 2010 US$) #  #                                 N/T       Mean          SD          Min         Max     Skew # East Asia & Pacific         43.1471  12048.778   6615.8248  -11964.6472   49541.463    0.824 # Europe & Central Asia       40.0536  12048.778  10971.0483  -33504.8721  76767.5254   0.4307 # Latin America & Caribbean        52  12048.778   2451.2636    -354.1639  23036.3668   0.1259 # Middle East & North Africa     42.1  12048.778   6455.0512  -18674.4049  63665.0446   1.8525 # North America                    60  12048.778  15470.4609  -29523.1017  50350.2816  -0.2451 # South Asia                    47.75  12048.778    617.0934   10026.9155   14455.865   0.9846 # Sub-Saharan Africa          50.6383  12048.778    1321.764    4846.3834  24883.1246   1.3879 #                                Kurt # East Asia & Pacific          8.9418 # Europe & Central Asia        7.4139 # Latin America & Caribbean    7.1939 # Middle East & North Africa  23.0457 # North America                3.2075 # South Asia                   5.6366 # Sub-Saharan Africa          28.0186 #  # , , Overall, LIFEEX: Life expectancy at birth, total (years) #  #                              N/T     Mean       SD      Min      Max     Skew    Kurt # East Asia & Pacific         1807  65.9445  10.1633   18.907   85.078   -0.856  4.3125 # Europe & Central Asia       3046  72.1625   5.7602   45.369  85.4171  -0.5594  4.0434 # Latin America & Caribbean   2107  68.3486   7.3768   41.762  82.1902  -1.0357  3.9379 # Middle East & North Africa  1226  66.2508   9.8306   29.919  82.8049  -0.8782  3.3054 # North America                144  76.2867   3.5734  68.8978  82.0488  -0.1963   1.976 # South Asia                   480  57.5585  11.3004   32.446   78.921  -0.2623  2.1147 # Sub-Saharan Africa          2860   51.581   8.6876   26.172  74.5146   0.1452  2.7245 #  # , , Between, LIFEEX: Life expectancy at birth, total (years) #  #                             N/T     Mean      SD      Min      Max     Skew    Kurt # East Asia & Pacific          32  65.9445  7.6833  49.7995  77.9008  -0.3832  2.4322 # Europe & Central Asia        55  72.1625  4.4378  60.1129  85.4171  -0.6584  2.8874 # Latin America & Caribbean    40  68.3486  4.9199  53.4918  82.1902  -0.9947  4.1617 # Middle East & North Africa   21  66.2508   5.922  52.5371  76.7395  -0.3181  3.0331 # North America                 3  76.2867  1.3589  74.8065  78.4175   0.1467  1.6356 # South Asia                    8  57.5585  5.6158  49.1972  69.3429   0.6643  3.1288 # Sub-Saharan Africa           48   51.581   5.657  40.9663  71.5749   1.1333   4.974 #  # , , Within, LIFEEX: Life expectancy at birth, total (years) #  #                                 N/T     Mean      SD      Min      Max     Skew    Kurt # East Asia & Pacific         56.4688  64.2963  6.6528  32.9068  83.9918  -0.3949  3.9528 # Europe & Central Asia       55.3818  64.2963  3.6723  46.3045  78.6265  -0.0307  3.7576 # Latin America & Caribbean    52.675  64.2963  5.4965  46.7831  79.5026  -0.3827  2.9936 # Middle East & North Africa   58.381  64.2963  7.8467  41.6187  78.8872  -0.6216   2.808 # North America                    48  64.2963  3.3049  54.7766  69.4306  -0.4327  2.3027 # South Asia                       60  64.2963  9.8062  41.4342  83.0122  -0.0946  2.1035 # Sub-Saharan Africa          59.5833  64.2963  6.5933  41.5678  84.4198   0.0811  2.7821 #  # , , Overall, GINI: Gini index (World Bank estimate) #  #                             N/T     Mean      SD   Min   Max     Skew    Kurt # East Asia & Pacific         154  37.7571  5.0318  27.8  49.1   0.3631  2.3047 # Europe & Central Asia       798  31.9114  4.5809  20.7  48.4   0.2989  2.5254 # Latin America & Caribbean   413  49.9557  5.4821  34.4  63.3  -0.0386  2.3631 # Middle East & North Africa   91  36.0143  5.2073    26  47.4   0.0241  1.9209 # North America                49  37.4816  3.6972    31  41.5  -0.4282  1.4577 # South Asia                   46  33.8804  3.9898  25.9  43.8   0.4205  2.7748 # Sub-Saharan Africa          193  44.6606  8.2003  29.8  65.8   0.6598  2.8451 #  # , , Between, GINI: Gini index (World Bank estimate) #  #                             N/T     Mean      SD      Min      Max     Skew    Kurt # East Asia & Pacific          23  37.7571  4.3005     30.8  45.8857   0.4912   2.213 # Europe & Central Asia        49  31.9114  4.0611  24.8667   40.935   0.3323   2.291 # Latin America & Caribbean    25  49.9557  4.0492     41.1     57.9     0.03  2.2573 # Middle East & North Africa   15  36.0143  4.7002    29.05     42.7  -0.2035  1.6815 # North America                 2  37.4816  3.3563  33.1222  40.0129  -0.5503  1.3029 # South Asia                    7  33.8804  3.0052  30.3556     38.8   0.2786  1.4817 # Sub-Saharan Africa           46  44.6606  6.8844    34.52  61.7143   0.9464  3.2302 #  # , , Within, GINI: Gini index (World Bank estimate) #  #                                 N/T     Mean      SD      Min      Max     Skew    Kurt # East Asia & Pacific          6.6957  38.5341  2.6125  31.0187  45.8901  -0.0585  3.0933 # Europe & Central Asia       16.2857  38.5341  2.1195  31.2841  50.1387   0.6622  6.1763 # Latin America & Caribbean     16.52  38.5341  3.6955  25.3917  48.8341  -0.0506  2.7603 # Middle East & North Africa   6.0667  38.5341  2.2415  31.7675   45.777   0.0408  4.7415 # North America                  24.5  38.5341  1.5507  33.0212  42.7119  -1.3213  6.8321 # South Asia                   6.5714  38.5341  2.6244  32.8341  45.0675  -0.1055  2.6885 # Sub-Saharan Africa           4.1957  38.5341  4.4553  27.9452  55.3591   0.6338  4.4174 #  # , , Overall, ODA: Net official development assistance and official aid received (constant 2018 US$) #  #                              N/T            Mean              SD           Min             Max # East Asia & Pacific         1537      352'017964      622'847624   -997'679993  4.04487988e+09 # Europe & Central Asia        787      402'455286      568'237036   -322'070007  4.34612988e+09 # Latin America & Caribbean   1972      172'880081      260'781049   -444'040009  2.99568994e+09 # Middle East & North Africa  1105      732'380009  1.52108993e+09   -141'789993  2.56715605e+10 # North America                 39      468717.916     10'653560.8  -15'869999.9     61'509998.3 # South Asia                   466  1.27049955e+09  1.61492889e+09   -247'369995  8.75425977e+09 # Sub-Saharan Africa          2702      486'371750      656'336230  -18'409999.8  1.18790801e+10 #                               Skew     Kurt # East Asia & Pacific          2.722  11.5221 # Europe & Central Asia       3.1305  15.2525 # Latin America & Caribbean   3.3259  22.4569 # Middle East & North Africa  6.6304  79.2238 # North America               4.8602  29.3092 # South Asia                  1.7923    6.501 # Sub-Saharan Africa          4.5456  48.8447 #  # , , Between, ODA: Net official development assistance and official aid received (constant 2018 US$) #  #                             N/T            Mean              SD          Min             Max # East Asia & Pacific          31      352'017964      457'183279  1'654615.38  1.63585532e+09 # Europe & Central Asia        32      402'455286      438'074771  12'516000.1  2.05456932e+09 # Latin America & Caribbean    37      172'880081      167'160838  2'225483.88      538'386665 # Middle East & North Africa   21      732'380009      775'418887   3'112820.5  2.86174883e+09 # North America                 1      468717.916               0   468717.916      468717.916 # South Asia                    8  1.27049955e+09  1.18347893e+09  27'152499.9  3.62337432e+09 # Sub-Saharan Africa           48      486'371750      397'995105  28'801206.9  1.55049113e+09 #                               Skew    Kurt # East Asia & Pacific         1.7771  5.1361 # Europe & Central Asia       2.0449  7.2489 # Latin America & Caribbean   0.8981  2.4954 # Middle East & North Africa  1.1363  3.6377 # North America                    -       - # South Asia                  0.7229  2.4072 # Sub-Saharan Africa          0.9871  3.1513 #  # , , Within, ODA: Net official development assistance and official aid received (constant 2018 US$) #  #                                 N/T        Mean              SD              Min             Max # East Asia & Pacific         49.5806  454'720131      422'992450  -2.04042108e+09  3.59673152e+09 # Europe & Central Asia       24.5938  454'720131      361'916875  -1.08796786e+09  3.30549004e+09 # Latin America & Caribbean   53.2973  454'720131      200'159960      -527'706542  3.28976141e+09 # Middle East & North Africa   52.619  454'720131  1.30860235e+09  -2.34610870e+09  2.45610972e+10 # North America                    39  454'720131     10'653560.8       438'381413      515'761411 # South Asia                    58.25  454'720131  1.09880524e+09  -2.44379420e+09  5.58560558e+09 # Sub-Saharan Africa          56.2917  454'720131      521'897637      -952'168698  1.12814455e+10 #                               Skew     Kurt # East Asia & Pacific         0.2908  14.4428 # Europe & Central Asia       2.3283  18.6937 # Latin America & Caribbean   3.7015  41.7506 # Middle East & North Africa  7.8663  117.987 # North America               4.8602  29.3092 # South Asia                  1.8418   9.4588 # Sub-Saharan Africa          5.2349  86.1042 l <- qsu(wlddev, by = ~ region, pid = ~ iso3c, cols = 9:12, vlabels = TRUE,          higher = TRUE, array = FALSE)  str(l, give.attr = FALSE) # List of 4 #  $ PCGDP: GDP per capita (constant 2010 US$)                                             :List of 3 #   ..$ Overall: 'qsu' num [1:7, 1:7] 1467 2243 1976 842 180 ... #   ..$ Between: 'qsu' num [1:7, 1:7] 34 56 38 20 3 ... #   ..$ Within : 'qsu' num [1:7, 1:7] 43.1 40.1 52 42.1 60 ... #  $ LIFEEX: Life expectancy at birth, total (years)                                       :List of 3 #   ..$ Overall: 'qsu' num [1:7, 1:7] 1807 3046 2107 1226 144 ... #   ..$ Between: 'qsu' num [1:7, 1:7] 32 55 40 21 3 ... #   ..$ Within : 'qsu' num [1:7, 1:7] 56.5 55.4 52.7 58.4 48 ... #  $ GINI: Gini index (World Bank estimate)                                                :List of 3 #   ..$ Overall: 'qsu' num [1:7, 1:7] 154 798 413 91 49 ... #   ..$ Between: 'qsu' num [1:7, 1:7] 23 49 25 15 2 ... #   ..$ Within : 'qsu' num [1:7, 1:7] 6.7 16.29 16.52 6.07 24.5 ... #  $ ODA: Net official development assistance and official aid received (constant 2018 US$):List of 3 #   ..$ Overall: 'qsu' num [1:7, 1:7] 1537 787 1972 1105 39 ... #   ..$ Between: 'qsu' num [1:7, 1:7] 31 32 37 21 1 ... #   ..$ Within : 'qsu' num [1:7, 1:7] 49.6 24.6 53.3 52.6 39 ... head(unlist2d(l, idcols = c(\"Variable\", \"Trans\"), row.names = \"Region\")) #                                    Variable   Trans                     Region    N      Mean # 1 PCGDP: GDP per capita (constant 2010 US$) Overall        East Asia & Pacific 1467 10513.244 # 2 PCGDP: GDP per capita (constant 2010 US$) Overall      Europe & Central Asia 2243 25992.962 # 3 PCGDP: GDP per capita (constant 2010 US$) Overall  Latin America & Caribbean 1976  7628.448 # 4 PCGDP: GDP per capita (constant 2010 US$) Overall Middle East & North Africa  842 13878.421 # 5 PCGDP: GDP per capita (constant 2010 US$) Overall              North America  180 48699.760 # 6 PCGDP: GDP per capita (constant 2010 US$) Overall                 South Asia  382  1235.926 #          SD        Min        Max      Skew      Kurt # 1 14383.551   132.0776  71992.152 1.6392248  4.741856 # 2 26435.132   366.9354 196061.417 2.2022472 10.197685 # 3  8818.505  1005.4085  88391.333 4.1701769 29.373869 # 4 18419.791   578.5996 116232.753 2.4177586  9.766883 # 5 24196.285 16405.9053 113236.091 0.9380056  2.968769 # 6  1611.223   265.9625   8476.564 2.7873830 10.340176 pwcor(wlddev[9:12], N = TRUE, P = TRUE) #               PCGDP        LIFEEX         GINI          ODA # PCGDP    1   (9470)   .57* (9022) -.44* (1735) -.16* (7128) # LIFEEX  .57* (9022)   1   (11670) -.35* (1742) -.02  (8142) # GINI   -.44* (1735)  -.35* (1742)   1   (1744) -.20* (1109) # ODA    -.16* (7128)  -.02  (8142) -.20* (1109)   1   (8608) print(pwcor(fmean(wlddev[9:12], wlddev$iso3c), N = TRUE, P = TRUE), show = \"lower.tri\") #              PCGDP      LIFEEX        GINI         ODA # PCGDP    1   (206)                                     # LIFEEX  .60* (199)   1   (207)                         # GINI   -.42* (165) -.40* (165)   1   (167)             # ODA    -.25* (172) -.21* (172) -.19* (145)   1   (178)  # N is same as overall N shown above... print(pwcor(fwithin(wlddev[9:12], wlddev$iso3c), P = TRUE), show = \"lower.tri\") #         PCGDP LIFEEX   GINI    ODA # PCGDP     1                        # LIFEEX   .31*    1                 # GINI    -.01   -.16*    1          # ODA     -.01    .17*  -.08*    1 pwnobs(wlddev) #         country iso3c  date  year decade region income  OECD PCGDP LIFEEX GINI  ODA   POP # country   13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # iso3c     13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # date      13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # year      13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # decade    13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # region    13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # income    13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # OECD      13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 8608 12919 # PCGDP      9470  9470  9470  9470   9470   9470   9470  9470  9470   9022 1735 7128  9470 # LIFEEX    11670 11670 11670 11670  11670  11670  11670 11670  9022  11670 1742 8142 11659 # GINI       1744  1744  1744  1744   1744   1744   1744  1744  1735   1742 1744 1109  1744 # ODA        8608  8608  8608  8608   8608   8608   8608  8608  7128   8142 1109 8608  8597 # POP       12919 12919 12919 12919  12919  12919  12919 12919  9470  11659 1744 8597 12919"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"ggdc10s---ggdc-10-sector-database","dir":"Articles","previous_headings":"1. Data and Summary Tools","what":"1.2 GGDC10S - GGDC 10-Sector Database","title":"Introduction to collapse","text":"Groningen Growth Development Centre 10-Sector Database provides long-run data sectoral productivity performance Africa, Asia, Latin America. Variables covered data set annual series value added (VA, local currency), persons employed (EMP) 10 broad sectors. first problem summarizing data value added (VA) local currency, second contains 2 different Variables (VA EMP) stacked column. One way solving first problem converting data percentages dividing overall VA EMP contained last column. different solution involving grouped-scaling introduced section 6.4. second problem nicely handled qsu, can also compute panel-statistics groups. statistics show dataset consistent: Employment data cover 42 countries 53 time-periods almost sectors. Agriculture largest sector terms employment, amounting 35% share employment across countries time, standard deviation (SD) around 27%. -country SD agricultural employment share 24% within SD 12%, indicating processes structural change gradual variation structure countries. next largest sectors agriculture manufacturing, wholesale retail trade government, claiming approx. 15% share economy. sectors -country SD also twice large within-country SD. terms value added, data covers 43 countries 50 time-periods. Agriculture, manufacturing, wholesale retail trade government also largest sectors terms VA, diminished agricultural share (around 17%) greater share manufacturing (around 20%). variation countries greater variation within countries, seems least terms agricultural VA share also considerable within-country SD 8%. also true finance real estate sector within SD 9%, suggesting (using bit common sense) diminishing VA share agriculture increased VA share finance real estate pattern characterizing countries sample. final step consider plot function can used plot structural transformation supported country. Botswana: plot chunk scplot_BWA","code":"head(GGDC10S) #   Country Regioncode             Region Variable Year      AGR      MIN       MAN        PU # 1     BWA        SSA Sub-saharan Africa       VA 1960       NA       NA        NA        NA # 2     BWA        SSA Sub-saharan Africa       VA 1961       NA       NA        NA        NA # 3     BWA        SSA Sub-saharan Africa       VA 1962       NA       NA        NA        NA # 4     BWA        SSA Sub-saharan Africa       VA 1963       NA       NA        NA        NA # 5     BWA        SSA Sub-saharan Africa       VA 1964 16.30154 3.494075 0.7365696 0.1043936 # 6     BWA        SSA Sub-saharan Africa       VA 1965 15.72700 2.495768 1.0181992 0.1350976 #         CON      WRT      TRA     FIRE      GOV      OTH      SUM # 1        NA       NA       NA       NA       NA       NA       NA # 2        NA       NA       NA       NA       NA       NA       NA # 3        NA       NA       NA       NA       NA       NA       NA # 4        NA       NA       NA       NA       NA       NA       NA # 5 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229 # 6 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710  namlab(GGDC10S, class = TRUE) #      Variable     Class                                                 Label # 1     Country character                                               Country # 2  Regioncode character                                           Region code # 3      Region character                                                Region # 4    Variable character                                              Variable # 5        Year   numeric                                                  Year # 6         AGR   numeric                                          Agriculture  # 7         MIN   numeric                                                Mining # 8         MAN   numeric                                         Manufacturing # 9          PU   numeric                                             Utilities # 10        CON   numeric                                          Construction # 11        WRT   numeric                         Trade, restaurants and hotels # 12        TRA   numeric                  Transport, storage and communication # 13       FIRE   numeric Finance, insurance, real estate and business services # 14        GOV   numeric                                   Government services # 15        OTH   numeric               Community, social and personal services # 16        SUM   numeric                               Summation of sector GDP  fnobs(GGDC10S) #    Country Regioncode     Region   Variable       Year        AGR        MIN        MAN         PU  #       5027       5027       5027       5027       5027       4364       4355       4355       4354  #        CON        WRT        TRA       FIRE        GOV        OTH        SUM  #       4355       4355       4355       4355       3482       4248       4364  fndistinct(GGDC10S) #    Country Regioncode     Region   Variable       Year        AGR        MIN        MAN         PU  #         43          6          6          2         67       4353       4224       4353       4237  #        CON        WRT        TRA       FIRE        GOV        OTH        SUM  #       4339       4344       4334       4349       3470       4238       4364  # The countries included: cat(funique(GGDC10S$Country, sort = TRUE)) # ARG BOL BRA BWA CHL CHN COL CRI DEW DNK EGY ESP ETH FRA GBR GHA HKG IDN IND ITA JPN KEN KOR MEX MOR MUS MWI MYS NGA NGA(alt) NLD PER PHL SEN SGP SWE THA TWN TZA USA VEN ZAF ZMB # Converting data to percentages of overall VA / EMP, dapply keeps the attributes, see section 6.1 pGGDC10S <- ftransformv(GGDC10S, 6:15, `*`, 100 / SUM)  # Summarizing the sectoral data by variable, overall, between and within countries su <- qsu(pGGDC10S, by = ~ Variable, pid = ~ Variable + Country,           cols = 6:16, higher = TRUE)  # This gives a 4D array of summary statistics str(su) #  'qsu' num [1:2, 1:7, 1:3, 1:11] 2225 2139 35.1 17.3 26.7 ... #  - attr(*, \"dimnames\")=List of 4 #   ..$ : chr [1:2] \"EMP\" \"VA\" #   ..$ : chr [1:7] \"N/T\" \"Mean\" \"SD\" \"Min\" ... #   ..$ : chr [1:3] \"Overall\" \"Between\" \"Within\" #   ..$ : chr [1:11] \"AGR\" \"MIN\" \"MAN\" \"PU\" ...  # Permuting this array to a more readible format aperm(su, c(4L, 2L, 3L, 1L)) # , , Overall, EMP #  #        N/T        Mean          SD       Min      Max     Skew     Kurt # AGR   2225     35.0949     26.7235     0.156      100   0.4856   2.0951 # MIN   2216      1.0349      1.4247    0.0043   9.4097   3.1281  15.0429 # MAN   2216     14.9768      8.0392    0.5822  45.2974   0.4272   2.8455 # PU    2215      0.5782      0.3601    0.0154   2.4786   1.2588   5.5822 # CON   2216      5.6583      2.9252    0.1417  15.9887  -0.0631   2.2725 # WRT   2216     14.9155      6.5573     0.809  32.8046  -0.1814   2.3226 # TRA   2216      4.8193       2.652    0.1506  15.0454   0.9477   4.4695 # FIRE  2216      4.6501      4.3518    0.0799  21.7717   1.2345   4.0831 # GOV   1780     13.1263      8.0844         0  34.8897   0.6301   2.5338 # OTH   2109      8.3977      6.6409     0.421  34.8942   1.4028   4.3191 # SUM   2225  36846.8741  96318.6544  173.8829   764200   5.0229  30.9814 #  # , , Between, EMP #  #       N/T        Mean         SD       Min         Max     Skew     Kurt # AGR    42     35.0949    24.1204    0.9997     88.3263   0.5202   2.2437 # MIN    42      1.0349     1.2304    0.0296      6.8532   2.7313   12.331 # MAN    42     14.9768     7.0375     1.718     32.3439  -0.0164   2.4321 # PU     42      0.5782     0.3041    0.0671      1.3226   0.5459   2.6905 # CON    42      5.6583     2.4748    0.5037     10.3691  -0.4442   2.3251 # WRT    42     14.9155      5.264    4.0003     26.7699  -0.5478   2.7294 # TRA    42      4.8193     2.4712     0.374     12.3887   0.9782   4.7857 # FIRE   42      4.6501     3.4468    0.1505     12.4402   0.6052   2.5883 # GOV    34     13.1263     7.2832    2.0086     29.1577   0.3858   2.1068 # OTH    40      8.3977      6.266    1.3508     26.4036   1.4349   4.3185 # SUM    42  36846.8741  89205.503  369.2353  485820.474   4.0761  19.3159 #  # , , Within, EMP #  #           N/T         Mean          SD          Min          Max     Skew     Kurt # AGR   52.9762      26.3768     11.5044      -5.3234     107.4891   1.6002  11.9683 # MIN   52.7619       3.4006      0.7182      -1.4068        7.509  -0.1988  15.0343 # MAN   52.7619       17.476      3.8861      -1.1061      40.3964   -0.082   7.3994 # PU    52.7381       1.3896      0.1929       0.6346       2.5461   0.5731   7.8523 # CON   52.7619       5.7633      1.5596       0.8964      12.9663   0.3077   4.1248 # WRT   52.7619      15.7581        3.91       3.7356      29.7615   0.3339   3.3386 # TRA   52.7619       6.3486      0.9623       2.3501      11.1064   0.2671   5.7162 # FIRE  52.7619       5.8228      2.6567      -2.9836      15.9974   0.5486   4.0288 # GOV   52.3529       13.263      3.5088      -2.1983       23.611  -0.5647   4.7286 # OTH    52.725       7.3941      2.1999      -2.3286      17.4413   0.2929   6.4631 # SUM   52.9762  21'566436.8  36327.1443  21'287906.3  21'844816.3   0.6649  34.2495 #  # , , Overall, VA #  #        N/T         Mean          SD       Min             Max     Skew      Kurt # AGR   2139      17.3082     15.5066    0.0318          95.222   1.3274    4.8827 # MIN   2139       5.8514      9.0975         0         59.0602   2.7193   10.9184 # MAN   2139      20.0651      8.0033     0.979         41.6281  -0.0348    2.6831 # PU    2139       2.2298      1.1088         0          9.1888   0.8899    6.2385 # CON   2139       5.8721      2.5113    0.5951         25.8575   1.5002    8.9578 # WRT   2139       16.631      5.1374    4.5187         39.7594   0.3455    3.2655 # TRA   2139       7.9329      3.1057    0.7957         25.9625   1.0122    5.7137 # FIRE  2139       7.0377     12.7077  -151.065         39.1705  -6.2254   59.8739 # GOV   1702       13.406      6.3521    0.7607         32.5107   0.4888    2.9043 # OTH   2139       6.4046      5.8416    0.2327         31.4474   1.4978    4.2051 # SUM   2139  43'961639.1  358'350627         0  8.06794210e+09  15.7682  289.4632 #  # , , Between, VA #  #       N/T         Mean          SD        Min             Max     Skew     Kurt # AGR    43      17.3082     13.1901     0.6058         63.8364   1.1328   4.7111 # MIN    43       5.8514      7.5705     0.0475         27.9214   1.7113    4.807 # MAN    43      20.0651      6.6423     4.1869         32.1138  -0.3591    2.619 # PU     43       2.2298      0.7457     0.4462           4.307   0.6196   3.8724 # CON    43       5.8721      1.8455     2.9405         12.9279   1.3285    6.505 # WRT    43       16.631      4.3779     8.4188         26.3876    0.292   2.4553 # TRA    43       7.9329      2.7222      2.037         14.8892   0.6362   3.6686 # FIRE   43       7.0377      9.0284   -35.6144         23.8658   -2.674  15.0975 # GOV    35       13.406       5.875     1.9757         27.7714   0.5198   3.0416 # OTH    43       6.4046      5.6137     1.1184         19.5299   1.3274   3.2043 # SUM    43  43'961639.1  185'785836  5077.7231  1.23317892e+09   5.8098  36.9778 #  # , , Within, VA #  #           N/T         Mean          SD              Min             Max     Skew      Kurt # AGR   49.7442      26.3768      8.1532            5.245         94.3499    1.234    9.5269 # MIN   49.7442       3.4006      5.0451          -20.051         35.7053    0.341    13.102 # MAN   49.7442       17.476      4.4647           1.1188         36.3501  -0.1928    3.9339 # PU    49.7442       1.3896      0.8206          -1.0904          6.2714   0.5258    5.3462 # CON   49.7442       5.7633      1.7031          -0.3464         18.6929   0.7493    6.3751 # WRT   49.7442      15.7581      2.6884           4.6513         32.6691   0.2338    4.4953 # TRA   49.7442       6.3486      1.4951           0.9187         18.5977   0.6995   10.1129 # FIRE  49.7442       5.8228      8.9428        -109.6278         54.1241  -2.7728   54.5971 # GOV   48.6286       13.263      2.4153           5.1249         22.8497   0.1663    3.3083 # OTH   49.7442       7.3941      1.6159          -0.9151         19.3116   0.7301    9.6613 # SUM   49.7442  21'566436.8  306'429102  -1.21124805e+09  6.85632962e+09  12.6639  253.1145 library(data.table) library(ggplot2) library(magrittr)  plotGGDC <- function(ctry) {   # Select and subset   fsubset(GGDC10S, Country == ctry, Variable, Year, AGR:SUM) %>%   # Convert to shares and replace negative values with NA   ftransform(fselect(., AGR:OTH) %>%              lapply(`*`, 1 / SUM) %>%              replace_outliers(0, NA, \"min\")) %>%   # Remove totals column and make proper variable labels   ftransform(Variable = recode_char(Variable,                                     VA = \"Value Added Share\",                                     EMP = \"Employment Share\"),              SUM = NULL) %>%   # Fast conversion to data.table   qDT %>%   # data.table's melt function   melt(1:2, variable.name = \"Sector\", na.rm = TRUE) %>%   # ggplot with some scales provided by the 'scales' package   ggplot(aes(x = Year, y = value, fill = Sector)) +     geom_area(position = \"fill\", alpha = 0.9) + labs(x = NULL, y = NULL) +     theme_linedraw(base_size = 14L) + facet_wrap( ~ Variable) +     scale_fill_manual(values = sub(\"#00FF66\", \"#00CC66\", rainbow(10L))) +     scale_x_continuous(breaks = scales::pretty_breaks(n = 7L), expand = c(0, 0)) +     scale_y_continuous(breaks = scales::pretty_breaks(n = 10L), expand = c(0, 0),                        labels = scales::percent) +     theme(axis.text.x = element_text(angle = 315, hjust = 0, margin = ggplot2::margin(t = 0)),           strip.background = element_rect(colour = \"grey20\", fill = \"grey20\"),           strip.text = element_text(face = \"bold\")) }  # Plotting the structural transformation of Botswana plotGGDC(\"BWA\")"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-data-manipulation","dir":"Articles","previous_headings":"","what":"2. Fast Data Manipulation","title":"Introduction to collapse","text":"lot R code concerned statistical computations preliminary data wrangling.  various reasons R development focused data frames main medium contain data, although matrices / arrays provide significantly faster methods common manipulations. first essential step towards optimizing R code thus speed frequent manipulations data frames. collapse introduces set highly optimized functions efficiently manipulate (mostly) data frames. manipulations can conducted non-standard evaluation standard evaluation (utilizing different functions), functions preserve data structure (.e. can used data.table, tbl_df, grouped_df, pdata.frame etc.).","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"selecting-and-replacing-columns","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.1 Selecting and Replacing Columns","title":"Introduction to collapse","text":"fselect analogue dplyr::select, executes 100x faster. can used select variables using expressions involving variable names: contrast dplyr::select, fselect replacement method can also return information selected columns data . fselect faster dplyr::select, also simpler offer special methods grouped tibbles (e.g. grouping columns always selected) dplyr-specific features select. see problem working statistical functions collapse grouped_df method, users careful replacing dplyr::select fselect dplyr scripts. collapse 1.6.0, fselect explicit support sf data frames. standard-evaluation analogue fselect function get_vars. get_vars can used select variables using names, indices, logical vectors, functions regular expressions evaluated column names: Replacing operations conducted analogous: get_vars 2x faster [.data.frame, get_vars<- 6-8x faster [<-.data.frame. addition get_vars, collapse offers set functions efficiently select replace data data type: num_vars, cat_vars (categorical = non-numeric columns), char_vars, fact_vars, logi_vars date_vars (date date-time columns).","code":"library(magrittr) # Pipe operators fselect(wlddev, country, year, PCGDP:ODA) %>% head(2) #       country year PCGDP LIFEEX GINI       ODA # 1 Afghanistan 1960    NA 32.446   NA 116769997 # 2 Afghanistan 1961    NA 32.962   NA 232080002  fselect(wlddev, -country, -year, -(PCGDP:ODA)) %>% head(2) #   iso3c       date decade     region     income  OECD     POP # 1   AFG 1961-01-01   1960 South Asia Low income FALSE 8996973 # 2   AFG 1962-01-01   1960 South Asia Low income FALSE 9169410  library(microbenchmark) microbenchmark(fselect = collapse::fselect(wlddev, country, year, PCGDP:ODA),                select = dplyr::select(wlddev, country, year, PCGDP:ODA)) # Unit: microseconds #     expr     min       lq      mean   median       uq      max neval #  fselect   2.911   3.4645   4.76297   4.3665   5.3710   20.459   100 #   select 382.284 393.0055 442.70734 410.3075 441.4265 2951.262   100 # Computing the log of columns fselect(wlddev, PCGDP:POP) <- lapply(fselect(wlddev, PCGDP:POP), log) head(wlddev, 2) #       country iso3c       date year decade     region     income  OECD PCGDP   LIFEEX GINI      ODA # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 3.479577   NA 18.57572 # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 3.495355   NA 19.26259 #        POP # 1 16.01240 # 2 16.03138 # Efficient deleting fselect(wlddev, country, year, PCGDP:POP) <- NULL head(wlddev, 2) #   iso3c       date decade     region     income  OECD # 1   AFG 1961-01-01   1960 South Asia Low income FALSE # 2   AFG 1962-01-01   1960 South Asia Low income FALSE rm(wlddev) fselect(wlddev, PCGDP:POP, return = \"names\") # [1] \"PCGDP\"  \"LIFEEX\" \"GINI\"   \"ODA\"    \"POP\" fselect(wlddev, PCGDP:POP, return = \"indices\") # [1]  9 10 11 12 13 fselect(wlddev, PCGDP:POP, return = \"named_indices\") #  PCGDP LIFEEX   GINI    ODA    POP  #      9     10     11     12     13 fselect(wlddev, PCGDP:POP, return = \"logical\") #  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE fselect(wlddev, PCGDP:POP, return = \"named_logical\") # country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX    GINI     ODA  #   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE    TRUE    TRUE    TRUE    TRUE  #     POP  #    TRUE get_vars(wlddev, 9:13) %>% head(1) #   PCGDP LIFEEX GINI       ODA     POP # 1    NA 32.446   NA 116769997 8996973 get_vars(wlddev, c(\"PCGDP\",\"LIFEEX\",\"GINI\",\"ODA\",\"POP\")) %>% head(1) #   PCGDP LIFEEX GINI       ODA     POP # 1    NA 32.446   NA 116769997 8996973 get_vars(wlddev, \"[[:upper:]]\", regex = TRUE) %>% head(1) #    OECD PCGDP LIFEEX GINI       ODA     POP # 1 FALSE    NA 32.446   NA 116769997 8996973 get_vars(wlddev, \"PC|LI|GI|OD|PO\", regex = TRUE) %>% head(1) #   PCGDP LIFEEX GINI       ODA     POP # 1    NA 32.446   NA 116769997 8996973 # Same as above, vectors of regular expressions are sequentially passed to grep get_vars(wlddev, c(\"PC\",\"LI\",\"GI\",\"OD\",\"PO\"), regex = TRUE) %>% head(1) #   PCGDP LIFEEX GINI       ODA     POP # 1    NA 32.446   NA 116769997 8996973 get_vars(wlddev, is.numeric) %>% head(1) #   year decade PCGDP LIFEEX GINI       ODA     POP # 1 1960   1960    NA 32.446   NA 116769997 8996973  # Returning other information get_vars(wlddev, is.numeric, return = \"names\") # [1] \"year\"   \"decade\" \"PCGDP\"  \"LIFEEX\" \"GINI\"   \"ODA\"    \"POP\" get_vars(wlddev, \"[[:upper:]]\", regex = TRUE, return = \"named_indices\") #   OECD  PCGDP LIFEEX   GINI    ODA    POP  #      8      9     10     11     12     13 get_vars(wlddev, 9:13) <- lapply(get_vars(wlddev, 9:13), log) get_vars(wlddev, 9:13) <- NULL head(wlddev, 2) #       country iso3c       date year decade     region     income  OECD # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE rm(wlddev) head(num_vars(wlddev), 2) #   year decade PCGDP LIFEEX GINI       ODA     POP # 1 1960   1960    NA 32.446   NA 116769997 8996973 # 2 1961   1960    NA 32.962   NA 232080002 9169410 head(cat_vars(wlddev), 2) #       country iso3c       date     region     income  OECD # 1 Afghanistan   AFG 1961-01-01 South Asia Low income FALSE # 2 Afghanistan   AFG 1962-01-01 South Asia Low income FALSE head(fact_vars(wlddev), 2) #   iso3c     region     income # 1   AFG South Asia Low income # 2   AFG South Asia Low income  # Replacing fact_vars(wlddev) <- fact_vars(wlddev)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"subsetting","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.2 Subsetting","title":"Introduction to collapse","text":"fsubset enhanced version base::subset using C functions data.table package fast subsetting operations. contrast base::subset, fsubset allows multiple comma-separated select arguments subset argument, also preserves attributes subsetted columns: also possible use standard evaluation fsubset, purposes function ss exists fast secure alternative [.data.frame: Thanks data.table C code optimized R code, fsubset fast. like base::subset, fsubset S3 generic methods vectors, matrices data frames. certain classes factors, fsubset.default also improves upon [, largest improvements data frame method.","code":"# Returning only value-added data after 1990 fsubset(GGDC10S, Variable == \"VA\" & Year > 1990, Country, Year, AGR:GOV) %>% head(2) #   Country Year      AGR      MIN      MAN       PU      CON      WRT      TRA     FIRE      GOV # 1     BWA 1991 303.1157 2646.950 472.6488 160.6079 580.0876 806.7509 232.7884 432.6965 1073.263 # 2     BWA 1992 333.4364 2690.939 537.4274 178.4532 678.7320 725.2577 285.1403 517.2141 1234.012 # Same thing fsubset(GGDC10S, Variable == \"VA\" & Year > 1990, -(Regioncode:Variable), -(OTH:SUM)) %>% head(2) #   Country Year      AGR      MIN      MAN       PU      CON      WRT      TRA     FIRE      GOV # 1     BWA 1991 303.1157 2646.950 472.6488 160.6079 580.0876 806.7509 232.7884 432.6965 1073.263 # 2     BWA 1992 333.4364 2690.939 537.4274 178.4532 678.7320 725.2577 285.1403 517.2141 1234.012 ss(GGDC10S, 1:2, 6:16)  # or fsubset(GGDC10S, 1:2, 6:16), but not recommended. #   AGR MIN MAN PU CON WRT TRA FIRE GOV OTH SUM # 1  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA # 2  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA ss(GGDC10S, -(1:2), c(\"AGR\",\"MIN\")) %>% head(2) #   AGR MIN # 1  NA  NA # 2  NA  NA microbenchmark(base = subset(GGDC10S, Variable == \"VA\" & Year > 1990, AGR:SUM),                collapse = fsubset(GGDC10S, Variable == \"VA\" & Year > 1990, AGR:SUM)) # Unit: microseconds #      expr     min       lq      mean   median      uq      max neval #      base 150.839 156.5585 199.63105 160.3510 166.993 3778.191   100 #  collapse  45.715  49.0975  51.55545  50.9015  52.357   82.861   100  microbenchmark(GGDC10S[1:10, 1:10], ss(GGDC10S, 1:10, 1:10)) # Unit: microseconds #                     expr    min     lq     mean median     uq    max neval #      GGDC10S[1:10, 1:10] 36.367 36.982 38.14599 37.515 38.294 76.219   100 #  ss(GGDC10S, 1:10, 1:10)  1.886  2.050  2.30666  2.214  2.419  8.405   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"reordering-rows-and-columns","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.3 Reordering Rows and Columns","title":"Introduction to collapse","text":"roworder fast analogue dplyr::arrange. syntax inspired data.table::setorder, negative variable names indicate descending sort. contrast data.table::setorder, roworder creates copy data frame (unless data already sorted). copy required, data.table::setorder faster. function roworderv standard evaluation analogue roworder: roworderv, also possible move exchange rows data frame: Similarly, pair colorder / colorderv facilitates efficient reordering columns data frame. functions require deep copy data fast. reorder columns reference, see also data.table::setcolorder.","code":"roworder(GGDC10S, -Variable, Country) %>% ss(1:2, 1:8) #   Country Regioncode        Region Variable Year          AGR MIN         MAN # 1     ARG        LAM Latin America       VA 1950 5.887857e-07   0 3.53443e-06 # 2     ARG        LAM Latin America       VA 1951 9.165327e-07   0 4.77277e-06  microbenchmark(collapse = collapse::roworder(GGDC10S, -Variable, Country),                dplyr = dplyr::arrange(GGDC10S, desc(Variable), Country)) # Unit: microseconds #      expr      min       lq      mean   median        uq      max neval #  collapse  113.406  152.151  176.7567  165.722  183.0855  538.330   100 #     dplyr 1240.168 1299.372 1618.5869 1384.755 1507.8160 8350.552   100 # Same as above roworderv(GGDC10S, c(\"Variable\", \"Country\"), decreasing = c(TRUE, FALSE)) %>% ss(1:2, 1:8) #   Country Regioncode        Region Variable Year          AGR MIN         MAN # 1     ARG        LAM Latin America       VA 1950 5.887857e-07   0 3.53443e-06 # 2     ARG        LAM Latin America       VA 1951 9.165327e-07   0 4.77277e-06 # If length(neworder) < fnrow(data), the default (pos = \"front\") brings rows to the front roworderv(GGDC10S, neworder = which(GGDC10S$Country == \"GHA\")) %>% ss(1:2, 1:8) #   Country Regioncode             Region Variable Year        AGR         MIN        MAN # 1     GHA        SSA Sub-saharan Africa       VA 1960 0.03576160 0.005103683 0.01744687 # 2     GHA        SSA Sub-saharan Africa       VA 1961 0.03823049 0.005456030 0.01865136  # pos = \"end\" brings rows to the end roworderv(GGDC10S, neworder = which(GGDC10S$Country == \"BWA\"), pos = \"end\") %>% ss(1:2, 1:8) #   Country Regioncode             Region Variable Year      AGR      MIN     MAN # 1     ETH        SSA Sub-saharan Africa       VA 1960       NA       NA      NA # 2     ETH        SSA Sub-saharan Africa       VA 1961 4495.614 11.86979 109.616  # pos = \"exchange\" arranges selected rows in the order they are passed, without affecting other rows roworderv(GGDC10S, neworder = with(GGDC10S, c(which(Country == \"GHA\"),                                               which(Country == \"BWA\"))), pos = \"exchange\") %>% ss(1:2, 1:8) #   Country Regioncode             Region Variable Year        AGR         MIN        MAN # 1     GHA        SSA Sub-saharan Africa       VA 1960 0.03576160 0.005103683 0.01744687 # 2     GHA        SSA Sub-saharan Africa       VA 1961 0.03823049 0.005456030 0.01865136 # The default is again pos = \"front\" which brings selected columns to the front / left colorder(GGDC10S, Variable, Country, Year) %>% head(2) #   Variable Country Year Regioncode             Region AGR MIN MAN PU CON WRT TRA FIRE GOV OTH SUM # 1       VA     BWA 1960        SSA Sub-saharan Africa  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA # 2       VA     BWA 1961        SSA Sub-saharan Africa  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"transforming-and-computing-new-columns","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.4 Transforming and Computing New Columns","title":"Introduction to collapse","text":"ftransform improved version base::transform data frames lists. ftransform can used compute new columns modify delete existing columns, always returns entire data frame. modification ftransformv exists transform specific columns using function: Instead passing comma-separated column = value expressions, also possible bulk-process data fransform passing single list expressions (data frame). useful complex transformations involving multiple steps: mode usage toggles automatic column matching replacement. Non-matching columns added data frame. Apart ftransform, function settransform(v) can used change input data frame reference: Another convenient addition provided function fcompute, can used compute new columns data frame environment returns computed columns new data frame: complex tasks see ?ftransform.","code":"ftransform(GGDC10S, AGR_perc = AGR / SUM * 100, # Computing Agricultural percentage                     Year = as.integer(Year),    # Coercing Year to integer                     AGR = NULL) %>% tail(2)     # Deleting column AGR #      Country Regioncode                       Region Variable Year      MIN      MAN       PU # 5026     EGY       MENA Middle East and North Africa      EMP 2011 27.56394 2373.814 317.9979 # 5027     EGY       MENA Middle East and North Africa      EMP 2012 24.78083 2348.434 324.9332 #           CON      WRT      TRA     FIRE      GOV OTH      SUM AGR_perc # 5026 2795.264 3020.236 2048.335 814.7403 5635.522  NA 22219.39 23.33961 # 5027 2931.196 3109.522 2065.004 832.4770 5735.623  NA 22532.56 22.90281  # Computing scalar results replicates them ftransform(GGDC10S, MIN_mean = fmean(MIN), Intercept = 1) %>% tail(2) #      Country Regioncode                       Region Variable Year      AGR      MIN      MAN # 5026     EGY       MENA Middle East and North Africa      EMP 2011 5185.919 27.56394 2373.814 # 5027     EGY       MENA Middle East and North Africa      EMP 2012 5160.590 24.78083 2348.434 #            PU      CON      WRT      TRA     FIRE      GOV OTH      SUM MIN_mean Intercept # 5026 317.9979 2795.264 3020.236 2048.335 814.7403 5635.522  NA 22219.39  1867909         1 # 5027 324.9332 2931.196 3109.522 2065.004 832.4770 5735.623  NA 22532.56  1867909         1 # Apply the log to columns 6-16 GGDC10S %>% ftransformv(6:16, log) %>% tail(2) #      Country Regioncode                       Region Variable Year      AGR      MIN      MAN # 5026     EGY       MENA Middle East and North Africa      EMP 2011 8.553702 3.316508 7.772253 # 5027     EGY       MENA Middle East and North Africa      EMP 2012 8.548806 3.210070 7.761504 #            PU      CON      WRT      TRA     FIRE      GOV OTH      SUM # 5026 5.762045 7.935682 8.013090 7.624782 6.702869 8.636845  NA 10.00872 # 5027 5.783620 7.983166 8.042224 7.632888 6.724406 8.654452  NA 10.02272  # Convert data to percentage terms GGDC10S %>% ftransformv(6:16, `*`, 100/SUM) %>% tail(2) #      Country Regioncode                       Region Variable Year      AGR       MIN      MAN # 5026     EGY       MENA Middle East and North Africa      EMP 2011 23.33961 0.1240535 10.68352 # 5027     EGY       MENA Middle East and North Africa      EMP 2012 22.90281 0.1099779 10.42240 #            PU      CON      WRT      TRA     FIRE      GOV OTH SUM # 5026 1.431173 12.58029 13.59279 9.218680 3.666798 25.36308  NA 100 # 5027 1.442061 13.00871 13.80013 9.164534 3.694551 25.45482  NA 100  # Apply log to numeric columns GGDC10S %>% ftransformv(is.numeric, log) %>% tail(2) #      Country Regioncode                       Region Variable     Year      AGR      MIN      MAN # 5026     EGY       MENA Middle East and North Africa      EMP 7.606387 8.553702 3.316508 7.772253 # 5027     EGY       MENA Middle East and North Africa      EMP 7.606885 8.548806 3.210070 7.761504 #            PU      CON      WRT      TRA     FIRE      GOV OTH      SUM # 5026 5.762045 7.935682 8.013090 7.624782 6.702869 8.636845  NA 10.00872 # 5027 5.783620 7.983166 8.042224 7.632888 6.724406 8.654452  NA 10.02272 # Same as above, but also replacing any generated infinite values with NA GGDC10S %>% ftransform(num_vars(.) %>% lapply(log) %>% replace_Inf) %>% tail(2) #      Country Regioncode                       Region Variable     Year      AGR      MIN      MAN # 5026     EGY       MENA Middle East and North Africa      EMP 7.606387 8.553702 3.316508 7.772253 # 5027     EGY       MENA Middle East and North Africa      EMP 7.606885 8.548806 3.210070 7.761504 #            PU      CON      WRT      TRA     FIRE      GOV OTH      SUM # 5026 5.762045 7.935682 8.013090 7.624782 6.702869 8.636845  NA 10.00872 # 5027 5.783620 7.983166 8.042224 7.632888 6.724406 8.654452  NA 10.02272 # Computing a new column and deleting some others by reference settransform(GGDC10S, FIRE_MAN = FIRE / MAN,                       Regioncode = NULL, Region = NULL) tail(GGDC10S, 2) #      Country Variable Year      AGR      MIN      MAN       PU      CON      WRT      TRA     FIRE # 5026     EGY      EMP 2011 5185.919 27.56394 2373.814 317.9979 2795.264 3020.236 2048.335 814.7403 # 5027     EGY      EMP 2012 5160.590 24.78083 2348.434 324.9332 2931.196 3109.522 2065.004 832.4770 #           GOV OTH      SUM  FIRE_MAN # 5026 5635.522  NA 22219.39 0.3432200 # 5027 5735.623  NA 22532.56 0.3544817 rm(GGDC10S)  # Bulk-processing the data into percentage terms settransformv(GGDC10S, 6:16, `*`, 100/SUM) tail(GGDC10S, 2) #      Country Regioncode                       Region Variable Year      AGR       MIN      MAN # 5026     EGY       MENA Middle East and North Africa      EMP 2011 23.33961 0.1240535 10.68352 # 5027     EGY       MENA Middle East and North Africa      EMP 2012 22.90281 0.1099779 10.42240 #            PU      CON      WRT      TRA     FIRE      GOV OTH SUM # 5026 1.431173 12.58029 13.59279 9.218680 3.666798 25.36308  NA 100 # 5027 1.442061 13.00871 13.80013 9.164534 3.694551 25.45482  NA 100  # Same thing via replacement ftransform(GGDC10S) <- fselect(GGDC10S, AGR:SUM) %>% lapply(`*`, 100/.$SUM) # Or using double pipes GGDC10S %<>% ftransformv(6:16, `*`, 100/SUM) rm(GGDC10S) fcompute(GGDC10S, AGR_perc = AGR / SUM * 100, FIRE_MAN = FIRE / MAN) %>% tail(2) #      AGR_perc  FIRE_MAN # 5026 23.33961 0.3432200 # 5027 22.90281 0.3544817"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"adding-and-binding-columns","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.5 Adding and Binding Columns","title":"Introduction to collapse","text":"cases multiple columns computed need added data frame (regardless whether names duplicated ), collapse introduces predicate add_vars. Together add_vars, function add_stub useful add prefix (default) postfix computed variables keeping variable names unique: default add_vars appends data frame towards (right) end, can also replace columns front positions data frame: add_vars can also used without replacement, serves efficient version cbind.data.frame, difference data structure attributes first argument preserved:","code":"# Efficient adding logged versions of some variables add_vars(wlddev) <- get_vars(wlddev, 9:13) %>% lapply(log10) %>% add_stub(\"log10.\") head(wlddev, 2) #       country iso3c       date year decade     region     income  OECD PCGDP LIFEEX GINI       ODA # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 32.446   NA 116769997 # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 32.962   NA 232080002 #       POP log10.PCGDP log10.LIFEEX log10.GINI log10.ODA log10.POP # 1 8996973          NA     1.511161         NA  8.067331  6.954096 # 2 9169410          NA     1.518014         NA  8.365638  6.962341 rm(wlddev) add_vars(wlddev, \"front\") <- get_vars(wlddev, 9:13) %>% lapply(log10) %>% add_stub(\"log10.\") head(wlddev, 2) #   log10.PCGDP log10.LIFEEX log10.GINI log10.ODA log10.POP     country iso3c       date year decade # 1          NA     1.511161         NA  8.067331  6.954096 Afghanistan   AFG 1961-01-01 1960   1960 # 2          NA     1.518014         NA  8.365638  6.962341 Afghanistan   AFG 1962-01-01 1961   1960 #       region     income  OECD PCGDP LIFEEX GINI       ODA     POP # 1 South Asia Low income FALSE    NA 32.446   NA 116769997 8996973 # 2 South Asia Low income FALSE    NA 32.962   NA 232080002 9169410 rm(wlddev)  add_vars(wlddev, c(10L,12L,14L,16L,18L)) <- get_vars(wlddev, 9:13) %>% lapply(log10) %>% add_stub(\"log10.\") head(wlddev, 2) #       country iso3c       date year decade     region     income  OECD PCGDP log10.PCGDP LIFEEX # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA          NA 32.446 # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA          NA 32.962 #   log10.LIFEEX GINI log10.GINI       ODA log10.ODA     POP log10.POP # 1     1.511161   NA         NA 116769997  8.067331 8996973  6.954096 # 2     1.518014   NA         NA 232080002  8.365638 9169410  6.962341 rm(wlddev) add_vars(wlddev, get_vars(wlddev, 9:13) %>% lapply(log) %>% add_stub(\"log.\"),                  get_vars(wlddev, 9:13) %>% lapply(log10) %>% add_stub(\"log10.\")) %>% head(2) #       country iso3c       date year decade     region     income  OECD PCGDP LIFEEX GINI       ODA # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 32.446   NA 116769997 # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 32.962   NA 232080002 #       POP log.PCGDP log.LIFEEX log.GINI  log.ODA  log.POP log10.PCGDP log10.LIFEEX log10.GINI # 1 8996973        NA   3.479577       NA 18.57572 16.01240          NA     1.511161         NA # 2 9169410        NA   3.495355       NA 19.26259 16.03138          NA     1.518014         NA #   log10.ODA log10.POP # 1  8.067331  6.954096 # 2  8.365638  6.962341  add_vars(wlddev,  get_vars(wlddev, 9:13) %>% lapply(log) %>% add_stub(\"log.\"),                   get_vars(wlddev, 9:13) %>% lapply(log10) %>% add_stub(\"log10.\"),          pos = c(10L,13L,16L,19L,22L,11L,14L,17L,20L,23L)) %>% head(2) #       country iso3c       date year decade     region     income  OECD PCGDP log.PCGDP log10.PCGDP # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA        NA          NA # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA        NA          NA #   LIFEEX log.LIFEEX log10.LIFEEX GINI log.GINI log10.GINI       ODA  log.ODA log10.ODA     POP # 1 32.446   3.479577     1.511161   NA       NA         NA 116769997 18.57572  8.067331 8996973 # 2 32.962   3.495355     1.518014   NA       NA         NA 232080002 19.26259  8.365638 9169410 #    log.POP log10.POP # 1 16.01240  6.954096 # 2 16.03138  6.962341  identical(cbind(wlddev, wlddev), add_vars(wlddev, wlddev)) # [1] TRUE microbenchmark(cbind(wlddev, wlddev), add_vars(wlddev, wlddev)) # Unit: microseconds #                      expr    min      lq     mean median      uq    max neval #     cbind(wlddev, wlddev) 13.694 14.1040 15.72760 14.391 14.7600 57.072   100 #  add_vars(wlddev, wlddev)  3.280  3.6285  4.13567  3.813  4.0385 19.352   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"renaming-columns","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.6 Renaming Columns","title":"Introduction to collapse","text":"frename fast substitute dplyr::rename: function setrename reference: functions limited data frames can applied R object ‘names’ attribute.","code":"frename(GGDC10S, AGR = Agriculture, MIN = Mining) %>% head(2) #   Country Regioncode             Region Variable Year Agriculture Mining MAN PU CON WRT TRA FIRE # 1     BWA        SSA Sub-saharan Africa       VA 1960          NA     NA  NA NA  NA  NA  NA   NA # 2     BWA        SSA Sub-saharan Africa       VA 1961          NA     NA  NA NA  NA  NA  NA   NA #   GOV OTH SUM # 1  NA  NA  NA # 2  NA  NA  NA frename(GGDC10S, tolower) %>% head(2) #   country regioncode             region variable year agr min man pu con wrt tra fire gov oth sum # 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA # 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA frename(GGDC10S, tolower, cols = .c(AGR, MIN)) %>% head(2) #   Country Regioncode             Region Variable Year agr min MAN PU CON WRT TRA FIRE GOV OTH SUM # 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA # 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA  NA   NA  NA  NA  NA setrename(GGDC10S, AGR = Agriculture, MIN = Mining) head(GGDC10S, 2) #   Country Regioncode             Region Variable Year Agriculture Mining MAN PU CON WRT TRA FIRE # 1     BWA        SSA Sub-saharan Africa       VA 1960          NA     NA  NA NA  NA  NA  NA   NA # 2     BWA        SSA Sub-saharan Africa       VA 1961          NA     NA  NA NA  NA  NA  NA   NA #   GOV OTH SUM # 1  NA  NA  NA # 2  NA  NA  NA setrename(GGDC10S, Agriculture = AGR, Mining = MIN) rm(GGDC10S)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"using-shortcuts","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.7 Using Shortcuts","title":"Introduction to collapse","text":"frequently required among functions introduced can abbreviated follows: fselect -> slt, fsubset -> sbt, ftransform(v) -> tfm(v), settransform(v) -> settfm(v), get_vars -> gv, num_vars -> nv, add_vars -> av. done make possible write faster parsimonious code, recommended personally kept scripts. lazy person may also decide code everything using shortcuts ctrl+F replacement long names finished script.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"missing-values-rows","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.8 Missing Values / Rows","title":"Introduction to collapse","text":"function na_omit much faster alternative stats::na.omit vectors, matrices data frames. default ‘na.action’ attribute containing removed cases omitted, can added option na.attr = TRUE. Like fsubset, na_omit preserves column attributes well attributes data frame . Another added feature removal cases missing certain columns : atomic vectors function na_rm also exists 2x faster x[!.na(x)]. na_omit na_rm return argument missing cases found. existence missing cases can checked using missing_cases, also considerably faster complete.cases data frames. also function na_insert randomly insert missing values vectors, matrices data frames. default na_insert(X, prop = 0.1) 10% values randomly set missing. Finally, function allNA provides much needed opposite anyNA atomic vectors.","code":"microbenchmark(na_omit(wlddev, na.attr = TRUE), na.omit(wlddev)) # Unit: microseconds #                             expr     min      lq      mean   median       uq       max neval #  na_omit(wlddev, na.attr = TRUE)  60.393  69.208   84.8126  79.9910   88.683   419.881   100 #                  na.omit(wlddev) 745.790 856.449 1721.5457 940.6015 1005.177 56344.414   100 na_omit(wlddev, cols = .c(PCGDP, LIFEEX)) %>% head(2) #       country iso3c       date year decade     region     income  OECD    PCGDP LIFEEX GINI # 1 Afghanistan   AFG 2003-01-01 2002   2000 South Asia Low income FALSE 330.3036 56.784   NA # 2 Afghanistan   AFG 2004-01-01 2003   2000 South Asia Low income FALSE 343.0809 57.271   NA #          ODA      POP # 1 1790479980 22600770 # 2 1972890015 23680871 # only removing missing data from numeric columns -> same and slightly faster than na_omit(wlddev) na_omit(wlddev, cols = is.numeric) %>% head(2) #   country iso3c       date year decade                region              income  OECD    PCGDP # 1 Albania   ALB 1997-01-01 1996   1990 Europe & Central Asia Upper middle income FALSE 1869.866 # 2 Albania   ALB 2003-01-01 2002   2000 Europe & Central Asia Upper middle income FALSE 2572.721 #   LIFEEX GINI       ODA     POP # 1 72.495 27.0 294089996 3168033 # 2 74.579 31.7 453309998 3051010"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"unique-values-rows","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.9 Unique Values / Rows","title":"Introduction to collapse","text":"Similar na_omit, function funique much faster alternative base::unique atomic vectors data frames. Like collapse functions also seeks preserve attributes.","code":"funique(GGDC10S$Variable)              # Unique values in order of appearance # [1] \"VA\"  \"EMP\" # attr(,\"label\") # [1] \"Variable\" # attr(,\"format.stata\") # [1] \"%9s\" funique(GGDC10S$Variable, sort = TRUE) # Sorted unique values # [1] \"EMP\" \"VA\"  # attr(,\"label\") # [1] \"Variable\" # attr(,\"format.stata\") # [1] \"%9s\"  # If all values/rows are unique, the original data is returned (no copy) identical(funique(GGDC10S), GGDC10S) # [1] TRUE  # Can remove duplicate rows by a subset of columns funique(GGDC10S, cols = .c(Country, Variable)) %>% ss(1:2, 1:8) #   Country Regioncode             Region Variable Year AGR MIN MAN # 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA # 2     BWA        SSA Sub-saharan Africa      EMP 1960  NA  NA  NA funique(GGDC10S, cols = .c(Country, Variable), sort = TRUE) %>% ss(1:2, 1:8) #   Country Regioncode        Region Variable Year          AGR      MIN          MAN # 1     ARG        LAM Latin America      EMP 1950 1.799565e+03 32.71936 1.603249e+03 # 2     ARG        LAM Latin America       VA 1950 5.887857e-07  0.00000 3.534430e-06"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"recoding-and-replacing-values","dir":"Articles","previous_headings":"2. Fast Data Manipulation","what":"2.10 Recoding and Replacing Values","title":"Introduction to collapse","text":"recode_num, recode_char, replace_NA, replace_Inf replace_outliers, collapse also introduces set functions efficiently recode replace numeric character values matrix-like objects (vectors, matrices, arrays, data frames, lists atomic objects). called data frame, recode_num, replace_Inf replace_outliers skip non-numeric columns, recode_char skips non-character columns, whereas replace_NA replaces missing values columns. recode_num recode_char follow syntax dplyr::recode provide less functionality except can efficiently applied matrices data frames, recode_char allows regular expression matching implemented via base::grepl: perhaps interesting function ensemble replace_outliers, replaces values falling outside 1- 2-sided numeric threshold outside certain number column- standard deviations value (default NA).","code":"# Efficient replacing missing values with 0 microbenchmark(replace_NA(GGDC10S, 0)) # Unit: microseconds #                    expr     min      lq     mean   median       uq      max neval #  replace_NA(GGDC10S, 0) 109.757 141.163 203.4982 151.0235 163.0775 4579.085   100  # Adding log-transformed sectoral data: Some NaN and Inf values generated add_vars(GGDC10S, 6:16*2-5) <- fselect(GGDC10S, AGR:SUM) %>%   lapply(log) %>% replace_Inf %>% add_stub(\"log.\") head(GGDC10S, 2) #   Country Regioncode             Region Variable Year AGR log.AGR MIN log.MIN MAN log.MAN PU log.PU # 1     BWA        SSA Sub-saharan Africa       VA 1960  NA      NA  NA      NA  NA      NA NA     NA # 2     BWA        SSA Sub-saharan Africa       VA 1961  NA      NA  NA      NA  NA      NA NA     NA #   CON log.CON WRT log.WRT TRA log.TRA FIRE log.FIRE GOV log.GOV OTH log.OTH SUM log.SUM # 1  NA      NA  NA      NA  NA      NA   NA       NA  NA      NA  NA      NA  NA      NA # 2  NA      NA  NA      NA  NA      NA   NA       NA  NA      NA  NA      NA  NA      NA rm(GGDC10S) month.name #  [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"      \"July\"      \"August\"    #  [9] \"September\" \"October\"   \"November\"  \"December\" recode_char(month.name, ber = \"C\", \"^J\" = \"A\", default = \"B\", regex = TRUE) #  [1] \"A\" \"B\" \"B\" \"B\" \"B\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" # replace all values below 2 and above 100 with NA replace_outliers(mtcars, c(2, 100)) %>% head(3) #                mpg cyl disp hp drat    wt  qsec vs am gear carb # Mazda RX4     21.0   6   NA NA 3.90 2.620 16.46 NA NA    4    4 # Mazda RX4 Wag 21.0   6   NA NA 3.90 2.875 17.02 NA NA    4    4 # Datsun 710    22.8   4   NA 93 3.85 2.320 18.61 NA NA    4   NA  # replace all value smaller than 2 with NA replace_outliers(mtcars, 2, single.limit = \"min\") %>% head(3) #                mpg cyl disp  hp drat    wt  qsec vs am gear carb # Mazda RX4     21.0   6  160 110 3.90 2.620 16.46 NA NA    4    4 # Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02 NA NA    4    4 # Datsun 710    22.8   4  108  93 3.85 2.320 18.61 NA NA    4   NA  # replace all value larger than 100 with NA replace_outliers(mtcars, 100, single.limit = \"max\") %>% head(3) #                mpg cyl disp hp drat    wt  qsec vs am gear carb # Mazda RX4     21.0   6   NA NA 3.90 2.620 16.46  0  1    4    4 # Mazda RX4 Wag 21.0   6   NA NA 3.90 2.875 17.02  0  1    4    4 # Datsun 710    22.8   4   NA 93 3.85 2.320 18.61  1  1    4    1  # replace all values above or below 3 column-standard-deviations from the column-mean with NA replace_outliers(mtcars, 3) %>% tail(3) #                mpg cyl disp  hp drat   wt qsec vs am gear carb # Ferrari Dino  19.7   6  145 175 3.62 2.77 15.5  0  1    5    6 # Maserati Bora 15.0   8  301 335 3.54 3.57 14.6  0  1    5   NA # Volvo 142E    21.4   4  121 109 4.11 2.78 18.6  1  1    4    2"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"quick-data-object-conversions","dir":"Articles","previous_headings":"","what":"3. Quick Data Object Conversions","title":"Introduction to collapse","text":"Apart code employed manipulation data actual statistical computations performed, frequently used data object conversions base functions like .data.frame, .matrix .factor significant share slowing R code. Optimally code written without conversions, sometimes necessary thus collapse provides set functions (qDF, qDT, qTBL, qM, qF, mrtl mctl) speed conversions quite bit. functions fast non-generic dispatch different objects internally, perform critical steps C++, , passed lists objects, check length first column. qDF, qDT qTBL efficiently convert vectors, matrices, higher-dimensional arrays suitable lists data.frame, data.table tibble respectively. default functions drop unnecessary attributes matrices lists / data frames conversion, can changed using keep.attr = TRUE argument. useful additional feature qDF qDT row.names.col argument, enabling saving names / row-names column converting vector, matrix, array data frame: conversion matrices list also programmers functions mrtl mctl, row- column- wise convert matrix plain list, data.frame data.table. reverse operation, qM converts vectors, higher-dimensional arrays, data frames suitable lists matrix. last, qF converts vectors factor quite bit faster .factor:","code":"str(EuStockMarkets) #  Time-Series [1:1860, 1:4] from 1991 to 1999: 1629 1614 1607 1621 1618 ... #  - attr(*, \"dimnames\")=List of 2 #   ..$ : NULL #   ..$ : chr [1:4] \"DAX\" \"SMI\" \"CAC\" \"FTSE\" # Efficient Conversion of data frames and matrices to data.table microbenchmark(qDT(wlddev), qDT(EuStockMarkets), as.data.table(wlddev), as.data.frame(EuStockMarkets)) # Unit: microseconds #                           expr    min      lq      mean   median       uq      max neval #                    qDT(wlddev)  3.075   3.608   4.21439   3.8950   4.2640   12.546   100 #            qDT(EuStockMarkets)  6.765   8.733  12.09254  12.5050  14.1040   30.217   100 #          as.data.table(wlddev) 64.206 122.180 253.75023 143.2745 173.1635 3653.346   100 #  as.data.frame(EuStockMarkets) 64.247  70.971  82.25174  79.6835  84.8700  339.849   100  # Converting a time series to data.frame head(qDF(AirPassengers)) #   AirPassengers # 1           112 # 2           118 # 3           132 # 4           129 # 5           121 # 6           135 # This saves the row-names in a column named 'car' head(qDT(mtcars, \"car\")) #                  car   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #               <char> <num> <num> <num> <num> <num> <num> <num> <num> <num> <num> <num> # 1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1     4     4 # 2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1     4     4 # 3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1     4     1 # 4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0     3     1 # 5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0     3     2 # 6:           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0     3     1  N_distinct <- fndistinct(GGDC10S) N_distinct #    Country Regioncode     Region   Variable       Year        AGR        MIN        MAN         PU  #         43          6          6          2         67       4353       4224       4353       4237  #        CON        WRT        TRA       FIRE        GOV        OTH        SUM  #       4339       4344       4334       4349       3470       4238       4364 # Converting a vector to data.frame, saving names head(qDF(N_distinct, \"variable\")) #     variable N_distinct # 1    Country         43 # 2 Regioncode          6 # 3     Region          6 # 4   Variable          2 # 5       Year         67 # 6        AGR       4353 # This converts the matrix to a list of 1860 row-vectors of length 4. microbenchmark(mrtl(EuStockMarkets)) # Unit: microseconds #                  expr     min       lq     mean  median       uq     max neval #  mrtl(EuStockMarkets) 139.728 151.4335 168.5522 155.841 164.6355 399.791   100 # Note: kit::psum is the most efficient way to do this microbenchmark(rowSums(qM(mtcars)), rowSums(mtcars), kit::psum(mtcars)) # Unit: nanoseconds #                 expr   min      lq      mean  median      uq      max neval #  rowSums(qM(mtcars))  5699  7933.5  12702.62  9122.5 11131.5   316315   100 #      rowSums(mtcars) 38868 41697.0  48003.21 44157.0 51496.0    95981   100 #    kit::psum(mtcars)   574   820.0 510905.51   943.0  1107.0 50967797   100 # Converting from character str(wlddev$country) #  chr [1:13176] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ... #  - attr(*, \"label\")= chr \"Country Name\" fndistinct(wlddev$country) # [1] 216 microbenchmark(qF(wlddev$country), as.factor(wlddev$country)) # Unit: microseconds #                       expr     min       lq      mean  median      uq     max neval #         qF(wlddev$country)  70.192  71.1965  73.77376  72.160  74.784 107.256   100 #  as.factor(wlddev$country) 263.794 275.7660 282.21530 278.841 283.761 360.431   100  # Converting from numeric str(wlddev$PCGDP) #  num [1:13176] NA NA NA NA NA NA NA NA NA NA ... #  - attr(*, \"label\")= chr \"GDP per capita (constant 2010 US$)\" fndistinct(wlddev$PCGDP) # [1] 9470 microbenchmark(qF(wlddev$PCGDP), as.factor(wlddev$PCGDP)) # Unit: microseconds #                     expr      min       lq     mean   median        uq       max neval #         qF(wlddev$PCGDP)  445.096  474.944  531.221  488.146  509.0765  3930.342   100 #  as.factor(wlddev$PCGDP) 9374.240 9546.132 9823.477 9633.196 9727.5165 13732.499   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"advanced-statistical-programming","dir":"Articles","previous_headings":"","what":"4. Advanced Statistical Programming","title":"Introduction to collapse","text":"introduced basic collapse data manipulation infrastructure preceding chapters, chapter introduces packages core functionality programming data.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-grouped-weighted-statistical-functions","dir":"Articles","previous_headings":"4. Advanced Statistical Programming","what":"4.1 Fast (Grouped, Weighted) Statistical Functions","title":"Introduction to collapse","text":"key feature collapse ’s broad set Fast Statistical Functions (fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs, fndistinct), able tangibly speed-column-wise, grouped weighted statistical computations vectors, matrices data frames. basic syntax common functions : x vector, matrix data frame, g takes groups supplied vector, factor, list vectors GRP object, w takes weight vector (supported fsum, fprod, fmean, fmedian, fmode, fnth, fvar fsd). TRA can used transform x using computed statistics one 10 available transformations (\"replace_fill\", \"replace\", \"-\", \"-+\", \"/\", \"%\", \"+\", \"*\", \"%%, \"-%%\", discussed section 6.3). na.rm efficiently skips missing values computation TRUE default. use.g.names = TRUE generates new row-names unique groups supplied g, drop = TRUE returns vector performing simple (non-grouped) computations matrix data frame columns. mind, let’s start simple examples. calculate simple column-wise means, sufficient type: Note separate methods vectors, matrices data frames written C++, thus conversions needed computations matrices data frames equally efficient. weight vector, weighted statistics easily computed: Fast grouped statistics can calculated simply passing grouping vectors lists grouping vectors fast functions:","code":"FUN(x, g = NULL, [w = NULL,] TRA = NULL, [na.rm = TRUE,] use.g.names = TRUE, drop = TRUE) fmean(mtcars$mpg) # Vector # [1] 20.09062  fmean(mtcars) #        mpg        cyl       disp         hp       drat         wt       qsec         vs         am  #  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250  #       gear       carb  #   3.687500   2.812500  fmean(mtcars, drop = FALSE)  # This returns a 1-row data-frame #        mpg    cyl     disp       hp     drat      wt     qsec     vs      am   gear   carb # 1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  m <- qM(mtcars) # Generate matrix fmean(m) #        mpg        cyl       disp         hp       drat         wt       qsec         vs         am  #  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250  #       gear       carb  #   3.687500   2.812500  fmean(m, drop = FALSE)  # This returns a 1-row matrix #           mpg    cyl     disp       hp     drat      wt     qsec     vs      am   gear   carb # [1,] 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 weights <- abs(rnorm(fnrow(mtcars))) # fnrow is a bit faster for data frames  fmean(mtcars, w = weights) # Weighted mean #         mpg         cyl        disp          hp        drat          wt        qsec          vs  #  20.8090714   5.8876772 214.9587303 142.8931066   3.7558442   3.0941361  17.8201120   0.5025300  #          am        gear        carb  #   0.4918237   3.8375831   2.7771280 fmedian(mtcars, w = weights) # Weighted median #    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb  #  21.00   6.00 160.00 113.00   3.77   3.17  18.00   1.00   0.00   4.00   2.00 fsd(mtcars, w = weights) # Frequency-weighted standard deviation #         mpg         cyl        disp          hp        drat          wt        qsec          vs  #   5.8799568   1.8416865 122.4274353  74.9459089   0.5413624   0.9689836   1.8516418   0.5089768  #          am        gear        carb  #   0.5089152   0.7557877   1.6744062 fmode(mtcars, w = weights) # Weighted statistical mode (i.e. the value with the largest sum of weights) #    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb  #  21.40   4.00 121.00 109.00   3.92   2.78  18.60   1.00   0.00   4.00   2.00 fmean(mtcars, mtcars$cyl) #        mpg cyl     disp        hp     drat       wt     qsec        vs        am     gear     carb # 4 26.66364   4 105.1364  82.63636 4.070909 2.285727 19.13727 0.9090909 0.7272727 4.090909 1.545455 # 6 19.74286   6 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 0.4285714 3.857143 3.428571 # 8 15.10000   8 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 0.1428571 3.285714 3.500000  fmean(mtcars, fselect(mtcars, cyl, vs, am)) #            mpg cyl     disp        hp     drat       wt     qsec vs am     gear     carb # 4.0.1 26.00000   4 120.3000  91.00000 4.430000 2.140000 16.70000  0  1 5.000000 2.000000 # 4.1.0 22.90000   4 135.8667  84.66667 3.770000 2.935000 20.97000  1  0 3.666667 1.666667 # 4.1.1 28.37143   4  89.8000  80.57143 4.148571 2.028286 18.70000  1  1 4.142857 1.428571 # 6.0.1 20.56667   6 155.0000 131.66667 3.806667 2.755000 16.32667  0  1 4.333333 4.666667 # 6.1.0 19.12500   6 204.5500 115.25000 3.420000 3.388750 19.21500  1  0 3.500000 2.500000 # 8.0.0 15.05000   8 357.6167 194.16667 3.120833 4.104083 17.14250  0  0 3.000000 3.083333 # 8.0.1 15.40000   8 326.0000 299.50000 3.880000 3.370000 14.55000  0  1 5.000000 6.000000  # Getting column indices ind <- fselect(mtcars, cyl, vs, am, return = \"indices\") fmean(get_vars(mtcars, -ind), get_vars(mtcars, ind)) #            mpg     disp        hp     drat       wt     qsec     gear     carb # 4.0.1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 2.000000 # 4.1.0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 1.666667 # 4.1.1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 1.428571 # 6.0.1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667 # 6.1.0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000 # 8.0.0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333 # 8.0.1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"factors-grouping-objects-and-grouped-data-frames","dir":"Articles","previous_headings":"4. Advanced Statistical Programming","what":"4.2 Factors, Grouping Objects and Grouped Data Frames","title":"Introduction to collapse","text":"programming can becomes efficient passing factors grouping objects g argument, otherwise vectors lists vectors grouped internally. programming purposes GRP objects preferable factors never require checks provide additional information grouping (group sizes original unique values group). GRP function creates grouping objects (class GRP) vectors lists columns. Grouping done efficiently via radix ordering C (using radixorder function): first three elements object provide information number groups, group row belongs, size group. print plot method provide information grouping: plot chunk GRPplot important elements GRP object directly handed compiled C++ code statistical functions, making repeated computations groups efficient. Yet another possibility creating grouped data frame (class grouped_df). can either done using dplyr::group_by, creates grouped tibble requires conversion grouping object using GRP.grouped_df, using efficient fgroup_by provided collapse: Now suppose wanted create new dataset contains mean, sd, min max variables mpg disp grouped cyl, vs :","code":"# This creates a factor, na.exclude = FALSE attaches a class 'na.included' f <- qF(mtcars$cyl, na.exclude = FALSE) # The 'na.included' attribute skips a missing value check on this factor attributes(f) # $levels # [1] \"4\" \"6\" \"8\" #  # $class # [1] \"factor\"      \"na.included\" # Saving data without grouping columns dat <- get_vars(mtcars, -ind) # Grouped standard-deviation fsd(dat, f) #        mpg     disp       hp      drat        wt     qsec      gear     carb # 4 4.509828 26.87159 20.93453 0.3654711 0.5695637 1.682445 0.5393599 0.522233 # 6 1.453567 41.56246 24.26049 0.4760552 0.3563455 1.706866 0.6900656 1.812654 # 8 2.560048 67.77132 50.97689 0.3723618 0.7594047 1.196014 0.7262730 1.556624  # Without option na.exclude = FALSE, anyNA needs to be called on the factor (noticeable on larger data). f2 <- qF(mtcars$cyl) microbenchmark(fsd(dat, f), fsd(dat, f2)) # Unit: microseconds #          expr   min    lq    mean median    uq    max neval #   fsd(dat, f) 6.027 6.232 6.51613 6.4165 6.601 11.152   100 #  fsd(dat, f2) 6.150 6.396 6.77771 6.5190 6.683 25.830   100 # This creates a 'GRP' object. g <- GRP(mtcars, ~ cyl + vs + am) # Using the formula interface, could also use c(\"cyl\",\"vs\",\"am\") or c(2,8:9) str(g) # Class 'GRP'  hidden list of 9 #  $ N.groups    : int 7 #  $ group.id    : int [1:32] 4 4 3 5 6 5 6 2 2 5 ... #  $ group.sizes : int [1:7] 1 3 7 3 4 12 2 #  $ groups      :'data.frame': 7 obs. of  3 variables: #   ..$ cyl: num [1:7] 4 4 4 6 6 8 8 #   ..$ vs : num [1:7] 0 1 1 0 1 0 0 #   ..$ am : num [1:7] 1 0 1 1 0 0 1 #  $ group.vars  : chr [1:3] \"cyl\" \"vs\" \"am\" #  $ ordered     : Named logi [1:2] TRUE FALSE #   ..- attr(*, \"names\")= chr [1:2] \"ordered\" \"sorted\" #  $ order       : int [1:32] 27 8 9 21 3 18 19 20 26 28 ... #   ..- attr(*, \"starts\")= int [1:7] 1 2 5 12 15 19 31 #   ..- attr(*, \"maxgrpn\")= int 12 #   ..- attr(*, \"sorted\")= logi FALSE #  $ group.starts: int [1:7] 27 8 3 1 4 5 29 #  $ call        : language GRP.default(X = mtcars, by = ~cyl + vs + am) print(g) # collapse grouping object of length 32 with 7 ordered groups #  # Call: GRP.default(X = mtcars, by = ~cyl + vs + am), X is unsorted #  # Distribution of group sizes:  #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #   1.000   2.500   3.000   4.571   5.500  12.000  #  # Groups with sizes:  # 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #     1     3     7     3     4    12     2 plot(g) fsd(dat, g) #             mpg      disp       hp      drat        wt       qsec      gear      carb # 4.0.1        NA        NA       NA        NA        NA         NA        NA        NA # 4.1.0 1.4525839 13.969371 19.65536 0.1300000 0.4075230 1.67143651 0.5773503 0.5773503 # 4.1.1 4.7577005 18.802128 24.14441 0.3783926 0.4400840 0.94546285 0.3779645 0.5345225 # 6.0.1 0.7505553  8.660254 37.52777 0.1616581 0.1281601 0.76872188 0.5773503 1.1547005 # 6.1.0 1.6317169 44.742634  9.17878 0.5919459 0.1162164 0.81590441 0.5773503 1.7320508 # 8.0.0 2.7743959 71.823494 33.35984 0.2302749 0.7683069 0.80164745 0.0000000 0.9003366 # 8.0.1 0.5656854 35.355339 50.20458 0.4808326 0.2828427 0.07071068 0.0000000 2.8284271  # Grouped computation with and without prior grouping microbenchmark(fsd(dat, g), fsd(dat, get_vars(mtcars, ind))) # Unit: microseconds #                             expr    min      lq     mean  median      uq     max neval #                      fsd(dat, g) 19.065 21.1765 23.68447 22.9600 24.9690  38.909   100 #  fsd(dat, get_vars(mtcars, ind)) 31.611 35.2600 44.56823 37.3715 41.1845 327.877   100 gmtcars <- fgroup_by(mtcars, cyl, vs, am) # fgroup_by() can also be abbreviated as gby() fmedian(gmtcars) #   cyl vs am   mpg  disp    hp  drat    wt  qsec gear carb # 1   4  0  1 26.00 120.3  91.0 4.430 2.140 16.70  5.0  2.0 # 2   4  1  0 22.80 140.8  95.0 3.700 3.150 20.01  4.0  2.0 # 3   4  1  1 30.40  79.0  66.0 4.080 1.935 18.61  4.0  1.0 # 4   6  0  1 21.00 160.0 110.0 3.900 2.770 16.46  4.0  4.0 # 5   6  1  0 18.65 196.3 116.5 3.500 3.440 19.17  3.5  2.5 # 6   8  0  0 15.20 355.0 180.0 3.075 3.810 17.35  3.0  3.0 # 7   8  0  1 15.40 326.0 299.5 3.880 3.370 14.55  5.0  6.0  head(fgroup_vars(gmtcars)) #                   cyl vs am # Mazda RX4           6  0  1 # Mazda RX4 Wag       6  0  1 # Datsun 710          4  1  1 # Hornet 4 Drive      6  1  0 # Hornet Sportabout   8  0  0 # Valiant             6  1  0  fmedian(gmtcars, keep.group_vars = FALSE) #     mpg  disp    hp  drat    wt  qsec gear carb # 1 26.00 120.3  91.0 4.430 2.140 16.70  5.0  2.0 # 2 22.80 140.8  95.0 3.700 3.150 20.01  4.0  2.0 # 3 30.40  79.0  66.0 4.080 1.935 18.61  4.0  1.0 # 4 21.00 160.0 110.0 3.900 2.770 16.46  4.0  4.0 # 5 18.65 196.3 116.5 3.500 3.440 19.17  3.5  2.5 # 6 15.20 355.0 180.0 3.075 3.810 17.35  3.0  3.0 # 7 15.40 326.0 299.5 3.880 3.370 14.55  5.0  6.0 # Standard evaluation dat <- get_vars(mtcars, c(\"mpg\", \"disp\")) add_vars(g[[\"groups\"]],          add_stub(fmean(dat, g, use.g.names = FALSE), \"mean_\"),          add_stub(fsd(dat, g, use.g.names = FALSE), \"sd_\"),          add_stub(fmin(dat, g, use.g.names = FALSE), \"min_\"),          add_stub(fmax(dat, g, use.g.names = FALSE), \"max_\")) #   cyl vs am mean_mpg mean_disp    sd_mpg   sd_disp min_mpg min_disp max_mpg max_disp # 1   4  0  1 26.00000  120.3000        NA        NA    26.0    120.3    26.0    120.3 # 2   4  1  0 22.90000  135.8667 1.4525839 13.969371    21.5    120.1    24.4    146.7 # 3   4  1  1 28.37143   89.8000 4.7577005 18.802128    21.4     71.1    33.9    121.0 # 4   6  0  1 20.56667  155.0000 0.7505553  8.660254    19.7    145.0    21.0    160.0 # 5   6  1  0 19.12500  204.5500 1.6317169 44.742634    17.8    167.6    21.4    258.0 # 6   8  0  0 15.05000  357.6167 2.7743959 71.823494    10.4    275.8    19.2    472.0 # 7   8  0  1 15.40000  326.0000 0.5656854 35.355339    15.0    301.0    15.8    351.0  # Non-Standard evaluation fgroup_by(mtcars, cyl, vs, am) %>% fselect(mpg, disp) %>% {   add_vars(fgroup_vars(., \"unique\"),            fmean(., keep.group_vars = FALSE) %>% add_stub(\"mean_\"),            fsd(., keep.group_vars = FALSE) %>% add_stub(\"sd_\"),            fmin(., keep.group_vars = FALSE) %>% add_stub(\"min_\"),            fmax(., keep.group_vars = FALSE) %>% add_stub(\"max_\")) } #   cyl vs am mean_mpg mean_disp    sd_mpg   sd_disp min_mpg min_disp max_mpg max_disp # 1   4  0  1 26.00000  120.3000        NA        NA    26.0    120.3    26.0    120.3 # 2   4  1  0 22.90000  135.8667 1.4525839 13.969371    21.5    120.1    24.4    146.7 # 3   4  1  1 28.37143   89.8000 4.7577005 18.802128    21.4     71.1    33.9    121.0 # 4   6  0  1 20.56667  155.0000 0.7505553  8.660254    19.7    145.0    21.0    160.0 # 5   6  1  0 19.12500  204.5500 1.6317169 44.742634    17.8    167.6    21.4    258.0 # 6   8  0  0 15.05000  357.6167 2.7743959 71.823494    10.4    275.8    19.2    472.0 # 7   8  0  1 15.40000  326.0000 0.5656854 35.355339    15.0    301.0    15.8    351.0"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"grouped-and-weighted-computations","dir":"Articles","previous_headings":"4. Advanced Statistical Programming","what":"4.3 Grouped and Weighted Computations","title":"Introduction to collapse","text":"also calculate groupwise-frequency weighted means standard-deviations using weight vector2. R overhead kind programming standard-evaluation low:","code":"# Grouped and weighted mean and sd and grouped min and max add_vars(g[[\"groups\"]],          add_stub(fmean(dat, g, weights, use.g.names = FALSE), \"w_mean_\"),          add_stub(fsd(dat, g, weights, use.g.names = FALSE), \"w_sd_\"),          add_stub(fmin(dat, g, use.g.names = FALSE), \"min_\"),          add_stub(fmax(dat, g, use.g.names = FALSE), \"max_\")) #   cyl vs am w_mean_mpg w_mean_disp  w_sd_mpg w_sd_disp min_mpg min_disp max_mpg max_disp # 1   4  0  1   26.00000   120.30000 0.0000000   0.00000    26.0    120.3    26.0    120.3 # 2   4  1  0   23.08757   136.62639 1.5306081  14.19412    21.5    120.1    24.4    146.7 # 3   4  1  1   27.34688    92.65353 4.8723476  21.44005    21.4     71.1    33.9    121.0 # 4   6  0  1   20.22046   151.00525 0.9349875  10.78832    19.7    145.0    21.0    160.0 # 5   6  1  0   19.52725   204.86661 1.7612203  50.80083    17.8    167.6    21.4    258.0 # 6   8  0  0   15.12267   359.56902 2.2886672  70.60949    10.4    275.8    19.2    472.0 # 7   8  0  1   15.51023   332.88960 0.4758366  29.73979    15.0    301.0    15.8    351.0  # Binding and reordering columns in a single step: Add columns in specific positions add_vars(g[[\"groups\"]],          add_stub(fmean(dat, g, weights, use.g.names = FALSE), \"w_mean_\"),          add_stub(fsd(dat, g, weights, use.g.names = FALSE), \"w_sd_\"),          add_stub(fmin(dat, g, use.g.names = FALSE), \"min_\"),          add_stub(fmax(dat, g, use.g.names = FALSE), \"max_\"),          pos = c(4,8,5,9,6,10,7,11)) #   cyl vs am w_mean_mpg  w_sd_mpg min_mpg max_mpg w_mean_disp w_sd_disp min_disp max_disp # 1   4  0  1   26.00000 0.0000000    26.0    26.0   120.30000   0.00000    120.3    120.3 # 2   4  1  0   23.08757 1.5306081    21.5    24.4   136.62639  14.19412    120.1    146.7 # 3   4  1  1   27.34688 4.8723476    21.4    33.9    92.65353  21.44005     71.1    121.0 # 4   6  0  1   20.22046 0.9349875    19.7    21.0   151.00525  10.78832    145.0    160.0 # 5   6  1  0   19.52725 1.7612203    17.8    21.4   204.86661  50.80083    167.6    258.0 # 6   8  0  0   15.12267 2.2886672    10.4    19.2   359.56902  70.60949    275.8    472.0 # 7   8  0  1   15.51023 0.4758366    15.0    15.8   332.88960  29.73979    301.0    351.0 microbenchmark(call = add_vars(g[[\"groups\"]],          add_stub(fmean(dat, g, weights, use.g.names = FALSE), \"w_mean_\"),          add_stub(fsd(dat, g, weights, use.g.names = FALSE), \"w_sd_\"),          add_stub(fmin(dat, g, use.g.names = FALSE), \"min_\"),          add_stub(fmax(dat, g, use.g.names = FALSE), \"max_\"))) # Unit: microseconds #  expr    min      lq     mean median     uq   max neval #  call 27.388 28.1875 29.56428 28.823 29.356 97.58   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"transformations-using-the-tra-argument","dir":"Articles","previous_headings":"4. Advanced Statistical Programming","what":"4.4 Transformations Using the TRA Argument","title":"Introduction to collapse","text":"final layer added complexity, utilize TRA argument generate groupwise-weighted demeaned, scaled data, additional columns giving group-minimum maximum values: also possible add_vars<- mtcars . default option add columns end, also specify positions: Together ftransform, things can become arbitrarily complex: full set 14 Fast Statistical Functions, additional vector- valued functions operators (fscale/STD, fbetween/B, fwithin/W, fhdbetween/HDB, fhdwithin/HDW, flag/L/F, fdiff/D, fgrowth/G) discussed later, collapse provides extraordinary new possibilities highly complex efficient statistical programming R. Computation speeds generally exceed packages like dplyr data.table, sometimes orders magnitude. Column-wise matrix computations also highly efficient comparable packages like matrixStats base R functions like colSums. particular ability perform grouped weighted computations matrices new R useful complex computations (aggregating input-output tables etc.). Note examples provide merely suggestions use features focused programming data frames (predicates get_vars, add_vars etc. made data frames). Equivalently efficient code written using vectors matrices.","code":"head(add_vars(get_vars(mtcars, ind),               add_stub(fmean(dat, g, weights, \"-\"), \"w_demean_\"), # This calculates weighted group means and uses them to demean the data               add_stub(fsd(dat, g, weights, \"/\"), \"w_scale_\"),    # This calculates weighted group sd's and uses them to scale the data               add_stub(fmin(dat, g, \"replace\"), \"min_\"),          # This replaces all observations by their group-minimum               add_stub(fmax(dat, g, \"replace\"), \"max_\")))         # This replaces all observations by their group-maximum #                   cyl vs am w_demean_mpg w_demean_disp w_scale_mpg w_scale_disp min_mpg min_disp # Mazda RX4           6  0  1    0.7795446     8.9947455   22.460194    14.830858    19.7    145.0 # Mazda RX4 Wag       6  0  1    0.7795446     8.9947455   22.460194    14.830858    19.7    145.0 # Datsun 710          4  1  1   -4.5468786    15.3464694    4.679469     5.037303    21.4     71.1 # Hornet 4 Drive      6  1  0    1.8727485    53.1333901   12.150666     5.078657    17.8    167.6 # Hornet Sportabout   8  0  0    3.5773335     0.4309751    8.170694     5.098465    10.4    275.8 # Valiant             6  1  0   -1.4272515    20.1333901   10.276966     4.429062    17.8    167.6 #                   max_mpg max_disp # Mazda RX4            21.0      160 # Mazda RX4 Wag        21.0      160 # Datsun 710           33.9      121 # Hornet 4 Drive       21.4      258 # Hornet Sportabout    19.2      472 # Valiant              21.4      258 # This defines the positions where we want to add these columns pos <- as.integer(c(2,8,3,9,4,10,5,11))  add_vars(mtcars, pos) <- c(add_stub(fmean(dat, g, weights, \"-\"), \"w_demean_\"),                            add_stub(fsd(dat, g, weights, \"/\"), \"w_scale_\"),                            add_stub(fmin(dat, g, \"replace\"), \"min_\"),                            add_stub(fmax(dat, g, \"replace\"), \"max_\")) head(mtcars) #                    mpg w_demean_mpg w_scale_mpg min_mpg max_mpg cyl disp w_demean_disp w_scale_disp # Mazda RX4         21.0    0.7795446   22.460194    19.7    21.0   6  160     8.9947455    14.830858 # Mazda RX4 Wag     21.0    0.7795446   22.460194    19.7    21.0   6  160     8.9947455    14.830858 # Datsun 710        22.8   -4.5468786    4.679469    21.4    33.9   4  108    15.3464694     5.037303 # Hornet 4 Drive    21.4    1.8727485   12.150666    17.8    21.4   6  258    53.1333901     5.078657 # Hornet Sportabout 18.7    3.5773335    8.170694    10.4    19.2   8  360     0.4309751     5.098465 # Valiant           18.1   -1.4272515   10.276966    17.8    21.4   6  225    20.1333901     4.429062 #                   min_disp max_disp  hp drat    wt  qsec vs am gear carb # Mazda RX4            145.0      160 110 3.90 2.620 16.46  0  1    4    4 # Mazda RX4 Wag        145.0      160 110 3.90 2.875 17.02  0  1    4    4 # Datsun 710            71.1      121  93 3.85 2.320 18.61  1  1    4    1 # Hornet 4 Drive       167.6      258 110 3.08 3.215 19.44  1  0    3    1 # Hornet Sportabout    275.8      472 175 3.15 3.440 17.02  0  0    3    2 # Valiant              167.6      258 105 2.76 3.460 20.22  1  0    3    1 rm(mtcars) # 2 different grouped and weighted computations (mutate operations) performed in one call settransform(mtcars, carb_dwmed_cyl = fmedian(carb, cyl, weights, \"-\"),                      carb_wsd_vs_am = fsd(carb, list(vs, am), weights, \"replace\"))  # Multivariate settransform(mtcars, c(fmedian(list(carb_dwmed_cyl = carb, mpg_dwmed_cyl = mpg), cyl, weights, \"-\"),                       fsd(list(carb_wsd_vs_am = carb, mpg_wsd_vs_am = mpg), list(vs, am), weights, \"replace\")))  # Nested (Computing the weighted 3rd quartile of mpg, grouped by cyl and carb being greater than it's weighted median, grouped by vs) settransform(mtcars,  mpg_gwQ3_cyl = fnth(mpg, 0.75, list(cyl, carb > fmedian(carb, vs, weights, 1L)), weights, 1L))  head(mtcars) #                    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_dwmed_cyl carb_wsd_vs_am # Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4              0      2.1897386 # Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4              0      2.1897386 # Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1             -1      0.5286617 # Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1             -3      1.3161442 # Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2             -2      0.9674070 # Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1             -3      1.3161442 #                   mpg_dwmed_cyl mpg_wsd_vs_am mpg_gwQ3_cyl # Mazda RX4                   1.3      4.567045     21.40000 # Mazda RX4 Wag               1.3      4.567045     21.40000 # Datsun 710                 -3.2      4.872348     27.95146 # Hornet 4 Drive              1.7      2.444036     21.40000 # Hornet Sportabout           3.5      2.288667     16.21512 # Valiant                    -1.6      2.444036     21.40000 rm(mtcars)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"advanced-data-aggregation","dir":"Articles","previous_headings":"","what":"5. Advanced Data Aggregation","title":"Introduction to collapse","text":"grouped statistical programming introduced previous section fastest customizable way dealing many data transformation problems. tasks multivariate aggregations single data frame however common demanded compact solution efficiently integrates multiple computational steps. purposes collap created fast multi-purpose aggregation command designed solve complex aggregation problems efficiently minimum coding. collap performs optimally together Fast Statistical Functions, also work functions. perform aggregation collap, one simply need type: collap also saves sum weights column. original idea behind collap however better demonstrated different dataset. Consider World Development Dataset wlddev introduced section 1: Suppose like aggregate data country decade, keep categorical information. collap extremely simple: Note columns data original order also retain attributes. understand result let us briefly examine syntax collap: clear X data supplies grouping information, can one- two-sided formula alternatively grouping vectors, factors, lists GRP objects (like Fast Statistical Functions). FUN provides function(s) applied numeric variables X defaults fmean, catFUN provides function(s) applied categorical variables X defaults fmode3. keep.col.order = TRUE specifies data returned original column-order. Thus example sufficient supply X collap rest us. Suppose want aggregate 4 series dataset. use multiple functions putting named unnamed list4: multiple functions, also request collap return long-format data: important feature collap highlight point custom argument, allows user circumvent broad distinction numeric categorical data (associated FUN catFUN arguments) specify exactly columns aggregate using functions: Since collapse 1.5.0, also possible perform weighted aggregations append functions _uw yield unweighted computation: Next collap, functions collapv provides programmers alternative allowing grouping weighting columns passed using column names indices, function collapg operates grouped data frames.","code":"collap(mtcars, mpg + disp ~ cyl + vs + am, list(fmean, fsd, fmin, fmax),        w = weights, keep.col.order = FALSE) #   cyl vs am  weights fmean.mpg fmean.disp   fsd.mpg fsd.disp fmin.mpg fmin.disp fmax.mpg fmax.disp # 1   4  0  1 1.416054  26.00000  120.30000 0.0000000  0.00000     26.0     120.3     26.0     120.3 # 2   4  1  0 3.232217  23.08757  136.62639 1.5306081 14.19412     21.5     120.1     24.4     146.7 # 3   4  1  1 7.893395  27.34688   92.65353 4.8723476 21.44005     21.4      71.1     33.9     121.0 # 4   6  0  1 1.866025  20.22046  151.00525 0.9349875 10.78832     19.7     145.0     21.0     160.0 # 5   6  1  0 3.237565  19.52725  204.86661 1.7612203 50.80083     17.8     167.6     21.4     258.0 # 6   8  0  0 8.054777  15.12267  359.56902 2.2886672 70.60949     10.4     275.8     19.2     472.0 # 7   8  0  1 2.881698  15.51023  332.88960 0.4758366 29.73979     15.0     301.0     15.8     351.0 head(wlddev) #       country iso3c       date year decade     region     income  OECD PCGDP LIFEEX GINI       ODA # 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 32.446   NA 116769997 # 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 32.962   NA 232080002 # 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA 33.471   NA 112839996 # 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA 33.971   NA 237720001 # 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA 34.463   NA 295920013 # 6 Afghanistan   AFG 1966-01-01 1965   1960 South Asia Low income FALSE    NA 34.948   NA 341839996 #       POP # 1 8996973 # 2 9169410 # 3 9351441 # 4 9543205 # 5 9744781 # 6 9956320 collap(wlddev, ~ iso3c + decade) %>% head #   country iso3c       date   year decade                    region      income  OECD    PCGDP # 1   Aruba   ABW 1961-01-01 1964.5   1960 Latin America & Caribbean High income FALSE       NA # 2   Aruba   ABW 1971-01-01 1974.5   1970 Latin America & Caribbean High income FALSE       NA # 3   Aruba   ABW 1981-01-01 1984.5   1980 Latin America & Caribbean High income FALSE 20267.30 # 4   Aruba   ABW 1991-01-01 1994.5   1990 Latin America & Caribbean High income FALSE 26611.44 # 5   Aruba   ABW 2001-01-01 2004.5   2000 Latin America & Caribbean High income FALSE 26664.99 # 6   Aruba   ABW 2011-01-01 2014.5   2010 Latin America & Caribbean High income FALSE 24926.17 #    LIFEEX GINI      ODA      POP # 1 67.2592   NA       NA  56984.3 # 2 70.6372   NA       NA  60080.6 # 3 73.0153   NA 49745999  61665.9 # 4 73.6069   NA 29971000  76946.7 # 5 74.2660   NA 23292000  97939.7 # 6 75.6546   NA       NA 103994.6 collap(X, by, FUN = fmean, catFUN = fmode, cols = NULL, w = NULL, wFUN = fsum,        custom = NULL, keep.by = TRUE, keep.w = TRUE, keep.col.order = TRUE,        sort.row = TRUE, parallel = FALSE, mc.cores = 1L,        return = c(\"wide\",\"list\",\"long\",\"long_dupl\"), give.names = \"auto\") # , ... # Same as collap(wlddev, ~ iso3c + decade, cols = 9:12) collap(wlddev, PCGDP + LIFEEX + GINI + ODA ~ iso3c + decade) %>% head #   iso3c decade    PCGDP  LIFEEX GINI      ODA # 1   ABW   1960       NA 67.2592   NA       NA # 2   ABW   1970       NA 70.6372   NA       NA # 3   ABW   1980 20267.30 73.0153   NA 49745999 # 4   ABW   1990 26611.44 73.6069   NA 29971000 # 5   ABW   2000 26664.99 74.2660   NA 23292000 # 6   ABW   2010 24926.17 75.6546   NA       NA collap(wlddev, ~ iso3c + decade, list(fmean, fmedian, fsd), cols = 9:12) %>% head #   iso3c decade fmean.PCGDP fmedian.PCGDP fsd.PCGDP fmean.LIFEEX fmedian.LIFEEX fsd.LIFEEX # 1   ABW   1960          NA            NA        NA      67.2592        67.2740 1.03046880 # 2   ABW   1970          NA            NA        NA      70.6372        70.6760 0.96813702 # 3   ABW   1980    20267.30      20280.81 4037.2695      73.0153        73.1260 0.38203753 # 4   ABW   1990    26611.44      26684.19  592.7919      73.6069        73.6100 0.08549392 # 5   ABW   2000    26664.99      26992.71 1164.6741      74.2660        74.2215 0.37614448 # 6   ABW   2010    24926.17      24599.50 1159.7344      75.6546        75.6540 0.42974339 #   fmean.GINI fmedian.GINI fsd.GINI fmean.ODA fmedian.ODA  fsd.ODA # 1         NA           NA       NA        NA          NA       NA # 2         NA           NA       NA        NA          NA       NA # 3         NA           NA       NA  49745999    39259998 23573651 # 4         NA           NA       NA  29971000    35155001 17270808 # 5         NA           NA       NA  23292000    16219999 42969712 # 6         NA           NA       NA        NA          NA       NA collap(wlddev, ~ iso3c + decade, list(fmean, fmedian, fsd), cols = 9:12, return = \"long\") %>% head #   Function iso3c decade    PCGDP  LIFEEX GINI      ODA # 1    fmean   ABW   1960       NA 67.2592   NA       NA # 2    fmean   ABW   1970       NA 70.6372   NA       NA # 3    fmean   ABW   1980 20267.30 73.0153   NA 49745999 # 4    fmean   ABW   1990 26611.44 73.6069   NA 29971000 # 5    fmean   ABW   2000 26664.99 74.2660   NA 23292000 # 6    fmean   ABW   2010 24926.17 75.6546   NA       NA collap(wlddev, ~ iso3c + decade,         custom = list(fmean = 9:10, fmedian = 11:12,                       ffirst = c(\"country\",\"region\",\"income\"),                       flast = c(\"year\",\"date\"),                       fmode = \"OECD\")) %>% head #   country iso3c       date year decade                    region      income  OECD    PCGDP  LIFEEX # 1   Aruba   ABW 1970-01-01 1969   1960 Latin America & Caribbean High income FALSE       NA 67.2592 # 2   Aruba   ABW 1980-01-01 1979   1970 Latin America & Caribbean High income FALSE       NA 70.6372 # 3   Aruba   ABW 1990-01-01 1989   1980 Latin America & Caribbean High income FALSE 20267.30 73.0153 # 4   Aruba   ABW 2000-01-01 1999   1990 Latin America & Caribbean High income FALSE 26611.44 73.6069 # 5   Aruba   ABW 2010-01-01 2009   2000 Latin America & Caribbean High income FALSE 26664.99 74.2660 # 6   Aruba   ABW 2020-01-01 2019   2010 Latin America & Caribbean High income FALSE 24926.17 75.6546 #   GINI      ODA # 1   NA       NA # 2   NA       NA # 3   NA 39259998 # 4   NA 35155001 # 5   NA 16219999 # 6   NA       NA # This aggregates using weighted mean and mode, and unweighted median, first and last value collap(wlddev, ~ region + year, w = ~ POP,         custom = list(fmean = 9:10, fmedian_uw = 11:12,                       ffirst_uw = c(\"country\",\"region\",\"income\"),                       flast_uw = c(\"year\",\"date\"),                       fmode = \"OECD\"), keep.w = FALSE) %>% head #          country       date year year              region              region              income # 1 American Samoa 1961-01-01 1960 1960 East Asia & Pacific East Asia & Pacific Upper middle income # 2 American Samoa 1962-01-01 1961 1961 East Asia & Pacific East Asia & Pacific Upper middle income # 3 American Samoa 1963-01-01 1962 1962 East Asia & Pacific East Asia & Pacific Upper middle income # 4 American Samoa 1964-01-01 1963 1963 East Asia & Pacific East Asia & Pacific Upper middle income # 5 American Samoa 1965-01-01 1964 1964 East Asia & Pacific East Asia & Pacific Upper middle income # 6 American Samoa 1966-01-01 1965 1965 East Asia & Pacific East Asia & Pacific Upper middle income #    OECD    PCGDP   LIFEEX GINI       ODA # 1 FALSE 1313.760 48.20996   NA  37295000 # 2 FALSE 1395.228 48.73451   NA  26630001 # 3 FALSE 1463.441 49.39960   NA 100040001 # 4 FALSE 1540.621 50.37529   NA  40389999 # 5 FALSE 1665.385 51.57330   NA  70059998 # 6 FALSE 1733.757 52.94426   NA  91545002"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"data-transformations","dir":"Articles","previous_headings":"","what":"6. Data Transformations","title":"Introduction to collapse","text":"ftransform TRA argument Fast Statistical Functions introduced earlier already provide significant scope transforming data, section introduces specialized functions covering advanced common use cases, sometimes greater efficiency.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"row-and-column-arithmetic","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.1 Row and Column Arithmetic","title":"Introduction to collapse","text":"dealing matrices matrix-like datasets, often perform operations applying vector rows columns data object question. mathematical operations base R (+, -, *, /, %%, …) operate column-wise quite inefficient used data frames. Even matrix code challenging efficiently apply vector v rows matrix X. reason collapse introduces set efficient row- column-wise arithmetic operators matrix-like objects: %rr%, %r+%, %r-%, %r*%, %r/%, %cr%, %c+%, %c-%, %c*%, %c/%.","code":"X <- qM(fselect(GGDC10S, AGR:SUM)) v <- fsum(X) v #         AGR         MIN         MAN          PU         CON         WRT         TRA        FIRE  # 11026503529  8134743462 24120129864  1461548426  7845957666 14776120961  6416089614  7216735147  #         GOV         OTH         SUM  #  5962229565  7155872037 94115930269  # This divides the rows of X by v all_obj_equal(t(t(X) / v), X / outer(rep(1, nrow(X)), v), X %r/% v) # [1] TRUE  # Base R vs. efficient base R vs. collapse microbenchmark(t(t(X) / v), X / outer(rep(1, nrow(X)), v), X %r/% v) # Unit: microseconds #                         expr     min       lq      mean   median       uq      max neval #                    t(t(X)/v) 194.873 234.3560 358.13500 284.6425 298.0905 3244.043   100 #  X/outer(rep(1, nrow(X)), v)  55.555  83.5580 101.45696 108.5885 113.5495  137.637   100 #                     X %r/% v  11.685  37.2075  83.87657  63.2630  72.7135 2744.663   100  # Data frame row operations dat <- fselect(GGDC10S, AGR:SUM) microbenchmark(dat %r/% v, # Same thing using mapply and collapse::copyAttrib                copyAttrib(mapply(`/`, dat, v, SIMPLIFY = FALSE), dat)) # Unit: microseconds #                                                    expr    min     lq      mean median      uq #                                              dat %r/% v 15.129 37.187 143.03998 40.139 46.5555 #  copyAttrib(mapply(`/`, dat, v, SIMPLIFY = FALSE), dat) 59.204 64.124  71.98944 66.379 76.7315 #       max neval #  5089.289   100 #   110.003   100  # Data frame column arithmetic is very slow microbenchmark(dat / dat$SUM, dat / 5, dat / dat,                dat %c/% dat$SUM, dat %c/% 5, dat %c/% dat) # Unit: microseconds #              expr      min        lq       mean    median        uq       max neval #       dat/dat$SUM 1275.264 1385.2260 1636.95411 1434.2825 1551.1940  5150.092   100 #             dat/5  276.012  295.4870 1181.83361  306.2905  327.4260 83176.208   100 #           dat/dat  295.323  320.1075  417.10858  330.5010  361.7020  3807.711   100 #  dat %c/% dat$SUM   20.295   45.4075  120.01479   48.5235   55.1245  3520.096   100 #        dat %c/% 5   17.179   44.5260   87.22996   48.7285   64.1035  3489.223   100 #      dat %c/% dat   20.459   46.2685   93.95601   51.0040   67.5065  3795.903   100"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"row-and-column-data-apply","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.1 Row and Column Data Apply","title":"Introduction to collapse","text":"dapply efficient apply command matrices data frames. can used apply functions rows (default) columns matrices data frames default returns objects type attributes unless result computation scalar. dapply preserves data structure: also delivers seamless conversions, .e. can apply functions data frame rows columns return matrix vice-versa: data frames, performance comparable lapply, dapply 2x faster apply row- column-wise operations matrices. important feature change structure data : attributes preserved unless result scalar drop = TRUE (default).","code":"dapply(mtcars, median) #     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear    carb  #  19.200   6.000 196.300 123.000   3.695   3.325  17.710   0.000   0.000   4.000   2.000  dapply(mtcars, median, MARGIN = 1) #           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive   Hornet Sportabout  #               4.000               4.000               4.000               3.215               3.440  #             Valiant          Duster 360           Merc 240D            Merc 230            Merc 280  #               3.460               4.000               4.000               4.000               4.000  #           Merc 280C          Merc 450SE          Merc 450SL         Merc 450SLC  Cadillac Fleetwood  #               4.000               4.070               3.730               3.780               5.250  # Lincoln Continental   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #               5.424               5.345               4.000               4.000               4.000  #       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28    Pontiac Firebird  #               3.700               3.520               3.435               4.000               3.845  #           Fiat X1-9       Porsche 914-2        Lotus Europa      Ford Pantera L        Ferrari Dino  #               4.000               4.430               4.000               5.000               6.000  #       Maserati Bora          Volvo 142E  #               8.000               4.000  dapply(mtcars, quantile) #         mpg cyl    disp    hp  drat      wt    qsec vs am gear carb # 0%   10.400   4  71.100  52.0 2.760 1.51300 14.5000  0  0    3    1 # 25%  15.425   4 120.825  96.5 3.080 2.58125 16.8925  0  0    3    2 # 50%  19.200   6 196.300 123.0 3.695 3.32500 17.7100  0  0    4    2 # 75%  22.800   8 326.000 180.0 3.920 3.61000 18.9000  1  1    4    4 # 100% 33.900   8 472.000 335.0 4.930 5.42400 22.9000  1  1    5    8  dapply(mtcars, quantile, MARGIN = 1) %>% head #                   0%    25%   50%    75% 100% # Mazda RX4          0 3.2600 4.000 18.730  160 # Mazda RX4 Wag      0 3.3875 4.000 19.010  160 # Datsun 710         1 1.6600 4.000 20.705  108 # Hornet 4 Drive     0 2.0000 3.215 20.420  258 # Hornet Sportabout  0 2.5000 3.440 17.860  360 # Valiant            0 1.8800 3.460 19.160  225  # This is considerably more efficient than log(mtcars): dapply(mtcars, log) %>% head #                        mpg      cyl     disp       hp     drat        wt     qsec   vs   am # Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 2.800933 -Inf    0 # Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 2.834389 -Inf    0 # Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 2.923699    0    0 # Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 2.967333    0 -Inf # Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 2.834389 -Inf -Inf # Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 3.006672    0 -Inf #                       gear      carb # Mazda RX4         1.386294 1.3862944 # Mazda RX4 Wag     1.386294 1.3862944 # Datsun 710        1.386294 0.0000000 # Hornet 4 Drive    1.098612 0.0000000 # Hornet Sportabout 1.098612 0.6931472 # Valiant           1.098612 0.0000000 is.data.frame(dapply(mtcars, log)) # [1] TRUE is.matrix(dapply(m, log)) # [1] TRUE identical(log(m), dapply(mtcars, log, return = \"matrix\")) # [1] TRUE identical(dapply(mtcars, log), dapply(m, log, return = \"data.frame\")) # [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"split-apply-combine-computing","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.2 Split-Apply-Combine Computing","title":"Introduction to collapse","text":"generalization dapply grouped computations using functions part Fast Statistical Functions introduced . fundamentally re-implementation lapply(split(x, g), FUN, ...) computing paradigm base R, substantially faster versatile functions like tapply, aggregate. however faster dplyr data.table larger grouped computations data frames requiring split-apply-combine computing. S3 generic methods vector, matrix, data.frame grouped_df5. also supports grouping (g) inputs Fast Statistical Functions (grouping vectors, factors, lists GRP objects). use demonstrated vectors matrices data frames.","code":"v <- iris$Sepal.Length   # A numeric vector f <- iris$Species        # A factor  ## default vector method BY(v, f, sum)                          # Sum by species, about 2x faster than tapply(v, f, sum) #     setosa versicolor  virginica  #      250.3      296.8      329.4  BY(v, f, quantile)                     # Species quantiles: by default stacked #       setosa.0%      setosa.25%      setosa.50%      setosa.75%     setosa.100%   versicolor.0%  #           4.300           4.800           5.000           5.200           5.800           4.900  #  versicolor.25%  versicolor.50%  versicolor.75% versicolor.100%    virginica.0%   virginica.25%  #           5.600           5.900           6.300           7.000           4.900           6.225  #   virginica.50%   virginica.75%  virginica.100%  #           6.500           6.900           7.900  BY(v, f, quantile, expand.wide = TRUE) # Wide format #             0%   25% 50% 75% 100% # setosa     4.3 4.800 5.0 5.2  5.8 # versicolor 4.9 5.600 5.9 6.3  7.0 # virginica  4.9 6.225 6.5 6.9  7.9  ## matrix method miris <- qM(num_vars(iris)) BY(miris, f, sum)                          # Also returns as matrix #            Sepal.Length Sepal.Width Petal.Length Petal.Width # setosa            250.3       171.4         73.1        12.3 # versicolor        296.8       138.5        213.0        66.3 # virginica         329.4       148.7        277.6       101.3  BY(miris, f, quantile) %>% head #               Sepal.Length Sepal.Width Petal.Length Petal.Width # setosa.0%              4.3       2.300        1.000         0.1 # setosa.25%             4.8       3.200        1.400         0.2 # setosa.50%             5.0       3.400        1.500         0.2 # setosa.75%             5.2       3.675        1.575         0.3 # setosa.100%            5.8       4.400        1.900         0.6 # versicolor.0%          4.9       2.000        3.000         1.0  BY(miris, f, quantile, expand.wide = TRUE)[, 1:5] #            Sepal.Length.0% Sepal.Length.25% Sepal.Length.50% Sepal.Length.75% Sepal.Length.100% # setosa                 4.3            4.800              5.0              5.2               5.8 # versicolor             4.9            5.600              5.9              6.3               7.0 # virginica              4.9            6.225              6.5              6.9               7.9  BY(miris, f, quantile, expand.wide = TRUE, return = \"list\")[1:2] # list of matrices # $Sepal.Length #             0%   25% 50% 75% 100% # setosa     4.3 4.800 5.0 5.2  5.8 # versicolor 4.9 5.600 5.9 6.3  7.0 # virginica  4.9 6.225 6.5 6.9  7.9 #  # $Sepal.Width #             0%   25% 50%   75% 100% # setosa     2.3 3.200 3.4 3.675  4.4 # versicolor 2.0 2.525 2.8 3.000  3.4 # virginica  2.2 2.800 3.0 3.175  3.8  ## data.frame method BY(num_vars(iris), f, sum)             # Also returns a data.frame etc... #            Sepal.Length Sepal.Width Petal.Length Petal.Width # setosa            250.3       171.4         73.1        12.3 # versicolor        296.8       138.5        213.0        66.3 # virginica         329.4       148.7        277.6       101.3  ## Conversions identical(BY(num_vars(iris), f, sum), BY(miris, f, sum, return = \"data.frame\")) # [1] TRUE identical(BY(miris, f, sum), BY(num_vars(iris), f, sum, return = \"matrix\")) # [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-grouped-replacing-and-sweeping-out-statistics","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.3 Fast (Grouped) Replacing and Sweeping-out Statistics","title":"Introduction to collapse","text":"TRA S3 generic efficiently transforms data either replacing data values supplied statistics sweeping statistics data. workhorse function behind row-wise arithmetic operators introduced (%rr%, %r+%, %r-%, %r*%, %r/%), generalizes grouped operations. 10 operations supported TRA : 1 - “replace_fill” : replace overwrite missing values (dplyr::mutate) 2 - “replace” : replace preserve missing values 3 - “-” : subtract (center) 4 - “-+” : subtract group-statistics add average group statistics 5 - “/” : divide (scale) 6 - “%” : compute percentages (divide multiply 100) 7 - “+” : add 8 - “*” : multiply 9 - “%%” : modulus 10 - “-%%” : subtract modulus TRA also incorporated argument Fast Statistical Functions. Therefore really necessary advisable use TRA function aggregate statistics transformed data required, sweep statistics otherwise obtained (e.g. regression correlation coefficients etc.). code computes column means iris-matrix obtained , uses demean matrix. functionality also offered base::sweep, TRA significantly faster. big advantage TRA also supports grouped operations: mentioned, calling TRA() function make much sense task can performed using Fast Statistical Functions arithmetic operators. however useful function call complex transformations involving grouped sweeping operations precomputed quantities.","code":"# Note: All examples below generalize to vectors or data frames stats <- fmean(miris)               # Saving stats  # 6 identical ways of centering a matrix microbenchmark(sweep(miris, 2, stats, \"-\"),  # base R                miris - outer(rep(1, nrow(iris)), stats),                TRA(miris, fmean(miris), \"-\"),                miris %r-% fmean(miris),      # The operator is actually a wrapper around TRA                fmean(miris, TRA = \"-\"),      # better for any operation if the stats are not needed                fwithin(miris))               # fastest, fwithin is discussed in section 6.5 # Unit: microseconds #                                      expr    min      lq     mean  median      uq    max neval #               sweep(miris, 2, stats, \"-\") 15.457 16.2975 17.57711 17.0355 17.6915 53.505   100 #  miris - outer(rep(1, nrow(iris)), stats)  4.715  5.6375  6.36812  6.0270  6.6010 21.402   100 #             TRA(miris, fmean(miris), \"-\")  3.075  3.3210  3.98930  3.6080  4.4895 14.678   100 #                   miris %r-% fmean(miris)  3.362  3.8130  4.68425  4.0590  4.5305 42.066   100 #                   fmean(miris, TRA = \"-\")  2.583  2.8085  3.79496  2.9930  4.2640 29.848   100 #                            fwithin(miris)  3.321  3.6080  5.26768  3.8130  4.9815 78.474   100  # Simple replacing [same as fmean(miris, TRA = \"replace\") or fbetween(miris)] TRA(miris, fmean(miris), \"replace\") %>% head(3) #      Sepal.Length Sepal.Width Petal.Length Petal.Width # [1,]     5.843333    3.057333        3.758    1.199333 # [2,]     5.843333    3.057333        3.758    1.199333 # [3,]     5.843333    3.057333        3.758    1.199333  # Simple scaling [same as fsd(miris, TRA = \"/\")] TRA(miris, fsd(miris), \"/\") %>% head(3) #      Sepal.Length Sepal.Width Petal.Length Petal.Width # [1,]     6.158928    8.029986    0.7930671   0.2623854 # [2,]     5.917402    6.882845    0.7930671   0.2623854 # [3,]     5.675875    7.341701    0.7364195   0.2623854 # Grouped centering [same as fmean(miris, f, TRA = \"-\") or fwithin(m, f)] TRA(miris, fmean(miris, f), \"-\", f) %>% head(3) #      Sepal.Length Sepal.Width Petal.Length Petal.Width # [1,]        0.094       0.072       -0.062      -0.046 # [2,]       -0.106      -0.428       -0.062      -0.046 # [3,]       -0.306      -0.228       -0.162      -0.046  # Grouped replacing [same as fmean(m, f, TRA = \"replace\") or fbetween(m, f)] TRA(miris, fmean(miris, f), \"replace\", f) %>% head(3) #      Sepal.Length Sepal.Width Petal.Length Petal.Width # [1,]        5.006       3.428        1.462       0.246 # [2,]        5.006       3.428        1.462       0.246 # [3,]        5.006       3.428        1.462       0.246  # Groupwise percentages [same as fsum(m, f, TRA = \"%\")] TRA(miris, fsum(miris, f), \"%\", f) %>% head(3) #      Sepal.Length Sepal.Width Petal.Length Petal.Width # [1,]     2.037555    2.042007     1.915185    1.626016 # [2,]     1.957651    1.750292     1.915185    1.626016 # [3,]     1.877747    1.866978     1.778386    1.626016"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-standardizing","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.4 Fast Standardizing","title":"Introduction to collapse","text":"function fscale can used efficiently standardize (.e. scale center) data using numerically stable online algorithm. ’s structure Fast Statistical Functions. standardization-operator STD also exists wrapper around fscale. difference default STD adds prefix standardized variables also provides enhanced method data frames (operators next section). Scaling fscale / STD can also done groupwise / weighted. example Groningen Growth Development Center 10-Sector Database provides annual series value added local currency persons employed 10 broad sectors several African, Asian, Latin American countries. wanted correlate data across countries sectors, needs standardized:","code":"# fscale doesn't rename columns fscale(mtcars) %>% head(2) #                     mpg        cyl       disp         hp      drat         wt       qsec         vs # Mazda RX4     0.1508848 -0.1049878 -0.5706198 -0.5350928 0.5675137 -0.6103996 -0.7771651 -0.8680278 # Mazda RX4 Wag 0.1508848 -0.1049878 -0.5706198 -0.5350928 0.5675137 -0.3497853 -0.4637808 -0.8680278 #                     am      gear      carb # Mazda RX4     1.189901 0.4235542 0.7352031 # Mazda RX4 Wag 1.189901 0.4235542 0.7352031  # By default adds a prefix STD(mtcars) %>% head(2) #                 STD.mpg    STD.cyl   STD.disp     STD.hp  STD.drat     STD.wt   STD.qsec     STD.vs # Mazda RX4     0.1508848 -0.1049878 -0.5706198 -0.5350928 0.5675137 -0.6103996 -0.7771651 -0.8680278 # Mazda RX4 Wag 0.1508848 -0.1049878 -0.5706198 -0.5350928 0.5675137 -0.3497853 -0.4637808 -0.8680278 #                 STD.am  STD.gear  STD.carb # Mazda RX4     1.189901 0.4235542 0.7352031 # Mazda RX4 Wag 1.189901 0.4235542 0.7352031  # See that is works STD(mtcars) %>% qsu #            N  Mean  SD      Min     Max # STD.mpg   32     0   1  -1.6079  2.2913 # STD.cyl   32     0   1  -1.2249  1.0149 # STD.disp  32    -0   1  -1.2879  1.9468 # STD.hp    32     0   1   -1.381  2.7466 # STD.drat  32    -0   1  -1.5646  2.4939 # STD.wt    32    -0   1  -1.7418  2.2553 # STD.qsec  32    -0   1   -1.874  2.8268 # STD.vs    32     0   1   -0.868   1.116 # STD.am    32    -0   1  -0.8141  1.1899 # STD.gear  32    -0   1  -0.9318  1.7789 # STD.carb  32    -0   1  -1.1222  3.2117  # We can also scale and center to a different mean and standard deviation: qsu(fscale(mtcars, mean = 5, sd = 3))[, .c(Mean, SD)] %>% t #       mpg  cyl  disp  hp  drat  wt  qsec  vs  am  gear  carb # Mean    5    5     5   5     5   5     5   5   5     5     5 # SD      3    3     3   3     3   3     3   3   3     3     3  # Or not center at all. In that case scaling is mean-preserving, in contrast to fsd(mtcars, TRA = \"/\") qsu(fscale(mtcars, mean = FALSE, sd = 3))[, .c(Mean, SD)] %>% t #           mpg     cyl      disp        hp    drat      wt     qsec      vs      am    gear    carb # Mean  20.0906  6.1875  230.7219  146.6875  3.5966  3.2172  17.8487  0.4375  0.4062  3.6875  2.8125 # SD          3       3         3         3       3       3        3       3       3       3       3 head(GGDC10S) #   Country Regioncode             Region Variable Year      AGR      MIN       MAN        PU # 1     BWA        SSA Sub-saharan Africa       VA 1960       NA       NA        NA        NA # 2     BWA        SSA Sub-saharan Africa       VA 1961       NA       NA        NA        NA # 3     BWA        SSA Sub-saharan Africa       VA 1962       NA       NA        NA        NA # 4     BWA        SSA Sub-saharan Africa       VA 1963       NA       NA        NA        NA # 5     BWA        SSA Sub-saharan Africa       VA 1964 16.30154 3.494075 0.7365696 0.1043936 # 6     BWA        SSA Sub-saharan Africa       VA 1965 15.72700 2.495768 1.0181992 0.1350976 #         CON      WRT      TRA     FIRE      GOV      OTH      SUM # 1        NA       NA       NA       NA       NA       NA       NA # 2        NA       NA       NA       NA       NA       NA       NA # 3        NA       NA       NA       NA       NA       NA       NA # 4        NA       NA       NA       NA       NA       NA       NA # 5 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229 # 6 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710 # Standardizing Sectors by Variable and Country STD_GGDC10S <- STD(GGDC10S, ~ Variable + Country, cols = 6:16) head(STD_GGDC10S) #   Variable Country    STD.AGR    STD.MIN    STD.MAN     STD.PU    STD.CON    STD.WRT    STD.TRA # 1       VA     BWA         NA         NA         NA         NA         NA         NA         NA # 2       VA     BWA         NA         NA         NA         NA         NA         NA         NA # 3       VA     BWA         NA         NA         NA         NA         NA         NA         NA # 4       VA     BWA         NA         NA         NA         NA         NA         NA         NA # 5       VA     BWA -0.7382911 -0.7165772 -0.6682536 -0.8051315 -0.6922839 -0.6032762 -0.5889923 # 6       VA     BWA -0.7392424 -0.7167359 -0.6680535 -0.8050172 -0.6917529 -0.6030211 -0.5887320 #     STD.FIRE    STD.GOV    STD.OTH    STD.SUM # 1         NA         NA         NA         NA # 2         NA         NA         NA         NA # 3         NA         NA         NA         NA # 4         NA         NA         NA         NA # 5 -0.6349956 -0.6561054 -0.5959744 -0.6758663 # 6 -0.6349359 -0.6558634 -0.5957137 -0.6757768  # Correlating Standardized Value-Added across countries fsubset(STD_GGDC10S, Variable == \"VA\", STD.AGR:STD.SUM) %>% pwcor #          STD.AGR STD.MIN STD.MAN STD.PU STD.CON STD.WRT STD.TRA STD.FIRE STD.GOV STD.OTH STD.SUM # STD.AGR       1      .88     .93    .88     .89     .90     .90      .86     .93     .88     .90 # STD.MIN      .88      1      .86    .84     .85     .85     .84      .83     .88     .84     .86 # STD.MAN      .93     .86      1     .95     .96     .97     .98      .95     .98     .97     .98 # STD.PU       .88     .84     .95     1      .95     .96     .96      .95     .96     .96     .97 # STD.CON      .89     .85     .96    .95      1      .98     .98      .97     .98     .97     .98 # STD.WRT      .90     .85     .97    .96     .98      1      .99      .98     .99     .99    1.00 # STD.TRA      .90     .84     .98    .96     .98     .99      1       .98     .99     .99     .99 # STD.FIRE     .86     .83     .95    .95     .97     .98     .98       1      .98     .98     .98 # STD.GOV      .93     .88     .98    .96     .98     .99     .99      .98      1      .99    1.00 # STD.OTH      .88     .84     .97    .96     .97     .99     .99      .98     .99      1      .99 # STD.SUM      .90     .86     .98    .97     .98    1.00     .99      .98    1.00     .99      1"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-centering-and-averaging","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.5 Fast Centering and Averaging","title":"Introduction to collapse","text":"slightly faster alternative fmean(x, g, w, TRA = \"-\"/\"-+\") fmean(x, g, w, TRA = \"replace\"/\"replace_fill\"), fwithin fbetween can used perform common (grouped, weighted) centering averaging tasks (also known - within- transformations language panel data econometrics). fbetween / fwithin faster fmean(..., TRA = ...) don’t materialize full set computed averages. operators W B also exist. demonstrate clearly utility operators exists fast transformation time series functions, code implements task demeaning 4 series country saving country-id using within-operator W opposed fwithin requires input passed externally like Fast Statistical Functions. also possible drop id’s W using argument keep.= FALSE. fbetween / B fwithin / W one additional computational option: plot chunk BWplot Another great utility operators can employed regression formulas manor efficient pleasing eyes. code demonstrates use W B efficiently run fixed-effects regressions lm. general recommended calling long names (.e. fwithin fscale etc.) programming since bit efficient R-side things require input terms data. purposes operators convenient. important note operators can everything functions can (.e. can also pass grouping vectors GRP objects ). just simple wrappers data frame method add 4 additional features: possibility formula input .e. W(mtcars, ~ cyl) W(mtcars, mpg ~ cyl) preserve grouping columns (cyl example) passed formula (default keep.= TRUE) ability subset many columns using cols argument (.e. W(mtcars, ~ cyl, cols = 4:7) W(mtcars, hp + drat + wt + qsec ~ cyl)) rename transformed columns adding prefix (default stub = \"W.\")","code":"## Simple centering and averaging fbetween(mtcars$mpg) %>% head # [1] 20.09062 20.09062 20.09062 20.09062 20.09062 20.09062  fwithin(mtcars$mpg) %>% head # [1]  0.909375  0.909375  2.709375  1.309375 -1.390625 -1.990625  all.equal(fbetween(mtcars) + fwithin(mtcars), mtcars) # [1] TRUE  ## Groupwise centering and averaging fbetween(mtcars$mpg, mtcars$cyl) %>% head # [1] 19.74286 19.74286 26.66364 19.74286 15.10000 19.74286  fwithin(mtcars$mpg, mtcars$cyl) %>% head # [1]  1.257143  1.257143 -3.863636  1.657143  3.600000 -1.642857  all.equal(fbetween(mtcars, mtcars$cyl) + fwithin(mtcars, mtcars$cyl), mtcars) # [1] TRUE # Center 4 series in this dataset by country W(wlddev, ~ iso3c, cols = 9:12) %>% head #   iso3c W.PCGDP  W.LIFEEX W.GINI       W.ODA # 1   AFG      NA -16.75117     NA -1370778502 # 2   AFG      NA -16.23517     NA -1255468497 # 3   AFG      NA -15.72617     NA -1374708502 # 4   AFG      NA -15.22617     NA -1249828497 # 5   AFG      NA -14.73417     NA -1191628485 # 6   AFG      NA -14.24917     NA -1145708502  # Same thing done manually using fwithin... add_vars(get_vars(wlddev, \"iso3c\"),          get_vars(wlddev, 9:12) %>%          fwithin(wlddev$iso3c) %>%          add_stub(\"W.\")) %>% head #   iso3c W.PCGDP  W.LIFEEX W.GINI       W.ODA # 1   AFG      NA -16.75117     NA -1370778502 # 2   AFG      NA -16.23517     NA -1255468497 # 3   AFG      NA -15.72617     NA -1374708502 # 4   AFG      NA -15.22617     NA -1249828497 # 5   AFG      NA -14.73417     NA -1191628485 # 6   AFG      NA -14.24917     NA -1145708502 # This replaces missing values with the group-mean: Same as fmean(x, g, TRA = \"replace_fill\") B(wlddev, ~ iso3c, cols = 9:12, fill = TRUE) %>% head #   iso3c  B.PCGDP B.LIFEEX B.GINI      B.ODA # 1   AFG 483.8351 49.19717     NA 1487548499 # 2   AFG 483.8351 49.19717     NA 1487548499 # 3   AFG 483.8351 49.19717     NA 1487548499 # 4   AFG 483.8351 49.19717     NA 1487548499 # 5   AFG 483.8351 49.19717     NA 1487548499 # 6   AFG 483.8351 49.19717     NA 1487548499  # This adds back the overall mean after subtracting out group means: Same as fmean(x, g, TRA = \"-+\") W(wlddev, ~ iso3c, cols = 9:12, mean = \"overall.mean\")  %>% head #   iso3c W.PCGDP W.LIFEEX W.GINI      W.ODA # 1   AFG      NA 47.54514     NA -916058371 # 2   AFG      NA 48.06114     NA -800748366 # 3   AFG      NA 48.57014     NA -919988371 # 4   AFG      NA 49.07014     NA -795108366 # 5   AFG      NA 49.56214     NA -736908354 # 6   AFG      NA 50.04714     NA -690988371  # Visual demonstration of centering on the overall mean vs. simple centering oldpar <- par(mfrow = c(1, 3)) plot(iris[1:2], col = iris$Species, main = \"Raw Data\")                       # Raw data plot(W(iris, ~ Species)[2:3], col = iris$Species, main = \"Simple Centering\") # Simple centering plot(W(iris, ~ Species, mean = \"overall.mean\")[2:3], col = iris$Species,     # Centering on overall mean: Preserves level of data      main = \"Added Overall Mean\") par(oldpar) # When using operators in formulas, we need to remove missing values beforehand to obtain the same results as a Fixed-Effects package data <- wlddev %>% fselect(iso3c, year, PCGDP, LIFEEX) %>% na_omit  # classical lm() -> iso3c is a factor, creates a matrix of 200+ country dummies. coef(lm(PCGDP ~ LIFEEX + iso3c, data))[1:2] # (Intercept)      LIFEEX  #   -2837.039     380.448  # Centering each variable individually coef(lm(W(PCGDP, iso3c) ~ W(LIFEEX, iso3c), data)) #      (Intercept) W(LIFEEX, iso3c)  #     5.596034e-13     3.804480e+02  # Centering the data coef(lm(W.PCGDP ~ W.LIFEEX, W(data, PCGDP + LIFEEX ~ iso3c))) #  (Intercept)     W.LIFEEX  # 5.596034e-13 3.804480e+02  # Adding the overall mean back to the data only changes the intercept coef(lm(W.PCGDP ~ W.LIFEEX, W(data, PCGDP + LIFEEX  ~ iso3c, mean = \"overall.mean\"))) # (Intercept)    W.LIFEEX  #  -14020.142     380.448  # Procedure suggested by Mundlak (1978) - controlling for group averages instead of demeaning coef(lm(PCGDP ~ LIFEEX + B(LIFEEX, iso3c), data)) #      (Intercept)           LIFEEX B(LIFEEX, iso3c)  #      -52254.7421         380.4480         585.8386"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"hd-centering-and-linear-prediction","dir":"Articles","previous_headings":"6. Data Transformations","what":"6.6 HD Centering and Linear Prediction","title":"Introduction to collapse","text":"Sometimes simple centering enough, example linear model multiple levels fixed-effects needs estimated, potentially involving interactions continuous covariates. purposes fhdwithin / HDW fhdbetween / HDB created efficient multi-purpose functions linear prediction partialling . operate splitting complex regression problems 2 parts: Factors factor-interactions projected using fixest::demean, efficient C++ routine centering vectors multiple factors, whereas continuous variables dealt using standard chol qr decomposition base R. examples show use HDW operator manually solving regression problem country time fixed effects. may wish test whether including time fixed-effects regression actually impacts fit. can done fast F-test: test shows time fixed-effects (accounted like year dummies) jointly significant. One can also use fhdbetween / HDB fhdwithin / HDW project interactions continuous covariates.","code":"data$year <- qF(data$year, na.exclude = FALSE) # the country code (iso3c) is already a factor  # classical lm() -> creates a matrix of 196 country dummies and 56 year dummies coef(lm(PCGDP ~ LIFEEX + iso3c + year, data))[1:2] # (Intercept)      LIFEEX  #  37388.0493   -333.0115  # Centering each variable individually coef(lm(HDW(PCGDP, list(iso3c, year)) ~ HDW(LIFEEX, list(iso3c, year)), data)) #                    (Intercept) HDW(LIFEEX, list(iso3c, year))  #                  -2.450245e-13                  -3.330115e+02  # Centering the entire data coef(lm(HDW.PCGDP ~ HDW.LIFEEX, HDW(data, PCGDP + LIFEEX ~ iso3c + year))) #   (Intercept)    HDW.LIFEEX  # -2.450245e-13 -3.330115e+02  # Procedure suggested by Mundlak (1978) - controlling for averages instead of demeaning coef(lm(PCGDP ~ LIFEEX + HDB(LIFEEX, list(iso3c, year)), data)) #                    (Intercept)                         LIFEEX HDB(LIFEEX, list(iso3c, year))  #                    -48141.1094                      -333.0115                      1236.2681 # The syntax is fFtest(y, exc, X, ...). 'exc' are exclusion restrictions. data %$% fFtest(PCGDP, year, list(LIFEEX, iso3c)) #                     R-Sq.  DF1  DF2  F-Stat.  P-Value # Full Model          0.894  258 8763  286.130    0.000 # Restricted Model    0.873  199 8822  304.661    0.000 # Exclusion Rest.     0.021   59 8763   29.280    0.000 wlddev$year <- as.numeric(wlddev$year)  # classical lm() -> full country-year interaction, -> 200+ country dummies, 200+ trends, year and ODA coef(lm(PCGDP ~ LIFEEX + iso3c * year + ODA, wlddev))[1:2] #   (Intercept)        LIFEEX  # -7.257955e+05  8.938626e+00  # Same using HDW coef(lm(HDW.PCGDP ~ HDW.LIFEEX, HDW(wlddev, PCGDP + LIFEEX ~ iso3c * year + ODA))) #  (Intercept)   HDW.LIFEEX  # 3.403288e-12 8.938626e+00  # example of a simple continuous problem HDW(iris[1:2], iris[3:4]) %>% head #   HDW.Sepal.Length HDW.Sepal.Width # 1       0.21483967       0.2001352 # 2       0.01483967      -0.2998648 # 3      -0.13098262      -0.1255786 # 4      -0.33933805      -0.1741510 # 5       0.11483967       0.3001352 # 6       0.41621663       0.6044681  # May include factors.. HDW(iris[1:2], iris[3:5]) %>% head #   HDW.Sepal.Length HDW.Sepal.Width # 1       0.14989286       0.1102684 # 2      -0.05010714      -0.3897316 # 3      -0.15951256      -0.1742640 # 4      -0.44070173      -0.3051992 # 5       0.04989286       0.2102684 # 6       0.17930818       0.3391766"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"time-series-and-panel-series","dir":"Articles","previous_headings":"","what":"7. Time Series and Panel Series","title":"Introduction to collapse","text":"collapse also presents essential contributions time series domain, particularly area (irregular) time series, panel data efficient secure computations (potentially unordered) time-dependent vectors (unbalanced) panels.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"panel-series-to-array-conversions","dir":"Articles","previous_headings":"7. Time Series and Panel Series","what":"7.1 Panel Series to Array Conversions","title":"Introduction to collapse","text":"facilitate exploration access panel data, psmat created S3 generic efficiently obtain matrices 3D-arrays panel data. plot chunk psmatplot Passing data frame panel series psmat generates 3D array: plot chunk psarplot plot chunk psarplot2 psmat can also output list panel series matrices, can used among things reshape data unlist2d (discussed detail List-Processing section).","code":"mts <- psmat(wlddev, PCGDP ~ iso3c, ~ year) str(mts) #  'psmat' num [1:216, 1:61] NA NA NA NA NA ... #  - attr(*, \"dimnames\")=List of 2 #   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #  - attr(*, \"transpose\")= logi FALSE plot(log10(mts), main = paste(\"Log10\", vlabels(wlddev$PCGDP)), xlab = \"Year\") # Get panel series array psar <- psmat(wlddev, ~ iso3c, ~ year, cols = 9:12) str(psar) #  'psmat' num [1:216, 1:61, 1:4] NA NA NA NA NA ... #  - attr(*, \"dimnames\")=List of 3 #   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #   ..$ : chr [1:4] \"PCGDP\" \"LIFEEX\" \"GINI\" \"ODA\" #  - attr(*, \"transpose\")= logi FALSE plot(psar) # Plot array of Panel Series aggregated by region: collap(wlddev, ~ region + year, cols = 9:12) %>%   psmat( ~ region, ~ year) %>%   plot(legend = TRUE, labs = vlabels(wlddev)[9:12]) # This gives list of ps-matrices psml <- psmat(wlddev, ~ iso3c, ~ year, 9:12, array = FALSE) str(psml, give.attr = FALSE) # List of 4 #  $ PCGDP : 'psmat' num [1:216, 1:61] NA NA NA NA NA ... #  $ LIFEEX: 'psmat' num [1:216, 1:61] 65.7 32.4 37.5 62.3 NA ... #  $ GINI  : 'psmat' num [1:216, 1:61] NA NA NA NA NA NA NA NA NA NA ... #  $ ODA   : 'psmat' num [1:216, 1:61] NA 116769997 -390000 NA NA ...  # Using unlist2d, can generate a data.frame unlist2d(psml, idcols = \"Variable\", row.names = \"Country\") %>% gv(1:10) %>% head #   Variable Country 1960 1961 1962 1963 1964 1965 1966 1967 # 1    PCGDP     ABW   NA   NA   NA   NA   NA   NA   NA   NA # 2    PCGDP     AFG   NA   NA   NA   NA   NA   NA   NA   NA # 3    PCGDP     AGO   NA   NA   NA   NA   NA   NA   NA   NA # 4    PCGDP     ALB   NA   NA   NA   NA   NA   NA   NA   NA # 5    PCGDP     AND   NA   NA   NA   NA   NA   NA   NA   NA # 6    PCGDP     ARE   NA   NA   NA   NA   NA   NA   NA   NA"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"panel-series-acf-pacf-and-ccf","dir":"Articles","previous_headings":"7. Time Series and Panel Series","what":"7.2 Panel Series ACF, PACF and CCF","title":"Introduction to collapse","text":"correlation structure panel data can also explored psacf, pspacf psccf. functions exact analogues stats::acf, stats::pacf stats::ccf. use fscale group-scale panel data panel-id provided, compute covariance sequence panel-lags (generated flag discussed ) group-scaled level-series, dividing variance group-scaled level series. Partial-ACF generated ACF using Yule-Walker decomposition (stats::pacf). plot chunk PSACF plot chunk PSACF plot chunk PSACF plot chunk PSACF","code":"# Panel-ACF of GDP per Capita psacf(wlddev, PCGDP ~ iso3c, ~year) # Panel-Partial-ACF of GDP per Capia pspacf(wlddev, PCGDP ~ iso3c, ~year) # Panel- Cross-Correlation function of GDP per Capia and Life-Expectancy wlddev %$% psccf(PCGDP, LIFEEX, iso3c, year) # Multivariate Panel-auto and cross-correlation function of 3 variables: psacf(wlddev, PCGDP + LIFEEX + ODA ~ iso3c, ~year)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-lags-and-leads","dir":"Articles","previous_headings":"7. Time Series and Panel Series","what":"7.3 Fast Lags and Leads","title":"Introduction to collapse","text":"flag corresponding lag- lead- operators L F S3 generics efficiently compute lags leads time series panel data. code shows compute simple lags leads classic Box & Jenkins airline data comes R. flag / L / F also work well (time series) matrices. regression daily closing prices major European stock indices run: Germany DAX (Ibis), Switzerland SMI, France CAC, UK FTSE. data sampled business time, .e. weekends holidays omitted. plot chunk mts Since v1.5.0, irregular time series supported: main innovation flag / L / F ability efficiently compute sequences lags leads panel data, panel data need ordered balanced: Optimal performance obtained panel-id factor, time variable also factor integer variable. case ordering vector data computed directly without prior sorting grouping, data accessed vector. Thus data need sorted compute fully-identified panel-lag, key advantage , say, shift function data.table. One intended area use, especially operators L F, substantially facilitate implementation dynamic models various contexts (independent estimation package). different ways L can used estimate dynamic panel-model using lm shown:","code":"# 1 lag L(AirPassengers) #      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec # 1949  NA 112 118 132 129 121 135 148 148 136 119 104 # 1950 118 115 126 141 135 125 149 170 170 158 133 114 # 1951 140 145 150 178 163 172 178 199 199 184 162 146 # 1952 166 171 180 193 181 183 218 230 242 209 191 172 # 1953 194 196 196 236 235 229 243 264 272 237 211 180 # 1954 201 204 188 235 227 234 264 302 293 259 229 203 # 1955 229 242 233 267 269 270 315 364 347 312 274 237 # 1956 278 284 277 317 313 318 374 413 405 355 306 271 # 1957 306 315 301 356 348 355 422 465 467 404 347 305 # 1958 336 340 318 362 348 363 435 491 505 404 359 310 # 1959 337 360 342 406 396 420 472 548 559 463 407 362 # 1960 405 417 391 419 461 472 535 622 606 508 461 390  # 3 identical ways of computing 1 lag all_identical(flag(AirPassengers), L(AirPassengers), F(AirPassengers,-1)) # [1] TRUE  # 1 lead and 3 lags - output as matrix L(AirPassengers, -1:3) %>% head #       F1  --  L1  L2  L3 # [1,] 118 112  NA  NA  NA # [2,] 132 118 112  NA  NA # [3,] 129 132 118 112  NA # [4,] 121 129 132 118 112 # [5,] 135 121 129 132 118 # [6,] 148 135 121 129 132  # ... this is still a time series object: attributes(L(AirPassengers, -1:3)) # $tsp # [1] 1949.000 1960.917   12.000 #  # $class # [1] \"ts\"     \"matrix\" #  # $dim # [1] 144   5 #  # $dimnames # $dimnames[[1]] # NULL #  # $dimnames[[2]] # [1] \"F1\" \"--\" \"L1\" \"L2\" \"L3\" str(EuStockMarkets) #  Time-Series [1:1860, 1:4] from 1991 to 1999: 1629 1614 1607 1621 1618 ... #  - attr(*, \"dimnames\")=List of 2 #   ..$ : NULL #   ..$ : chr [1:4] \"DAX\" \"SMI\" \"CAC\" \"FTSE\"  # Data is recorded on 260 days per year, 1991-1999 tsp(EuStockMarkets) # [1] 1991.496 1998.646  260.000 freq <- frequency(EuStockMarkets)  # There is some obvious seasonality stl(EuStockMarkets[, \"DAX\"], freq) %>% plot # 1 annual lead and 1 annual lag L(EuStockMarkets, -1:1*freq) %>% head #      F260.DAX     DAX L260.DAX F260.SMI    SMI L260.SMI F260.CAC    CAC L260.CAC F260.FTSE   FTSE # [1,]  1755.98 1628.75       NA   1846.6 1678.1       NA   1907.3 1772.8       NA    2515.8 2443.6 # [2,]  1754.95 1613.63       NA   1854.8 1688.5       NA   1900.6 1750.5       NA    2521.2 2460.2 # [3,]  1759.90 1606.51       NA   1845.3 1678.6       NA   1880.9 1718.0       NA    2493.9 2448.2 # [4,]  1759.84 1621.04       NA   1854.5 1684.1       NA   1873.5 1708.1       NA    2476.1 2470.4 # [5,]  1776.50 1618.16       NA   1870.5 1686.6       NA   1883.6 1723.1       NA    2497.1 2484.7 # [6,]  1769.98 1610.61       NA   1862.6 1671.6       NA   1868.5 1714.3       NA    2469.0 2466.8 #      L260.FTSE # [1,]        NA # [2,]        NA # [3,]        NA # [4,]        NA # [5,]        NA # [6,]        NA  # DAX regressed on it's own 2 annual lags and the lags of the other indicators lm(DAX ~., data = L(EuStockMarkets, 0:2*freq)) %>% summary #  # Call: # lm(formula = DAX ~ ., data = L(EuStockMarkets, 0:2 * freq)) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -240.46  -51.28  -12.01   45.19  358.02  #  # Coefficients: #               Estimate Std. Error t value Pr(>|t|)     # (Intercept) -564.02041   93.94903  -6.003 2.49e-09 *** # L260.DAX      -0.12577    0.03002  -4.189 2.99e-05 *** # L520.DAX      -0.12528    0.04103  -3.053  0.00231 **  # SMI            0.32601    0.01726  18.890  < 2e-16 *** # L260.SMI       0.27499    0.02517  10.926  < 2e-16 *** # L520.SMI       0.04602    0.02602   1.769  0.07721 .   # CAC            0.59637    0.02349  25.389  < 2e-16 *** # L260.CAC      -0.14283    0.02763  -5.169 2.72e-07 *** # L520.CAC       0.05196    0.03657   1.421  0.15557     # FTSE           0.01002    0.02403   0.417  0.67675     # L260.FTSE      0.04509    0.02807   1.606  0.10843     # L520.FTSE      0.10601    0.02717   3.902  0.00010 *** # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 83.06 on 1328 degrees of freedom #   (520 observations deleted due to missingness) # Multiple R-squared:  0.9943,  Adjusted R-squared:  0.9942  # F-statistic: 2.092e+04 on 11 and 1328 DF,  p-value: < 2.2e-16 t <- seq_row(EuStockMarkets)[-4L]  flag(EuStockMarkets[-4L, ], -1:1, t = t) %>% head #       F1.DAX     DAX  L1.DAX F1.SMI    SMI L1.SMI F1.CAC    CAC L1.CAC F1.FTSE   FTSE L1.FTSE # [1,] 1613.63 1628.75      NA 1688.5 1678.1     NA 1750.5 1772.8     NA  2460.2 2443.6      NA # [2,] 1606.51 1613.63 1628.75 1678.6 1688.5 1678.1 1718.0 1750.5 1772.8  2448.2 2460.2  2443.6 # [3,]      NA 1606.51 1613.63     NA 1678.6 1688.5     NA 1718.0 1750.5      NA 2448.2  2460.2 # [4,] 1610.61 1618.16      NA 1671.6 1686.6     NA 1714.3 1723.1     NA  2466.8 2484.7      NA # [5,] 1630.75 1610.61 1618.16 1682.9 1671.6 1686.6 1734.5 1714.3 1723.1  2487.9 2466.8  2484.7 # [6,] 1640.17 1630.75 1610.61 1703.6 1682.9 1671.6 1757.4 1734.5 1714.3  2508.4 2487.9  2466.8 # This lags all 4 series L(wlddev, 1L, ~ iso3c, ~ year, cols = 9:12) %>% head #   iso3c year L1.PCGDP L1.LIFEEX L1.GINI    L1.ODA # 1   AFG 1960       NA        NA      NA        NA # 2   AFG 1961       NA    32.446      NA 116769997 # 3   AFG 1962       NA    32.962      NA 232080002 # 4   AFG 1963       NA    33.471      NA 112839996 # 5   AFG 1964       NA    33.971      NA 237720001 # 6   AFG 1965       NA    34.463      NA 295920013  # Without t: Works here because data is ordered, but gives a message L(wlddev, 1L, ~ iso3c, cols = 9:12) %>% head #   iso3c L1.PCGDP L1.LIFEEX L1.GINI    L1.ODA # 1   AFG       NA        NA      NA        NA # 2   AFG       NA    32.446      NA 116769997 # 3   AFG       NA    32.962      NA 232080002 # 4   AFG       NA    33.471      NA 112839996 # 5   AFG       NA    33.971      NA 237720001 # 6   AFG       NA    34.463      NA 295920013  # 1 lead and 2 lags of Life Expectancy # after removing the 4th row, thus creating an unbalanced panel wlddev %>% ss(-4L) %>%   L(-1:2, LIFEEX ~ iso3c, ~year) %>% head #   iso3c year F1.LIFEEX LIFEEX L1.LIFEEX L2.LIFEEX # 1   AFG 1960    32.962 32.446        NA        NA # 2   AFG 1961    33.471 32.962    32.446        NA # 3   AFG 1962        NA 33.471    32.962    32.446 # 4   AFG 1964    34.948 34.463        NA    33.471 # 5   AFG 1965    35.430 34.948    34.463        NA # 6   AFG 1966    35.914 35.430    34.948    34.463 # Different ways of regressing GDP on it's lags and life-Expectancy and it's lags  # 1 - Precomputing lags lm(PCGDP ~ ., L(wlddev, 0:2, PCGDP + LIFEEX ~ iso3c, ~ year, keep.ids = FALSE)) %>% summary #  # Call: # lm(formula = PCGDP ~ ., data = L(wlddev, 0:2, PCGDP + LIFEEX ~  #     iso3c, ~year, keep.ids = FALSE)) #  # Residuals: #      Min       1Q   Median       3Q      Max  # -16776.5   -102.2    -17.2     91.5  12277.1  #  # Coefficients: #               Estimate Std. Error t value Pr(>|t|)     # (Intercept) -333.93994   61.04617  -5.470 4.62e-08 *** # L1.PCGDP       1.31959    0.01021 129.270  < 2e-16 *** # L2.PCGDP      -0.31707    0.01029 -30.815  < 2e-16 *** # LIFEEX       -17.77368   35.47772  -0.501    0.616     # L1.LIFEEX     45.76286   65.87124   0.695    0.487     # L2.LIFEEX    -21.43005   34.98964  -0.612    0.540     # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 787.3 on 8609 degrees of freedom #   (4561 observations deleted due to missingness) # Multiple R-squared:  0.9976,  Adjusted R-squared:  0.9976  # F-statistic: 7.26e+05 on 5 and 8609 DF,  p-value: < 2.2e-16  # 2 - Ad-hoc computation in lm formula lm(PCGDP ~ L(PCGDP, 1:2, iso3c, year) + L(LIFEEX, 0:2, iso3c, year), wlddev) %>% summary #  # Call: # lm(formula = PCGDP ~ L(PCGDP, 1:2, iso3c, year) + L(LIFEEX, 0:2,  #     iso3c, year), data = wlddev) #  # Residuals: #      Min       1Q   Median       3Q      Max  # -16776.5   -102.2    -17.2     91.5  12277.1  #  # Coefficients: #                                 Estimate Std. Error t value Pr(>|t|)     # (Intercept)                   -333.93994   61.04617  -5.470 4.62e-08 *** # L(PCGDP, 1:2, iso3c, year)L1     1.31959    0.01021 129.270  < 2e-16 *** # L(PCGDP, 1:2, iso3c, year)L2    -0.31707    0.01029 -30.815  < 2e-16 *** # L(LIFEEX, 0:2, iso3c, year)--  -17.77368   35.47772  -0.501    0.616     # L(LIFEEX, 0:2, iso3c, year)L1   45.76286   65.87124   0.695    0.487     # L(LIFEEX, 0:2, iso3c, year)L2  -21.43005   34.98964  -0.612    0.540     # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 787.3 on 8609 degrees of freedom #   (4561 observations deleted due to missingness) # Multiple R-squared:  0.9976,  Adjusted R-squared:  0.9976  # F-statistic: 7.26e+05 on 5 and 8609 DF,  p-value: < 2.2e-16  # 3 - Precomputing panel-identifiers g = qF(wlddev$iso3c, na.exclude = FALSE) t = qF(wlddev$year, na.exclude = FALSE) lm(PCGDP ~ L(PCGDP, 1:2, g, t) + L(LIFEEX, 0:2, g, t), wlddev) %>% summary #  # Call: # lm(formula = PCGDP ~ L(PCGDP, 1:2, g, t) + L(LIFEEX, 0:2, g,  #     t), data = wlddev) #  # Residuals: #      Min       1Q   Median       3Q      Max  # -16776.5   -102.2    -17.2     91.5  12277.1  #  # Coefficients: #                          Estimate Std. Error t value Pr(>|t|)     # (Intercept)            -333.93994   61.04617  -5.470 4.62e-08 *** # L(PCGDP, 1:2, g, t)L1     1.31959    0.01021 129.270  < 2e-16 *** # L(PCGDP, 1:2, g, t)L2    -0.31707    0.01029 -30.815  < 2e-16 *** # L(LIFEEX, 0:2, g, t)--  -17.77368   35.47772  -0.501    0.616     # L(LIFEEX, 0:2, g, t)L1   45.76286   65.87124   0.695    0.487     # L(LIFEEX, 0:2, g, t)L2  -21.43005   34.98964  -0.612    0.540     # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 787.3 on 8609 degrees of freedom #   (4561 observations deleted due to missingness) # Multiple R-squared:  0.9976,  Adjusted R-squared:  0.9976  # F-statistic: 7.26e+05 on 5 and 8609 DF,  p-value: < 2.2e-16"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"fast-differences-and-growth-rates","dir":"Articles","previous_headings":"7. Time Series and Panel Series","what":"7.4 Fast Differences and Growth Rates","title":"Introduction to collapse","text":"Similarly flag / L / F, fdiff / D / Dlog computes sequences suitably lagged / leaded iterated differences, quasi-differences (quasi-)log-differences time series panel data, fgrowth / G computes growth rates. Using Airpassengers data, seasonal decomposition shows significant seasonality: plot chunk stl can test statistical significance seasonality jointly testing set monthly dummies regressed differenced series. Given seasonal fluctuations increasing magnitude, using growth rates test seems appropriate: test shows significant seasonality, accounting 87% variation growth rate series. can plot series together ordinary, seasonal (12-month) deseasonalized monthly growth rate using: plot chunk Gplot evident taking annualized growth rate also removes periodic behavior. can also compute second differences growth rates growth rates. plot ordinary annual first second differences data: plot chunk Dplot general, fdiff / D fgrowth / G can compute sequences lagged / leaded iterated differences / growth rates. also works panel data. code gives example: Calls flag / L / F, fdiff / D fgrowth / G can nested. example , L.matrix called right-half ob sequence: fdiff / D fgrowth / G also come data frame method, making computation growth-variables datasets easy: code estimates dynamic panel model regressing 10-year growth rate GDP per capita ’s 10-year lagged level 10-year growth rate life-expectancy: go step , code regresses 10-year growth rate GDP 10-year lagged levels 10-year growth rates GDP life expectancy, country time-fixed effects projected using HDW. standard errors unreliable without bootstrapping, example nicely demonstrates potential complex estimations brought collapse. One inconveniences computations requires declaring panel-identifiers iso3c year function. great remedy plm classes pseries pdata.frame collapse built support. shows one run regression plm: learn integration collapse plm, consult corresponding vignette.","code":"stl(AirPassengers, \"periodic\") %>% plot f <- qF(cycle(AirPassengers)) fFtest(fgrowth(AirPassengers), f) #   R-Sq.     DF1     DF2 F-Stat. P-value  #   0.874      11     131  82.238   0.000 G(AirPassengers, c(0, 1, 12)) %>% cbind(W.G1 = W(G(AirPassengers), f)) %>%   plot(main = \"Growth Rate of Airpassengers\") D(AirPassengers, c(1,12), 1:2) %>% plot # sequence of leaded/lagged and iterated differences y = 1:10 D(y, -2:2, 1:3) #       F2D1 F2D2 F2D3 FD1 FD2 FD3 -- D1 D2 D3 L2D1 L2D2 L2D3 #  [1,]   -2    0    0  -1   0   0  1 NA NA NA   NA   NA   NA #  [2,]   -2    0    0  -1   0   0  2  1 NA NA   NA   NA   NA #  [3,]   -2    0    0  -1   0   0  3  1  0 NA    2   NA   NA #  [4,]   -2    0    0  -1   0   0  4  1  0  0    2   NA   NA #  [5,]   -2    0   NA  -1   0   0  5  1  0  0    2    0   NA #  [6,]   -2    0   NA  -1   0   0  6  1  0  0    2    0   NA #  [7,]   -2   NA   NA  -1   0   0  7  1  0  0    2    0    0 #  [8,]   -2   NA   NA  -1   0  NA  8  1  0  0    2    0    0 #  [9,]   NA   NA   NA  -1  NA  NA  9  1  0  0    2    0    0 # [10,]   NA   NA   NA  NA  NA  NA 10  1  0  0    2    0    0 g = rep(1:2, each = 5) t = rep(1:5, 2)  D(y, -2:2, 1:2, g, t) #       F2D1 F2D2 FD1 FD2 -- D1 D2 L2D1 L2D2 #  [1,]   -2    0  -1   0  1 NA NA   NA   NA #  [2,]   -2   NA  -1   0  2  1 NA   NA   NA #  [3,]   -2   NA  -1   0  3  1  0    2   NA #  [4,]   NA   NA  -1  NA  4  1  0    2   NA #  [5,]   NA   NA  NA  NA  5  1  0    2    0 #  [6,]   -2    0  -1   0  6 NA NA   NA   NA #  [7,]   -2   NA  -1   0  7  1 NA   NA   NA #  [8,]   -2   NA  -1   0  8  1  0    2   NA #  [9,]   NA   NA  -1  NA  9  1  0    2   NA # [10,]   NA   NA  NA  NA 10  1  0    2    0 L(D(y, 0:2, 1:2, g, t), 0:1, g, t) #       -- L1.-- D1 L1.D1 D2 L1.D2 L2D1 L1.L2D1 L2D2 L1.L2D2 #  [1,]  1    NA NA    NA NA    NA   NA      NA   NA      NA #  [2,]  2     1  1    NA NA    NA   NA      NA   NA      NA #  [3,]  3     2  1     1  0    NA    2      NA   NA      NA #  [4,]  4     3  1     1  0     0    2       2   NA      NA #  [5,]  5     4  1     1  0     0    2       2    0      NA #  [6,]  6    NA NA    NA NA    NA   NA      NA   NA      NA #  [7,]  7     6  1    NA NA    NA   NA      NA   NA      NA #  [8,]  8     7  1     1  0    NA    2      NA   NA      NA #  [9,]  9     8  1     1  0     0    2       2   NA      NA # [10,] 10     9  1     1  0     0    2       2    0      NA G(GGDC10S, 1L, 1L, ~ Variable + Country, ~ Year, cols = 6:10) %>% head #   Variable Country Year    G1.AGR    G1.MIN   G1.MAN    G1.PU   G1.CON # 1       VA     BWA 1960        NA        NA       NA       NA       NA # 2       VA     BWA 1961        NA        NA       NA       NA       NA # 3       VA     BWA 1962        NA        NA       NA       NA       NA # 4       VA     BWA 1963        NA        NA       NA       NA       NA # 5       VA     BWA 1964        NA        NA       NA       NA       NA # 6       VA     BWA 1965 -3.524492 -28.57143 38.23529 29.41176 103.9604 summary(lm(G(PCGDP,10,1,iso3c,year) ~              L(PCGDP,10,iso3c,year) +              G(LIFEEX,10,1,iso3c,year), data = wlddev)) #  # Call: # lm(formula = G(PCGDP, 10, 1, iso3c, year) ~ L(PCGDP, 10, iso3c,  #     year) + G(LIFEEX, 10, 1, iso3c, year), data = wlddev) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -104.32  -21.97   -3.96   13.26 1714.58  #  # Coefficients: #                                 Estimate Std. Error t value Pr(>|t|)     # (Intercept)                    2.740e+01  1.089e+00  25.168  < 2e-16 *** # L(PCGDP, 10, iso3c, year)     -3.337e-04  4.756e-05  -7.016 2.49e-12 *** # G(LIFEEX, 10, 1, iso3c, year)  4.617e-01  1.124e-01   4.107 4.05e-05 *** # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 58.43 on 7113 degrees of freedom #   (6060 observations deleted due to missingness) # Multiple R-squared:  0.01132, Adjusted R-squared:  0.01104  # F-statistic: 40.73 on 2 and 7113 DF,  p-value: < 2.2e-16 moddat <- HDW(L(G(wlddev, c(0, 10), 1, ~iso3c, ~year, 9:10), c(0, 10), ~iso3c, ~year), ~iso3c + qF(year))[-c(1,5)] summary(lm(HDW.L10G1.PCGDP ~. , moddat)) #  # Call: # lm(formula = HDW.L10G1.PCGDP ~ ., data = moddat) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -807.68  -10.80   -0.64   10.23  779.99  #  # Coefficients: #                        Estimate Std. Error t value Pr(>|t|)     # (Intercept)           1.907e-15  4.930e-01   0.000 1.000000     # HDW.L10.PCGDP        -2.500e-03  1.292e-04 -19.347  < 2e-16 *** # HDW.L10.L10G1.PCGDP  -5.885e-01  1.082e-02 -54.412  < 2e-16 *** # HDW.L10.LIFEEX        1.056e+00  2.885e-01   3.661 0.000254 *** # HDW.L10G1.LIFEEX      6.927e-01  1.154e-01   6.002 2.08e-09 *** # HDW.L10.L10G1.LIFEEX  8.749e-01  1.108e-01   7.899 3.39e-15 *** # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 35.69 on 5235 degrees of freedom # Multiple R-squared:  0.4029,  Adjusted R-squared:  0.4023  # F-statistic: 706.4 on 5 and 5235 DF,  p-value: < 2.2e-16 pwlddev <- plm::pdata.frame(wlddev, index = c(\"iso3c\", \"year\")) moddat <- HDW(L(G(pwlddev, c(0, 10), 1, 9:10), c(0, 10)))[-c(1,5)] summary(lm(HDW.L10G1.PCGDP ~. , moddat)) #  # Call: # lm(formula = HDW.L10G1.PCGDP ~ ., data = moddat) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -677.61  -12.45   -1.02   10.86  913.22  #  # Coefficients: #                        Estimate Std. Error t value Pr(>|t|)     # (Intercept)           0.1456192  0.5187976   0.281 0.778962     # HDW.L10.PCGDP        -0.0022910  0.0001253 -18.291  < 2e-16 *** # HDW.L10.L10G1.PCGDP  -0.5859896  0.0113538 -51.612  < 2e-16 *** # HDW.L10.LIFEEX        0.8701877  0.2456255   3.543 0.000399 *** # HDW.L10G1.LIFEEX      0.6910533  0.1132028   6.105 1.11e-09 *** # HDW.L10.L10G1.LIFEEX  0.8990853  0.1068241   8.417  < 2e-16 *** # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 37.51 on 5235 degrees of freedom #   (7935 observations deleted due to missingness) # Multiple R-squared:  0.3784,  Adjusted R-squared:  0.3778  # F-statistic: 637.4 on 5 and 5235 DF,  p-value: < 2.2e-16"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"list-processing-and-a-panel-var-example","dir":"Articles","previous_headings":"","what":"8. List Processing and a Panel-VAR Example","title":"Introduction to collapse","text":"collapse also provides ensemble list-processing functions grew necessity working complex nested lists data objects. example provided section also somewhat complex, demonstrates utility functions also providing nice data-transformation task. summarizing GGDC10S data section 1, evident certain sectors high share economic activity almost countries sample. prompts question whether exist common patterns interaction important sectors across countries. One way empirically study (Structural) Panel-Vector-Autoregression (PSVAR) value added 6 important sectors (excluding government): Agriculture, manufacturing, wholesale retail trade, construction, transport storage finance real estate. use vars package6. Since vars natively support panel-VAR, need create central varest object manually run SVAR function impose identification restrictions. start exploring harmonizing data: plot chunk AGRmat plot shows quite heterogeneity levels (VA local currency) trend growth rates. panel-VAR estimation really interested sectoral relationships within countries. Thus need harmonize sectoral data . One way taking growth rates log-differences data, VAR’s usually estimated levels unless data cointegrated (value added series , general, exhibit unit-root behavior). Thus harmonize data opt subtracting country-sector specific cubic trend data logs: plot chunk AGRmatplot seems done decent job curbing heterogeneity. series however high variance around cubic trend. Therefore final step standardize data bring variances line: plot chunk AGRmatplot2 Now looks pretty good, can terms harmonization without differencing data. code applies transformations sectors: plot chunk psmatplot2 Since data annual, let us estimate Panel-VAR one lag: prepared data, code estimates panel-VAR using lm creates varest object: significant serial-correlation test suggests panel-VAR one lag ill-identified, sample size also quite large test prone reject, test likely also still picking remaining cross-sectional heterogeneity. purposes vignette shall bother us. default VAR identified using Choleski ordering direct impact matrix first variable (Agriculture) assumed directly impacted sector current period, descends last variable (Finance Real Estate), assumed impacted sectors current period. structural identification usually necessary impose restrictions direct impact matrix line economic theory. difficult conceive theories average worldwide interaction broad economic sectors, aid identification compute correlation matrix growth rates restrict lowest coefficients 0, better just imposing random Choleski ordering. Now object quite involved, brings us actual subject section:","code":"library(vars) # The 6 most important non-government sectors (see section 1) sec <- c(\"AGR\", \"MAN\", \"WRT\", \"CON\", \"TRA\", \"FIRE\") # This creates a data.frame containing the value added of the 6 most important non-government sectors data <- fsubset(GGDC10S, Variable == \"VA\", c(\"Country\", \"Year\", sec)) %>%   na_omit(cols = sec) # Let's look at the log VA in agriculture across countries: AGRmat <- psmat(data, AGR ~ Country, ~ Year, transpose = TRUE) %>% log   # Converting to panel series matrix plot(AGRmat) # Subtracting a country specific cubic growth trend AGRmat <- dapply(AGRmat, fhdwithin, poly(seq_row(AGRmat), 3), fill = TRUE)  plot(AGRmat) # Standardizing the cubic log-detrended data AGRmat <- fscale(AGRmat) plot(AGRmat) # Taking logs settransformv(data, 3:8, log) # Projecting out country FE and cubic trends from complete cases gv(data, 3:8) <- HDW(data, ~ qF(Country)*poly(Year, 3), fill = TRUE) # Scaling gv(data, 3:8) <- STD(data, ~ Country, cols = 3:8, keep.by = FALSE)  # Check the plot psmat(data, ~ Country, ~ Year) %>% plot # This adds one lag of all series to the data add_vars(data) <- L(data, 1, ~ Country, ~ Year, keep.ids = FALSE) # This removes missing values from all but the first row and drops identifier columns (vars is made for time series without gaps) data <- rbind(ss(data, 1, -(1:2)), na_omit(ss(data, -1, -(1:2)))) head(data) #   STD.HDW.AGR STD.HDW.MAN STD.HDW.WRT STD.HDW.CON STD.HDW.TRA STD.HDW.FIRE L1.STD.HDW.AGR # 1  0.65713943   2.2350584    1.946383 -0.03574399   1.0877811    1.0476507             NA # 2 -0.14377115   1.8693570    1.905081  1.23225734   1.0542315    0.9105622     0.65713943 # 3 -0.09209878  -0.8212004    1.997253 -0.01783824   0.6718465    0.6134260    -0.14377115 # 4 -0.25213869  -1.7830320   -1.970855 -2.68332505  -1.8475551    0.4382902    -0.09209878 # 5 -0.31623401  -4.2931567   -1.822211 -2.75551916  -0.7066491   -2.1982640    -0.25213869 # 6 -0.72691916  -1.3219387   -2.079333 -0.12148295  -1.1398220   -2.2230474    -0.31623401 #   L1.STD.HDW.MAN L1.STD.HDW.WRT L1.STD.HDW.CON L1.STD.HDW.TRA L1.STD.HDW.FIRE # 1             NA             NA             NA             NA              NA # 2      2.2350584       1.946383    -0.03574399      1.0877811       1.0476507 # 3      1.8693570       1.905081     1.23225734      1.0542315       0.9105622 # 4     -0.8212004       1.997253    -0.01783824      0.6718465       0.6134260 # 5     -1.7830320      -1.970855    -2.68332505     -1.8475551       0.4382902 # 6     -4.2931567      -1.822211    -2.75551916     -0.7066491      -2.1982640 # saving the names of the 6 sectors nam <- names(data)[1:6]  pVAR <- list(varresult = setNames(lapply(seq_len(6), function(i)    # list of 6 lm's each regressing                lm(as.formula(paste0(nam[i], \"~ -1 + . \")),          # the sector on all lags of                get_vars(data, c(i, 7:fncol(data))))), nam),         # itself and other sectors, removing the missing first row              datamat = ss(data, -1),                                # The full data containing levels and lags of the sectors, removing the missing first row              y = do.call(cbind, get_vars(data, 1:6)),               # Only the levels data as matrix              type = \"none\",                                         # No constant or tend term: We harmonized the data already              p = 1,                                                 # The lag-order              K = 6,                                                 # The number of variables              obs = fnrow(data)-1,                                   # The number of non-missing obs              totobs = fnrow(data),                                  # The total number of obs              restrictions = NULL,              call = quote(VAR(y = data)))  class(pVAR) <- \"varest\" serial.test(pVAR) #  #   Portmanteau Test (asymptotic) #  # data:  Residuals of VAR object pVAR # Chi-squared = 1680.8, df = 540, p-value < 2.2e-16 # This computes the pairwise correlations between standardized sectoral growth rates across countries corr <- fsubset(GGDC10S, Variable == \"VA\") %>%   # Subset rows: Only VA            fgroup_by(Country) %>%                # Group by country                 get_vars(sec) %>%                # Select the 6 sectors                    fgrowth %>%                   # Compute Sectoral growth rates (a time-variable can be passed, but not necessary here as the data is ordered)                       fscale %>%                 # Scale and center (i.e. standardize)                          pwcor                   # Compute Pairwise correlations  corr #        AGR   MAN   WRT   CON   TRA  FIRE # AGR     1    .55   .59   .39   .52   .41 # MAN    .55    1    .67   .54   .65   .48 # WRT    .59   .67    1    .56   .66   .52 # CON    .39   .54   .56    1    .53   .46 # TRA    .52   .65   .66   .53    1    .51 # FIRE   .41   .48   .52   .46   .51    1  # We need to impose K*(K-1)/2 = 15 (with K = 6 variables) restrictions for identification corr[corr <= sort(corr)[15]] <- 0 corr #        AGR   MAN   WRT   CON   TRA  FIRE # AGR     1    .55   .59   .00   .00   .00 # MAN    .55    1    .67   .54   .65   .00 # WRT    .59   .67    1    .56   .66   .00 # CON    .00   .54   .56    1    .00   .00 # TRA    .00   .65   .66   .00    1    .00 # FIRE   .00   .00   .00   .00   .00    1  # The rest is unknown (i.e. will be estimated) corr[corr > 0 & corr < 1] <- NA  # Using a diagonal shock vcov matrix (standard assumption for SVAR) Bmat <- diag(6) diag(Bmat) <- NA   # This estimates the Panel-SVAR using Maximum Likelihood: pSVAR <- SVAR(pVAR, Amat = unclass(corr), Bmat = Bmat, estmethod = \"direct\") pSVAR #  # SVAR Estimation Results: # ========================  #  #  # Estimated A matrix: #              STD.HDW.AGR STD.HDW.MAN STD.HDW.WRT STD.HDW.CON STD.HDW.TRA STD.HDW.FIRE # STD.HDW.AGR       1.0000    -0.59223     0.51301      0.0000     0.00000            0 # STD.HDW.MAN      -0.2547     1.00000    -0.07819     -0.1711     0.14207            0 # STD.HDW.WRT      -0.3924    -0.56875     1.00000     -0.0135    -0.01391            0 # STD.HDW.CON       0.0000     0.02595    -0.18541      1.0000     0.00000            0 # STD.HDW.TRA       0.0000    -0.03321    -0.05370      0.0000     1.00000            0 # STD.HDW.FIRE      0.0000     0.00000     0.00000      0.0000     0.00000            1 #  # Estimated B matrix: #              STD.HDW.AGR STD.HDW.MAN STD.HDW.WRT STD.HDW.CON STD.HDW.TRA STD.HDW.FIRE # STD.HDW.AGR        0.678      0.0000      0.0000      0.0000      0.0000       0.0000 # STD.HDW.MAN        0.000      0.6248      0.0000      0.0000      0.0000       0.0000 # STD.HDW.WRT        0.000      0.0000      0.4155      0.0000      0.0000       0.0000 # STD.HDW.CON        0.000      0.0000      0.0000      0.5028      0.0000       0.0000 # STD.HDW.TRA        0.000      0.0000      0.0000      0.0000      0.5593       0.0000 # STD.HDW.FIRE       0.000      0.0000      0.0000      0.0000      0.0000       0.6475 # psVAR$var$varresult is a list containing the 6 linear models fitted above, it is not displayed in full here. str(pSVAR, give.attr = FALSE, max.level = 3) # List of 13 #  $ A      : num [1:6, 1:6] 1 -0.255 -0.392 0 0 ... #  $ Ase    : num [1:6, 1:6] 0 0 0 0 0 0 0 0 0 0 ... #  $ B      : num [1:6, 1:6] 0.678 0 0 0 0 ... #  $ Bse    : num [1:6, 1:6] 0 0 0 0 0 0 0 0 0 0 ... #  $ LRIM   : NULL #  $ Sigma.U: num [1:6, 1:6] 43.898 24.88 23.941 4.873 0.661 ... #  $ LR     :List of 5 #   ..$ statistic: Named num 1130 #   ..$ parameter: Named num 1 #   ..$ p.value  : Named num 0 #   ..$ method   : chr \"LR overidentification\" #   ..$ data.name: symbol data #  $ opt    :List of 5 #   ..$ par        : num [1:20] -0.2547 -0.3924 -0.5922 -0.5688 0.0259 ... #   ..$ value      : num 10924 #   ..$ counts     : Named int [1:2] 501 NA #   ..$ convergence: int 1 #   ..$ message    : NULL #  $ start  : num [1:20] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ... #  $ type   : chr \"AB-model\" #  $ var    :List of 10 #   ..$ varresult   :List of 6 #   .. ..$ STD.HDW.AGR :List of 13 #   .. ..$ STD.HDW.MAN :List of 13 #   .. ..$ STD.HDW.WRT :List of 13 #   .. ..$ STD.HDW.CON :List of 13 #   .. ..$ STD.HDW.TRA :List of 13 #   .. ..$ STD.HDW.FIRE:List of 13 #   ..$ datamat     :'data.frame':  2060 obs. of  12 variables: #   .. ..$ STD.HDW.AGR    : num [1:2060] -0.1438 -0.0921 -0.2521 -0.3162 -0.7269 ... #   .. ..$ STD.HDW.MAN    : num [1:2060] 1.869 -0.821 -1.783 -4.293 -1.322 ... #   .. ..$ STD.HDW.WRT    : num [1:2060] 1.91 2 -1.97 -1.82 -2.08 ... #   .. ..$ STD.HDW.CON    : num [1:2060] 1.2323 -0.0178 -2.6833 -2.7555 -0.1215 ... #   .. ..$ STD.HDW.TRA    : num [1:2060] 1.054 0.672 -1.848 -0.707 -1.14 ... #   .. ..$ STD.HDW.FIRE   : num [1:2060] 0.911 0.613 0.438 -2.198 -2.223 ... #   .. ..$ L1.STD.HDW.AGR : num [1:2060] 0.6571 -0.1438 -0.0921 -0.2521 -0.3162 ... #   .. ..$ L1.STD.HDW.MAN : num [1:2060] 2.235 1.869 -0.821 -1.783 -4.293 ... #   .. ..$ L1.STD.HDW.WRT : num [1:2060] 1.95 1.91 2 -1.97 -1.82 ... #   .. ..$ L1.STD.HDW.CON : num [1:2060] -0.0357 1.2323 -0.0178 -2.6833 -2.7555 ... #   .. ..$ L1.STD.HDW.TRA : num [1:2060] 1.088 1.054 0.672 -1.848 -0.707 ... #   .. ..$ L1.STD.HDW.FIRE: num [1:2060] 1.048 0.911 0.613 0.438 -2.198 ... #   ..$ y           : num [1:2061, 1:6] 0.6571 -0.1438 -0.0921 -0.2521 -0.3162 ... #   ..$ type        : chr \"none\" #   ..$ p           : num 1 #   ..$ K           : num 6 #   ..$ obs         : num 2060 #   ..$ totobs      : int 2061 #   ..$ restrictions: NULL #   ..$ call        : language VAR(y = data) #  $ iter   : Named int 501 #  $ call   : language SVAR(x = pVAR, estmethod = \"direct\", Amat = unclass(corr), Bmat = Bmat)"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"list-search-and-identification","dir":"Articles","previous_headings":"8. List Processing and a Panel-VAR Example","what":"8.1 List Search and Identification","title":"Introduction to collapse","text":"dealing list-like object, might interested complexity measuring level nesting. can done ldepth: might interested knowing whether list-object contains non-atomic elements like call, terms formulas. function .regular collapse package checks object atomic list-like, recursive version is_unlistable checks whether objects nested structure atomic list-like: Evidently object unlistable, viewing structure know contains several call terms objects. might also want know object saves kind residuals fitted values. can done using has_elem, also supports regular expression search element names: might also want know whether object contains kind data-matrix. can checked calling: functions can sometimes helpful exploring objects. much greater advantage functions search check lists ability write complex programs (demonstrated ).","code":"# The list-tree of this object has 5 levels of nesting ldepth(pSVAR) # [1] 5  # This data has a depth of 1, thus this dataset does not contain list-columns ldepth(data) # [1] 1 # Is this object composed only of atomic elements e.g. can it be unlisted? is_unlistable(pSVAR) # [1] FALSE # Does this object contain an element with \"fitted\" in its name? has_elem(pSVAR, \"fitted\", regex = TRUE) # [1] TRUE  # Does this object contain an element with \"residuals\" in its name? has_elem(pSVAR, \"residuals\", regex = TRUE) # [1] TRUE # Is there a matrix stored in this object? has_elem(pSVAR, is.matrix) # [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"list-subsetting","dir":"Articles","previous_headings":"8. List Processing and a Panel-VAR Example","what":"8.2 List Subsetting","title":"Introduction to collapse","text":"gathered information pSVAR object, section introduces several extractor functions pull elements lists: get_elem can used pull elements lists simplified format7. plot chunk PVARplot Similarly, pull plot fitted values: plot chunk PVARfittedplot main quantities interest SVAR analysis computed: impulse response functions (IRF’s) forecast error variance decompositions (FEVD’s): pIRF object contains IRF’s lower upper confidence bounds atomic elements providing information object: separately access top-level atomic list elements using atomic_elem list_elem: also recursive versions atomic_elem list_elem named reg_elem irreg_elem can used split nested lists atomic non-atomic parts. covered vignette.","code":"# This is the path to the residuals from a single equation str(pSVAR$var$varresult$STD.HDW.AGR$residuals) #  Named num [1:2060] -0.7234 -0.1962 -0.1993 0.0739 -0.1418 ... #  - attr(*, \"names\")= chr [1:2060] \"2\" \"3\" \"4\" \"5\" ...  # get_elem gets the residuals from all 6 equations and puts them in a top-level list resid <- get_elem(pSVAR, \"residuals\") str(resid, give.attr = FALSE) # List of 6 #  $ STD.HDW.AGR : Named num [1:2060] -0.7234 -0.1962 -0.1993 0.0739 -0.1418 ... #  $ STD.HDW.MAN : Named num [1:2060] 0.363 -1.989 -1.167 -3.082 1.474 ... #  $ STD.HDW.WRT : Named num [1:2060] 0.37 0.628 -3.054 -0.406 -0.384 ... #  $ STD.HDW.CON : Named num [1:2060] 1.035 -1.093 -2.62 -0.611 2.307 ... #  $ STD.HDW.TRA : Named num [1:2060] 0.1481 -0.2599 -2.2361 0.8619 -0.0915 ... #  $ STD.HDW.FIRE: Named num [1:2060] -0.11396 -0.33092 0.11754 -2.10521 -0.00968 ...  # Quick conversion to matrix and plotting qM(resid) %>% plot.ts(main = \"Panel-VAR Residuals\") # Regular expression search and retrieval of fitted values get_elem(pSVAR, \"^fi\", regex = TRUE) %>% qM %>%   plot.ts(main = \"Panel-VAR Fitted Values\") # This computes orthogonalized impulse response functions pIRF <- irf(pSVAR) # This computes the forecast error variance decompositions pFEVD <- fevd(pSVAR) # See the structure of a vars IRF object: str(pIRF, give.attr = FALSE) # List of 11 #  $ irf       :List of 6 #   ..$ STD.HDW.AGR : num [1:11, 1:6] 0.611 0.399 0.268 0.185 0.132 ... #   ..$ STD.HDW.MAN : num [1:11, 1:6] 0.1774 0.1549 0.134 0.1142 0.0959 ... #   ..$ STD.HDW.WRT : num [1:11, 1:6] -0.1807 -0.1071 -0.0647 -0.0402 -0.0259 ... #   ..$ STD.HDW.CON : num [1:11, 1:6] 0.0215 0.0383 0.0442 0.0438 0.0403 ... #   ..$ STD.HDW.TRA : num [1:11, 1:6] -0.02595 -0.01257 -0.00721 -0.00511 -0.00421 ... #   ..$ STD.HDW.FIRE: num [1:11, 1:6] 0 0.0122 0.0147 0.0132 0.0104 ... #  $ Lower     :List of 6 #   ..$ STD.HDW.AGR : num [1:11, 1:6] 0.1137 -0.0144 -0.0393 -0.0446 -0.0439 ... #   ..$ STD.HDW.MAN : num [1:11, 1:6] -0.6474 -0.3434 -0.2069 -0.125 -0.0734 ... #   ..$ STD.HDW.WRT : num [1:11, 1:6] -0.659 -0.427 -0.311 -0.236 -0.189 ... #   ..$ STD.HDW.CON : num [1:11, 1:6] -0.721 -0.417 -0.258 -0.183 -0.123 ... #   ..$ STD.HDW.TRA : num [1:11, 1:6] -0.4161 -0.2568 -0.169 -0.1231 -0.0894 ... #   ..$ STD.HDW.FIRE: num [1:11, 1:6] 0 -0.0157 -0.022 -0.0227 -0.0211 ... #  $ Upper     :List of 6 #   ..$ STD.HDW.AGR : num [1:11, 1:6] 1.218 0.801 0.565 0.389 0.275 ... #   ..$ STD.HDW.MAN : num [1:11, 1:6] 0.906 0.601 0.439 0.328 0.239 ... #   ..$ STD.HDW.WRT : num [1:11, 1:6] 0.846 0.601 0.428 0.319 0.239 ... #   ..$ STD.HDW.CON : num [1:11, 1:6] 0.716 0.514 0.4 0.305 0.234 ... #   ..$ STD.HDW.TRA : num [1:11, 1:6] 0.2866 0.21 0.1591 0.1207 0.0899 ... #   ..$ STD.HDW.FIRE: num [1:11, 1:6] 0 0.0363 0.0471 0.0461 0.0405 ... #  $ response  : chr [1:6] \"STD.HDW.AGR\" \"STD.HDW.MAN\" \"STD.HDW.WRT\" \"STD.HDW.CON\" ... #  $ impulse   : chr [1:6] \"STD.HDW.AGR\" \"STD.HDW.MAN\" \"STD.HDW.WRT\" \"STD.HDW.CON\" ... #  $ ortho     : logi TRUE #  $ cumulative: logi FALSE #  $ runs      : num 100 #  $ ci        : num 0.05 #  $ boot      : logi TRUE #  $ model     : chr \"svarest\" # Pool-out top-level atomic elements in the list str(atomic_elem(pIRF)) # List of 8 #  $ response  : chr [1:6] \"STD.HDW.AGR\" \"STD.HDW.MAN\" \"STD.HDW.WRT\" \"STD.HDW.CON\" ... #  $ impulse   : chr [1:6] \"STD.HDW.AGR\" \"STD.HDW.MAN\" \"STD.HDW.WRT\" \"STD.HDW.CON\" ... #  $ ortho     : logi TRUE #  $ cumulative: logi FALSE #  $ runs      : num 100 #  $ ci        : num 0.05 #  $ boot      : logi TRUE #  $ model     : chr \"svarest\""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"recursive-apply-and-unlisting-in-2d","dir":"Articles","previous_headings":"8. List Processing and a Panel-VAR Example","what":"8.3 Recursive Apply and Unlisting in 2D","title":"Introduction to collapse","text":"vars supplies simple plot methods IRF FEVD objects using base graphics.       section however want generate nicer compact plots using ggplot2, also compute statistics IRF data. Starting latter, code sums 10-period impulse response coefficients sector response sectoral impulse stores data frame: function rapply2d used similar base::rapply, difference result simplified / unlisted default rapply2d treat data frames like atomic objects apply functions . unlist2d efficient generalization base::unlist 2-dimensions, one also think recursive generalization .call(rbind, ...). efficiently unlists nested lists data objects creates data frame identifier columns level nesting left, content list columns right. cumulative coefficients suggest Agriculture responds mostly ’s shock, bit shocks Manufacturing Wholesale Retail Trade. Similar patters can observed Manufacturing Wholesale Retail Trade. Thus three sectors seem interlinked countries. remaining three sectors mostly affected dynamics, also Agriculture Manufacturing. Let us use ggplot2 create nice compact plots IRF’s FEVD’s. task unlist2d extremely helpful creating data frame representation required. Starting IRF’s, discard upper lower bounds just use impulses: plot chunk IRFplot round things , thing FEVD’s: plot chunk FEVDplot IRF’s FEVD’s show Agriculture, Manufacturing Wholesale Retail Trade broadly interlinked, even short-run, Agriculture Manufacturing explain variation Construction, Transport Finance longer horizons. course identification strategy used example really structural theory based. better strategy aggregate World Input-Output Database use shares identification (another nice collapse exercise, vignette).","code":"# Computing the cumulative impact after 10 periods list_elem(pIRF) %>%                            # Pull out the sublist elements containing the IRF coefficients + CI's   rapply2d(function(x) round(fsum(x), 2)) %>%  # Recursively apply the column-sums to coefficient matrices (could also use colSums)   unlist2d(c(\"Type\", \"Impulse\"))               # Recursively row-bind the result to a data.frame and add identifier columns #     Type      Impulse STD.HDW.AGR STD.HDW.MAN STD.HDW.WRT STD.HDW.CON STD.HDW.TRA STD.HDW.FIRE # 1    irf  STD.HDW.AGR        1.92        1.08        1.68        0.83        0.72         0.54 # 2    irf  STD.HDW.MAN        0.98        2.22        2.12        1.09        0.97         1.05 # 3    irf  STD.HDW.WRT       -0.47       -0.27        0.65        0.17        0.03        -0.02 # 4    irf  STD.HDW.CON        0.33        0.39        0.34        2.00        0.55         0.38 # 5    irf  STD.HDW.TRA       -0.07       -0.11       -0.24       -0.30        1.31        -0.20 # 6    irf STD.HDW.FIRE        0.07       -0.07        0.02       -0.09       -0.06         1.84 # 7  Lower  STD.HDW.AGR       -0.18       -2.08       -3.14       -0.68       -2.46        -0.68 # 8  Lower  STD.HDW.MAN       -1.52        0.38       -1.30       -0.86       -1.82         0.12 # 9  Lower  STD.HDW.WRT       -2.38       -2.65       -0.22       -2.68       -2.01        -1.20 # 10 Lower  STD.HDW.CON       -2.01       -2.47       -2.16        0.53       -1.68        -0.80 # 11 Lower  STD.HDW.TRA       -1.32       -1.34       -1.17       -1.64        0.31        -0.69 # 12 Lower STD.HDW.FIRE       -0.16       -0.26       -0.16       -0.27       -0.20         0.96 # 13 Upper  STD.HDW.AGR        3.97        3.18        3.21        3.69        2.61         1.58 # 14 Upper  STD.HDW.MAN        3.19        3.85        3.00        3.60        3.05         1.78 # 15 Upper  STD.HDW.WRT        3.06        2.66        4.41        2.49        3.31         1.47 # 16 Upper  STD.HDW.CON        2.85        3.30        3.20        3.88        2.59         1.76 # 17 Upper  STD.HDW.TRA        1.08        1.93        1.76        0.72        2.82         0.63 # 18 Upper STD.HDW.FIRE        0.30        0.15        0.30        0.12        0.18         2.21 # This binds the matrices after adding integer row-names to them to a data.table  data <- pIRF$irf %>%                      # Get only the coefficient matrices, discard the confidence bounds            unlist2d(idcols = \"Impulse\",   # Recursive unlisting to data.table creating a factor id-column                     row.names = \"Time\",   # and saving generated rownames in a variable called 'Time'                     id.factor = TRUE,     # -> Create Id column ('Impulse') as factor                     DT = TRUE)            # -> Output as data.table (default is data.frame)  head(data, 3) #        Impulse  Time STD.HDW.AGR STD.HDW.MAN STD.HDW.WRT STD.HDW.CON STD.HDW.TRA STD.HDW.FIRE #         <fctr> <int>       <num>       <num>       <num>       <num>       <num>        <num> # 1: STD.HDW.AGR     1   0.6113132   0.1896711   0.3488940  0.05976606  0.02503336   0.00000000 # 2: STD.HDW.AGR     2   0.3986337   0.1892803   0.3014961  0.09430567  0.07263670   0.03669857 # 3: STD.HDW.AGR     3   0.2676944   0.1654161   0.2491999  0.10769335  0.09330830   0.06042380  data <- melt(data, 1:2)                   # Using data.table's melt head(data, 3) #        Impulse  Time    variable     value #         <fctr> <int>      <fctr>     <num> # 1: STD.HDW.AGR     1 STD.HDW.AGR 0.6113132 # 2: STD.HDW.AGR     2 STD.HDW.AGR 0.3986337 # 3: STD.HDW.AGR     3 STD.HDW.AGR 0.2676944  # Here comes the plot:   ggplot(data, aes(x = Time, y = value, color = Impulse)) +     geom_line(size = I(1)) + geom_hline(yintercept = 0) +     labs(y = NULL, title = \"Orthogonal Impulse Response Functions\") +     scale_color_manual(values = rainbow(6)) +     facet_wrap(~ variable) +     theme_light(base_size = 14) +     scale_x_continuous(breaks = scales::pretty_breaks(n=7), expand = c(0, 0))+     scale_y_continuous(breaks = scales::pretty_breaks(n=7), expand = c(0, 0))+     theme(axis.text = element_text(colour = \"black\"),       plot.title = element_text(hjust = 0.5),       strip.background = element_rect(fill = \"white\", colour = NA),       strip.text = element_text(face = \"bold\", colour = \"grey30\"),       axis.ticks = element_line(colour = \"black\"),       panel.border = element_rect(colour = \"black\")) data <- unlist2d(pFEVD, idcols = \"variable\", row.names = \"Time\", id.factor = TRUE, DT = TRUE) %>%             melt(c(\"variable\", \"Time\"), variable.name = \"Sector\") head(data, 3) #       variable  Time      Sector     value #         <fctr> <int>      <fctr>     <num> # 1: STD.HDW.AGR     1 STD.HDW.AGR 0.8513029 # 2: STD.HDW.AGR     2 STD.HDW.AGR 0.8385913 # 3: STD.HDW.AGR     3 STD.HDW.AGR 0.8264789  # Here comes the plot:   ggplot(data, aes(x = Time, y = value, fill = Sector)) +     geom_area(position = \"fill\", alpha = 0.8) +     labs(y = NULL, title = \"Forecast Error Variance Decompositions\") +     scale_fill_manual(values = rainbow(6)) +     facet_wrap(~ set_class(variable, \"factor\")) +     theme_linedraw(base_size = 14) +     scale_x_continuous(breaks = scales::pretty_breaks(n=7), expand = c(0, 0))+     scale_y_continuous(breaks = scales::pretty_breaks(n=7), expand = c(0, 0))+     theme(plot.title = element_text(hjust = 0.5),       strip.background = element_rect(fill = \"white\", colour = NA),       strip.text = element_text(face = \"bold\", colour = \"grey30\"))"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"going-further","dir":"Articles","previous_headings":"","what":"Going Further","title":"Introduction to collapse","text":"learn collapse, just examine documentation help(\"collapse-documentation\") organized, extensive contains lots examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_intro.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Introduction to collapse","text":"Timmer, M. P., de Vries, G. J., & de Vries, K. (2015). “Patterns Structural Change Developing Countries.” . J. Weiss, & M. Tribe (Eds.), Routledge Handbook Industry Development. (pp. 65-83). Routledge. Mundlak, Yair. 1978. “Pooling Time Series Cross Section Data.” Econometrica 46 (1): 69–85.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"collapse's Handling of R Objects","text":"collapse provides class-agnostic architecture permitting computations broad range R objects. provides explicit support base R classes data types (logical, integer, double, character, list, data.frame, matrix, factor, Date, POSIXct, ts) popular extensions, including integer64, data.table, tibble, grouped_df, xts/zoo, pseries, pdata.frame, units, sf (geometric operations). also introduces GRP_df performant class-agnostic grouped data frame, indexed_series indexed_frame classes modern class-agnostic successors pseries, pdata.frame. objects inherit classes succeed handled .pseries, .pdata.frame, .grouped_df methods, also support original (plm / dplyr) implementations (details ). objects handled internally C R level using general principles extended specific considerations classes. start summarizing general principles, enable usage collapse classes explicitly support.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"general-principles","dir":"Articles","previous_headings":"","what":"General Principles","title":"collapse's Handling of R Objects","text":"general, collapse preserves attributes classes R objects statistical data manipulation operations unless preservation involves high-risk yielding something wrong/useless. Risky operations change dimensions internal data type (typeof()) R object. collapse’s R C code, exist 3 principal types R objects: atomic vectors, matrices, lists - often assumed data frames. data manipulation functions collapse, like fmutate(), support lists, whereas statistical functions - like S3 generic Fast Statistical Functions like fmean() - generally support 3 types objects. S3 generic functions initially dispatch .default, .matrix, .data.frame, (hidden) .list methods.  .list method generally dispatches .data.frame method. basic methods, non-generic functions collapse, decide exactly handle object based statistical operation performed attribute handling principles mostly implemented C. simplest case arises operation preserves dimensions object, fscale(x) fmutate(data, across(:c, log)). case, attributes x / data fully preserved1. Another simple case matrices lists arises statistical operation reduces single dimension fmean(x), , drop = TRUE default Fast Statistical Functions, attributes apart (column-)names dropped (named) vector means returned. atomic vectors, statistical operation like fmean(x) preserve attributes (except ts objects), object useful properties labels units. complex cases involve changing dimensions object. number rows preserved e.g. fmutate(data, a_b = / b) flag(x, -1:1), (column-)names attribute object modified. number rows reduced e.g. fmean(x, g), attributes also retained suitable modifications (row-)names attribute. However, x matrix, attributes row- column-names retained !.object(x), , matrix ‘class’ attribute. atomic vectors, attributes retained !inherits(x, \"ts\"), aggregating time series break class. also applies columns data frame aggregated. data transformed using statistics provided TRA() function e.g. TRA(x, STATS, operation, groups) like-named argument Fast Statistical Functions, operations simply modify input (x) statistical sense (\"replace_na\", \"-\", \"-+\", \"/\", \"+\", \"*\", \"%%\", \"-%%\") just copy attributes transformed object. Operations \"fill\" \"replace\" tricky, since x replaced STATS, different class data type. following rules apply: (1) result data type STATS; (2) .object(STATS), attributes STATS preserved; (3) otherwise attributes x preserved unless .object(x) && typeof(x) != typeof(STATS); (4) exemption rule made x factor integer replacement offered STATS e.g. fnobs(factor, group, TRA = \"fill\"). case, attributes x copied except ‘class’ ‘levels’ attributes. rules devised considering possibility x may important information attached preserved data transformations, \"label\" attribute. Another rather complex case arises manipulating data collapse using base R functions, e.g. (mtcars$mpg, mtcars$cyl, mad) mtcars |> fgroup_by(cyl, vs, ) |> fsummarise(mad_mpg = mad(mpg)). case, collapse internally uses base R functions lapply unlist(), following efficient splitting gsplit() (preserves attributes). Concretely, result computed y = unlist(lapply(gsplit(x, g), FUN, ...), FALSE, FALSE), examples x mtcars$mpg, g grouping variable(s), FUN = mad, y mad(x) group. follow policy attribute preservation closely possible, collapse calls internal function y_final = copyMostAttributes(y, x), copies attributes x y deemed compatible2 (\\approx data type). deemed incompatible, copyMostAttributes still checks x \"label\" attribute copies one y. summarize general principles: collapse just tries preserve attributes cases except likely break something, beholding way commonly used R classes objects behave. likely operations break something aggregating matrices class (mts/xts) univariate time series (ts), data replaced another object, applying unknown function vector groups assembling result unlist(). latter cases, particular attention paid integer vectors factors, often count something generating integers, malformed factors need avoided. following section provides details collapse functions supported classes.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"object-conversions","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"Object Conversions","title":"collapse's Handling of R Objects","text":"Quick conversion functions qDF, qDT, qTBL() qM (create data.frame’s, data.table’s, tibble’s matrices arbitrary R objects) default (keep.attr = FALSE) perform strict conversions, attributes non-essential class dropped input object. ensure , following conversion, objects behave exactly way users expect. different behavior functions like .data.frame(), .data.table(), as_tibble() .matrix() e.g. .matrix(EuStockMarkets) just returns EuStockMarkets whereas qM(EuStockMarkets) returns plain matrix without time series attributes. behavior can changed setting keep.attr = TRUE, .e. qM(EuStockMarkets, keep.attr = TRUE).","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"selecting-columns-by-data-type","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"Selecting Columns by Data Type","title":"collapse's Handling of R Objects","text":"Functions num_vars(), cat_vars() (opposite num_vars()), char_vars() etc. implemented C avoid need check data frame columns applying R function .numeric(). .numeric, C implementation equivalent is_numeric_C <- function(x) typeof(x) %% c(\"integer\", \"double\") && !inherits(x, c(\"factor\", \"Date\", \"POSIXct\", \"yearmon\", \"yearqtr\")). course respect behavior classes define methods .numeric e.g. .numeric.foo <- function(x) FALSE, y = structure(rnorm(100), class = \"foo\"), .numeric(y) FALSE num_vars(data.frame(y)) still returns . Correct behavior case requires get_vars(data.frame(y), .numeric). particular case aware using collap() FUN catFUN arguments, C code (is_numeric_C) used internally decide whether column numeric categorical. collapse support statistical operations complex data.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"parsing-of-time-ids","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"Parsing of Time-IDs","title":"collapse's Handling of R Objects","text":"Time Series Functions flag, fdiff, fgrowth psacf/pspacf/psccf (operators L/F/D/Dlog/G) t argument pass time-ids fully identified temporal operations time series panel data. t plain numeric vector factor, coerced integer using .integer(), integer steps used time steps. premised observation common form temporal identifier numeric variable denoting calendar years. hand t numeric time object .object(t) && .numeric(unclass(t)) (e.g. Date, POSIXct, etc.), passed timeid() computes greatest common divisor vector generates integer time-id way. Users therefore advised use appropriate classes represent time steps e.g. monthly data zoo::yearmon appropriate. also possible pass non-numeric t, character list/data.frame. cases ordered grouping applied generate integer time-id, rather avoided.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"xtszoo-time-series","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"xts/zoo Time Series","title":"collapse's Handling of R Objects","text":"xts/zoo time series handled .zoo methods relevant functions. methods simple follow pattern: FUN.zoo <- function(x, ...) (.matrix(x)) FUN.matrix(x, ...) else FUN.default(x, ....). Thus general principles apply. Time-Series function automatically use index indexed computations, partly consistency native methods also case (e.g. lag.xts perform indexed lag), partly , outlined , index necessarily accurately reflect time structure. Thus user must exercise discretion perform indexed lag xts/zoo. example: flag(xts_daily, 1:3, t = index(xts_daily)) flag(xts_monthly, 1:3, t = zoo::.yearmon(index(xts_monthly))).","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"support-for-sf-and-units","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"Support for sf and units","title":"collapse's Handling of R Objects","text":"collapse internally supports sf seeking avoid undue destruction removal ‘geometry’ column data manipulation operations. simply implemented additional check C programs used subset columns data: object sf data frame, ‘geometry’ column added column selection. functions like funique() roworder() internal facilities avoid sorting grouping ‘geometry’ column. functions like descr() qsu() simply omit geometry column statistical calculations. short vignette describes integration collapse sf bit detail. summary: collapse supports sf seeking appropriately deal ‘geometry’ column. perform geometrical operations. example, subsetting fsubset(), bounding box attribute geometry unaltered likely large. Regarding units objects, relevant functions also simple methods form FUN.units <- function(x, ...) copyMostAttrib((.matrix(x)) FUN.matrix(x, ...), x) else FUN.default(x, ....). According general principles, default method preserves units class, whereas matrix method FUN aggregates data. use copyMostAttrib(), copies attributes apart \"dim\", \"dimnames\", \"names\", ensures returned objects still units.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"support-for-data-table","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"Support for data.table","title":"collapse's Handling of R Objects","text":"collapse provides quite thorough support data.table. simplest level support avoids assigning descriptive (character) row names data.table’s e.g. fmean(mtcars, mtcars$cyl) row-names corresponding groups fmean(qDT(mtcars), mtcars$cyl) . collapse supports data.table’s reference semantics (set*, :=). able add columns reference (e.g. DT[, new := 1]), data.table’s implemented overallocated lists3. collapse copied C code data.table overallocation generate \".internal.selfref\" attribute, qDT() creates valid fully functional data.table. enable seamless data manipulation combining collapse data.table, data manipulation functions collapse call C code end return valid (overallocated) data.table. However, overallocation comes computational cost 2-3 microseconds, opted also adding .data.frame methods statistical functions. Concretely, means res <- DT |> fgroup_by(id) |> fsummarise(mu_a = fmean()) gives fully functional data.table .e. res[, new := 1] works, res2 <- DT |> fgroup_by(id) |> fmean() gives non-overallocated data.table res2[, new := 1] still work issue warning. case, res2 <- DT |> fgroup_by(id) |> fmean() |> qDT() can used avoid warning. , , seems reasonable trade-flexibility performance. details examples provided collapse data.table vignette.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"class-agnostic-grouped-and-indexed-data-frames","dir":"Articles","previous_headings":"Specific Functions and Classes","what":"Class-Agnostic Grouped and Indexed Data Frames","title":"collapse's Handling of R Objects","text":"indicated introductory remarks, collapse provides fast class-agnostic grouped data frame created fgroup_by(), fast class-agnostic indexed time series panel data, created findex_by()/reindex(). Class-agnostic means object grouped/indexed continues behave except collapse operations utilizing ‘groups’/‘index_df’ attributes. grouped data frame implemented follows: fgroup_by() saves class input data, calls GRP() columns grouped, attaches resulting ‘GRP’ object \"groups\" attribute. assigns class attribute follows words: class \"GRP_df\" added front, followed classes original object4, followed \"grouped_df\" finally \"data.frame\", present. \"GRP_df\" class dealing appropriately object methods print() subsetting ([, [[), e.g. print.GRP_df fetches grouping object, prints fungroup(.X)5, prints summary grouping. [.GRP_df works similarly: saves groups, calls [ fungroup(.X), attaches groups result list number rows. collapse issues printing handling grouped data.table’s, tibbles, sf data frames, etc. - continue behave usual. Now collapse various functions .grouped_df method deal grouped data frames. example fmean.grouped_df, nutshell, fetches attached ‘GRP’ object using GRP.grouped_df, calls fmean.data.frame fungroup(data), passing ‘GRP’ object g argument grouped computation. general principles outlined apply resulting object attributes input. architecture additional advantage: allows GRP.grouped_df examine grouping object check created collapse (class ‘GRP’) dplyr. latter case, efficient C routine called convert dplyr grouping object ‘GRP’ object .grouped_df methods collapse apply data frames created either dplyr::group_by() fgroup_by(). indexed_frame works similarly. inherits pdata.frame .pdata.frame methods collapse deal indexed_frame’s arbitrary classes pdata.frame’s created plm. notable difference grouped_df pdata.frame indexed_frame deeply indexed data structure: variable inside indexed_frame indexed_series contains index_df attribute external pointer index_df attribute frame. Functions pseries methods operating indexed_series stored inside frame ((data, flag(column))) can fetch index pointer. allows worry-free application inside arbitrary data masking environments (, %$%, attach, etc..) estimation commands (glm, feols, lmrob etc..) without duplication index memory. may guessed, indexed_series also class-agnostic inherit pseries. vector matrix class can become indexed_series. levels generality indexed series frames allow one, two variables index support time series complex panels, natively deal irregularity time6, provide rich set methods subsetting manipulation also subset index_df attribute, including internal methods fsubset(), funique(), roworder(v) na_omit(). indexed_frame indexed_series rich general structure permitting fully time-aware computations nearly R object. See ?indexing information.","code":"clx <- class(.X) # .X is the data frame being grouped, clx is its class m <- match(c(\"GRP_df\", \"grouped_df\", \"data.frame\"), clx, nomatch = 0L) class(.X) <- c(\"GRP_df\",  if(length(mp <- m[m != 0L])) clx[-mp] else clx, \"grouped_df\", if(m[3L]) \"data.frame\")"},{"path":"https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"collapse's Handling of R Objects","text":"collapse handles R objects preserving fairly intelligent manner, allowing seamless compatibility many common data classes R, statistical workflows preserve attributes (labels, units, etc.) data. implemented general principles specific considerations/exemptions mostly implemented C - detailed vignette. main benefits design generality execution speed: collapse much fewer R-level method dispatches function calls frameworks used perform statistical data manipulation operations, behaves predictably, may also work well simple new class. main disadvantage general principles exemptions hard-coded C thus may work specific classes. prominent example collapse simply fails lubridate’s interval class (#186, #418), \"starts\" attribute length data preserved subset collapse operations.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/developing_with_collapse.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Developing with collapse","text":"collapse offers integrated suite C/C++-based statistical data manipulation functions, many low-level tools memory efficient programming, class-agnostic architecture seamlessly supports vectors, matrices, data frame-like objects. features make ideal backend high-performance statistical packages. vignette meant provide recommendations developing collapse. complementary earlier blog post programming collapse readers also encouraged consult. vignette adds 3 important points writing efficient R/collapse code.","code":""},{"path":"https://sebkrantz.github.io/collapse/articles/developing_with_collapse.html","id":"point-1-be-minimalistic-in-computations","dir":"Articles","previous_headings":"","what":"Point 1: Be Minimalistic in Computations","title":"Developing with collapse","text":"collapse supports different types R objects (vectors, matrices, data frames + variants) can perform grouped operations using different types grouping information (plain vectors, ‘qG’1 objects, factors, ‘GRP’ objects, grouped indexed data frames). Grouping can sorted unsorted. key efficient code use minimal required operations/objects get job done. Suppose want sum object x groups using grouping vector g. grouping needed , done using internal grouping fsum() without creating external grouping objects - fsum(x, g) aggregation fsum(x, g, TRA = \"fill\") expansion: expansion case efficient internally uses unsorted grouping. Apart default sorted aggregation, functions efficiently convert input g minimally required information. aggregation case, can improve performance also using unsorted grouping, e.g., fsum(x, qF(g, sort = FALSE)) fsum(x, qG(g, sort = FALSE), use.g.names = FALSE) group-names needed. advisable also set argument na.exclude = FALSE qF()/qG() add class ‘na.included’ precludes internal missing value checks fsum() friends. g plain vector first-appearance order groups kept even g factor, use group(g) instead qG(g, sort = FALSE, na.exclude = FALSE).2 Set use.g.names = FALSE needed (can abbreviate use = FALSE), , data missing values, set na.rm = FALSE maximum performance. Factors ‘qG’ objects efficient inputs statistical/transformation functions except fmedian(), fnth(), fmode(), fndistinct(), split-apply-combine operations using ()/gsplit(). repeated grouped operations involving , makes sense create ‘GRP’ objects using GRP(). objects expensive create provide complete information.3 sorting needed, set sort = FALSE, aggregation unique groups/names needed set return.groups = FALSE. always use group() unsorted grouping simple functions? can , qF()/qG() bit smarter comes handling input factors/‘qG’ objects whereas group() hashes every vector: rare cases grouped/indexed data frames created fgroup_by()/findex_by() needed package code. Likewise, functions like fsummarise()/fmutate() essentially wrappers. example (use = FALSE abbreviates use.g.names = FALSE) clear: nothing prevents using wrappers - quite efficient - want change inputs programmatically makes sense go one level - code also become safer.4 general, think carefully vectorize minimalistic memory efficient way. find can craft parsimonious efficient code solve complicated problems. example, merging multiple spatial datasets, map features (businesses) multiple sources, , unwilling match features individually across data sources, decided keep richest source covering feature type location. creating feature importance indicator comparable across sources, deduplication expression ended single line form: fsubset(data, source == fmode(source, list(location, type), importance, \"fill\")) - keep features importance-weighted frequent source location type. effective collapse solution apparent, packages may offer efficient solutions. Check fastverse suggested packages list. example want efficiently replace multiple items vector, kit::vswitch()/nswitch() can pretty magical. Also functions like data.table::set()/rowid() etc. great, e.g., recent issue: collapse equivalent grouped dplyr::slice_head(n)? fsubset(data, data.table::rowid(id1, id2, ...) <= n).","code":"fmean(mtcars$mpg, mtcars$cyl) #        4        6        8  # 26.66364 19.74286 15.10000 fmean(mtcars$mpg, mtcars$cyl, TRA = \"fill\") #  [1] 19.74286 19.74286 26.66364 19.74286 15.10000 19.74286 15.10000 26.66364 26.66364 19.74286 # [11] 19.74286 15.10000 15.10000 15.10000 15.10000 15.10000 15.10000 26.66364 26.66364 26.66364 # [21] 26.66364 15.10000 15.10000 15.10000 15.10000 26.66364 26.66364 26.66364 15.10000 19.74286 # [31] 15.10000 26.66364 x <- rnorm(1e7) # 10 million random obs g <- sample.int(1e6, 1e7, TRUE) # 1 Million random groups oldopts <- set_collapse(na.rm = FALSE) # No missing values: maximum performance microbenchmark::microbenchmark(   internal = fsum(x, g),   internal_expand = fsum(x, g, TRA = \"fill\"),   qF1 = fsum(x, qF(g, sort = FALSE)),   qF2 = fsum(x, qF(g, sort = FALSE, na.exclude = FALSE)),   qG1 = fsum(x, qG(g, sort = FALSE), use = FALSE),   qG2 = fsum(x, qG(g, sort = FALSE, na.exclude = FALSE), use = FALSE),   group = fsum(x, group(g), use = FALSE), # Same as above basically   GRP1 = fsum(x, GRP(g)),    GRP2 = fsum(x, GRP(g, sort = FALSE)),    GRP3 = fsum(x, GRP(g, sort = FALSE, return.groups = FALSE), use = FALSE) ) # Unit: milliseconds #             expr       min        lq      mean    median        uq      max neval #         internal 119.62078 124.61575 133.51499 129.24721 136.84295 187.9376   100 #  internal_expand  87.45751  93.53473 101.63398  97.34573 105.04102 195.5121   100 #              qF1  98.40816 101.62102 110.80120 105.03839 112.72224 265.5931   100 #              qF2  86.75518  89.82823 100.47122  93.89814 103.04776 194.9115   100 #              qG1  88.38563  92.44846 103.28242  97.29579 105.35159 202.8058   100 #              qG2  72.94851  76.86912  87.05558  79.43137  86.15307 262.4734   100 #            group  74.08335  77.19435  87.62058  82.58726  90.61506 162.0318   100 #             GRP1 145.13799 149.54178 163.89938 154.71379 164.11361 297.5056   100 #             GRP2  95.83557  99.05297 109.58577 103.34950 112.50322 266.9996   100 #             GRP3  82.56629  86.15699  97.54058  90.40781  98.05956 328.7744   100 f <- qF(g); f2 <- qF(g, na.exclude = FALSE) gg <- group(g) # Same as qG(g, sort = FALSE, na.exclude = FALSE) grp <- GRP(g) # Simple functions: factors are efficient inputs microbenchmark::microbenchmark(   factor = fsum(x, f),   factor_nona = fsum(x, f2),   qG_nona = fsum(x, gg),   qG_nona_nonam = fsum(x, gg, use = FALSE),   GRP = fsum(x, grp),   GRP_nonam = fsum(x, grp, use = FALSE) ) # Unit: milliseconds #           expr      min       lq     mean   median       uq      max neval #         factor 16.02514 16.49498 17.50705 17.11619 18.16497 21.72975   100 #    factor_nona 12.72911 13.15124 14.41943 13.87850 15.03540 23.27144   100 #        qG_nona 14.30178 14.95450 20.48179 15.67930 17.34989 57.15597   100 #  qG_nona_nonam 11.57118 12.00423 13.12157 12.49071 13.61801 23.31219   100 #            GRP 12.83345 13.08907 14.45512 13.95154 15.21594 21.46473   100 #      GRP_nonam 12.67589 13.22139 14.15271 13.76600 14.84057 20.36359   100  # Complex functions: more information helps microbenchmark::microbenchmark(   qG = fmedian(x, gg, use = FALSE),   GRP = fmedian(x, grp, use = FALSE), times = 10) # Unit: milliseconds #  expr      min       lq     mean   median       uq      max neval #    qG 258.4450 261.9357 267.2520 264.2608 267.4161 297.1552    10 #   GRP 191.8623 193.0631 196.0935 193.4358 194.6245 210.3685    10 set_collapse(oldopts) microbenchmark::microbenchmark(   factor_factor = qF(f),   # This checks NA's and adds 'na.included' class -> full deep copy   factor_factor2 = qF(f, na.exclude = FALSE),    # NA checking costs.. incurred in fsum() and friends   check_na = collapse:::is.nmfactor(f),    check_na2 = collapse:::is.nmfactor(f2),   factor_qG = qF(gg),   qG_factor = qG(f),   qG_qG = qG(gg),   group_factor = group(f),   group_qG = group(gg) ) # Unit: nanoseconds #            expr      min         lq        mean     median         uq      max neval #   factor_factor     1107     2562.5     6925.31     7298.0     9676.0    19270   100 #  factor_factor2  5926960  6147663.0  6898849.83  6235136.5  6421686.5 15325349   100 #        check_na  3440474  3503880.5  3525056.59  3513597.5  3524770.0  3927185   100 #       check_na2      287     1496.5     3325.10     3341.5     4243.5     9922   100 #       factor_qG     2583    11644.0    15105.63    15887.5    18614.0    31898   100 #       qG_factor     1927     4284.5    10171.28     9614.5    13796.5    50799   100 #           qG_qG     1476     2583.0     6674.39     6498.5     8897.0    23124   100 #    group_factor 16066629 16300165.0 17378151.76 16489011.0 16858872.0 54181582   100 #        group_qG 13824175 14194917.5 15083957.81 14347396.5 14700345.0 22289117   100 mtcars |>   fgroup_by(cyl, vs, am) |>   fsummarise(mpg = fsum(mpg),              across(c(carb, hp, qsec), fmean)) #   cyl vs am   mpg     carb        hp     qsec # 1   4  0  1  26.0 2.000000  91.00000 16.70000 # 2   4  1  0  68.7 1.666667  84.66667 20.97000 # 3   4  1  1 198.6 1.428571  80.57143 18.70000 # 4   6  0  1  61.7 4.666667 131.66667 16.32667 # 5   6  1  0  76.5 2.500000 115.25000 19.21500 # 6   8  0  0 180.6 3.083333 194.16667 17.14250 # 7   8  0  1  30.8 6.000000 299.50000 14.55000 g <- GRP(mtcars, c(\"cyl\", \"vs\", \"am\"))  add_vars(g$groups,   get_vars(mtcars, \"mpg\") |> fsum(g, use = FALSE),   get_vars(mtcars, c(\"carb\", \"hp\", \"qsec\")) |> fmean(g, use = FALSE) ) #   cyl vs am   mpg     carb        hp     qsec # 1   4  0  1  26.0 2.000000  91.00000 16.70000 # 2   4  1  0  68.7 1.666667  84.66667 20.97000 # 3   4  1  1 198.6 1.428571  80.57143 18.70000 # 4   6  0  1  61.7 4.666667 131.66667 16.32667 # 5   6  1  0  76.5 2.500000 115.25000 19.21500 # 6   8  0  0 180.6 3.083333 194.16667 17.14250 # 7   8  0  1  30.8 6.000000 299.50000 14.55000"},{"path":"https://sebkrantz.github.io/collapse/articles/developing_with_collapse.html","id":"point-2-think-about-memory-and-optimize","dir":"Articles","previous_headings":"","what":"Point 2: Think About Memory and Optimize","title":"Developing with collapse","text":"R programs inefficient 2 principal reasons: (1) operations vectorized; (2) many intermediate objects/copies created. collapse’s vectorized statistical functions help (1), also provides many efficient programming functions deal (2). One source inefficiency R code widespread use logical vectors. example x == 0 creates logical vector 1 million elements just indicate R elements x 0. collapse, setv(x, 0, NA) efficient equivalent. also works don’t want replace NA another vector y: much better setv() quite versatile also works indices logical vectors instead elements search . can also invert query setting invert = TRUE. complex workflows, may wish save logical vector, e.g., xmiss <- .na(x), use repeatedly. One aspect note logical vectors inefficient subsetting compared indices: Thus, indices always preferable. collapse, can created directly using whichNA(xNA) case, whichv(x, 0) (x == 0) number. Also exist invert = TRUE argument covering != case. convenience, infix operators x %==% 0 x %!=% 0 wrap whichv(x, 0) whichv(x, 0, invert = TRUE), respectively. Similarly, fmatch() supports faster matching associated operators %iin% %!iin% also return indices, e.g., letters %iin% c(\"\", \"b\") returns 1:2. can also used subsetting: Likewise, anyNA(), allNA(), anyv() allv() help avoid expressions like (x == 0) favor anyv(x, 0). convenience functions exist na_rm(x) common x[!.na(x)] expression extremely inefficient. Another hint particularly data frame subsetting ss() function, argument check = FALSE avoid checks indices (small effect data size): Another common source inefficiencies copies produced statistical operations. example particular case res <- kit::psum(x, y, z) offers efficient solution5. general solution collapse’s %+=%, %-=%, %*=% %/=% operators wrappers around setop() function also works matrices data frames.6 function also rowwise argument operations vectors matrix/data.frame rows: functions like na_locf()/na_focb() also set = TRUE arguments perform operations reference.7 also setTRA() (grouped) transformations reference, wrapping TRA(..., set = TRUE). Since TRA added argument Fast Statistical Functions, set = TRUE can passed modify reference. example: setTRA(iris$Sepal.Length, fmedian(iris$Sepal.Length, iris$Species), \"fill\", iris$Species), replacing values Sepal.Length vector species median reference: set argument can invoked anywhere, also inside fmutate() calls /without groups. can also done combination transformations (sweeping operations). example, following turns columns matrix proportions. summary, think really needed complete task keep things minimum terms computations memory. Let’s final exercise regard create hyper-efficient function univariate linear regression groups: expression computed greg() amounts sum(y * (x - mean(x)))/sum((x - mean(x))^2) group, equivalent cov(x, y)/var(x), efficient, requiring exactly one full copy x create group-demeaned vector, dmx, using w (weights) argument fsum() sum products (y * dmx dmx * dmx) fly, including division reference avoiding additional copy. One much better coding grouped regression directly C.","code":"x <- abs(round(rnorm(1e6))) x[x == 0] <- NA y <- rnorm(1e6) setv(x, NA, y) # Replaces missing x with y x[is.na(x)] <- y[is.na(x)] xNA <- na_insert(x, prop = 0.4) xmiss <- is.na(xNA) ind <- which(xmiss) bench::mark(x[xmiss], x[ind]) # # A tibble: 2 × 6 #   expression      min   median `itr/sec` mem_alloc `gc/sec` #   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> # 1 x[xmiss]     3.34ms   3.58ms      269.    8.39MB     4.21 # 2 x[ind]     771.74µs 972.11µs     1025.    3.05MB     6.61 bench::mark(   `%in%` = fsubset(wlddev, iso3c %in% c(\"USA\", \"DEU\", \"ITA\", \"GBR\")),   `%iin%` = fsubset(wlddev, iso3c %iin% c(\"USA\", \"DEU\", \"ITA\", \"GBR\")) ) # # A tibble: 2 × 6 #   expression      min   median `itr/sec` mem_alloc `gc/sec` #   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> # 1 %in%        146.8µs  165.7µs     6008.     3.8MB     2.12 # 2 %iin%        17.3µs   23.6µs    39878.   130.4KB    23.9 ind <- wlddev$iso3c %!iin% c(\"USA\", \"DEU\", \"ITA\", \"GBR\") microbenchmark::microbenchmark(   withcheck = ss(wlddev, ind),   nocheck = ss(wlddev, ind, check = FALSE) ) # Unit: microseconds #       expr    min       lq     mean   median       uq     max neval #  withcheck 48.749 106.6615 124.4366 122.1595 143.8895 256.619   100 #    nocheck 47.355 105.5750 126.9225 119.6380 150.8595 344.113   100 x <- rnorm(100); y <- rnorm(100); z <- rnorm(100) res <- x + y + z # Creates 2 copies res <- x + y res %+=% z m <- qM(mtcars) setop(m, \"*\", seq_col(m), rowwise = TRUE) head(m / qM(mtcars)) #                   mpg cyl disp hp drat wt qsec  vs  am gear carb # Mazda RX4           1   2    3  4    5  6    7 NaN   9   10   11 # Mazda RX4 Wag       1   2    3  4    5  6    7 NaN   9   10   11 # Datsun 710          1   2    3  4    5  6    7   8   9   10   11 # Hornet 4 Drive      1   2    3  4    5  6    7   8 NaN   10   11 # Hornet Sportabout   1   2    3  4    5  6    7 NaN NaN   10   11 # Valiant             1   2    3  4    5  6    7   8 NaN   10   11 fmedian(iris$Sepal.Length, iris$Species, TRA = \"fill\", set = TRUE) head(iris) #   Sepal.Length Sepal.Width Petal.Length Petal.Width Species # 1            5         3.5          1.4         0.2  setosa # 2            5         3.0          1.4         0.2  setosa # 3            5         3.2          1.3         0.2  setosa # 4            5         3.1          1.5         0.2  setosa # 5            5         3.6          1.4         0.2  setosa # 6            5         3.9          1.7         0.4  setosa fsum(m, TRA = \"/\", set = TRUE) fsum(m) # Check #  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb  #    1    1    1    1    1    1    1    1    1    1    1 greg <- function(y, x, g) {   g <- group(g)   dmx <- fmean(x, g, TRA = \"-\", na.rm = FALSE)   (fsum(y, g, dmx, use = FALSE, na.rm = FALSE) %/=%    fsum(dmx, g, dmx, use = FALSE, na.rm = FALSE)) }  # Test y <- rnorm(1e7) x <- rnorm(1e7) g <- sample.int(1e6, 1e7, TRUE)  microbenchmark::microbenchmark(greg(y, x, g), group(g)) # Unit: milliseconds #           expr       min        lq     mean    median        uq      max neval #  greg(y, x, g) 131.39639 138.68961 153.1586 145.78243 161.48137 305.5862   100 #       group(g)  62.41733  64.80468  72.2558  68.87266  73.21657 153.1643   100"},{"path":"https://sebkrantz.github.io/collapse/articles/developing_with_collapse.html","id":"point-3-internally-favor-primitive-r-objects-and-functions","dir":"Articles","previous_headings":"","what":"Point 3: Internally Favor Primitive R Objects and Functions","title":"Developing with collapse","text":"partly reiterates Point 1 now focus internal data representation rather grouping computing. point also bluntly stated : ‘vectors, matrices lists good, data frames complex objects bad’. Many frameworks seem imply opposite - tidyverse encourages cast data tidy tibble, data.table offers efficient data frame. objects internally complex, , case data.table, efficient internal C-level algorithms large-data manipulation. always take step back ask : statistical software writing, need complexity? Complex objects require complex methods manipulate , thus, using , incur cost everything goes methods. Vectors, matrices, lists much efficient R collapse provides many options manipulate directly. may surprise hear , internally, collapse use data frame-like objects . Instead, objects cast lists using unclass(data), class(data) <- NULL, attributes(data) <- NULL. advisable want write fast package code data frame-like objects. benchmark illustrates basically everything data.frame expensive equivalent list. means illustration, let’s recreate pwnobs() function collapse counts pairwise missing values. list method written R. basic implementation :8 Now tips can optimize follows: Evidently, optimized function 6x faster (small) dataset changed nothing loops computation. larger data difference less stark, never know ’s going methods written scale. advice : try avoid , use simple objects take full control code. also makes code robust can create class-agnostic code. latter intent vignette collapse’s object handling also helpful. use collapse functions discussion void - collapse functions designed data frames, including join(), pivot(), fsubset(), etc., internally handle data list equally efficient data frames lists. However, want use base R semantics ([, etc.) alongside collapse functions, makes sense unclass incoming data frame-like objects reclass end. don’t want internally convert data frames lists, least use functions .subset(), .subset2(), collapse::get_vars() efficiently extract columns attr() extract/set attributes. matrices, use dimnames() directly instead rownames() colnames() wrap . Also avoid .data.frame() friends coerce/recreate data frame-like objects. quite easy construct data.frame list: can also use collapse functions qDF(), qDT() qTBL() efficiently convert/create data.frame’s, data.table’s, tibble’s: collapse also provides functions like setattrib(), copyMostAttrib(), etc., efficiently attach attributes . another efficient workflow general data frame-like objects save attributes ax <- attributes(data), manipulate list attributes(data) <- NULL, modify ax$names ax$row.names needed use setattrib(data, ax) returning.","code":"l <- unclass(mtcars) nam <- names(mtcars) microbenchmark::microbenchmark(names(mtcars), attr(mtcars, \"names\"), names(l),                names(mtcars) <- nam, attr(mtcars, \"names\") <- nam, names(l) <- nam,                mtcars[[\"mpg\"]], .subset2(mtcars, \"mpg\"), l[[\"mpg\"]],                mtcars[3:8], .subset(mtcars, 3:8), l[3:8],                ncol(mtcars), length(mtcars), length(unclass(mtcars)), length(l),                nrow(mtcars), length(.subset2(mtcars, 1L)), length(l[[1L]])) # Unit: nanoseconds #                          expr  min   lq    mean median     uq   max neval #                 names(mtcars)  164  205  240.26    246  246.0   410   100 #         attr(mtcars, \"names\")   41   82  109.88     82  123.0  1476   100 #                      names(l)    0    0   24.60     41   41.0    82   100 #          names(mtcars) <- nam  451  492  651.90    656  697.0  3321   100 #  attr(mtcars, \"names\") <- nam  287  369  480.52    451  492.0  4346   100 #               names(l) <- nam  164  246  276.34    246  287.0   533   100 #               mtcars[[\"mpg\"]] 2009 2091 2363.65   2173 2296.0 15539   100 #       .subset2(mtcars, \"mpg\")   41   41   68.88     82   82.0   164   100 #                    l[[\"mpg\"]]   41   82   78.31     82   82.0   205   100 #                   mtcars[3:8] 5166 5371 5607.98   5453 5576.0 15908   100 #          .subset(mtcars, 3:8)  246  246  321.03    287  328.0  2788   100 #                        l[3:8]  246  287  305.45    287  328.0   492   100 #                  ncol(mtcars) 1025 1107 1200.07   1189 1230.0  2255   100 #                length(mtcars)  164  205  249.28    246  266.5   492   100 #       length(unclass(mtcars))  123  164  176.71    164  164.0   861   100 #                     length(l)    0    0   18.86      0   41.0   287   100 #                  nrow(mtcars) 1025 1107 1239.84   1148 1230.0  6642   100 #  length(.subset2(mtcars, 1L))   41   82  113.57     82  123.0  1845   100 #               length(l[[1L]])   41   82  100.45     82  123.0   492   100 pwnobs_list <- function(X) {     dg <- fnobs(X)     n <- ncol(X)     nr <- nrow(X)     N.mat <- diag(dg)     for (i in 1:(n - 1L)) {         miss <- is.na(X[[i]])         for (j in (i + 1L):n) N.mat[i, j] <- N.mat[j, i] <- nr - sum(miss | is.na(X[[j]]))     }     rownames(N.mat) <- names(dg)     colnames(N.mat) <- names(dg)     N.mat }  mtcNA <- na_insert(mtcars, prop = 0.2) pwnobs_list(mtcNA) #      mpg cyl disp hp drat wt qsec vs am gear carb # mpg   26  20   20 20   20 20   21 22 21   21   22 # cyl   20  26   21 20   22 21   22 22 22   23   20 # disp  20  21   26 22   22 23   22 22 21   21   22 # hp    20  20   22 26   21 23   22 20 20   21   21 # drat  20  22   22 21   26 23   21 21 20   21   21 # wt    20  21   23 23   23 26   22 21 21   20   20 # qsec  21  22   22 22   21 22   26 22 20   22   20 # vs    22  22   22 20   21 21   22 26 20   23   21 # am    21  22   21 20   20 21   20 20 26   20   21 # gear  21  23   21 21   21 20   22 23 20   26   20 # carb  22  20   22 21   21 20   20 21 21   20   26 pwnobs_list_opt <- function(X) {     dg <- fnobs.data.frame(X)     class(X) <- NULL     n <- length(X)     nr <- length(X[[1L]])     N.mat <- diag(dg)     for (i in 1:(n - 1L)) {         miss <- is.na(X[[i]])         for (j in (i + 1L):n) N.mat[i, j] <- N.mat[j, i] <- nr - sum(miss | is.na(X[[j]]))     }     dimnames(N.mat) <- list(names(dg), names(dg))     N.mat }  identical(pwnobs_list(mtcNA), pwnobs_list_opt(mtcNA)) # [1] TRUE  microbenchmark::microbenchmark(pwnobs_list(mtcNA), pwnobs_list_opt(mtcNA)) # Unit: microseconds #                    expr     min       lq      mean  median      uq     max neval #      pwnobs_list(mtcNA) 153.217 160.1255 185.09696 179.744 215.004 241.654   100 #  pwnobs_list_opt(mtcNA)  27.429  31.1600  33.38507  32.964  35.137  45.387   100 attr(l, \"row.names\") <- .set_row_names(length(l[[1L]])) class(l) <- \"data.frame\" head(l, 2) #   mpg cyl disp  hp drat    wt  qsec vs am gear carb # 1  21   6  160 110  3.9 2.620 16.46  0  1    4    4 # 2  21   6  160 110  3.9 2.875 17.02  0  1    4    4 library(data.table) library(tibble) microbenchmark::microbenchmark(qDT(mtcars), as.data.table(mtcars),                                qTBL(mtcars), as_tibble(mtcars)) # Unit: microseconds #                   expr    min     lq     mean  median      uq      max neval #            qDT(mtcars)  2.952  3.280  6.35705  3.5670  3.8130  269.534   100 #  as.data.table(mtcars) 34.194 36.572 44.93641 37.4535 39.2985  697.410   100 #           qTBL(mtcars)  2.419  2.583  3.19267  2.8700  2.9930   38.704   100 #      as_tibble(mtcars) 48.257 49.569 71.56304 50.4095 52.5005 2050.533   100  l <- unclass(mtcars) microbenchmark::microbenchmark(qDF(l), as.data.frame(l), as.data.table(l), as_tibble(l)) # Unit: microseconds #              expr     min       lq      mean   median      uq     max neval #            qDF(l)   1.722   2.2140   4.51779   2.4600   2.747 199.424   100 #  as.data.frame(l) 210.412 225.1515 242.65973 248.3370 254.569 301.186   100 #  as.data.table(l)  70.889  77.2030  90.30086  83.0045  88.683 798.393   100 #      as_tibble(l)  55.350  61.8690  68.20924  67.0760  72.898 139.769   100"},{"path":"https://sebkrantz.github.io/collapse/articles/developing_with_collapse.html","id":"some-notes-on-global-options","dir":"Articles","previous_headings":"","what":"Some Notes on Global Options","title":"Developing with collapse","text":"collapse set global options can set using set_collapse() retrieved using get_collapse().9 confers responsibilities upon package developers setting options inside package also affects collapse behaves outside package. general, rules apply setting R options options() par(): need reset using .exit() user choices unaffected even package function breaks. example, want block code multithreaded without missing value skipping maximum performance: Namespace masking (options mask remove) set inside packages may unintended side-effects user (e.g., collapse appears top search() path afterwards). Conversely, user choices set_collapse() also affect package code, except namespace masking specify explicitly collapse functions using (e.g., via importFrom(\"collapse\", \"fmean\") NAMESPACE collapse::fmean() code). Particularly options na.rm, nthreads, sort, set user, impact code, unless explicitly set targeted arguments (e.g., nthreads na.rm statistical functions like fmean(), sort arguments grouping functions like GRP()/qF()/qG()/fgroup_by()). general view necessary - user sets set_collapse(na.rm = FALSE) data missing values, good also speeds package functions. However, package code generates missing values expects collapse functions skip take care using either set_collapse() + .exit() explicitly setting na.rm = TRUE relevant functions. Also watch internally-grouped aggregations using Fast Statistical Functions, affected global defaults: Statistical functions sort arguments, thus, crucial output remains sorted, ensure sorted factor, ‘qG’, ‘GRP’ object passed: course, can also check options user set adjust code, e.g. ","code":"fast_function <- function(x, ...) {      # Your code...    oldopts <- set_collapse(nthreads = 4, na.rm = FALSE)   on.exit(set_collapse(oldopts))       # Multithreaded code... } fmean(mtcars$mpg, mtcars$cyl) #        4        6        8  # 26.66364 19.74286 15.10000 oldopts <- set_collapse(sort = FALSE) fmean(mtcars$mpg, mtcars$cyl) #        6        4        8  # 19.74286 26.66364 15.10000 fmean(mtcars$mpg, qF(mtcars$cyl, sort = TRUE)) #        4        6        8  # 26.66364 19.74286 15.10000 set_collapse(oldopts) # Your code ... if(!get_collapse(\"sort\")) {   oldopts <- set_collapse(sort = TRUE)   on.exit(set_collapse(oldopts))  } # Critical code ..."},{"path":"https://sebkrantz.github.io/collapse/articles/developing_with_collapse.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Developing with collapse","text":"collapse can become game-changer statistical software development R, enabling write programs effectively run like C accomplishing complex statistical/data tasks lines code. however requires taking closer look package, particular documentation, following advice given vignette.","code":""},{"path":"https://sebkrantz.github.io/collapse/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sebastian Krantz. Author, maintainer. Matt Dowle. Contributor. Arun Srinivasan. Contributor. Morgan Jacob. Contributor. Dirk Eddelbuettel. Contributor. Laurent Berge. Contributor. Kevin Tappe. Contributor. Alina Cherkas. Contributor. R Core Team contributors worldwide. Contributor. Martyn Plummer. Copyright holder. 1999-2016 R Core Team. Copyright holder.","code":""},{"path":"https://sebkrantz.github.io/collapse/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Krantz, S. (2024). collapse: Advanced Fast Statistical Computing Data Transformation R [Preprint]. arXiv. https://arxiv.org/abs/2403.05038 Krantz (2025). collapse: Advanced Fast Data Transformation R. R package version 2.1.1. doi:10.5281/zenodo.8433090. https://sebkrantz.github.io/collapse/.","code":"@Misc{krantz2024collapse,   title = {collapse: Advanced and Fast Statistical Computing and Data Transformation in R},   author = {Sebastian Krantz},   year = {2024},   eprint = {2403.05038},   archiveprefix = {arXiv},   primaryclass = {stat.CO},   url = {https://arxiv.org/abs/2403.05038}, } @Manual{rcollapse,   title = {collapse: Advanced and Fast Data Transformation in R},   author = {Sebastian Krantz},   year = {2025},   note = {R package version 2.1.1},   doi = {10.5281/zenodo.8433090},   url = {https://sebkrantz.github.io/collapse/}, }"},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"collapse-","dir":"","previous_headings":"","what":"Advanced and Fast Data Transformation in R","title":"Advanced and Fast Data Transformation in R","text":"collapse large C/C++-based package data transformation statistical computing R. aims : Facilitate complex data transformation, exploration computing tasks R. Help make R code fast, flexible, parsimonious programmer friendly. novel class-agnostic architecture supports basic R objects popular extensions, including units, integer64, xts/zoo, tibble, grouped_df, data.table, sf, pseries pdata.frame. Key Features: Advanced statistical programming: full set fast statistical functions supporting grouped weighted computations vectors, matrices data frames. Fast programmable grouping, ordering, matching, deduplication, factor generation interactions. Fast data manipulation: Fast flexible functions data manipulation, data object conversions memory efficient R programming. Advanced aggregation: Fast easy multi-type, weighted parallelized data aggregation. Advanced transformations: Fast row/column arithmetic, (grouped) sweeping statistics (reference), (grouped, weighted) scaling (higher-dimensional) centering averaging. Advanced time-computations: Fast flexible indexed time series panel data classes, lags/leads, differences (compound) growth rates (irregular) time series panels, panel-autocorrelation functions panel data array conversions. List processing: Recursive list search, filtering, splitting, apply unlisting data frame. Advanced data exploration: Fast (grouped, weighted, multi-level) descriptive statistical tools. collapse written C C++, algorithms much faster base R’s, extremely low evaluation overheads, scales well (benchmarks: linux | windows), excels complex statistical tasks.","code":""},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Advanced and Fast Data Transformation in R","text":"","code":"# Install the current version on CRAN install.packages(\"collapse\")  # Install a stable development version (Windows/Mac binaries) from R-universe install.packages(\"collapse\", repos = \"https://fastverse.r-universe.dev\")  # Install a stable development version from GitHub (requires compilation) remotes::install_github(\"SebKrantz/collapse\")  # Install previous versions from the CRAN Archive (requires compilation) install.packages(\"https://cran.r-project.org/src/contrib/Archive/collapse/collapse_2.0.19.tar.gz\",                   repos = NULL, type = \"source\")  # Older stable versions: 1.9.6, 1.8.9, 1.7.6, 1.6.5, 1.5.3, 1.4.2, 1.3.2, 1.2.1"},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Advanced and Fast Data Transformation in R","text":"collapse installs built-structured documentation, implemented via set separate help pages. Calling help('collapse-documentation') brings top-level documentation page, providing overview entire package links documentation pages. addition several vignettes, among one Documentation Resources.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"article-on-arxiv","dir":"","previous_headings":"Documentation","what":"Article on arXiv","title":"Advanced and Fast Data Transformation in R","text":"article collapse submitted Journal Statistical Software March 2024 updated/revised February 2025.","code":""},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"presentation-at-user-2022","dir":"","previous_headings":"Documentation","what":"Presentation at useR 2022","title":"Advanced and Fast Data Transformation in R","text":"Video Recording | Slides","code":""},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"example-usage","dir":"","previous_headings":"","what":"Example Usage","title":"Advanced and Fast Data Transformation in R","text":"Evaluated extensive sets examples provided package page (also accessible R calling example('collapse-package')), vignettes documentation.","code":"library(collapse) data(\"iris\")            # iris dataset in base R v <- iris$Sepal.Length  # Vector d <- num_vars(iris)     # Saving numeric variables (could also be a matrix, statistical functions are S3 generic) g <- iris$Species       # Grouping variable (could also be a list of variables)  ## Advanced Statistical Programming -----------------------------------------------------------------------------  # Simple (column-wise) statistics... fmedian(v)                       # Vector fsd(qM(d))                       # Matrix (qM is a faster as.matrix) fmode(d)                         # data.frame fmean(qM(d), drop = FALSE)       # Still a matrix fmax(d, drop = FALSE)            # Still a data.frame  # Fast grouped and/or weighted statistics w <- abs(rnorm(fnrow(iris))) fmedian(d, w = w)                 # Simple weighted statistics fnth(d, 0.75, g)                  # Grouped statistics (grouped third quartile) fmedian(d, g, w)                  # Groupwise-weighted statistics fsd(v, g, w)                      # Similarly for vectors fmode(qM(d), g, w, ties = \"max\")  # Or matrices (grouped and weighted maximum mode) ...  # A fast set of data manipulation functions allows complex piped programming at high speeds library(magrittr)                            # Pipe operators iris %>% fgroup_by(Species) %>% fndistinct   # Grouped distinct value counts iris %>% fgroup_by(Species) %>% fmedian(w)   # Weighted group medians  iris %>% add_vars(w) %>%                     # Adding weight vector to dataset   fsubset(Sepal.Length < fmean(Sepal.Length), Species, Sepal.Width:w) %>% # Fast selecting and subsetting   fgroup_by(Species) %>%                     # Grouping (efficiently creates a grouped tibble)   fvar(w) %>%                                # Frequency-weighted group-variance, default (keep.w = TRUE)     roworder(sum.w)                            # also saves group weights in a column called 'sum.w'  # Can also use dplyr (but dplyr manipulation verbs are a lot slower) library(dplyr) iris %>% add_vars(w) %>%    filter(Sepal.Length < fmean(Sepal.Length)) %>%    select(Species, Sepal.Width:w) %>%    group_by(Species) %>%    fvar(w) %>% arrange(sum.w)    ## Fast Data Manipulation ---------------------------------------------------------------------------------------  head(GGDC10S)  # Pivot Wider: Only SUM (total) SUM <- GGDC10S |> pivot(c(\"Country\", \"Year\"), \"SUM\", \"Variable\", how = \"wider\") head(SUM)  # Joining with data from wlddev wlddev |>     join(SUM, on = c(\"iso3c\" = \"Country\", \"year\" = \"Year\"), how = \"inner\")  # Recast pivoting + supplying new labels for generated columns pivot(GGDC10S, values = 6:16, names = list(\"Variable\", \"Sectorcode\"),       labels = list(to = \"Sector\",                     new = c(Sectorcode = \"GGDC10S Sector Code\",                             Sector = \"Long Sector Description\",                             VA = \"Value Added\",                             EMP = \"Employment\")),        how = \"recast\", na.rm = TRUE)  ## Advanced Aggregation -----------------------------------------------------------------------------------------  collap(iris, Sepal.Length + Sepal.Width ~ Species, fmean)  # Simple aggregation using the mean.. collap(iris, ~ Species, list(fmean, fmedian, fmode))       # Multiple functions applied to each column add_vars(iris) <- w                                        # Adding weights, return in long format.. collap(iris, ~ Species, list(fmean, fmedian, fmode), w = ~ w, return = \"long\")  # Generate some additional logical data settransform(iris, AWMSL = Sepal.Length > fmedian(Sepal.Length, w = w),                     AWMSW = Sepal.Width > fmedian(Sepal.Width, w = w))  # Multi-type data aggregation: catFUN applies to all categorical columns (here AMWSW) collap(iris, ~ Species + AWMSL, list(fmean, fmedian, fmode),         catFUN = fmode, w = ~ w, return = \"long\")  # Custom aggregation gives the greatest possible flexibility: directly mapping functions to columns collap(iris, ~ Species + AWMSL,         custom = list(fmean = 2:3, fsd = 3:4, fmode = \"AWMSL\"), w = ~ w,         wFUN = list(fsum, fmin, fmax), # Here also aggregating the weight vector with 3 different functions        keep.col.order = FALSE)        # Column order not maintained -> grouping and weight variables first  # Can also use grouped tibble: weighted median for numeric, weighted mode for categorical columns iris %>% fgroup_by(Species, AWMSL) %>% collapg(fmedian, fmode, w = w)  ## Advanced Transformations -------------------------------------------------------------------------------------  # All Fast Statistical Functions have a TRA argument, supporting 10 different replacing and sweeping operations fmode(d, TRA = \"replace\")     # Replacing values with the mode fsd(v, TRA = \"/\")             # dividing by the overall standard deviation (scaling) fsum(d, TRA = \"%\")            # Computing percentages fsd(d, g, TRA = \"/\")          # Grouped scaling fmin(d, g, TRA = \"-\")         # Setting the minimum value in each species to 0 ffirst(d, g, TRA = \"%%\")      # Taking modulus of first value in each species fmedian(d, g, w, \"-\")         # Groupwise centering by the weighted median fnth(d, 0.95, g, w, \"%\")      # Expressing data in percentages of the weighted species-wise 95th percentile fmode(d, g, w, \"replace\",     # Replacing data by the species-wise weighted minimum-mode       ties = \"min\")  # TRA() can also be called directly to replace or sweep with a matching set of computed statistics TRA(v, sd(v), \"/\")                       # Same as fsd(v, TRA = \"/\") TRA(d, fmedian(d, g, w), \"-\", g)         # Same as fmedian(d, g, w, \"-\") TRA(d, BY(d, g, quantile, 0.95), \"%\", g) # Same as fnth(d, 0.95, g, TRA = \"%\") (apart from quantile algorithm)  # For common uses, there are some faster and more advanced functions fbetween(d, g)                           # Grouped averaging [same as fmean(d, g, TRA = \"replace\") but faster] fwithin(d, g)                            # Grouped centering [same as fmean(d, g, TRA = \"-\") but faster] fwithin(d, g, w)                         # Grouped and weighted centering [same as fmean(d, g, w, \"-\")] fwithin(d, g, w, theta = 0.76)           # Quasi-centering i.e. d - theta*fbetween(d, g, w) fwithin(d, g, w, mean = \"overall.mean\")  # Preserving the overall weighted mean of the data  fscale(d)                                # Scaling and centering (default mean = 0, sd = 1) fscale(d, mean = 5, sd = 3)              # Custom scaling and centering fscale(d, mean = FALSE, sd = 3)          # Mean preserving scaling fscale(d, g, w)                          # Grouped and weighted scaling and centering fscale(d, g, w, mean = \"overall.mean\",   # Setting group means to overall weighted mean,        sd = \"within.sd\")                 # and group sd's to fsd(fwithin(d, g, w), w = w)  get_vars(iris, 1:2)                      # Use get_vars for fast selecting data.frame columns, gv is shortcut fhdbetween(gv(iris, 1:2), gv(iris, 3:5)) # Linear prediction with factors and continuous covariates fhdwithin(gv(iris, 1:2), gv(iris, 3:5))  # Linear partialling out factors and continuous covariates  # This again opens up new possibilities for data manipulation... iris %>%     ftransform(ASWMSL = Sepal.Length > fmedian(Sepal.Length, Species, w, \"replace\")) %>%   fgroup_by(ASWMSL) %>% collapg(w = w, keep.col.order = FALSE)  iris %>% fgroup_by(Species) %>% num_vars %>% fwithin(w)  # Weighted demeaning   ## Time Series and Panel Series ---------------------------------------------------------------------------------  flag(AirPassengers, -1:3)                      # A sequence of lags and leads EuStockMarkets %>%                             # A sequence of first and second seasonal differences   fdiff(0:1 * frequency(.), 1:2)   fdiff(EuStockMarkets, rho = 0.95)              # Quasi-difference [x - rho*flag(x)] fdiff(EuStockMarkets, log = TRUE)              # Log-difference [log(x/flag(x))] EuStockMarkets %>% fgrowth(c(1, frequency(.))) # Ordinary and seasonal growth rate EuStockMarkets %>% fgrowth(logdiff = TRUE)     # Log-difference growth rate [log(x/flag(x))*100]  # Creating panel data pdata <- EuStockMarkets %>% list(`A` = ., `B` = .) %>%           unlist2d(idcols = \"Id\", row.names = \"Time\")    L(pdata, -1:3, ~Id, ~Time)                   # Sequence of fully identified panel-lags (L is operator for flag)  pdata %>% fgroup_by(Id) %>% flag(-1:3, Time) # Same thing..  # collapse also supports indexed series and data frames (and plm panel data classes) pdata <- findex_by(pdata, Id, Time)          L(pdata, -1:3)          # Same as above, ... psacf(pdata)            # Multivariate panel-ACF psmat(pdata) %>% plot   # 3D-array of time series from panel data + plotting  HDW(pdata)              # This projects out id and time fixed effects.. (HDW is operator for fhdwithin) W(pdata, effect = \"Id\") # Only Id effects.. (W is operator for fwithin)  ## List Processing ----------------------------------------------------------------------------------------------  # Some nested list of heterogenous data objects.. l <- list(a = qM(mtcars[1:8]),                                   # Matrix           b = list(c = mtcars[4:11],                             # data.frame                    d = list(e = mtcars[2:10],                              f = fsd(mtcars))))                   # Vector  ldepth(l)                       # List has 4 levels of nesting (considering that mtcars is a data.frame) is_unlistable(l)                # Can be unlisted has_elem(l, \"f\")                # Contains an element by the name of \"f\" has_elem(l, is.matrix)          # Contains a matrix  get_elem(l, \"f\")                # Recursive extraction of elements.. get_elem(l, c(\"c\",\"f\"))          get_elem(l, c(\"c\",\"f\"), keep.tree = TRUE) unlist2d(l, row.names = TRUE)   # Intelligent recursive row-binding to data.frame    rapply2d(l, fmean) %>% unlist2d # Taking the mean of all elements and repeating  # Application: extracting and tidying results from (potentially nested) lists of model objects list(mod1 = lm(mpg ~ carb, mtcars),       mod2 = lm(mpg ~ carb + hp, mtcars)) %>%   lapply(summary) %>%    get_elem(\"coef\", regex = TRUE) %>%   # Regular expression search and extraction   unlist2d(idcols = \"Model\", row.names = \"Predictor\")  ## Summary Statistics -------------------------------------------------------------------------------------------  irisNA <- na_insert(iris, prop = 0.15)  # Randmonly set 15% missing fnobs(irisNA)                           # Observation count pwnobs(irisNA)                          # Pairwise observation count fnobs(irisNA, g)                        # Grouped observation count fndistinct(irisNA)                      # Same with distinct values... (default na.rm = TRUE skips NA's) fndistinct(irisNA, g)    descr(iris)                                   # Detailed statistical description of data  varying(iris, ~ Species)                      # Show which variables vary within Species varying(pdata)                                # Which are time-varying ?  qsu(iris, w = ~ w)                            # Fast (one-pass) summary (with weights) qsu(iris, ~ Species, w = ~ w, higher = TRUE)  # Grouped summary + higher moments qsu(pdata, higher = TRUE)                     # Panel-data summary (between and within entities) pwcor(num_vars(irisNA), N = TRUE, P = TRUE)   # Pairwise correlations with p-value and observations pwcor(W(pdata, keep.ids = FALSE), P = TRUE)   # Within-correlations"},{"path":"https://sebkrantz.github.io/collapse/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Advanced and Fast Data Transformation in R","text":"collapse instrumental research project, please consider citing using citation(\"collapse\").","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/BY.html","id":null,"dir":"Reference","previous_headings":"","what":"Split-Apply-Combine Computing  — BY","title":"Split-Apply-Combine Computing  — BY","text":"S3 generic efficiently applies functions vectors matrix- data frame columns groups. Similar dapply seeks retain structure attributes data, can also output various standard formats. simple parallelism also available.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/BY.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split-Apply-Combine Computing  — BY","text":"","code":"BY(x, ...)  # Default S3 method BY(x, g, FUN, ..., use.g.names = TRUE, sort = .op[[\"sort\"]], reorder = TRUE,    expand.wide = FALSE, parallel = FALSE, mc.cores = 1L,    return = c(\"same\", \"vector\", \"list\"))  # S3 method for class 'matrix' BY(x, g, FUN, ..., use.g.names = TRUE, sort = .op[[\"sort\"]], reorder = TRUE,    expand.wide = FALSE, parallel = FALSE, mc.cores = 1L,    return = c(\"same\", \"matrix\", \"data.frame\", \"list\"))  # S3 method for class 'data.frame' BY(x, g, FUN, ..., use.g.names = TRUE, sort = .op[[\"sort\"]], reorder = TRUE,    expand.wide = FALSE, parallel = FALSE, mc.cores = 1L,    return = c(\"same\", \"matrix\", \"data.frame\", \"list\"))  # S3 method for class 'grouped_df' BY(x, FUN, ..., reorder = TRUE, keep.group_vars = TRUE, use.g.names = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/BY.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split-Apply-Combine Computing  — BY","text":"x vector, matrix, data frame alike object. g GRP object, factor / atomic vector / list atomic vectors (internally converted GRP object) used group x. FUN function, can scalar- vector-valued. vector valued functions see also reorder expand.wide. ... arguments FUN, .data.frame 'grouped_df' method. Since v1.9.0 data length arguments also split groups. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). vector-valued functions (row-)names generated function creates names statistics e.g. quantile() adds names, range() log() . row-names generated data.table's. sort logical. Sort groups? Internally passed GRP, effective g already factor GRP object. reorder logical. vector-valued function passed preserves data length, TRUE reorder result elements/rows match original data. FALSE just combines data order groups (.e. elements first group first-appearance order followed elements second group etc..). Note reorder = FALSE, grouping variables, names rownames retained grouping sorted data, see GRP. expand.wide logical. FUN vector-valued function returning vector fixed length > 1 (quantile function), expand.wide can used return result wider format (instead stacking resulting vectors fixed length output column). parallel logical. TRUE implements simple parallel execution internally calling mclapply instead lapply. Parallelism across columns, except default method. mc.cores integer. Argument mclapply indicating number cores use parallel execution. Can use detectCores() select available cores. return integer string indicating type object return. default 1 - \"\" returns object type (.e. class attributes retained underlying data type , just names dimensions adjusted). 2 - \"matrix\" always returns output matrix, 3 - \"data.frame\" always returns data frame 4 - \"list\" returns raw (uncombined) output. Note: 4 - \"list\" works together expand.wide return list matrices. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. See also Note.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/BY.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split-Apply-Combine Computing  — BY","text":"re-implementation Split-Apply-Combine computing paradigm. faster tapply, , aggregate (d)plyr, preserves data attributes just like dapply. principally wrapper around lapply(gsplit(x, g), FUN, ...), uses gsplit optimized splitting also strongly optimizes internal code compared base R functions. details look documentation dapply works similar (apart splitting performed ). function intended simple cases involving flexible computation statistics across groups using single function e.g. iris |> gby(Species) |> (IQR) simpler iris |> gby(Species) |> smr(acr(.fns = IQR)) etc..","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/BY.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split-Apply-Combine Computing  — BY","text":"X FUN applied every column split g.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/BY.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split-Apply-Combine Computing  — BY","text":"","code":"v <- iris$Sepal.Length   # A numeric vector g <- GRP(iris$Species)   # A grouping  ## default vector method BY(v, g, sum)                                # Sum by species #>     setosa versicolor  virginica  #>      250.3      296.8      329.4  head(BY(v, g, scale))                        # Scale by species (please use fscale instead) #> [1]  0.26667447 -0.30071802 -0.86811050 -1.15180675 -0.01702177  1.11776320 BY(v, g, fquantile)                          # Species quantiles: by default stacked #>       setosa.0%      setosa.25%      setosa.50%      setosa.75%     setosa.100%  #>           4.300           4.800           5.000           5.200           5.800  #>   versicolor.0%  versicolor.25%  versicolor.50%  versicolor.75% versicolor.100%  #>           4.900           5.600           5.900           6.300           7.000  #>    virginica.0%   virginica.25%   virginica.50%   virginica.75%  virginica.100%  #>           4.900           6.225           6.500           6.900           7.900  BY(v, g, fquantile, expand.wide = TRUE)      # Wide format #>             0%   25% 50% 75% 100% #> setosa     4.3 4.800 5.0 5.2  5.8 #> versicolor 4.9 5.600 5.9 6.3  7.0 #> virginica  4.9 6.225 6.5 6.9  7.9  ## matrix method m <- qM(num_vars(iris)) BY(m, g, sum)                          # Also return as matrix #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            250.3       171.4         73.1        12.3 #> versicolor        296.8       138.5        213.0        66.3 #> virginica         329.4       148.7        277.6       101.3 BY(m, g, sum, return = \"data.frame\")   # Return as data.frame.. also works for computations below #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            250.3       171.4         73.1        12.3 #> versicolor        296.8       138.5        213.0        66.3 #> virginica         329.4       148.7        277.6       101.3 head(BY(m, g, scale)) #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]   0.26667447   0.1899414   -0.3570112  -0.4364923 #> [2,]  -0.30071802  -1.1290958   -0.3570112  -0.4364923 #> [3,]  -0.86811050  -0.6014810   -0.9328358  -0.4364923 #> [4,]  -1.15180675  -0.8652884    0.2188133  -0.4364923 #> [5,]  -0.01702177   0.4537488   -0.3570112  -0.4364923 #> [6,]   1.11776320   1.2451711    1.3704625   1.4613004 BY(m, g, fquantile) #>                 Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa.0%              4.300       2.300        1.000         0.1 #> setosa.25%             4.800       3.200        1.400         0.2 #> setosa.50%             5.000       3.400        1.500         0.2 #> setosa.75%             5.200       3.675        1.575         0.3 #> setosa.100%            5.800       4.400        1.900         0.6 #> versicolor.0%          4.900       2.000        3.000         1.0 #> versicolor.25%         5.600       2.525        4.000         1.2 #> versicolor.50%         5.900       2.800        4.350         1.3 #> versicolor.75%         6.300       3.000        4.600         1.5 #> versicolor.100%        7.000       3.400        5.100         1.8 #> virginica.0%           4.900       2.200        4.500         1.4 #> virginica.25%          6.225       2.800        5.100         1.8 #> virginica.50%          6.500       3.000        5.550         2.0 #> virginica.75%          6.900       3.175        5.875         2.3 #> virginica.100%         7.900       3.800        6.900         2.5 BY(m, g, fquantile, expand.wide = TRUE) #>            Sepal.Length.0% Sepal.Length.25% Sepal.Length.50% Sepal.Length.75% #> setosa                 4.3            4.800              5.0              5.2 #> versicolor             4.9            5.600              5.9              6.3 #> virginica              4.9            6.225              6.5              6.9 #>            Sepal.Length.100% Sepal.Width.0% Sepal.Width.25% Sepal.Width.50% #> setosa                   5.8            2.3           3.200             3.4 #> versicolor               7.0            2.0           2.525             2.8 #> virginica                7.9            2.2           2.800             3.0 #>            Sepal.Width.75% Sepal.Width.100% Petal.Length.0% Petal.Length.25% #> setosa               3.675              4.4             1.0              1.4 #> versicolor           3.000              3.4             3.0              4.0 #> virginica            3.175              3.8             4.5              5.1 #>            Petal.Length.50% Petal.Length.75% Petal.Length.100% Petal.Width.0% #> setosa                 1.50            1.575               1.9            0.1 #> versicolor             4.35            4.600               5.1            1.0 #> virginica              5.55            5.875               6.9            1.4 #>            Petal.Width.25% Petal.Width.50% Petal.Width.75% Petal.Width.100% #> setosa                 0.2             0.2             0.3              0.6 #> versicolor             1.2             1.3             1.5              1.8 #> virginica              1.8             2.0             2.3              2.5 ml <- BY(m, g, fquantile, expand.wide = TRUE, # Return as list of matrices          return = \"list\") ml #> $Sepal.Length #>             0%   25% 50% 75% 100% #> setosa     4.3 4.800 5.0 5.2  5.8 #> versicolor 4.9 5.600 5.9 6.3  7.0 #> virginica  4.9 6.225 6.5 6.9  7.9 #>  #> $Sepal.Width #>             0%   25% 50%   75% 100% #> setosa     2.3 3.200 3.4 3.675  4.4 #> versicolor 2.0 2.525 2.8 3.000  3.4 #> virginica  2.2 2.800 3.0 3.175  3.8 #>  #> $Petal.Length #>             0% 25%  50%   75% 100% #> setosa     1.0 1.4 1.50 1.575  1.9 #> versicolor 3.0 4.0 4.35 4.600  5.1 #> virginica  4.5 5.1 5.55 5.875  6.9 #>  #> $Petal.Width #>             0% 25% 50% 75% 100% #> setosa     0.1 0.2 0.2 0.3  0.6 #> versicolor 1.0 1.2 1.3 1.5  1.8 #> virginica  1.4 1.8 2.0 2.3  2.5 #>  # Unlisting to Data Frame unlist2d(ml, idcols = \"Variable\", row.names = \"Species\") #>        Variable    Species  0%   25%  50%   75% 100% #> 1  Sepal.Length     setosa 4.3 4.800 5.00 5.200  5.8 #> 2  Sepal.Length versicolor 4.9 5.600 5.90 6.300  7.0 #> 3  Sepal.Length  virginica 4.9 6.225 6.50 6.900  7.9 #> 4   Sepal.Width     setosa 2.3 3.200 3.40 3.675  4.4 #> 5   Sepal.Width versicolor 2.0 2.525 2.80 3.000  3.4 #> 6   Sepal.Width  virginica 2.2 2.800 3.00 3.175  3.8 #> 7  Petal.Length     setosa 1.0 1.400 1.50 1.575  1.9 #> 8  Petal.Length versicolor 3.0 4.000 4.35 4.600  5.1 #> 9  Petal.Length  virginica 4.5 5.100 5.55 5.875  6.9 #> 10  Petal.Width     setosa 0.1 0.200 0.20 0.300  0.6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]  ## data.frame method BY(num_vars(iris), g, sum)             # Also returns a data.fram #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            250.3       171.4         73.1        12.3 #> versicolor        296.8       138.5        213.0        66.3 #> virginica         329.4       148.7        277.6       101.3 BY(num_vars(iris), g, sum, return = 2) # Return as matrix.. also works for computations below #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            250.3       171.4         73.1        12.3 #> versicolor        296.8       138.5        213.0        66.3 #> virginica         329.4       148.7        277.6       101.3 head(BY(num_vars(iris), g, scale)) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   0.26667447   0.1899414   -0.3570112  -0.4364923 #> 2  -0.30071802  -1.1290958   -0.3570112  -0.4364923 #> 3  -0.86811050  -0.6014810   -0.9328358  -0.4364923 #> 4  -1.15180675  -0.8652884    0.2188133  -0.4364923 #> 5  -0.01702177   0.4537488   -0.3570112  -0.4364923 #> 6   1.11776320   1.2451711    1.3704625   1.4613004 BY(num_vars(iris), g, fquantile) #>                 Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa.0%              4.300       2.300        1.000         0.1 #> setosa.25%             4.800       3.200        1.400         0.2 #> setosa.50%             5.000       3.400        1.500         0.2 #> setosa.75%             5.200       3.675        1.575         0.3 #> setosa.100%            5.800       4.400        1.900         0.6 #> versicolor.0%          4.900       2.000        3.000         1.0 #> versicolor.25%         5.600       2.525        4.000         1.2 #> versicolor.50%         5.900       2.800        4.350         1.3 #> versicolor.75%         6.300       3.000        4.600         1.5 #> versicolor.100%        7.000       3.400        5.100         1.8 #> virginica.0%           4.900       2.200        4.500         1.4 #> virginica.25%          6.225       2.800        5.100         1.8 #> virginica.50%          6.500       3.000        5.550         2.0 #> virginica.75%          6.900       3.175        5.875         2.3 #> virginica.100%         7.900       3.800        6.900         2.5 BY(num_vars(iris), g, fquantile, expand.wide = TRUE) #>            Sepal.Length.0% Sepal.Length.25% Sepal.Length.50% Sepal.Length.75% #> setosa                 4.3            4.800              5.0              5.2 #> versicolor             4.9            5.600              5.9              6.3 #> virginica              4.9            6.225              6.5              6.9 #>            Sepal.Length.100% Sepal.Width.0% Sepal.Width.25% Sepal.Width.50% #> setosa                   5.8            2.3           3.200             3.4 #> versicolor               7.0            2.0           2.525             2.8 #> virginica                7.9            2.2           2.800             3.0 #>            Sepal.Width.75% Sepal.Width.100% Petal.Length.0% Petal.Length.25% #> setosa               3.675              4.4             1.0              1.4 #> versicolor           3.000              3.4             3.0              4.0 #> virginica            3.175              3.8             4.5              5.1 #>            Petal.Length.50% Petal.Length.75% Petal.Length.100% Petal.Width.0% #> setosa                 1.50            1.575               1.9            0.1 #> versicolor             4.35            4.600               5.1            1.0 #> virginica              5.55            5.875               6.9            1.4 #>            Petal.Width.25% Petal.Width.50% Petal.Width.75% Petal.Width.100% #> setosa                 0.2             0.2             0.3              0.6 #> versicolor             1.2             1.3             1.5              1.8 #> virginica              1.8             2.0             2.3              2.5 BY(num_vars(iris), g, fquantile,       # Return as list of matrices    expand.wide = TRUE, return = \"list\") #> $Sepal.Length #>             0%   25% 50% 75% 100% #> setosa     4.3 4.800 5.0 5.2  5.8 #> versicolor 4.9 5.600 5.9 6.3  7.0 #> virginica  4.9 6.225 6.5 6.9  7.9 #>  #> $Sepal.Width #>             0%   25% 50%   75% 100% #> setosa     2.3 3.200 3.4 3.675  4.4 #> versicolor 2.0 2.525 2.8 3.000  3.4 #> virginica  2.2 2.800 3.0 3.175  3.8 #>  #> $Petal.Length #>             0% 25%  50%   75% 100% #> setosa     1.0 1.4 1.50 1.575  1.9 #> versicolor 3.0 4.0 4.35 4.600  5.1 #> virginica  4.5 5.1 5.55 5.875  6.9 #>  #> $Petal.Width #>             0% 25% 50% 75% 100% #> setosa     0.1 0.2 0.2 0.3  0.6 #> versicolor 1.0 1.2 1.3 1.5  1.8 #> virginica  1.4 1.8 2.0 2.3  2.5 #>   ## grouped data frame method giris <- fgroup_by(iris, Species) giris |> BY(sum)                      # Compute sum #>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa        250.3       171.4         73.1        12.3 #> 2 versicolor        296.8       138.5        213.0        66.3 #> 3  virginica        329.4       148.7        277.6       101.3 giris |> BY(sum, use.g.names = TRUE,  # Use row.names and              keep.group_vars = FALSE)  # remove 'Species' and groups attribute #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            250.3       171.4         73.1        12.3 #> versicolor        296.8       138.5        213.0        66.3 #> virginica         329.4       148.7        277.6       101.3 giris |> BY(sum, return = \"matrix\")   # Return matrix #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]        250.3       171.4         73.1        12.3 #> [2,]        296.8       138.5        213.0        66.3 #> [3,]        329.4       148.7        277.6       101.3 giris |> BY(sum, return = \"matrix\",   # Matrix with row.names              use.g.names = TRUE) #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            250.3       171.4         73.1        12.3 #> versicolor        296.8       138.5        213.0        66.3 #> virginica         329.4       148.7        277.6       101.3 giris |> BY(.quantile)                # Compute quantiles (output is stacked) #>       Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1      setosa        4.300       2.300        1.000         0.1 #> 2      setosa        4.800       3.200        1.400         0.2 #> 3      setosa        5.000       3.400        1.500         0.2 #> 4      setosa        5.200       3.675        1.575         0.3 #> 5      setosa        5.800       4.400        1.900         0.6 #> 6  versicolor        4.900       2.000        3.000         1.0 #> 7  versicolor        5.600       2.525        4.000         1.2 #> 8  versicolor        5.900       2.800        4.350         1.3 #> 9  versicolor        6.300       3.000        4.600         1.5 #> 10 versicolor        7.000       3.400        5.100         1.8 #> 11  virginica        4.900       2.200        4.500         1.4 #> 12  virginica        6.225       2.800        5.100         1.8 #> 13  virginica        6.500       3.000        5.550         2.0 #> 14  virginica        6.900       3.175        5.875         2.3 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] giris |> BY(.quantile, names = TRUE,  # Wide output                expand.wide = TRUE) #>      Species Sepal.Length.0% Sepal.Length.25% Sepal.Length.50% Sepal.Length.75% #> 1     setosa             4.3            4.800              5.0              5.2 #> 2 versicolor             4.9            5.600              5.9              6.3 #> 3  virginica             4.9            6.225              6.5              6.9 #>   Sepal.Length.100% Sepal.Width.0% Sepal.Width.25% Sepal.Width.50% #> 1               5.8            2.3           3.200             3.4 #> 2               7.0            2.0           2.525             2.8 #> 3               7.9            2.2           2.800             3.0 #>   Sepal.Width.75% Sepal.Width.100% Petal.Length.0% Petal.Length.25% #> 1           3.675              4.4             1.0              1.4 #> 2           3.000              3.4             3.0              4.0 #> 3           3.175              3.8             4.5              5.1 #>   Petal.Length.50% Petal.Length.75% Petal.Length.100% Petal.Width.0% #> 1             1.50            1.575               1.9            0.1 #> 2             4.35            4.600               5.1            1.0 #> 3             5.55            5.875               6.9            1.4 #>   Petal.Width.25% Petal.Width.50% Petal.Width.75% Petal.Width.100% #> 1             0.2             0.2             0.3              0.6 #> 2             1.2             1.3             1.5              1.8 #> 3             1.8             2.0             2.3              2.5"},{"path":"https://sebkrantz.github.io/collapse/reference/GGDC10S.html","id":null,"dir":"Reference","previous_headings":"","what":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","title":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","text":"GGDC 10-Sector Database provides long-run internationally comparable dataset sectoral productivity performance Africa, Asia, Latin America. Variables covered data set annual series value added (local currency), persons employed 10 broad sectors.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/GGDC10S.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","text":"","code":"data(\"GGDC10S\")"},{"path":"https://sebkrantz.github.io/collapse/reference/GGDC10S.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","text":"data frame 5027 observations following 16 variables. Country char: Country (43 countries) Regioncode char: ISO3 Region code Region char: Region (6 World Regions) Variable char: Variable (Value Added Employment) Year num: Year (67 Years, 1947-2013) AGR num: Agriculture MIN num: Mining MAN num: Manufacturing PU num: Utilities CON num: Construction WRT num: Trade, restaurants hotels TRA num: Transport, storage communication FIRE num: Finance, insurance, real estate business services GOV num: Government services OTH num: Community, social personal services SUM num: Summation sector GDP","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/GGDC10S.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","text":"https://www.rug.nl/ggdc/productivity/10-sector/","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/GGDC10S.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","text":"Timmer, M. P., de Vries, G. J., & de Vries, K. (2015). \"Patterns Structural Change Developing Countries.\" . J. Weiss, & M. Tribe (Eds.), Routledge Handbook Industry Development. (pp. 65-83). Routledge.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/GGDC10S.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Groningen Growth and Development Centre 10-Sector Database — GGDC10S","text":"","code":"namlab(GGDC10S, class = TRUE) #>      Variable     Class                                                 Label #> 1     Country character                                               Country #> 2  Regioncode character                                           Region code #> 3      Region character                                                Region #> 4    Variable character                                              Variable #> 5        Year   numeric                                                  Year #> 6         AGR   numeric                                          Agriculture  #> 7         MIN   numeric                                                Mining #> 8         MAN   numeric                                         Manufacturing #> 9          PU   numeric                                             Utilities #> 10        CON   numeric                                          Construction #> 11        WRT   numeric                         Trade, restaurants and hotels #> 12        TRA   numeric                  Transport, storage and communication #> 13       FIRE   numeric Finance, insurance, real estate and business services #> 14        GOV   numeric                                   Government services #> 15        OTH   numeric               Community, social and personal services #> 16        SUM   numeric                               Summation of sector GDP # aperm(qsu(GGDC10S, ~ Variable, ~ Variable + Country, vlabels = TRUE)) # \\donttest{ library(ggplot2)  ## World Regions Structural Change Plot  GGDC10S |>   fmutate(across(AGR:OTH, `*`, 1 / SUM),           Variable = ifelse(Variable == \"VA\",\"Value Added Share\", \"Employment Share\")) |>   replace_outliers(0, NA, \"min\") |>   collap( ~ Variable + Region + Year, cols = 6:15) |> qDT() |>   pivot(1:3, names = list(variable = \"Sector\"), na.rm = TRUE) |>    ggplot(aes(x = Year, y = value, fill = Sector)) +     geom_area(position = \"fill\", alpha = 0.9) + labs(x = NULL, y = NULL) +     theme_linedraw(base_size = 14) +     facet_grid(Variable ~ Region, scales = \"free_x\") +     scale_fill_manual(values = sub(\"#00FF66\", \"#00CC66\", rainbow(10))) +     scale_x_continuous(breaks = scales::pretty_breaks(n = 7), expand = c(0, 0))+     scale_y_continuous(breaks = scales::pretty_breaks(n = 10), expand = c(0, 0),                        labels = scales::percent) +     theme(axis.text.x = element_text(angle = 315, hjust = 0, margin = ggplot2::margin(t = 0)),           strip.background = element_rect(colour = \"grey30\", fill = \"grey30\"))   # A function to plot the structural change of an arbitrary country  plotGGDC <- function(ctry) {    GGDC10S |>   fsubset(Country == ctry, Variable, Year, AGR:SUM) |>   fmutate(across(AGR:OTH, `*`, 1 / SUM), SUM = NULL,           Variable = ifelse(Variable == \"VA\",\"Value Added Share\", \"Employment Share\")) |>   replace_outliers(0, NA, \"min\") |> qDT() |>   pivot(1:2, names = list(variable = \"Sector\"), na.rm = TRUE) |>    ggplot(aes(x = Year, y = value, fill = Sector)) +     geom_area(position = \"fill\", alpha = 0.9) + labs(x = NULL, y = NULL) +     theme_linedraw(base_size = 14) + facet_wrap( ~ Variable) +     scale_fill_manual(values = sub(\"#00FF66\", \"#00CC66\", rainbow(10))) +     scale_x_continuous(breaks = scales::pretty_breaks(n = 7), expand = c(0, 0)) +     scale_y_continuous(breaks = scales::pretty_breaks(n = 10), expand = c(0, 0),                        labels = scales::percent) +     theme(axis.text.x = element_text(angle = 315, hjust = 0, margin = ggplot2::margin(t = 0)),           strip.background = element_rect(colour = \"grey20\", fill = \"grey20\"),           strip.text = element_text(face = \"bold\")) }  plotGGDC(\"BWA\")   # }"},{"path":"https://sebkrantz.github.io/collapse/reference/GRP.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Grouping / collapse Grouping Objects — GRP","title":"Fast Grouping / collapse Grouping Objects — GRP","text":"GRP performs fast, ordered unordered, groupings vectors data frames (lists vectors) using radixorder group. output list-like object class 'GRP' can printed, plotted used efficient input collapse's fast statistical transformation functions operators (see macros .FAST_FUN .OPERATOR_FUN), well collap, TRA. fgroup_by similar dplyr::group_by faster class-agnostic. creates grouped data frame 'GRP' object attached - fast dplyr-like programming collapse's fast functions. also several conversion methods 'GRP' objects. Notable among GRP.grouped_df, returns 'GRP' object grouped data frame created dplyr::group_by fgroup_by, duo GRP.factor as_factor_GRP. gsplit efficiently splits vector based 'GRP' object, greorder helps recombine results. workhorses behind functions like , collap, fsummarise fmutate evaluated base R user-defined functions.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/GRP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Grouping / collapse Grouping Objects — GRP","text":"","code":"GRP(X, ...)  # Default S3 method GRP(X, by = NULL, sort = .op[[\"sort\"]], decreasing = FALSE, na.last = TRUE,     return.groups = TRUE, return.order = sort, method = \"auto\",     call = TRUE, ...)  # S3 method for class 'factor' GRP(X, ..., group.sizes = TRUE, drop = FALSE, return.groups = TRUE,     call = TRUE)  # S3 method for class 'qG' GRP(X, ..., group.sizes = TRUE, return.groups = TRUE, call = TRUE)  # S3 method for class 'pseries' GRP(X, effect = 1L, ..., group.sizes = TRUE, return.groups = TRUE,     call = TRUE)  # S3 method for class 'pdata.frame' GRP(X, effect = 1L, ..., group.sizes = TRUE, return.groups = TRUE,     call = TRUE)  # S3 method for class 'grouped_df' GRP(X, ..., return.groups = TRUE, call = TRUE)  # Identify 'GRP' objects is_GRP(x)  # S3 method for class 'GRP' length(x)                          # Length of data being grouped GRPN(x, expand = TRUE, ...)        # Group sizes (default: expanded to match data length) GRPid(x, sort = FALSE, ...)        # Group id (data length, same as GRP(.)$group.id) GRPnames(x, force.char = TRUE, sep = \".\")  # Group names  as_factor_GRP(x, ordered = FALSE, sep = \".\") # 'GRP'-object to (ordered) factor conversion  # Efficiently split a vector using a 'GRP' object gsplit(x, g, use.g.names = FALSE, ...)  # Efficiently reorder y = unlist(gsplit(x, g)) such that identical(greorder(y, g), x) greorder(x, g, ...)  # Fast, class-agnostic pendant to dplyr::group_by for use with fast functions, see details fgroup_by(.X, ..., sort = .op[[\"sort\"]], decreasing = FALSE, na.last = TRUE,           return.groups = TRUE, return.order = sort, method = \"auto\") # Standard-evaluation analogue (slim wrapper around GRP.default(), for programming) group_by_vars(X, by = NULL, ...) # Shorthand for fgroup_by gby(.X, ..., sort = .op[[\"sort\"]], decreasing = FALSE, na.last = TRUE,     return.groups = TRUE, return.order = sort, method = \"auto\")  # Get grouping columns from a grouped data frame created with dplyr::group_by or fgroup_by fgroup_vars(X, return = \"data\")  # Ungroup grouped data frame created with dplyr::group_by or fgroup_by fungroup(X, ...)  # S3 method for class 'GRP' print(x, n = 6, ...)  # S3 method for class 'GRP' plot(x, breaks = \"auto\", type = \"l\", horizontal = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/GRP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Grouping / collapse Grouping Objects — GRP","text":"X vector, list columns data frame (default method), suitable object (conversion / extractor methods). .X data frame list. x, g 'GRP' object. gsplit/greorder, x can vector type, NULL return integer indices groups. gsplit/greorder/GRPN/GRPid also support vectors data frames passed g/x. X data frame list, can indicate columns use grouping (default columns used). Columns must passed using vector column names, indices, one-sided formula .e. ~ col1 + col2, logical vector (converted indices) selector function e.g. is_categorical. sort logical. FALSE, groups ordered simply grouped order first appearance unique elements / rows. often provides performance gain data sorted beforehand. See also method. ordered logical. TRUE adds class 'ordered' .e. generates ordered factor. decreasing logical. sort order increasing decreasing? Can vector length equal number arguments X / (argument passed radixorder). na.last logical. missing values encountered grouping vector/columns, assign last group (argument passed radixorder). return.groups logical. Include unique groups created GRP object. return.order logical. sort = TRUE, include output radixorder created GRP object. brings performance improvements gsplit (thus also benefits grouped execution base R functions). method character. algorithm use grouping: either \"radix\", \"hash\" \"auto\". \"auto\" chose \"radix\" sort = TRUE, yielding ordered grouping via radixorder, \"hash\"-based grouping first-appearance order via group otherwise. possibly put method = \"radix\" sort = FALSE, group character data first appearance order sort numeric data (good hybrid option). method = \"hash\" currently support sorting, thus putting sort = TRUE simply ignored. group.sizes logical. TRUE tabulates factor levels using tabulate create vector group sizes; FALSE leaves slot empty converting factors. drop logical. TRUE efficiently drops unused factor levels beforehand using fdroplevels. call logical. TRUE calls match.call saves final slot GRP object. expand logical. TRUE returns vector length data. FALSE returns group sizes (computed first-appearance-order groups x already 'GRP' object). force.char logical. Always output group names character vector, even single numeric vector passed GRP.default. sep character. separator passed paste creating group names multiple grouping variables pasting together. effect plm / indexed data methods: Select panel identifier used grouping variable. 1L takes first variable index, 2L second etc., identifiers can also passed character string. one variable can supplied. return integer string specifying fgroup_vars return. options :  use.g.names logical. TRUE returns named list, like split. FALSE slightly efficient.   n integer. Number groups print . breaks integer. Number breaks histogram group-sizes. type linetype plot. horizontal logical. TRUE arranges plots next , instead . Note size group plotted objects less 10,000 groups. ... fgroup_by: unquoted comma-separated column names, sequences columns, expressions involving columns, column names, indices, logical vectors selector functions. See Examples. group_by_vars, gsplit, greorder, GRPN GRPid: arguments passed GRP (g/x already 'GRP' object). example argument used data frame passed.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/GRP.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Grouping / collapse Grouping Objects — GRP","text":"GRP central function collapse package provides, form integer vectors, key pieces information efficiently perform grouped operations C/C++ level. statistical function require information (1) number groups (2) integer group-id indicating values / rows belong group (3) information size group. Provided , collapse's Fast Statistical Functions pre-allocate intermediate result vectors right sizes (cases) perform grouped statistical computations single pass data. sorting functionality GRP.default lets groups receive different integer-id's depending whether groups sorted sort = TRUE (FALSE gives first-appearance order), order (argument decreasing). affects order values/rows output whenever aggregation performed. elements object provide information whether data sorted variables defining grouping (6) ordering vector (7). also feed optimizations gsplit/greorder benefit execution base R functions across groups. Complimentary GRP, function fgroup_by significantly faster class-agnostic alternative dplyr::group_by programming collapse. creates grouped data frame 'GRP' object attached \"groups\" attribute. data frame classes 'GRP_df', ..., 'grouped_df' 'data.frame', ... stands classes input frame inherits 'data.table', 'sf', 'tbl_df', 'indexed_frame' etc.. collapse functions 'grouped_df' method respond 'grouped_df' objects created either fgroup_by dplyr::group_by. method GRP.grouped_df takes \"groups\" attribute 'grouped_df' converts 'GRP' object created dplyr::group_by. 'GRP_df' class front responds print.GRP_df first calls print(fungroup(x), ...) prints one line object indicating grouping variables, followed, square brackets, statistics group sizes: [N | Mean (SD) Min-Max]. mean rounded full number standard deviation (SD) one digit. Minimum maximum displayed SD non-zero. also exist method [.GRP_df calls NextMethod makes sure grouping information preserved dropped depending dimensions result (subsetting rows aggregation data.table drops grouping object). GRP.default supports vector list input also return 'GRP' objects passed. also hidden method GRP.GRP simply returns grouping objects (re-grouping functionality offered). Apart GRP.grouped_df several conversion methods: conversion factors 'GRP' objects GRP.factor involves obtaining number groups calling ng <- fnlevels(f) computing count level using tabulate(f, ng). integer group-id (2) already given factor removing levels class attributes replacing missing values ng + 1L. levels put list moved position (4) 'GRP' object, reserved unique groups. Finally, sortedness check !.unsorted(id) run group-id check data represented factor sorted (6). GRP.qG works similarly (see also qG), 'pseries' 'pdata.frame' methods simply group one factors index (selected using effect argument) . Creating factor 'GRP' object using as_factor_GRP involve computations, may involve interacting multiple grouping columns using paste function produce unique factor levels.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/GRP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Grouping / collapse Grouping Objects — GRP","text":"list-like object class `GRP' containing information number groups, observations (rows) belonging group, size group, unique group names / definitions, whether groups ordered data grouped sorted , ordering vector used perform ordering group start positions. object structured follows:","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/GRP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Grouping / collapse Grouping Objects — GRP","text":"","code":"## default method GRP(mtcars$cyl) #> collapse grouping object of length 32 with 3 ordered groups #>  #> Call: GRP.default(X = mtcars$cyl), X is unsorted #>  #> Distribution of group sizes:  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    7.00    9.00   11.00   10.67   12.50   14.00  #>  #> Groups with sizes:  #>  4  6  8  #> 11  7 14  GRP(mtcars, ~ cyl + vs + am)       # Or GRP(mtcars, c(\"cyl\",\"vs\",\"am\")) or GRP(mtcars, c(2,8:9)) #> collapse grouping object of length 32 with 7 ordered groups #>  #> Call: GRP.default(X = mtcars, by = ~cyl + vs + am), X is unsorted #>  #> Distribution of group sizes:  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   2.500   3.000   4.571   5.500  12.000  #>  #> Groups with sizes:  #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>     1     3     7     3     4    12     2  g <- GRP(mtcars, ~ cyl + vs + am)  # Saving the object print(g)                           # Printing it #> collapse grouping object of length 32 with 7 ordered groups #>  #> Call: GRP.default(X = mtcars, by = ~cyl + vs + am), X is unsorted #>  #> Distribution of group sizes:  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   2.500   3.000   4.571   5.500  12.000  #>  #> Groups with sizes:  #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>     1     3     7     3     4    12     2  plot(g)                            # Plotting it  GRPnames(g)                        # Retain group names #> [1] \"4.0.1\" \"4.1.0\" \"4.1.1\" \"6.0.1\" \"6.1.0\" \"8.0.0\" \"8.0.1\" GRPid(g)                           # Retain group id (same as g$group.id), useful inside fmutate() #>  [1] 4 4 3 5 6 5 6 2 2 5 5 6 6 6 6 6 6 3 3 3 2 6 6 6 6 3 1 3 7 4 7 3 fsum(mtcars, g)                    # Compute the sum of mtcars, grouped by variables cyl, vs and am #>         mpg cyl   disp   hp  drat     wt   qsec vs am gear carb #> 4.0.1  26.0   4  120.3   91  4.43  2.140  16.70  0  1    5    2 #> 4.1.0  68.7  12  407.6  254 11.31  8.805  62.91  3  0   11    5 #> 4.1.1 198.6  28  628.6  564 29.04 14.198 130.90  7  7   29   10 #> 6.0.1  61.7  18  465.0  395 11.42  8.265  48.98  0  3   13   14 #> 6.1.0  76.5  24  818.2  461 13.68 13.555  76.86  4  0   14   10 #> 8.0.0 180.6  96 4291.4 2330 37.45 49.249 205.71  0  0   36   37 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] gsplit(mtcars$mpg, g)              # Use the object to split a vector #> [[1]] #> [1] 26 #>  #> [[2]] #> [1] 24.4 22.8 21.5 #>  #> [[3]] #> [1] 22.8 32.4 30.4 33.9 27.3 30.4 21.4 #>  #> [[4]] #> [1] 21.0 21.0 19.7 #>  #> [[5]] #> [1] 21.4 18.1 19.2 17.8 #>  #> [[6]] #>  [1] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 13.3 19.2 #>  #> [[7]] #> [1] 15.8 15.0 #>  gsplit(NULL, g)                    # The indices of the groups #> [[1]] #> [1] 27 #>  #> [[2]] #> [1]  8  9 21 #>  #> [[3]] #> [1]  3 18 19 20 26 28 32 #>  #> [[4]] #> [1]  1  2 30 #>  #> [[5]] #> [1]  4  6 10 11 #>  #> [[6]] #>  [1]  5  7 12 13 14 15 16 17 22 23 24 25 #>  #> [[7]] #> [1] 29 31 #>  identical(mtcars$mpg,              # greorder and unlist undo the effect of gsplit           greorder(unlist(gsplit(mtcars$mpg, g)), g)) #> [1] TRUE  ## Convert factor to GRP object and vice-versa GRP(iris$Species) #> collapse grouping object of length 150 with 3 ordered groups #>  #> Call: GRP.factor(X = iris$Species), X is sorted #>  #> Distribution of group sizes:  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      50      50      50      50      50      50  #>  #> Groups with sizes:  #>     setosa versicolor  virginica  #>         50         50         50  as_factor_GRP(g) #>  [1] 6.0.1 6.0.1 4.1.1 6.1.0 8.0.0 6.1.0 8.0.0 4.1.0 4.1.0 6.1.0 6.1.0 8.0.0 #> [13] 8.0.0 8.0.0 8.0.0 8.0.0 8.0.0 4.1.1 4.1.1 4.1.1 4.1.0 8.0.0 8.0.0 8.0.0 #> [25] 8.0.0 4.1.1 4.0.1 4.1.1 8.0.1 6.0.1 8.0.1 4.1.1 #> Levels: 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1   ## dplyr integration library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union mtcars |> group_by(cyl,vs,am) |> GRP()    # Get GRP object from a dplyr grouped tibble #> collapse grouping object of length 32 with 7 ordered groups #>  #> Call: GRP.grouped_df(X = group_by(mtcars, cyl, vs, am)), X is unsorted #>  #> Distribution of group sizes:  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   2.500   3.000   4.571   5.500  12.000  #>  #> Groups with sizes:  #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>     1     3     7     3     4    12     2  mtcars |> group_by(cyl,vs,am) |> fmean()  # Grouped mean using dplyr grouping #> # A tibble: 7 × 11 #>     cyl    vs    am   mpg  disp    hp  drat    wt  qsec  gear  carb #>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1     4     0     1  26   120.   91    4.43  2.14  16.7  5     2    #> 2     4     1     0  22.9 136.   84.7  3.77  2.94  21.0  3.67  1.67 #> 3     4     1     1  28.4  89.8  80.6  4.15  2.03  18.7  4.14  1.43 #> 4     6     0     1  20.6 155   132.   3.81  2.76  16.3  4.33  4.67 #> 5     6     1     0  19.1 205.  115.   3.42  3.39  19.2  3.5   2.5  #> 6     8     0     0  15.1 358.  194.   3.12  4.10  17.1  3     3.08 #> 7     8     0     1  15.4 326   300.   3.88  3.37  14.6  5     6    mtcars |> fgroup_by(cyl,vs,am) |> fmean() # Faster alternative with collapse grouping #>   cyl vs am      mpg     disp        hp     drat       wt     qsec     gear #> 1   4  0  1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 #> 2   4  1  0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 #> 3   4  1  1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 #> 4   6  0  1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 #> 5   6  1  0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 #> 6   8  0  0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 #>       carb #> 1 2.000000 #> 2 1.666667 #> 3 1.428571 #> 4 4.666667 #> 5 2.500000 #> 6 3.083333 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  mtcars |> fgroup_by(cyl,vs,am)            # Print method for grouped data frame #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]   ## Adding a column of group sizes. mtcars |> fgroup_by(cyl,vs,am) |> fsummarise(Sizes = GRPN()) #>   cyl vs am Sizes #> 1   4  0  1     1 #> 2   4  1  0     3 #> 3   4  1  1     7 #> 4   6  0  1     3 #> 5   6  1  0     4 #> 6   8  0  0    12 #> 7   8  0  1     2 # Note: can also set_collapse(mask = \"n\") to use n() instead, see help(\"collapse-options\") # Other usage modes: mtcars |> fgroup_by(cyl,vs,am) |> fmutate(Sizes = GRPN()) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb Sizes #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     3 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     3 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1     7 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1     4 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2    12 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fmutate(Sizes = GRPN(list(cyl,vs,am)))  # Same thing, slightly more efficient #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb Sizes #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     3 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     3 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1     7 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1     4 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2    12 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ]  ## Various options for programming and interactive use fgroup_by(GGDC10S, Variable, Decade = floor(Year / 10) * 10) |> head(3) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA fgroup_by(GGDC10S, 1:3, 5) |> head(3) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA fgroup_by(GGDC10S, c(\"Variable\", \"Country\")) |> head(3) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA fgroup_by(GGDC10S, is.character) |> head(3) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA fgroup_by(GGDC10S, Country:Variable, Year) |> head(3) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA fgroup_by(GGDC10S, Country:Region, Var = Variable, Year) |> head(3) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA  ## Note that you can create a grouped data frame without materializing the unique grouping columns fgroup_by(GGDC10S, Variable, Country, return.groups = FALSE) |> fmutate(across(AGR:SUM, fscale)) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #> 4     BWA        SSA Sub-saharan Africa       VA 1963  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA #> 4  NA   NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5023 rows ] #>  #> Grouped by:  Variable, Country  [85 | 59 (7.7) 4-65]  fgroup_by(GGDC10S, Variable, Country, return.groups = FALSE) |> fselect(AGR:SUM) |> fmean() #>           AGR        MIN         MAN          PU         CON         WRT #> 1   1419.8013   52.08903  1931.76020  101.720936   742.40444  1982.17750 #> 2    964.2103   56.03295   235.03321    5.346433   122.78267   281.51638 #> 3  17191.3529  206.02389  6991.37096  364.573404  3524.73837  8509.46124 #> 4    188.0574   10.47964    18.05585    3.093238    25.34108    36.32971 #> 5    701.9112  101.41742   624.90303   29.358568   296.00226   694.66129 #> 6 287744.2384 7049.52662 67143.63401 1606.064456 20851.66641 28908.10661 #>            TRA       FIRE         GOV         OTH         SUM #> 1   648.511948  627.79291  2043.47128   992.44753  10542.1770 #> 2   115.472843   44.56442          NA   395.56503   2220.5242 #> 3  2054.373148 4413.54448  5307.28010  5710.26650  54272.9850 #> 4     8.362217   15.33622    61.10484    27.60635    393.7666 #> 5   258.240876  272.43836          NA  1003.43964   3982.3726 #> 6 13928.607381 4929.26308 22669.24815 30990.11860 485820.4737 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 79 rows ]  ## Note also that setting sort = FALSE on unsorted data can be much faster... if not required... library(microbenchmark) microbenchmark(gby(GGDC10S, Variable, Country), gby(GGDC10S, Variable, Country, sort = FALSE)) #> Warning: less accurate nanosecond times to avoid potential integer overflows #> Unit: microseconds #>                                           expr    min      lq     mean  median #>                gby(GGDC10S, Variable, Country) 75.973 76.9775 79.27391 77.5310 #>  gby(GGDC10S, Variable, Country, sort = FALSE) 25.953 26.7115 27.23220 27.1215 #>      uq     max neval #>  78.228 227.222   100 #>  27.511  34.563   100"},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"TRA S3 generic efficiently transforms data either (column-wise) replacing data values supplied statistics sweeping statistics data. TRA supports grouped operations data transformation reference, thus generalization sweep.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"","code":"TRA(x, STATS, FUN = \"-\", ...) setTRA(x, STATS, FUN = \"-\", ...) # Shorthand for invisible(TRA(..., set = TRUE))  # Default S3 method TRA(x, STATS, FUN = \"-\", g = NULL, set = FALSE, ...)  # S3 method for class 'matrix' TRA(x, STATS, FUN = \"-\", g = NULL, set = FALSE, ...)  # S3 method for class 'data.frame' TRA(x, STATS, FUN = \"-\", g = NULL, set = FALSE, ...)  # S3 method for class 'grouped_df' TRA(x, STATS, FUN = \"-\", keep.group_vars = TRUE, set = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"x atomic vector, matrix, data frame grouped data frame (class 'grouped_df'). STATS matching set summary statistics. See Details Examples. FUN integer character string indicating operation perform. 11 supported operations:  g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. Number groups must match rows STATS. See Details. set logical. TRUE transforms data reference .e. performs -place modification data without creating copy. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. See Details Examples. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"Without groups (g = NULL), TRA little column based version sweep, albeit many times efficient. case methods support atomic vector statistics length NCOL(x) passed STATS. matrix data frame methods also support 1-row matrix 1-row data frame / list, respectively. TRA always preserves attributes x. groups passed g, STATS needs type x appropriate dimensions [NCOL(x) == NCOL(STATS) NROW(STATS) equals number groups (.e. number levels g factor)]. condition satisfied, TRA assume first row STATS set statistics computed first group/level g, second row second group/level etc. groupwise replacing sweeping accordingly. example Let x = c(1.2, 4.6, 2.5, 9.1, 8.7, 3.3), g integer vector 3 groups g = c(1,3,3,2,1,2) STATS = fmean(x,g) = c(4.95, 6.20, 3.55). = TRA(x,STATS,\"-\",g) = c(-3.75,  1.05, -1.05,  2.90,  3.75, -2.90) [fmean(x, g, TRA = \"-\")] equivalent following -loop: (1:6) [] = x[] - STATS[g[]]. Correct computation requires g used fmean g passed TRA exactly vector. Using g = c(1,3,3,2,1,2) fmean g = c(3,1,1,2,3,2) TRA give right result. safest way programming TRA thus repeatedly employ factor GRP object grouped computations. Atomic vectors passed g converted factors (see qF) lists converted GRP objects. also done Fast Statistical Functions , thus together functions, TRA can also safely used atomic- list-groups (long functions apply sorted grouping, default collapse). x grouped data frame ('grouped_df'), TRA matches columns x STATS also checks grouping columns x STATS. TRA.grouped_df transform columns x matching counterparts found STATS (exempting grouping columns) return x (columns order). keep.group_vars = FALSE, grouping columns dropped computation, however \"groups\" attribute dropped (can removed using fungroup() dplyr::ungroup()).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"x columns replaced swept using STATS, (optionally) grouped g.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"cases need call TRA() function, TRA-argument Fast Statistical Functions (ensuring exact grouping vector used computing statistics subsequent transformation). addition functions fbetween/B fwithin/W fscale/STD provide optimized solutions frequent scaling, centering averaging tasks.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/TRA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform Data by (Grouped) Replacing or Sweeping out Statistics — TRA","text":"","code":"v <- iris$Sepal.Length          # A numeric vector f <- iris$Species               # A factor dat <- num_vars(iris)           # Numeric columns m <- qM(dat)                    # Matrix of numeric data  head(TRA(v, fmean(v)))                # Simple centering [same as fmean(v, TRA = \"-\") or W(v)] #> [1] -0.7433333 -0.9433333 -1.1433333 -1.2433333 -0.8433333 -0.4433333 head(TRA(m, fmean(m)))                # [same as sweep(m, 2, fmean(m)), fmean(m, TRA = \"-\") or W(m)] #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]   -0.7433333  0.44266667       -2.358  -0.9993333 #> [2,]   -0.9433333 -0.05733333       -2.358  -0.9993333 #> [3,]   -1.1433333  0.14266667       -2.458  -0.9993333 #> [4,]   -1.2433333  0.04266667       -2.258  -0.9993333 #> [5,]   -0.8433333  0.54266667       -2.358  -0.9993333 #> [6,]   -0.4433333  0.84266667       -2.058  -0.7993333 head(TRA(dat, fmean(dat)))            # [same as fmean(dat, TRA = \"-\") or W(dat)] #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   -0.7433333  0.44266667       -2.358  -0.9993333 #> 2   -0.9433333 -0.05733333       -2.358  -0.9993333 #> 3   -1.1433333  0.14266667       -2.458  -0.9993333 #> 4   -1.2433333  0.04266667       -2.258  -0.9993333 #> 5   -0.8433333  0.54266667       -2.358  -0.9993333 #> 6   -0.4433333  0.84266667       -2.058  -0.7993333 head(TRA(v, fmean(v), \"replace\"))     # Simple replacing [same as fmean(v, TRA = \"replace\") or B(v)] #> [1] 5.843333 5.843333 5.843333 5.843333 5.843333 5.843333 head(TRA(m, fmean(m), \"replace\"))     # [same as sweep(m, 2, fmean(m)), fmean(m, TRA = 1L) or B(m)] #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]     5.843333    3.057333        3.758    1.199333 #> [2,]     5.843333    3.057333        3.758    1.199333 #> [3,]     5.843333    3.057333        3.758    1.199333 #> [4,]     5.843333    3.057333        3.758    1.199333 #> [5,]     5.843333    3.057333        3.758    1.199333 #> [6,]     5.843333    3.057333        3.758    1.199333 head(TRA(dat, fmean(dat), \"replace\")) # [same as fmean(dat, TRA = \"replace\") or B(dat)] #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.843333    3.057333        3.758    1.199333 #> 2     5.843333    3.057333        3.758    1.199333 #> 3     5.843333    3.057333        3.758    1.199333 #> 4     5.843333    3.057333        3.758    1.199333 #> 5     5.843333    3.057333        3.758    1.199333 #> 6     5.843333    3.057333        3.758    1.199333 head(TRA(m, fsd(m), \"/\"))             # Simple scaling... [same as fsd(m, TRA = \"/\")]... #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]     6.158928    8.029986    0.7930671   0.2623854 #> [2,]     5.917402    6.882845    0.7930671   0.2623854 #> [3,]     5.675875    7.341701    0.7364195   0.2623854 #> [4,]     5.555112    7.112273    0.8497148   0.2623854 #> [5,]     6.038165    8.259414    0.7930671   0.2623854 #> [6,]     6.521218    8.947698    0.9630101   0.5247707  # Note: All grouped examples also apply for v and dat... head(TRA(m, fmean(m, f), \"-\", f))       # Centering [same as fmean(m, f, TRA = \"-\") or W(m, f)] #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]        0.094       0.072       -0.062      -0.046 #> [2,]       -0.106      -0.428       -0.062      -0.046 #> [3,]       -0.306      -0.228       -0.162      -0.046 #> [4,]       -0.406      -0.328        0.038      -0.046 #> [5,]       -0.006       0.172       -0.062      -0.046 #> [6,]        0.394       0.472        0.238       0.154 head(TRA(m, fmean(m, f), \"replace\", f)) # Replacing [same fmean(m, f, TRA = \"replace\") or B(m, f)] #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]        5.006       3.428        1.462       0.246 #> [2,]        5.006       3.428        1.462       0.246 #> [3,]        5.006       3.428        1.462       0.246 #> [4,]        5.006       3.428        1.462       0.246 #> [5,]        5.006       3.428        1.462       0.246 #> [6,]        5.006       3.428        1.462       0.246 head(TRA(m, fsd(m, f), \"/\", f))         # Scaling [same as fsd(m, f, TRA = \"/\")] #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]     14.46851    9.233260     8.061544    1.897793 #> [2,]     13.90112    7.914223     8.061544    1.897793 #> [3,]     13.33372    8.441838     7.485720    1.897793 #> [4,]     13.05003    8.178031     8.637369    1.897793 #> [5,]     14.18481    9.497068     8.061544    1.897793 #> [6,]     15.31960   10.288490     9.789018    3.795585  head(TRA(m, fmean(m, f), \"-+\", f))      # Centering on the overall mean ... #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]     5.937333    3.129333        3.696    1.153333 #> [2,]     5.737333    2.629333        3.696    1.153333 #> [3,]     5.537333    2.829333        3.596    1.153333 #> [4,]     5.437333    2.729333        3.796    1.153333 #> [5,]     5.837333    3.229333        3.696    1.153333 #> [6,]     6.237333    3.529333        3.996    1.353333                                         # [same as fmean(m, f, TRA = \"-+\") or                                         #           W(m, f, mean = \"overall.mean\")] head(TRA(TRA(m, fmean(m, f), \"-\", f),   # Also the same thing done manually !!      fmean(m), \"+\")) #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]     5.937333    3.129333        3.696    1.153333 #> [2,]     5.737333    2.629333        3.696    1.153333 #> [3,]     5.537333    2.829333        3.596    1.153333 #> [4,]     5.437333    2.729333        3.796    1.153333 #> [5,]     5.837333    3.229333        3.696    1.153333 #> [6,]     6.237333    3.529333        3.996    1.353333  # Grouped data method library(magrittr) iris %>% fgroup_by(Species) %>% TRA(fmean(.)) #>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1         0.094       0.072       -0.062      -0.046  setosa #> 2        -0.106      -0.428       -0.062      -0.046  setosa #> 3        -0.306      -0.228       -0.162      -0.046  setosa #> 4        -0.406      -0.328        0.038      -0.046  setosa #> 5        -0.006       0.172       -0.062      -0.046  setosa #> 6         0.394       0.472        0.238       0.154  setosa #> 7        -0.406      -0.028       -0.062       0.054  setosa #> 8        -0.006      -0.028        0.038      -0.046  setosa #> 9        -0.606      -0.528       -0.062      -0.046  setosa #> 10       -0.106      -0.328        0.038      -0.146  setosa #> 11        0.394       0.272        0.038      -0.046  setosa #> 12       -0.206      -0.028        0.138      -0.046  setosa #> 13       -0.206      -0.428       -0.062      -0.146  setosa #> 14       -0.706      -0.428       -0.362      -0.146  setosa #>  [ reached 'max' / getOption(\"max.print\") -- omitted 136 rows ] #>  #> Grouped by:  Species  [3 | 50 (0)]  iris %>% fgroup_by(Species) %>% fmean(TRA = \"-\")        # Same thing #>    Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   setosa        0.094       0.072       -0.062      -0.046 #> 2   setosa       -0.106      -0.428       -0.062      -0.046 #> 3   setosa       -0.306      -0.228       -0.162      -0.046 #> 4   setosa       -0.406      -0.328        0.038      -0.046 #> 5   setosa       -0.006       0.172       -0.062      -0.046 #> 6   setosa        0.394       0.472        0.238       0.154 #> 7   setosa       -0.406      -0.028       -0.062       0.054 #> 8   setosa       -0.006      -0.028        0.038      -0.046 #> 9   setosa       -0.606      -0.528       -0.062      -0.046 #> 10  setosa       -0.106      -0.328        0.038      -0.146 #> 11  setosa        0.394       0.272        0.038      -0.046 #> 12  setosa       -0.206      -0.028        0.138      -0.046 #> 13  setosa       -0.206      -0.428       -0.062      -0.146 #> 14  setosa       -0.706      -0.428       -0.362      -0.146 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 136 rows ] #>  #> Grouped by:  Species  [3 | 50 (0)]  iris %>% fgroup_by(Species) %>% TRA(fmean(.)[c(2,4)])   # Only transforming 2 columns #>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1         0.094         3.5       -0.062         0.2  setosa #> 2        -0.106         3.0       -0.062         0.2  setosa #> 3        -0.306         3.2       -0.162         0.2  setosa #> 4        -0.406         3.1        0.038         0.2  setosa #> 5        -0.006         3.6       -0.062         0.2  setosa #> 6         0.394         3.9        0.238         0.4  setosa #> 7        -0.406         3.4       -0.062         0.3  setosa #> 8        -0.006         3.4        0.038         0.2  setosa #> 9        -0.606         2.9       -0.062         0.2  setosa #> 10       -0.106         3.1        0.038         0.1  setosa #> 11        0.394         3.7        0.038         0.2  setosa #> 12       -0.206         3.4        0.138         0.2  setosa #> 13       -0.206         3.0       -0.062         0.1  setosa #> 14       -0.706         3.0       -0.362         0.1  setosa #>  [ reached 'max' / getOption(\"max.print\") -- omitted 136 rows ] #>  #> Grouped by:  Species  [3 | 50 (0)]  iris %>% fgroup_by(Species) %>% TRA(fmean(.)[c(2,4)],   # Dropping species column                                         keep.group_vars = FALSE) #>    Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1         0.094         3.5       -0.062         0.2 #> 2        -0.106         3.0       -0.062         0.2 #> 3        -0.306         3.2       -0.162         0.2 #> 4        -0.406         3.1        0.038         0.2 #> 5        -0.006         3.6       -0.062         0.2 #> 6         0.394         3.9        0.238         0.4 #> 7        -0.406         3.4       -0.062         0.3 #> 8        -0.006         3.4        0.038         0.2 #> 9        -0.606         2.9       -0.062         0.2 #> 10       -0.106         3.1        0.038         0.1 #> 11        0.394         3.7        0.038         0.2 #> 12       -0.206         3.4        0.138         0.2 #> 13       -0.206         3.0       -0.062         0.1 #> 14       -0.706         3.0       -0.362         0.1 #> 15        0.794         4.0       -0.262         0.2 #> 16        0.694         4.4        0.038         0.4 #> 17        0.394         3.9       -0.162         0.4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 133 rows ] #>  #> Grouped by:  Species  [3 | 50 (0)]"},{"path":"https://sebkrantz.github.io/collapse/reference/across.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Functions Across Multiple Columns — across","title":"Apply Functions Across Multiple Columns — across","text":"across() can used inside fmutate fsummarise apply one functions selection columns. overall similar dplyr::across, support rlang features, additional features (arguments), optimized work collapse's, .FAST_FUN, yielding much faster computations.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/across.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Functions Across Multiple Columns — across","text":"","code":"across(.cols = NULL, .fns, ..., .names = NULL,        .apply = \"auto\", .transpose = \"auto\")  # acr(...) can be used to abbreviate across(...)"},{"path":"https://sebkrantz.github.io/collapse/reference/across.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Functions Across Multiple Columns — across","text":".cols select columns using column names expressions (e.g. :b c(, b, c:f)), column indices, logical vectors, functions yielding logical value e.g. .numeric. NULL applies functions columns except grouping columns. .fns function, character vector functions list functions. Vectors / lists can named yield alternative names result (see .names). argument evaluated inside substitute(), content (names vectors/lists) checked .FAST_FUN .OPERATOR_FUN. Matching functions receive vectorized execution, functions applied data standard way. ... arguments .fns. Arguments evaluated data environment split groups well (non-vectorized functions, length data). .names controls naming computed columns. NULL generates names form coli_funj multiple functions used. .names = TRUE enables single function, .names = FALSE disables multiple functions (sensible functions .OPERATOR_FUN rename columns (.apply = FALSE)). Setting .names = \"flip\" generates names form funj_coli. also possible supply function two arguments column function names e.g. function(c, f) paste0(f, \"_\", c). Finally, can supply custom vector names must match length(.cols) * length(.fns). .apply controls whether functions applied column--column (TRUE) multiple columns (FALSE). default, \"auto\", latter vectorized functions, efficient data frame method. can also sensible use .apply = FALSE non-vectorized functions, especially multivariate functions like lm pwcor, functions renaming data. See Examples. .transpose multiple .fns, .transpose controls whether result ordered first column, function (TRUE), vice-versa (FALSE). \"auto\" former functions yield results dimensions (dimensions may differ .apply = FALSE). See Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/across.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Apply Functions Across Multiple Columns — across","text":"across() support purr-style lambdas, support dplyr-style predicate functions e.g. across((.numeric), sum), simply use across(.numeric, sum). contrast dplyr, can also compute grouping columns. Also note across() function collapse known expression internally transformed fsummarise()/fmutate() something else. Thus, called using qualified names, .e., collapse::across() work necessary collapse attached.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/across.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Functions Across Multiple Columns — across","text":"","code":"# Basic (Weighted) Summaries fsummarise(wlddev, across(PCGDP:GINI, fmean, w = POP)) #>      PCGDP   LIFEEX     GINI #> 1 7956.238 65.88068 39.52428  wlddev |> fgroup_by(region, income) |>     fsummarise(across(PCGDP:GINI, fmean, w = POP)) #>                        region              income      PCGDP   LIFEEX     GINI #> 1         East Asia & Pacific         High income 29172.7552 76.83283 32.79182 #> 2         East Asia & Pacific Lower middle income  1756.6480 64.25623 36.07647 #> 3         East Asia & Pacific Upper middle income  2357.6168 68.40768 39.94810 #> 4       Europe & Central Asia         High income 29335.5511 75.66616 32.25404 #> 5       Europe & Central Asia          Low income   803.2234 62.45228 32.22326 #> 6       Europe & Central Asia Lower middle income  2256.9684 68.48909 28.97857 #> 7       Europe & Central Asia Upper middle income  7772.5035 68.01573 38.70512 #> 8   Latin America & Caribbean         High income 10217.0626 73.04484 49.41109 #> 9   Latin America & Caribbean          Low income  1317.9024 55.45075 41.10000 #> 10  Latin America & Caribbean Lower middle income  1913.8993 63.86360 50.65115 #> 11  Latin America & Caribbean Upper middle income  7564.8294 69.46947 52.90072 #> 12 Middle East & North Africa         High income 25889.0715 72.38335 36.93006 #> 13 Middle East & North Africa          Low income  1049.8255 63.62748 35.89218 #> 14 Middle East & North Africa Lower middle income  2015.0739 65.55189 33.21199 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ]  # Note that for these we don't actually need across... fselect(wlddev, PCGDP:GINI) |> fmean(w = wlddev$POP, drop = FALSE) #>      PCGDP   LIFEEX     GINI #> 1 7956.238 65.88068 39.52428 wlddev |> fgroup_by(region, income) |>     fselect(PCGDP:GINI, POP) |> fmean(POP, keep.w = FALSE) #>                        region              income      PCGDP   LIFEEX     GINI #> 1         East Asia & Pacific         High income 29172.7552 76.83283 32.79182 #> 2         East Asia & Pacific Lower middle income  1756.6480 64.25623 36.07647 #> 3         East Asia & Pacific Upper middle income  2357.6168 68.40768 39.94810 #> 4       Europe & Central Asia         High income 29335.5511 75.66616 32.25404 #> 5       Europe & Central Asia          Low income   803.2234 62.45228 32.22326 #> 6       Europe & Central Asia Lower middle income  2256.9684 68.48909 28.97857 #> 7       Europe & Central Asia Upper middle income  7772.5035 68.01573 38.70512 #> 8   Latin America & Caribbean         High income 10217.0626 73.04484 49.41109 #> 9   Latin America & Caribbean          Low income  1317.9024 55.45075 41.10000 #> 10  Latin America & Caribbean Lower middle income  1913.8993 63.86360 50.65115 #> 11  Latin America & Caribbean Upper middle income  7564.8294 69.46947 52.90072 #> 12 Middle East & North Africa         High income 25889.0715 72.38335 36.93006 #> 13 Middle East & North Africa          Low income  1049.8255 63.62748 35.89218 #> 14 Middle East & North Africa Lower middle income  2015.0739 65.55189 33.21199 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ] collap(wlddev, PCGDP + LIFEEX + GINI ~ region + income, w = ~ POP, keep.w = FALSE) #>                        region              income      PCGDP   LIFEEX     GINI #> 1         East Asia & Pacific         High income 29172.7552 76.83283 32.79182 #> 2         East Asia & Pacific Lower middle income  1756.6480 64.25623 36.07647 #> 3         East Asia & Pacific Upper middle income  2357.6168 68.40768 39.94810 #> 4       Europe & Central Asia         High income 29335.5511 75.66616 32.25404 #> 5       Europe & Central Asia          Low income   803.2234 62.45228 32.22326 #> 6       Europe & Central Asia Lower middle income  2256.9684 68.48909 28.97857 #> 7       Europe & Central Asia Upper middle income  7772.5035 68.01573 38.70512 #> 8   Latin America & Caribbean         High income 10217.0626 73.04484 49.41109 #> 9   Latin America & Caribbean          Low income  1317.9024 55.45075 41.10000 #> 10  Latin America & Caribbean Lower middle income  1913.8993 63.86360 50.65115 #> 11  Latin America & Caribbean Upper middle income  7564.8294 69.46947 52.90072 #> 12 Middle East & North Africa         High income 25889.0715 72.38335 36.93006 #> 13 Middle East & North Africa          Low income  1049.8255 63.62748 35.89218 #> 14 Middle East & North Africa Lower middle income  2015.0739 65.55189 33.21199 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ]  # But if we want to use some base R function that reguires argument splitting... wlddev |> na_omit(cols = \"POP\") |> fgroup_by(region, income) |>     fsummarise(across(PCGDP:GINI, weighted.mean, w = POP, na.rm = TRUE)) #>                        region              income      PCGDP   LIFEEX     GINI #> 1         East Asia & Pacific         High income 29172.7552 76.83283 32.79182 #> 2         East Asia & Pacific Lower middle income  1756.6480 64.25623 36.07647 #> 3         East Asia & Pacific Upper middle income  2357.6168 68.40768 39.94810 #> 4       Europe & Central Asia         High income 29335.5511 75.66616 32.25404 #> 5       Europe & Central Asia          Low income   803.2234 62.45228 32.22326 #> 6       Europe & Central Asia Lower middle income  2256.9684 68.48909 28.97857 #> 7       Europe & Central Asia Upper middle income  7772.5035 68.01573 38.70512 #> 8   Latin America & Caribbean         High income 10217.0626 73.04484 49.41109 #> 9   Latin America & Caribbean          Low income  1317.9024 55.45075 41.10000 #> 10  Latin America & Caribbean Lower middle income  1913.8993 63.86360 50.65115 #> 11  Latin America & Caribbean Upper middle income  7564.8294 69.46947 52.90072 #> 12 Middle East & North Africa         High income 25889.0715 72.38335 36.93006 #> 13 Middle East & North Africa          Low income  1049.8255 63.62748 35.89218 #> 14 Middle East & North Africa Lower middle income  2015.0739 65.55189 33.21199 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ]  # Or if we want to apply different functions... wlddev |> fgroup_by(region, income) |>     fsummarise(across(PCGDP:GINI, list(mu = fmean, sd = fsd), w = POP),                POP_sum = fsum(POP), OECD = fmean(OECD)) #>                  region              income   PCGDP_mu   PCGDP_sd LIFEEX_mu #> 1   East Asia & Pacific         High income 29172.7552 14714.1754  76.83283 #> 2   East Asia & Pacific Lower middle income  1756.6480  1064.2676  64.25623 #> 3   East Asia & Pacific Upper middle income  2357.6168  2457.9024  68.40768 #> 4 Europe & Central Asia         High income 29335.5511 13038.1111  75.66616 #> 5 Europe & Central Asia          Low income   803.2234   307.7395  62.45228 #> 6 Europe & Central Asia Lower middle income  2256.9684   970.2648  68.48909 #> 7 Europe & Central Asia Upper middle income  7772.5035  3184.4987  68.01573 #>   LIFEEX_sd  GINI_mu  GINI_sd     POP_sum      OECD #> 1  5.964994 32.79182 1.230489 11407808149 0.3076923 #> 2  7.536813 36.07647 4.358228 22174820629 0.0000000 #> 3  7.689033 39.94810 3.120103 69639871478 0.0000000 #> 4  4.175866 32.25404 3.023778 27285316560 0.7027027 #> 5  6.050875 32.22326 1.547793   311485944 0.0000000 #> 6  2.452041 28.97857 4.573107  4511786205 0.0000000 #> 7  4.796135 38.70512 4.233085 16972478305 0.0625000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 16 rows ] # Note that the above still detects fmean as a fast function, the names of the list # are irrelevant, but the function name must be typed or passed as a character vector, # Otherwise functions will be executed by groups e.g. function(x) fmean(x) won't vectorize  # Same, naming in a different way wlddev |> fgroup_by(region, income) |>     fsummarise(across(PCGDP:GINI, list(mu = fmean, sd = fsd), w = POP, .names = \"flip\"),                sum_POP = fsum(POP), OECD = fmean(OECD)) #>                  region              income   mu_PCGDP   sd_PCGDP mu_LIFEEX #> 1   East Asia & Pacific         High income 29172.7552 14714.1754  76.83283 #> 2   East Asia & Pacific Lower middle income  1756.6480  1064.2676  64.25623 #> 3   East Asia & Pacific Upper middle income  2357.6168  2457.9024  68.40768 #> 4 Europe & Central Asia         High income 29335.5511 13038.1111  75.66616 #> 5 Europe & Central Asia          Low income   803.2234   307.7395  62.45228 #> 6 Europe & Central Asia Lower middle income  2256.9684   970.2648  68.48909 #> 7 Europe & Central Asia Upper middle income  7772.5035  3184.4987  68.01573 #>   sd_LIFEEX  mu_GINI  sd_GINI     sum_POP      OECD #> 1  5.964994 32.79182 1.230489 11407808149 0.3076923 #> 2  7.536813 36.07647 4.358228 22174820629 0.0000000 #> 3  7.689033 39.94810 3.120103 69639871478 0.0000000 #> 4  4.175866 32.25404 3.023778 27285316560 0.7027027 #> 5  6.050875 32.22326 1.547793   311485944 0.0000000 #> 6  2.452041 28.97857 4.573107  4511786205 0.0000000 #> 7  4.796135 38.70512 4.233085 16972478305 0.0625000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 16 rows ]  # Or we want to do more advanced things.. # Such as nesting data frames.. qTBL(wlddev) |> fgroup_by(region, income) |>     fsummarise(across(c(PCGDP, LIFEEX, ODA),                function(x) list(Nest = list(x)),                .apply = FALSE)) #> # A tibble: 23 × 3 #>    region                    income              Nest                 #>    <fct>                     <fct>               <list>               #>  1 East Asia & Pacific       High income         <tibble [793 × 3]>   #>  2 East Asia & Pacific       Lower middle income <tibble [793 × 3]>   #>  3 East Asia & Pacific       Upper middle income <tibble [610 × 3]>   #>  4 Europe & Central Asia     High income         <tibble [2,257 × 3]> #>  5 Europe & Central Asia     Low income          <tibble [61 × 3]>    #>  6 Europe & Central Asia     Lower middle income <tibble [244 × 3]>   #>  7 Europe & Central Asia     Upper middle income <tibble [976 × 3]>   #>  8 Latin America & Caribbean High income         <tibble [1,037 × 3]> #>  9 Latin America & Caribbean Low income          <tibble [61 × 3]>    #> 10 Latin America & Caribbean Lower middle income <tibble [244 × 3]>   #> # ℹ 13 more rows # Or linear models.. qTBL(wlddev) |> fgroup_by(region, income) |>     fsummarise(across(c(PCGDP, LIFEEX, ODA),                function(x) list(Mods = list(lm(PCGDP ~., x))),                .apply = FALSE)) #> # A tibble: 23 × 3 #>    region                    income              Mods   #>    <fct>                     <fct>               <list> #>  1 East Asia & Pacific       High income         <lm>   #>  2 East Asia & Pacific       Lower middle income <lm>   #>  3 East Asia & Pacific       Upper middle income <lm>   #>  4 Europe & Central Asia     High income         <lm>   #>  5 Europe & Central Asia     Low income          <lm>   #>  6 Europe & Central Asia     Lower middle income <lm>   #>  7 Europe & Central Asia     Upper middle income <lm>   #>  8 Latin America & Caribbean High income         <lm>   #>  9 Latin America & Caribbean Low income          <lm>   #> 10 Latin America & Caribbean Lower middle income <lm>   #> # ℹ 13 more rows # Or cumputing grouped correlation matrices qTBL(wlddev) |> fgroup_by(region, income) |>     fsummarise(across(c(PCGDP, LIFEEX, ODA),       function(x) qDF(pwcor(x), \"Variable\"), .apply = FALSE)) #> # A tibble: 69 × 6 #>    region                income              Variable  PCGDP  LIFEEX     ODA #>    <fct>                 <fct>               <chr>     <dbl>   <dbl>   <dbl> #>  1 East Asia & Pacific   High income         PCGDP     1      0.662  -0.388  #>  2 East Asia & Pacific   High income         LIFEEX    0.662  1      -0.444  #>  3 East Asia & Pacific   High income         ODA      -0.388 -0.444   1      #>  4 East Asia & Pacific   Lower middle income PCGDP     1      0.395  -0.146  #>  5 East Asia & Pacific   Lower middle income LIFEEX    0.395  1       0.206  #>  6 East Asia & Pacific   Lower middle income ODA      -0.146  0.206   1      #>  7 East Asia & Pacific   Upper middle income PCGDP     1      0.700  -0.378  #>  8 East Asia & Pacific   Upper middle income LIFEEX    0.700  1       0.0796 #>  9 East Asia & Pacific   Upper middle income ODA      -0.378  0.0796  1      #> 10 Europe & Central Asia High income         PCGDP     1      0.586  -0.329  #> # ℹ 59 more rows  # Here calculating 1- and 10-year lags and growth rates of these variables qTBL(wlddev) |> fgroup_by(country) |>     fmutate(across(c(PCGDP, LIFEEX, ODA), list(L, G),                    n = c(1, 10), t = year, .names = FALSE)) #> # A tibble: 13,176 × 25 #>    country  iso3c date        year decade region income OECD  PCGDP LIFEEX  GINI #>    <chr>    <fct> <date>     <int>  <int> <fct>  <fct>  <lgl> <dbl>  <dbl> <dbl> #>  1 Afghani… AFG   1961-01-01  1960   1960 South… Low i… FALSE    NA   32.4    NA #>  2 Afghani… AFG   1962-01-01  1961   1960 South… Low i… FALSE    NA   33.0    NA #>  3 Afghani… AFG   1963-01-01  1962   1960 South… Low i… FALSE    NA   33.5    NA #>  4 Afghani… AFG   1964-01-01  1963   1960 South… Low i… FALSE    NA   34.0    NA #>  5 Afghani… AFG   1965-01-01  1964   1960 South… Low i… FALSE    NA   34.5    NA #>  6 Afghani… AFG   1966-01-01  1965   1960 South… Low i… FALSE    NA   34.9    NA #>  7 Afghani… AFG   1967-01-01  1966   1960 South… Low i… FALSE    NA   35.4    NA #>  8 Afghani… AFG   1968-01-01  1967   1960 South… Low i… FALSE    NA   35.9    NA #>  9 Afghani… AFG   1969-01-01  1968   1960 South… Low i… FALSE    NA   36.4    NA #> 10 Afghani… AFG   1970-01-01  1969   1960 South… Low i… FALSE    NA   36.9    NA #> # ℹ 13,166 more rows #> # ℹ 14 more variables: ODA <dbl>, POP <dbl>, L1.PCGDP <dbl>, G1.PCGDP <dbl>, #> #   L10.PCGDP <dbl>, L10G1.PCGDP <dbl>, L1.LIFEEX <dbl>, G1.LIFEEX <dbl>, #> #   L10.LIFEEX <dbl>, L10G1.LIFEEX <dbl>, L1.ODA <dbl>, G1.ODA <dbl>, #> #   L10.ODA <dbl>, L10G1.ODA <dbl> #>  #> Grouped by:  country  [216 | 61 (0)]   # Same but variables in different order qTBL(wlddev) |> fgroup_by(country) |>     fmutate(across(c(PCGDP, LIFEEX, ODA), list(L, G), n = c(1, 10),                    t = year, .names = FALSE, .transpose = FALSE)) #> # A tibble: 13,176 × 25 #>    country  iso3c date        year decade region income OECD  PCGDP LIFEEX  GINI #>    <chr>    <fct> <date>     <int>  <int> <fct>  <fct>  <lgl> <dbl>  <dbl> <dbl> #>  1 Afghani… AFG   1961-01-01  1960   1960 South… Low i… FALSE    NA   32.4    NA #>  2 Afghani… AFG   1962-01-01  1961   1960 South… Low i… FALSE    NA   33.0    NA #>  3 Afghani… AFG   1963-01-01  1962   1960 South… Low i… FALSE    NA   33.5    NA #>  4 Afghani… AFG   1964-01-01  1963   1960 South… Low i… FALSE    NA   34.0    NA #>  5 Afghani… AFG   1965-01-01  1964   1960 South… Low i… FALSE    NA   34.5    NA #>  6 Afghani… AFG   1966-01-01  1965   1960 South… Low i… FALSE    NA   34.9    NA #>  7 Afghani… AFG   1967-01-01  1966   1960 South… Low i… FALSE    NA   35.4    NA #>  8 Afghani… AFG   1968-01-01  1967   1960 South… Low i… FALSE    NA   35.9    NA #>  9 Afghani… AFG   1969-01-01  1968   1960 South… Low i… FALSE    NA   36.4    NA #> 10 Afghani… AFG   1970-01-01  1969   1960 South… Low i… FALSE    NA   36.9    NA #> # ℹ 13,166 more rows #> # ℹ 14 more variables: ODA <dbl>, POP <dbl>, L1.PCGDP <dbl>, L10.PCGDP <dbl>, #> #   L1.LIFEEX <dbl>, L10.LIFEEX <dbl>, L1.ODA <dbl>, L10.ODA <dbl>, #> #   G1.PCGDP <dbl>, L10G1.PCGDP <dbl>, G1.LIFEEX <dbl>, L10G1.LIFEEX <dbl>, #> #   G1.ODA <dbl>, L10G1.ODA <dbl> #>  #> Grouped by:  country  [216 | 61 (0)]"},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"Fast operators perform row- column-wise replacing sweeping operations vectors matrices, data frames, lists. See also setop math reference setTRA sweeping reference.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"","code":"## Perform the operation with v and each row of X  X %rr% v    # Replace rows of X with v X %r+% v    # Add v to each row of X X %r-% v    # Subtract v from each row of X X %r*% v    # Multiply each row of X with v X %r/% v    # Divide each row of X by v  ## Perform a column-wise operation between V and X  X %cr% V    # Replace columns of X with V X %c+% V    # Add V to columns of X X %c-% V    # Subtract V from columns of X X %c*% V    # Multiply columns of X with V X %c/% V    # Divide columns of X by V"},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"X vector, matrix, data frame list like object (rows (r) columns (c) matching v / V). v row operations: atomic vector matching NCOL(X). X data frame, v can also list scalar atomic elements. also possible sweep lists vectors v lists matrices data frames X. V column operations: suitable scalar, vector, matrix / data frame matching NROW(X). X can also list vectors / matrices case V can scalar / vector / matrix matching list scalars / vectors / matrices.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"matrix data frame X, default behavior R calling X op v (multiplication X * v) perform operation v column X. equivalent operation performed X %cop% V, difference computes significantly faster X/V data frame / list. complex frequently required task perform operation v row X. provided based efficient C++ code %rop% set functions, e.g. X %r*% v efficiently multiplies v row X.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"X operation v / V performed row column. attributes X preserved.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"Computations Output: functions quite simple, work X LHS .e. v %op% X likely fail. row operations simple wrappers around TRA provides operations including grouped replacing sweeping (v matrix data frame less rows X mapped rows X grouping vectors). One consequence just like TRA, row-wise mathematical operations (+, -, *, /) always yield numeric output, even X v may integer. different column- operations depend base R may also preserve integer data. Rules Arithmetic: Since operators defined simple infix functions, normal rules arithmetic respected. %c+% b %c*% c evaluates (%c+% b) %c*% c. chained infix operations, just evaluated sequentially left right. Performance Notes: function setop related set %op=% operators well setTRA function can used perform operations reference, faster copies output required!! Furthermore, Fast Statistical Functions, using fmedian(X, TRA = \"-\") tiny bit faster X %r-% fmedian(X). Also use fwithin(X) fast centering using mean, fscale(X) fast scaling centering mean-preserving scaling.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/arithmetic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Row/Column Arithmetic for Matrix-Like Objects — arithmetic","text":"","code":"## Using data frame's / lists v <- mtcars$cyl mtcars %cr% v #>                   mpg cyl disp hp drat wt qsec vs am gear carb #> Mazda RX4           6   6    6  6    6  6    6  6  6    6    6 #> Mazda RX4 Wag       6   6    6  6    6  6    6  6  6    6    6 #> Datsun 710          4   4    4  4    4  4    4  4  4    4    4 #> Hornet 4 Drive      6   6    6  6    6  6    6  6  6    6    6 #> Hornet Sportabout   8   8    8  8    8  8    8  8  8    8    8 #> Valiant             6   6    6  6    6  6    6  6  6    6    6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] mtcars %c-% v #>                    mpg cyl disp  hp  drat     wt  qsec vs am gear carb #> Mazda RX4         15.0   0  154 104 -2.10 -3.380 10.46 -6 -5   -2   -2 #> Mazda RX4 Wag     15.0   0  154 104 -2.10 -3.125 11.02 -6 -5   -2   -2 #> Datsun 710        18.8   0  104  89 -0.15 -1.680 14.61 -3 -3    0   -3 #> Hornet 4 Drive    15.4   0  252 104 -2.92 -2.785 13.44 -5 -6   -3   -5 #> Hornet Sportabout 10.7   0  352 167 -4.85 -4.560  9.02 -8 -8   -5   -6 #> Valiant           12.1   0  219  99 -3.24 -2.540 14.22 -5 -6   -3   -5 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] mtcars %r-% seq_col(mtcars) #>                    mpg cyl disp  hp  drat     wt  qsec vs am gear carb #> Mazda RX4         20.0   4  157 106 -1.10 -3.380  9.46 -8 -8   -6   -7 #> Mazda RX4 Wag     20.0   4  157 106 -1.10 -3.125 10.02 -8 -8   -6   -7 #> Datsun 710        21.8   2  105  89 -1.15 -3.680 11.61 -7 -8   -6  -10 #> Hornet 4 Drive    20.4   4  255 106 -1.92 -2.785 12.44 -7 -9   -7  -10 #> Hornet Sportabout 17.7   6  357 171 -1.85 -2.560 10.02 -8 -9   -7   -9 #> Valiant           17.1   4  222 101 -2.24 -2.540 13.22 -7 -9   -7  -10 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] mtcars %r-% lapply(mtcars, quantile, 0.28) #>                     mpg cyl    disp    hp    drat     wt    qsec vs am gear #> Mazda RX4         5.296   2  25.536  7.56  0.7724 -0.102 -0.5216  0  1    1 #> Mazda RX4 Wag     5.296   2  25.536  7.56  0.7724  0.153  0.0384  0  1    1 #> Datsun 710        7.096   0 -26.464 -9.44  0.7224 -0.402  1.6284  1  1    1 #> Hornet 4 Drive    5.696   2 123.536  7.56 -0.0476  0.493  2.4584  1  0    0 #> Hornet Sportabout 2.996   4 225.536 72.56  0.0224  0.718  0.0384  0  0    0 #> Valiant           2.396   2  90.536  2.56 -0.3676  0.738  3.2384  1  0    0 #>                   carb #> Mazda RX4            2 #> Mazda RX4 Wag        2 #> Datsun 710          -1 #> Hornet 4 Drive      -1 #> Hornet Sportabout    0 #> Valiant             -1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  mtcars %c*% 5       # Significantly faster than mtcars * 5 #>                     mpg cyl disp  hp  drat     wt   qsec vs am gear carb #> Mazda RX4         105.0  30  800 550 19.50 13.100  82.30  0  5   20   20 #> Mazda RX4 Wag     105.0  30  800 550 19.50 14.375  85.10  0  5   20   20 #> Datsun 710        114.0  20  540 465 19.25 11.600  93.05  5  5   20    5 #> Hornet 4 Drive    107.0  30 1290 550 15.40 16.075  97.20  5  0   15    5 #> Hornet Sportabout  93.5  40 1800 875 15.75 17.200  85.10  0  0   15   10 #> Valiant            90.5  30 1125 525 13.80 17.300 101.10  5  0   15    5 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] mtcars %c*% mtcars  # Significantly faster than mtcars * mtcars #>                      mpg cyl   disp    hp    drat        wt     qsec vs am gear #> Mazda RX4         441.00  36  25600 12100 15.2100  6.864400 270.9316  0  1   16 #> Mazda RX4 Wag     441.00  36  25600 12100 15.2100  8.265625 289.6804  0  1   16 #> Datsun 710        519.84  16  11664  8649 14.8225  5.382400 346.3321  1  1   16 #> Hornet 4 Drive    457.96  36  66564 12100  9.4864 10.336225 377.9136  1  0    9 #> Hornet Sportabout 349.69  64 129600 30625  9.9225 11.833600 289.6804  0  0    9 #> Valiant           327.61  36  50625 11025  7.6176 11.971600 408.8484  1  0    9 #>                   carb #> Mazda RX4           16 #> Mazda RX4 Wag       16 #> Datsun 710           1 #> Hornet 4 Drive       1 #> Hornet Sportabout    4 #> Valiant              1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  ## Using matrices X <- qM(mtcars) X %cr% v #>                     mpg cyl disp hp drat wt qsec vs am gear carb #> Mazda RX4             6   6    6  6    6  6    6  6  6    6    6 #> Mazda RX4 Wag         6   6    6  6    6  6    6  6  6    6    6 #> Datsun 710            4   4    4  4    4  4    4  4  4    4    4 #> Hornet 4 Drive        6   6    6  6    6  6    6  6  6    6    6 #> Hornet Sportabout     8   8    8  8    8  8    8  8  8    8    8 #> Valiant               6   6    6  6    6  6    6  6  6    6    6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] X %c-% v #>                      mpg cyl  disp  hp  drat     wt  qsec vs am gear carb #> Mazda RX4           15.0   0 154.0 104 -2.10 -3.380 10.46 -6 -5   -2   -2 #> Mazda RX4 Wag       15.0   0 154.0 104 -2.10 -3.125 11.02 -6 -5   -2   -2 #> Datsun 710          18.8   0 104.0  89 -0.15 -1.680 14.61 -3 -3    0   -3 #> Hornet 4 Drive      15.4   0 252.0 104 -2.92 -2.785 13.44 -5 -6   -3   -5 #> Hornet Sportabout   10.7   0 352.0 167 -4.85 -4.560  9.02 -8 -8   -5   -6 #> Valiant             12.1   0 219.0  99 -3.24 -2.540 14.22 -5 -6   -3   -5 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] X %r-% dapply(X, quantile, 0.28) #>                        mpg cyl    disp     hp    drat     wt    qsec vs am gear #> Mazda RX4            5.296   2  25.536   7.56  0.7724 -0.102 -0.5216  0  1    1 #> Mazda RX4 Wag        5.296   2  25.536   7.56  0.7724  0.153  0.0384  0  1    1 #> Datsun 710           7.096   0 -26.464  -9.44  0.7224 -0.402  1.6284  1  1    1 #> Hornet 4 Drive       5.696   2 123.536   7.56 -0.0476  0.493  2.4584  1  0    0 #> Hornet Sportabout    2.996   4 225.536  72.56  0.0224  0.718  0.0384  0  0    0 #> Valiant              2.396   2  90.536   2.56 -0.3676  0.738  3.2384  1  0    0 #>                     carb #> Mazda RX4              2 #> Mazda RX4 Wag          2 #> Datsun 710            -1 #> Hornet 4 Drive        -1 #> Hornet Sportabout      0 #> Valiant               -1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  ## Chained Operations library(magrittr) # Needed here to evaluate infix operators in sequence mtcars %>% fwithin() %r-% rnorm(11) %c*% 5 %>%     tfm(mpg = fsum(mpg)) %>% qsu() #>        N       Mean        SD        Min        Max #> mpg   32  -245.2386         0  -245.2386  -245.2386 #> cyl   32    -2.1457    8.9296   -13.0832     6.9168 #> disp  32    -0.6105  619.6935  -798.7199  1205.7801 #> hp    32     5.6901  342.8143  -467.7474   947.2526 #> drat  32     2.7901    2.6734    -1.3927     9.4573 #> wt    32    -5.2627    4.8923   -13.7839     5.7711 #> qsec  32    -3.3884    8.9347   -20.1322    21.8678 #> vs    32    -0.1925    2.5201      -2.38       2.62 #> am    32     1.7819     2.495    -0.2493     4.7507 #> gear  32    -3.9142     3.689    -7.3517     2.6483 #> carb  32    -4.0221     8.076   -13.0846    21.9154"},{"path":"https://sebkrantz.github.io/collapse/reference/collap.html","id":null,"dir":"Reference","previous_headings":"","what":"Advanced Data Aggregation — collap","title":"Advanced Data Aggregation — collap","text":"collap fast versatile multi-purpose data aggregation command. performs simple weighted aggregations, multi-type aggregations automatically applying different functions numeric categorical columns, multi-function aggregations applying multiple functions column, fully custom aggregations user passes list mapping functions columns.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Advanced Data Aggregation — collap","text":"","code":"# Main function: allows formula and data input to `by` and `w` arguments collap(X, by, FUN = fmean, catFUN = fmode, cols = NULL, w = NULL, wFUN = fsum,        custom = NULL, ..., keep.by = TRUE, keep.w = TRUE, keep.col.order = TRUE,        sort = .op[[\"sort\"]], decreasing = FALSE, na.last = TRUE, return.order = sort,        method = \"auto\", parallel = FALSE, mc.cores = 2L,        return = c(\"wide\",\"list\",\"long\",\"long_dupl\"), give.names = \"auto\")  # Programmer function: allows column names and indices input to `by` and `w` arguments collapv(X, by, FUN = fmean, catFUN = fmode, cols = NULL, w = NULL, wFUN = fsum,         custom = NULL, ..., keep.by = TRUE, keep.w = TRUE, keep.col.order = TRUE,         sort = .op[[\"sort\"]], decreasing = FALSE, na.last = TRUE, return.order = sort,         method = \"auto\", parallel = FALSE, mc.cores = 2L,         return = c(\"wide\",\"list\",\"long\",\"long_dupl\"), give.names = \"auto\")  # Auxiliary function: for grouped data ('grouped_df') input + non-standard evaluation collapg(X, FUN = fmean, catFUN = fmode, cols = NULL, w = NULL, wFUN = fsum,         custom = NULL, keep.group_vars = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/collap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Advanced Data Aggregation — collap","text":"X data frame, object coercible data frame using qDF. collap: one-two sided formula, .e. ~ group1 var1 + var2 ~ group1 + group2, atomic vector, list vectors GRP object used group X. collapv: names indices grouping columns, logical vector selector function is_categorical selecting grouping columns. FUN function, list functions (.e. list(fsum, fmean, fsd) list(sd = fsd, myfun1 = function(x)..)), character vector function names, automatically applied numeric variables. catFUN FUN, applied categorical (non-numeric) typed columns (is_categorical). cols select columns aggregate using function, column names, indices logical vector. Note: cols ignored two-sided formula passed . w weights. Can passed numeric vector alternatively formula .e. ~ weightvar collap column name / index etc. .e. \"weightvar\" collapv. collapg supports non-standard evaluations weightvar can indicated without quotes. wFUN FUN: Function(s) aggregate weight variable keep.w = TRUE. default sum weights computed group. custom named list specifying fully customized aggregation task. names list function names content columns aggregate using function (input cols). example custom = list(fmean = 1:6, fsd = 7:9, fmode = 10:11) tells collap aggregate columns 1-6 X using mean, columns 7-9 using standard deviation etc. Notes: custom lets collap ignore inputs passed FUN, catFUN cols. Since v1.6.0 can also rename columns e.g. custom = list(fmean = c(newname = \"col1\", \"col2\"), fmode = c(newname = 3)). keep., keep.group_vars logical. FALSE omit grouping variables output. TRUE keeps variables, even passed externally list vector (unlike collapse functions). keep.w logical. FALSE omit weight variable output .e. aggregation weights. TRUE aggregates adds weights, even passed externally vector (unlike collapse functions). keep.col.order logical. Retain original column order post-aggregation. sort, decreasing, na.last, return.order, method logical / character. Arguments passed GRP.default affecting row-order aggregated data frame grouping algorithm. parallel logical. Use mclapply instead lapply parallelize computation column level. available Windows. mc.cores integer. Argument mclapply setting number cores use, default 2. return character. Control output format aggregating multiple functions performing custom aggregation. \"wide\" (default) returns wider data frame added columns additional function. \"list\" returns list data frames - one function. \"long\" adds column \"Function\" row-binds results different functions using data.table::rbindlist. \"long_dupl\" special option aggregating multi-type data using multiple FUN one catFUN vice-versa. case format long data aggregated using one function duplicated. See Examples. give.names logical. Create unique names aggregated columns adding prefix 'FUN.var'. 'auto' automatically create prefixes whenever multiple functions applied column. ... additional arguments passed functions supplied FUN, catFUN, wFUN custom. Since v1.9.0 also split groups non-Fast Statistical Functions. behavior Fast Statistical Functions unused arguments regulated option(\"collapse_unused_arg_action\") defaults \"warning\". collapg also allows arguments collap except sort, decreasing, na.last, return.order, method keep..","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Advanced Data Aggregation — collap","text":"collap automatically checks function passed whether Fast Statistical Function (.e. whether function name contained .FAST_STAT_FUN). function fast statistical function, collap grouping calls function carry grouped computations (vectorized C/C++), resulting high aggregation speeds, even weights. function one .FAST_STAT_FUN, called internally perform computation. resulting computations function put list recombined produce desired output format controlled return argument. substantially slower, particularly many groups. setting parallel = TRUE non-windows computer, aggregations efficiently parallelized column level using mclapply utilizing mc.cores cores. Fast Statistical Function support multithreading .e. nthreads argument can passed collap. Using C-level multithreading much effective R-level parallelism, also works Windows, two never combined. w argument used, weights passed functions except wFUN. may undesirable settings like collap(data, ~ id, custom = list(fsum = ..., fmean = ...), w = ~ weights) wish aggregate columns using weighted mean, others using simple sum another unweighted statistic.  Therefore possible append Fast Statistical Functions _uw yield unweighted computation. example one can specify: collap(data, ~ id, custom = list(fsum_uw = ..., fmean = ...), w = ~ weights) get weighted mean simple sum. Note _uw functions available use outside collap. Thus one also needs quote passing FUN catFUN arguments, e.g. use collap(data, ~ id, fmean, \"fmode_uw\", w = ~ weights).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Advanced Data Aggregation — collap","text":"X aggregated. X data frame coerced one using qDF aggregated.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/collap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Advanced Data Aggregation — collap","text":"","code":"## A Simple Introduction -------------------------------------- head(iris) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa collap(iris, ~ Species)                                        # Default: FUN = fmean for numeric #>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1        5.006       3.428        1.462       0.246     setosa #> 2        5.936       2.770        4.260       1.326 versicolor #> 3        6.588       2.974        5.552       2.026  virginica collapv(iris, 5)                                               # Same using collapv #>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1        5.006       3.428        1.462       0.246     setosa #> 2        5.936       2.770        4.260       1.326 versicolor #> 3        6.588       2.974        5.552       2.026  virginica collap(iris, ~ Species, fmedian)                               # Using the median #>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1          5.0         3.4         1.50         0.2     setosa #> 2          5.9         2.8         4.35         1.3 versicolor #> 3          6.5         3.0         5.55         2.0  virginica collap(iris, ~ Species, fmedian, keep.col.order = FALSE)       # Groups in-front #>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa          5.0         3.4         1.50         0.2 #> 2 versicolor          5.9         2.8         4.35         1.3 #> 3  virginica          6.5         3.0         5.55         2.0 collap(iris, Sepal.Width + Petal.Width ~ Species, fmedian)     # Only '.Width' columns #>   Sepal.Width Petal.Width    Species #> 1         3.4         0.2     setosa #> 2         2.8         1.3 versicolor #> 3         3.0         2.0  virginica collapv(iris, 5, cols = c(2, 4))                               # Same using collapv #>   Sepal.Width Petal.Width    Species #> 1       3.428       0.246     setosa #> 2       2.770       1.326 versicolor #> 3       2.974       2.026  virginica collap(iris, ~ Species, list(fmean, fmedian))                  # Two functions #>   fmean.Sepal.Length fmedian.Sepal.Length fmean.Sepal.Width fmedian.Sepal.Width #> 1              5.006                  5.0             3.428                 3.4 #> 2              5.936                  5.9             2.770                 2.8 #> 3              6.588                  6.5             2.974                 3.0 #>   fmean.Petal.Length fmedian.Petal.Length fmean.Petal.Width fmedian.Petal.Width #> 1              1.462                 1.50             0.246                 0.2 #> 2              4.260                 4.35             1.326                 1.3 #> 3              5.552                 5.55             2.026                 2.0 #>      Species #> 1     setosa #> 2 versicolor #> 3  virginica collap(iris, ~ Species, list(fmean, fmedian), return = \"long\") # Long format #>   Function Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1    fmean        5.006       3.428        1.462       0.246     setosa #> 2    fmean        5.936       2.770        4.260       1.326 versicolor #> 3    fmean        6.588       2.974        5.552       2.026  virginica #> 4  fmedian        5.000       3.400        1.500       0.200     setosa #> 5  fmedian        5.900       2.800        4.350       1.300 versicolor #> 6  fmedian        6.500       3.000        5.550       2.000  virginica collapv(iris, 5, custom = list(fmean = 1:2, fmedian = 3:4))    # Custom aggregation #>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1        5.006       3.428         1.50         0.2     setosa #> 2        5.936       2.770         4.35         1.3 versicolor #> 3        6.588       2.974         5.55         2.0  virginica collapv(iris, 5, custom = list(fmean = 1:2, fmedian = 3:4),    # Raw output, no column reordering         return = \"list\") #> $fmean #>      Species Sepal.Length Sepal.Width #> 1     setosa        5.006       3.428 #> 2 versicolor        5.936       2.770 #> 3  virginica        6.588       2.974 #>  #> $fmedian #>      Species Petal.Length Petal.Width #> 1     setosa         1.50         0.2 #> 2 versicolor         4.35         1.3 #> 3  virginica         5.55         2.0 #>  collapv(iris, 5, custom = list(fmean = 1:2, fmedian = 3:4),    # A strange choice..         return = \"long\") #>   Function Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1    fmean        5.006       3.428           NA          NA     setosa #> 2    fmean        5.936       2.770           NA          NA versicolor #> 3    fmean        6.588       2.974           NA          NA  virginica #> 4  fmedian           NA          NA         1.50         0.2     setosa #> 5  fmedian           NA          NA         4.35         1.3 versicolor #> 6  fmedian           NA          NA         5.55         2.0  virginica collap(iris, ~ Species, w = ~ Sepal.Length)                    # Using Sepal.Length as weights, .. #>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1        250.3    3.447423     1.465202   0.2480224     setosa #> 2        296.8    2.784063     4.290195   1.3352089 versicolor #> 3        329.4    2.987948     5.597116   2.0333030  virginica weights <- abs(rnorm(fnrow(iris))) collap(iris, ~ Species, w = weights)                           # Some random weights.. #>    weights Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1 49.80442     5.008467    3.439900     1.475742   0.2414042     setosa #> 2 38.71629     5.918589    2.770690     4.235565   1.3112717 versicolor #> 3 43.96431     6.530104    2.968984     5.520582   2.0136350  virginica collap(iris, iris$Species, w = weights)                        # Note this behavior.. #>      Species  weights Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa 49.80442     5.008467    3.439900     1.475742   0.2414042 #> 2 versicolor 38.71629     5.918589    2.770690     4.235565   1.3112717 #> 3  virginica 43.96431     6.530104    2.968984     5.520582   2.0136350 #>      Species #> 1     setosa #> 2 versicolor #> 3  virginica collap(iris, iris$Species, w = weights,        keep.by = FALSE, keep.w = FALSE) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1     5.008467    3.439900     1.475742   0.2414042     setosa #> 2     5.918589    2.770690     4.235565   1.3112717 versicolor #> 3     6.530104    2.968984     5.520582   2.0136350  virginica   ## Multi-Type Aggregation -------------------------------------- head(wlddev)                                                    # World Development Panel Data #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(collap(wlddev, ~ country + decade))                        # Aggregate by country and decade #>       country iso3c       date   year decade     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 1964.5   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1971-01-01 1974.5   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 1984.5   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 1991-01-01 1994.5   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2001-01-01 2004.5   2000 South Asia Low income FALSE #>     PCGDP  LIFEEX GINI        ODA      POP #> 1      NA 34.6908   NA  222288999  9886773 #> 2      NA 39.9053   NA  236169998 12451803 #> 3      NA 46.4176   NA   71666001 12291854 #> 4      NA 53.0097   NA  317255000 16931903 #> 5 379.373 58.0881   NA 3054051961 24870022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(collap(wlddev, ~ country + decade, fmedian, ffirst))       # Different functions #>       country iso3c       date   year decade     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 1964.5   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1971-01-01 1974.5   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 1984.5   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 1991-01-01 1994.5   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2001-01-01 2004.5   2000 South Asia Low income FALSE #>      PCGDP  LIFEEX GINI        ODA      POP #> 1       NA 34.7055   NA  234900002  9850550 #> 2       NA 39.8430   NA  246509995 12551055 #> 3       NA 46.4005   NA   48539999 12071250 #> 4       NA 53.1200   NA  285175003 17593192 #> 5 361.2596 58.0310   NA 2984469971 25190480 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(collap(wlddev, ~ country + decade, cols = is.numeric))     # Aggregate only numeric columns #>       country   year decade decade    PCGDP  LIFEEX GINI        ODA      POP #> 1 Afghanistan 1964.5   1960   1960       NA 34.6908   NA  222288999  9886773 #> 2 Afghanistan 1974.5   1970   1970       NA 39.9053   NA  236169998 12451803 #> 3 Afghanistan 1984.5   1980   1980       NA 46.4176   NA   71666001 12291854 #> 4 Afghanistan 1994.5   1990   1990       NA 53.0097   NA  317255000 16931903 #> 5 Afghanistan 2004.5   2000   2000 379.3730 58.0881   NA 3054051961 24870022 #> 6 Afghanistan 2014.5   2010   2010 567.4047 63.0715   NA 5023859033 33741195 head(collap(wlddev, ~ country + decade, cols = 9:13))           # Only the 5 series #>       country decade    PCGDP  LIFEEX GINI        ODA      POP #> 1 Afghanistan   1960       NA 34.6908   NA  222288999  9886773 #> 2 Afghanistan   1970       NA 39.9053   NA  236169998 12451803 #> 3 Afghanistan   1980       NA 46.4176   NA   71666001 12291854 #> 4 Afghanistan   1990       NA 53.0097   NA  317255000 16931903 #> 5 Afghanistan   2000 379.3730 58.0881   NA 3054051961 24870022 #> 6 Afghanistan   2010 567.4047 63.0715   NA 5023859033 33741195 head(collap(wlddev, PCGDP + LIFEEX ~ country + decade))         # Only GDP and life-expactancy #>       country decade    PCGDP  LIFEEX #> 1 Afghanistan   1960       NA 34.6908 #> 2 Afghanistan   1970       NA 39.9053 #> 3 Afghanistan   1980       NA 46.4176 #> 4 Afghanistan   1990       NA 53.0097 #> 5 Afghanistan   2000 379.3730 58.0881 #> 6 Afghanistan   2010 567.4047 63.0715 head(collap(wlddev, PCGDP + LIFEEX ~ country + decade, fsum))   # Using the sum instead #>       country decade    PCGDP  LIFEEX #> 1 Afghanistan   1960       NA 346.908 #> 2 Afghanistan   1970       NA 399.053 #> 3 Afghanistan   1980       NA 464.176 #> 4 Afghanistan   1990       NA 530.097 #> 5 Afghanistan   2000 3034.984 580.881 #> 6 Afghanistan   2010 5674.047 630.715 head(collap(wlddev, PCGDP + LIFEEX ~ country + decade, sum,     # Same using base::sum -> slower!             na.rm = TRUE)) #>       country decade    PCGDP  LIFEEX #> 1 Afghanistan   1960    0.000 346.908 #> 2 Afghanistan   1970    0.000 399.053 #> 3 Afghanistan   1980    0.000 464.176 #> 4 Afghanistan   1990    0.000 530.097 #> 5 Afghanistan   2000 3034.984 580.881 #> 6 Afghanistan   2010 5674.047 630.715 head(collap(wlddev, wlddev[c(\"country\",\"decade\")], fsum,        # Same, exploring different inputs             cols = 9:10)) #>       country decade    PCGDP  LIFEEX #> 1 Afghanistan   1960       NA 346.908 #> 2 Afghanistan   1970       NA 399.053 #> 3 Afghanistan   1980       NA 464.176 #> 4 Afghanistan   1990       NA 530.097 #> 5 Afghanistan   2000 3034.984 580.881 #> 6 Afghanistan   2010 5674.047 630.715 head(collap(wlddev[9:10], wlddev[c(\"country\",\"decade\")], fsum)) #>       country decade    PCGDP  LIFEEX #> 1 Afghanistan   1960       NA 346.908 #> 2 Afghanistan   1970       NA 399.053 #> 3 Afghanistan   1980       NA 464.176 #> 4 Afghanistan   1990       NA 530.097 #> 5 Afghanistan   2000 3034.984 580.881 #> 6 Afghanistan   2010 5674.047 630.715 head(collapv(wlddev, c(\"country\",\"decade\"), fsum))              # ..names/indices with collapv #>       country iso3c       date  year decade     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 19645   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1971-01-01 19745   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 19845   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 1991-01-01 19945   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2001-01-01 20045   2000 South Asia Low income FALSE #>      PCGDP  LIFEEX GINI         ODA       POP #> 1       NA 346.908   NA  2222889992  98867731 #> 2       NA 399.053   NA  2361699982 124518028 #> 3       NA 464.176   NA   716660007 122918537 #> 4       NA 530.097   NA  3172550003 169319030 #> 5 3034.984 580.881   NA 30540519608 248700217 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(collapv(wlddev, c(1,5), fsum)) #>       country iso3c       date  year decade     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 19645   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1971-01-01 19745   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 19845   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 1991-01-01 19945   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2001-01-01 20045   2000 South Asia Low income FALSE #>      PCGDP  LIFEEX GINI         ODA       POP #> 1       NA 346.908   NA  2222889992  98867731 #> 2       NA 399.053   NA  2361699982 124518028 #> 3       NA 464.176   NA   716660007 122918537 #> 4       NA 530.097   NA  3172550003 169319030 #> 5 3034.984 580.881   NA 30540519608 248700217 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  g <- GRP(wlddev, ~ country + decade)                            # Precomputing the grouping head(collap(wlddev, g, keep.by = FALSE))                        # This is slightly faster now #>       country iso3c       date   year decade     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 1964.5   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1971-01-01 1974.5   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 1984.5   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 1991-01-01 1994.5   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2001-01-01 2004.5   2000 South Asia Low income FALSE #>     PCGDP  LIFEEX GINI        ODA      POP #> 1      NA 34.6908   NA  222288999  9886773 #> 2      NA 39.9053   NA  236169998 12451803 #> 3      NA 46.4176   NA   71666001 12291854 #> 4      NA 53.0097   NA  317255000 16931903 #> 5 379.373 58.0881   NA 3054051961 24870022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] # Aggregate categorical data using not the mode but the last element head(collap(wlddev, ~ country + decade, fmean, flast)) #>       country iso3c       date   year decade     region     income  OECD #> 1 Afghanistan   AFG 1970-01-01 1964.5   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1980-01-01 1974.5   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1990-01-01 1984.5   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 2000-01-01 1994.5   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2010-01-01 2004.5   2000 South Asia Low income FALSE #>     PCGDP  LIFEEX GINI        ODA      POP #> 1      NA 34.6908   NA  222288999  9886773 #> 2      NA 39.9053   NA  236169998 12451803 #> 3      NA 46.4176   NA   71666001 12291854 #> 4      NA 53.0097   NA  317255000 16931903 #> 5 379.373 58.0881   NA 3054051961 24870022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(collap(wlddev, ~ country + decade, catFUN = flast,         # Aggregate only categorical data             cols = is_categorical)) #>       country     country iso3c       date decade     region     income  OECD #> 1 Afghanistan Afghanistan   AFG 1970-01-01   1960 South Asia Low income FALSE #> 2 Afghanistan Afghanistan   AFG 1980-01-01   1970 South Asia Low income FALSE #> 3 Afghanistan Afghanistan   AFG 1990-01-01   1980 South Asia Low income FALSE #> 4 Afghanistan Afghanistan   AFG 2000-01-01   1990 South Asia Low income FALSE #> 5 Afghanistan Afghanistan   AFG 2010-01-01   2000 South Asia Low income FALSE #> 6 Afghanistan Afghanistan   AFG 2020-01-01   2010 South Asia Low income FALSE   ## Weighted Aggregation ---------------------------------------- # We aggregate to region level using population weights head(collap(wlddev, ~ region + year, w = ~ POP))                # Takes weighted mean for numeric.. #>   country iso3c       date year decade              region              income #> 1   China   CHN 1961-01-01 1960   1960 East Asia & Pacific Upper middle income #> 2   China   CHN 1962-01-01 1961   1960 East Asia & Pacific Upper middle income #> 3   China   CHN 1963-01-01 1962   1960 East Asia & Pacific Upper middle income #> 4   China   CHN 1964-01-01 1963   1960 East Asia & Pacific Upper middle income #> 5   China   CHN 1965-01-01 1964   1960 East Asia & Pacific Upper middle income #>    OECD    PCGDP   LIFEEX GINI       ODA        POP #> 1 FALSE 1313.760 48.20996   NA 764164132 1018832214 #> 2 FALSE 1395.228 48.73451   NA 774544481 1021806689 #> 3 FALSE 1463.441 49.39960   NA 915939856 1035694621 #> 4 FALSE 1540.621 50.37529   NA 748978431 1060888744 #> 5 FALSE 1665.385 51.57330   NA 619226983 1085690423 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] # ..and weighted mode for categorical data. The weight vector is aggregated using fsum  head(collap(wlddev, ~ region + year, w = ~ POP,                 # Aggregating weights using sum             wFUN = list(sum = fsum, max = fmax)))               # and max (corresponding to mode) #>   country iso3c       date year decade              region              income #> 1   China   CHN 1961-01-01 1960   1960 East Asia & Pacific Upper middle income #> 2   China   CHN 1962-01-01 1961   1960 East Asia & Pacific Upper middle income #> 3   China   CHN 1963-01-01 1962   1960 East Asia & Pacific Upper middle income #> 4   China   CHN 1964-01-01 1963   1960 East Asia & Pacific Upper middle income #> 5   China   CHN 1965-01-01 1964   1960 East Asia & Pacific Upper middle income #>    OECD    PCGDP   LIFEEX GINI       ODA    sum.POP   max.POP #> 1 FALSE 1313.760 48.20996   NA 764164132 1018832214 667070000 #> 2 FALSE 1395.228 48.73451   NA 774544481 1021806689 660330000 #> 3 FALSE 1463.441 49.39960   NA 915939856 1035694621 665770000 #> 4 FALSE 1540.621 50.37529   NA 748978431 1060888744 682335000 #> 5 FALSE 1665.385 51.57330   NA 619226983 1085690423 698355000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]   ## Multi-Function Aggregation ---------------------------------- head(collap(wlddev, ~ country + decade, list(mean = fmean, N = fnobs),  # Saving mean and Nobs             cols = 9:13)) #>       country decade mean.PCGDP N.PCGDP mean.LIFEEX N.LIFEEX mean.GINI N.GINI #> 1 Afghanistan   1960         NA       0     34.6908       10        NA      0 #> 2 Afghanistan   1970         NA       0     39.9053       10        NA      0 #> 3 Afghanistan   1980         NA       0     46.4176       10        NA      0 #> 4 Afghanistan   1990         NA       0     53.0097       10        NA      0 #> 5 Afghanistan   2000    379.373       8     58.0881       10        NA      0 #>     mean.ODA N.ODA mean.POP N.POP #> 1  222288999    10  9886773    10 #> 2  236169998    10 12451803    10 #> 3   71666001    10 12291854    10 #> 4  317255000    10 16931903    10 #> 5 3054051961    10 24870022    10 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  head(collap(wlddev, ~ country + decade,                         # Same using base R -> slower             list(mean = mean,                  N = function(x, ...) sum(!is.na(x))),             cols = 9:13, na.rm = TRUE)) #>       country decade mean.PCGDP N.PCGDP mean.LIFEEX N.LIFEEX mean.GINI N.GINI #> 1 Afghanistan   1960        NaN       0     34.6908       10       NaN      0 #> 2 Afghanistan   1970        NaN       0     39.9053       10       NaN      0 #> 3 Afghanistan   1980        NaN       0     46.4176       10       NaN      0 #> 4 Afghanistan   1990        NaN       0     53.0097       10       NaN      0 #> 5 Afghanistan   2000    379.373       8     58.0881       10       NaN      0 #>     mean.ODA N.ODA mean.POP N.POP #> 1  222288999    10  9886773    10 #> 2  236169998    10 12451803    10 #> 3   71666001    10 12291854    10 #> 4  317255000    10 16931903    10 #> 5 3054051961    10 24870022    10 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  lapply(collap(wlddev, ~ country + decade,                       # List output format        list(mean = fmean, N = fnobs), cols = 9:13, return = \"list\"), head) #> $mean #>       country decade    PCGDP  LIFEEX GINI        ODA      POP #> 1 Afghanistan   1960       NA 34.6908   NA  222288999  9886773 #> 2 Afghanistan   1970       NA 39.9053   NA  236169998 12451803 #> 3 Afghanistan   1980       NA 46.4176   NA   71666001 12291854 #> 4 Afghanistan   1990       NA 53.0097   NA  317255000 16931903 #> 5 Afghanistan   2000 379.3730 58.0881   NA 3054051961 24870022 #> 6 Afghanistan   2010 567.4047 63.0715   NA 5023859033 33741195 #>  #> $N #>       country decade PCGDP LIFEEX GINI ODA POP #> 1 Afghanistan   1960     0     10    0  10  10 #> 2 Afghanistan   1970     0     10    0  10  10 #> 3 Afghanistan   1980     0     10    0  10  10 #> 4 Afghanistan   1990     0     10    0  10  10 #> 5 Afghanistan   2000     8     10    0  10  10 #> 6 Afghanistan   2010    10     10    0  10  10 #>   head(collap(wlddev, ~ country + decade,                         # Long output format      list(mean = fmean, N = fnobs), cols = 9:13, return = \"long\")) #>   Function     country decade    PCGDP  LIFEEX GINI        ODA      POP #> 1     mean Afghanistan   1960       NA 34.6908   NA  222288999  9886773 #> 2     mean Afghanistan   1970       NA 39.9053   NA  236169998 12451803 #> 3     mean Afghanistan   1980       NA 46.4176   NA   71666001 12291854 #> 4     mean Afghanistan   1990       NA 53.0097   NA  317255000 16931903 #> 5     mean Afghanistan   2000 379.3730 58.0881   NA 3054051961 24870022 #> 6     mean Afghanistan   2010 567.4047 63.0715   NA 5023859033 33741195  head(collap(wlddev, ~ country + decade,                         # Also aggregating categorical data,      list(mean = fmean, N = fnobs), return = \"long_dupl\"))      # and duplicating it 2 times #>   Function     country iso3c       date   year decade     region     income #> 1     mean Afghanistan   AFG 1961-01-01 1964.5   1960 South Asia Low income #> 2     mean Afghanistan   AFG 1971-01-01 1974.5   1970 South Asia Low income #> 3     mean Afghanistan   AFG 1981-01-01 1984.5   1980 South Asia Low income #> 4     mean Afghanistan   AFG 1991-01-01 1994.5   1990 South Asia Low income #> 5     mean Afghanistan   AFG 2001-01-01 2004.5   2000 South Asia Low income #>    OECD   PCGDP  LIFEEX GINI        ODA      POP #> 1 FALSE      NA 34.6908   NA  222288999  9886773 #> 2 FALSE      NA 39.9053   NA  236169998 12451803 #> 3 FALSE      NA 46.4176   NA   71666001 12291854 #> 4 FALSE      NA 53.0097   NA  317255000 16931903 #> 5 FALSE 379.373 58.0881   NA 3054051961 24870022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  head(collap(wlddev, ~ country + decade,                         # Now also using 2 functions on      list(mean = fmean, N = fnobs), list(mode = fmode, last = flast),   # categorical data             keep.col.order = FALSE)) #>       country decade mean.year mean.PCGDP mean.LIFEEX mean.GINI  mean.ODA #> 1 Afghanistan   1960    1964.5         NA     34.6908        NA 222288999 #> 2 Afghanistan   1970    1974.5         NA     39.9053        NA 236169998 #>   mean.POP N.year N.PCGDP N.LIFEEX N.GINI N.ODA N.POP mode.iso3c  mode.date #> 1  9886773     10       0       10      0    10    10        AFG 1961-01-01 #> 2 12451803     10       0       10      0    10    10        AFG 1971-01-01 #>   mode.region mode.income mode.OECD last.iso3c  last.date last.region #> 1  South Asia  Low income     FALSE        AFG 1970-01-01  South Asia #> 2  South Asia  Low income     FALSE        AFG 1980-01-01  South Asia #>   last.income last.OECD #> 1  Low income     FALSE #> 2  Low income     FALSE #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]  head(collap(wlddev, ~ country + decade,                         # More functions, string input,             c(\"fmean\",\"fsum\",\"fnobs\",\"fsd\",\"fvar\"),             # parallelized execution             c(\"fmode\",\"ffirst\",\"flast\",\"fndistinct\"),           # (choose more than 1 cores,             parallel = TRUE, mc.cores = 1L,                     # depending on your machine)             keep.col.order = FALSE)) #>       country decade fmean.year fmean.PCGDP fmean.LIFEEX fmean.GINI fmean.ODA #> 1 Afghanistan   1960     1964.5          NA      34.6908         NA 222288999 #>   fmean.POP fsum.year fsum.PCGDP fsum.LIFEEX fsum.GINI   fsum.ODA fsum.POP #> 1   9886773     19645         NA     346.908        NA 2222889992 98867731 #>   fnobs.year fnobs.PCGDP fnobs.LIFEEX fnobs.GINI fnobs.ODA fnobs.POP fsd.year #> 1         10           0           10          0        10        10  3.02765 #>   fsd.PCGDP fsd.LIFEEX fsd.GINI  fsd.ODA  fsd.POP fvar.year fvar.PCGDP #> 1        NA   1.490964       NA 80884369 637640.4  9.166667         NA #>   fvar.LIFEEX fvar.GINI     fvar.ODA     fvar.POP fmode.iso3c fmode.date #> 1    2.222975        NA 6.542281e+15 406585329709         AFG 1961-01-01 #>   fmode.region fmode.income fmode.OECD ffirst.iso3c ffirst.date ffirst.region #> 1   South Asia   Low income      FALSE          AFG  1961-01-01    South Asia #>   ffirst.income ffirst.OECD flast.iso3c flast.date flast.region flast.income #> 1    Low income       FALSE         AFG 1970-01-01   South Asia   Low income #>   flast.OECD fndistinct.iso3c fndistinct.date fndistinct.region #> 1      FALSE                1              10                 1 #>   fndistinct.income fndistinct.OECD #> 1                 1               1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]   ## Custom Aggregation ------------------------------------------ head(collap(wlddev, ~ country + decade,                         # Custom aggregation             custom = list(fmean = 11:13, fsd = 9:10, fmode = 7:8))) #>       country decade     income  OECD    PCGDP   LIFEEX GINI        ODA #> 1 Afghanistan   1960 Low income FALSE       NA 1.490964   NA  222288999 #> 2 Afghanistan   1970 Low income FALSE       NA 1.738383   NA  236169998 #> 3 Afghanistan   1980 Low income FALSE       NA 2.161460   NA   71666001 #> 4 Afghanistan   1990 Low income FALSE       NA 1.695424   NA  317255000 #> 5 Afghanistan   2000 Low income FALSE 53.66524 1.565630   NA 3054051961 #> 6 Afghanistan   2010 Low income FALSE 18.07999 1.274644   NA 5023859033 #>        POP #> 1  9886773 #> 2 12451803 #> 3 12291854 #> 4 16931903 #> 5 24870022 #> 6 33741195  head(collap(wlddev, ~ country + decade,                         # Using column names             custom = list(fmean = \"PCGDP\", fsd = c(\"LIFEEX\",\"GINI\"),                           flast = \"date\"))) #>       country       date decade    PCGDP   LIFEEX GINI #> 1 Afghanistan 1970-01-01   1960       NA 1.490964   NA #> 2 Afghanistan 1980-01-01   1970       NA 1.738383   NA #> 3 Afghanistan 1990-01-01   1980       NA 2.161460   NA #> 4 Afghanistan 2000-01-01   1990       NA 1.695424   NA #> 5 Afghanistan 2010-01-01   2000 379.3730 1.565630   NA #> 6 Afghanistan 2020-01-01   2010 567.4047 1.274644   NA  head(collap(wlddev, ~ country + decade,                         # Weighted parallelized custom             custom = list(fmean = 9:12, fsd = 9:10,             # aggregation                           fmode = 7:8), w = ~ POP,             wFUN = list(fsum, fmax),             parallel = TRUE, mc.cores = 1L)) #>       country decade fmode.income fmode.OECD fmean.PCGDP fsd.PCGDP fmean.LIFEEX #> 1 Afghanistan   1960   Low income      FALSE          NA        NA     34.77716 #> 2 Afghanistan   1970   Low income      FALSE          NA        NA     40.00367 #> 3 Afghanistan   1980   Low income      FALSE          NA        NA     46.32098 #> 4 Afghanistan   1990   Low income      FALSE          NA        NA     53.25897 #> 5 Afghanistan   2000   Low income      FALSE    382.5583  51.11423     58.23630 #>   fsd.LIFEEX fmean.GINI  fmean.ODA  fsum.POP fmax.POP #> 1   1.413369         NA  223006447  98867731 10893776 #> 2   1.643608         NA  236798314 124518028 13411056 #> 3   2.061847         NA   70613923 122918537 13356511 #> 4   1.556185         NA  306818649 169319030 20170844 #> 5   1.476279         NA 3240143310 248700217 28394813 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  head(collap(wlddev, ~ country + decade,                         # No column reordering             custom = list(fmean = 9:12, fsd = 9:10,                           fmode = 7:8), w = ~ POP,             wFUN = list(fsum, fmax),             parallel = TRUE, mc.cores = 1L, keep.col.order = FALSE)) #>       country decade  fsum.POP fmax.POP fmean.PCGDP fmean.LIFEEX fmean.GINI #> 1 Afghanistan   1960  98867731 10893776          NA     34.77716         NA #> 2 Afghanistan   1970 124518028 13411056          NA     40.00367         NA #> 3 Afghanistan   1980 122918537 13356511          NA     46.32098         NA #> 4 Afghanistan   1990 169319030 20170844          NA     53.25897         NA #> 5 Afghanistan   2000 248700217 28394813    382.5583     58.23630         NA #>    fmean.ODA fsd.PCGDP fsd.LIFEEX fmode.income fmode.OECD #> 1  223006447        NA   1.413369   Low income      FALSE #> 2  236798314        NA   1.643608   Low income      FALSE #> 3   70613923        NA   2.061847   Low income      FALSE #> 4  306818649        NA   1.556185   Low income      FALSE #> 5 3240143310  51.11423   1.476279   Low income      FALSE #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## Piped Use -------------------------------------------------- iris |> fgroup_by(Species) |> collapg() #>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa        5.006       3.428        1.462       0.246 #> 2 versicolor        5.936       2.770        4.260       1.326 #> 3  virginica        6.588       2.974        5.552       2.026 wlddev |> fgroup_by(country, decade) |> collapg() |> head() #>       country decade iso3c       date   year     region     income  OECD #> 1 Afghanistan   1960   AFG 1961-01-01 1964.5 South Asia Low income FALSE #> 2 Afghanistan   1970   AFG 1971-01-01 1974.5 South Asia Low income FALSE #> 3 Afghanistan   1980   AFG 1981-01-01 1984.5 South Asia Low income FALSE #> 4 Afghanistan   1990   AFG 1991-01-01 1994.5 South Asia Low income FALSE #> 5 Afghanistan   2000   AFG 2001-01-01 2004.5 South Asia Low income FALSE #>     PCGDP  LIFEEX GINI        ODA      POP #> 1      NA 34.6908   NA  222288999  9886773 #> 2      NA 39.9053   NA  236169998 12451803 #> 3      NA 46.4176   NA   71666001 12291854 #> 4      NA 53.0097   NA  317255000 16931903 #> 5 379.373 58.0881   NA 3054051961 24870022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] wlddev |> fgroup_by(region, year) |> collapg(w = POP) |> head() #>                region year        POP country iso3c       date decade #> 1 East Asia & Pacific 1960 1018832214   China   CHN 1961-01-01   1960 #> 2 East Asia & Pacific 1961 1021806689   China   CHN 1962-01-01   1960 #> 3 East Asia & Pacific 1962 1035694621   China   CHN 1963-01-01   1960 #> 4 East Asia & Pacific 1963 1060888744   China   CHN 1964-01-01   1960 #> 5 East Asia & Pacific 1964 1085690423   China   CHN 1965-01-01   1960 #>                income  OECD    PCGDP   LIFEEX GINI       ODA #> 1 Upper middle income FALSE 1313.760 48.20996   NA 764164132 #> 2 Upper middle income FALSE 1395.228 48.73451   NA 774544481 #> 3 Upper middle income FALSE 1463.441 49.39960   NA 915939856 #> 4 Upper middle income FALSE 1540.621 50.37529   NA 748978431 #> 5 Upper middle income FALSE 1665.385 51.57330   NA 619226983 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] wlddev |> fgroup_by(country, decade) |> collapg(fmedian, flast) |> head() #>       country decade iso3c       date   year     region     income  OECD #> 1 Afghanistan   1960   AFG 1970-01-01 1964.5 South Asia Low income FALSE #> 2 Afghanistan   1970   AFG 1980-01-01 1974.5 South Asia Low income FALSE #> 3 Afghanistan   1980   AFG 1990-01-01 1984.5 South Asia Low income FALSE #> 4 Afghanistan   1990   AFG 2000-01-01 1994.5 South Asia Low income FALSE #> 5 Afghanistan   2000   AFG 2010-01-01 2004.5 South Asia Low income FALSE #>      PCGDP  LIFEEX GINI        ODA      POP #> 1       NA 34.7055   NA  234900002  9850550 #> 2       NA 39.8430   NA  246509995 12551055 #> 3       NA 46.4005   NA   48539999 12071250 #> 4       NA 53.1200   NA  285175003 17593192 #> 5 361.2596 58.0310   NA 2984469971 25190480 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] wlddev |> fgroup_by(country, decade) |>   collapg(custom = list(fmean = 9:12, fmode = 5:7, flast = 3)) |> head() #>       country decade       date decade     region     income    PCGDP  LIFEEX #> 1 Afghanistan   1960 1970-01-01   1960 South Asia Low income       NA 34.6908 #> 2 Afghanistan   1970 1980-01-01   1970 South Asia Low income       NA 39.9053 #> 3 Afghanistan   1980 1990-01-01   1980 South Asia Low income       NA 46.4176 #> 4 Afghanistan   1990 2000-01-01   1990 South Asia Low income       NA 53.0097 #> 5 Afghanistan   2000 2010-01-01   2000 South Asia Low income 379.3730 58.0881 #> 6 Afghanistan   2010 2020-01-01   2010 South Asia Low income 567.4047 63.0715 #>   GINI        ODA #> 1   NA  222288999 #> 2   NA  236169998 #> 3   NA   71666001 #> 4   NA  317255000 #> 5   NA 3054051961 #> 6   NA 5023859033"},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-documentation.html","id":null,"dir":"Reference","previous_headings":"","what":"Collapse Documentation & Overview — collapse-documentation","title":"Collapse Documentation & Overview — collapse-documentation","text":"following table fully summarizes contents collapse. documentation structured hierarchically: main overview page, linking topical overview pages associated function pages (unless functions documented topic page).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-documentation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Collapse Documentation & Overview — collapse-documentation","text":"added top-level documentation infrastructure collapse allows effectively navigate package.  Calling ?FUN brings documentation page documenting function, contains links associated topic pages closely related functions. can also call topical documentation pages directly console. links pages contained global macro .COLLAPSE_TOPICS (e.g. calling help(.COLLAPSE_TOPICS[1]) brings page).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-documentation.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Collapse Documentation & Overview — collapse-documentation","text":"Maintainer: Sebastian Krantz sebastian.krantz@graduateinstitute.ch","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":null,"dir":"Reference","previous_headings":"","what":"collapse Package Options — collapse-options","title":"collapse Package Options — collapse-options","text":"collapse globally configurable extent packages : default value key function arguments governing behavior algorithms, exported namespace, can adjusted interactively set_collapse() function. options saved internal environment called .op (safety performance reasons) visible documentation functions fmean. contents environment can accessed using get_collapse(). also options can set using options (retrievable using getOption). options mainly affect package startup behavior.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"collapse Package Options — collapse-options","text":"","code":"set_collapse(...) get_collapse(opts = NULL)"},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"collapse Package Options — collapse-options","text":"... either comma separated options, single list options. available options :  opts character. vector options receive .op, NULL list options.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"collapse Package Options — collapse-options","text":"set_collapse() returns old content .op invisibly list. get_collapse(), called one option, returns value option, otherwise list.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"collapse Package Options — collapse-options","text":"Setting keywords \"fast-fun\", \"fast-stat-fun\", \"fast-trfm-fun\" \"\" set_collapse(mask = ...) also adjust internal optimization flags, e.g. (f)summarise (f)mutate, functions - expressions containing - receive vectorized execution (see examples (f)summarise (f)mutate). Users aware expressions like fmutate(mu = sum(var) / lenth(var)): usually gets executed groups, keywords set,vectorized (like fmutate(mu = fsum(var) / lenth(var))) implying grouped sum divided overall length. case fmutate(mu = base::sum(var) / lenth(var)) needs specified retain original result. Note passing individual functions like set_collapse(mask = \"(f)sum\") change internal optimization flags functions. ensure consistency .e. can either (setting appropriate keywords) comes vectorized stats basic R names. Note also masking change documentation links, need look f- version function get right documentation. safe way set options affecting startup behavior using .Rprofile file user project directory (see also , user-level file located file.path(Sys.getenv(\"HOME\"), \".Rprofile\") can edited using file.edit(Sys.getenv(\"HOME\"), \".Rprofile\")), using .fastverse configuration file project directory. options(\"collapse_remove\") fact remove functions namespace reversed set_collapse(remove = NULL) package loaded. reversed re-loading collapse.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":"options-set-using-options-","dir":"Reference","previous_headings":"","what":"Options Set Using options()","title":"collapse Package Options — collapse-options","text":"\"collapse_unused_arg_action\" regulates generic functions (Fast Statistical Functions) package react unknown argument passed method. default action \"warning\" issues warning. options \"error\", \"message\" \"none\", whereby latter enables silent swallowing arguments. \"collapse_export_F\", set TRUE, exports lead operator F package namespace loading package. operator exported default v1.9.0, now hidden inside package due many problems base::F. Alternatively, operator can accessed using collapse:::F. \"collapse_nthreads\", \"collapse_na_rm\", \"collapse_sort\", \"collapse_stable_algo\", \"collapse_verbose\", \"collapse_digits\", \"collapse_mask\" \"collapse_remove\" can set loading package initialize .op different defaults (e.g. using .Rprofile file). loaded, options effect, users need use set_collapse() change . See also Note.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"collapse Package Options — collapse-options","text":"","code":"# Setting new values oldopts <- set_collapse(nthreads = 2, na.rm = FALSE)  # Getting the values get_collapse() #> $nthreads #> [1] 2 #>  #> $remove #> NULL #>  #> $stable.algo #> [1] TRUE #>  #> $sort #> [1] TRUE #>  #> $digits #> [1] 2 #>  #> $stub #> [1] TRUE #>  #> $verbose #> [1] 1 #>  #> $mask #> NULL #>  #> $na.rm #> [1] FALSE #>  get_collapse(\"nthreads\") #> [1] 2  # Resetting set_collapse(oldopts) rm(oldopts)  if (FALSE) { # \\dontrun{ ## This is a typical working setup I use: library(fastverse) # Loading other stats packages with fastverse_extend(): # displays versions, checks conflicts, and installs if unavailable fastverse_extend(qs, fixest, grf, glmnet, install = TRUE) # Now setting collapse options with some namespace modification set_collapse(   nthreads = 4,   sort = FALSE,   mask = c(\"manip\", \"helper\", \"special\", \"mean\", \"scale\"),   remove = \"old\" ) # Final conflicts check (optional) fastverse_conflicts()  # For some simpler scripts I also use set_collapse(   nthreads = 4,   sort = FALSE,   mask = \"all\",   remove = c(\"old\", \"between\") # I use data.table::between > fbetween )  # This is now collapse code mtcars |>   subset(mpg > 12) |>   group_by(cyl) |>   sum() } # }  ## Changing what happens with unused arguments oldopts <- options(collapse_unused_arg_action = \"message\") # default: \"warning\" fmean(mtcars$mpg, bla = 1) #> Unused argument (bla = 1) passed to fmean.default #> [1] 20.09062  # Now nothing happens, same as base R options(collapse_unused_arg_action = \"none\") fmean(mtcars$mpg, bla = 1) #> [1] 20.09062 mean(mtcars$mpg, bla = 1) #> [1] 20.09062  options(oldopts) rm(oldopts)"},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-package.html","id":null,"dir":"Reference","previous_headings":"","what":" Advanced and Fast Data Transformation — collapse-package","title":" Advanced and Fast Data Transformation — collapse-package","text":"collapse C/C++ based package data transformation statistical computing R. aims : facilitate complex data transformation, exploration computing tasks R. help make R code fast, flexible, parsimonious programmer friendly. also implements class-agnostic approach data manipulation R, supporting major classes.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-package.html","id":"getting-started","dir":"Reference","previous_headings":"","what":"Getting Started","title":" Advanced and Fast Data Transformation — collapse-package","text":"Read short vignette documentation resources, check built documentation.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":" Advanced and Fast Data Transformation — collapse-package","text":"collapse provides integrated suite statistical data manipulation functions greatly extend enhance capabilities base R. nutshell, collapse provides: Fast C/C++ based (grouped, weighted) computations embedded highly optimized R code. complex statistical, time series / panel data recursive (list-processing) operations. flexible generic approach supporting preserving many R objects. Optimized programming standard non-standard evaluation. statistical functions collapse S3 generic core methods vectors, matrices data frames, internally support grouped weighted computations carried C/C++.  Functions core methods seek preserve object attributes (including column attributes variable labels), ensuring flexibility effective workflows broad range R objects (including time-series classes). See vignette collapse's handling R objects. Missing values efficiently skipped C/C++ level. package default na.rm = TRUE. can changed using set_collapse(na.rm = FALSE). Missing weights generally supported.   collapse installs built-hierarchical documentation facilitating use package. package coded C C++ built Rcpp, also uses C/C++ functions data.table, kit, fixest, weights, stats RcppArmadillo / RcppEigen.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-package.html","id":"author-s-","dir":"Reference","previous_headings":"","what":"Author(s)","title":" Advanced and Fast Data Transformation — collapse-package","text":"Maintainer: Sebastian Krantz sebastian.krantz@graduateinstitute.ch","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-package.html","id":"developing-bug-reporting","dir":"Reference","previous_headings":"","what":"Developing / Bug Reporting","title":" Advanced and Fast Data Transformation — collapse-package","text":"Please report issues https://github.com/SebKrantz/collapse/issues. Please send pull-requests 'development' branch repository.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":" Advanced and Fast Data Transformation — collapse-package","text":"","code":"## Note: this set of examples is is certainly non-exhaustive and does not ## showcase many recent features, but remains a very good starting point  ## Let's start with some statistical programming v <- iris$Sepal.Length d <- num_vars(iris)    # Saving numeric variables f <- iris$Species      # Factor  # Simple statistics fmean(v)               # vector #> [1] 5.843333 fmean(qM(d))           # matrix (qM is a faster as.matrix) #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>     5.843333     3.057333     3.758000     1.199333  fmean(d)               # data.frame #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>     5.843333     3.057333     3.758000     1.199333   # Preserving data structure fmean(qM(d), drop = FALSE)     # Still a matrix #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]     5.843333    3.057333        3.758    1.199333 fmean(d, drop = FALSE)         # Still a data.frame #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.843333    3.057333        3.758    1.199333  # Weighted statistics, supported by most functions... w <- abs(rnorm(fnrow(iris))) fmean(d, w = w) #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>     5.839603     3.086031     3.721517     1.199056   # Grouped statistics... fmean(d, f) #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            5.006       3.428        1.462       0.246 #> versicolor        5.936       2.770        4.260       1.326 #> virginica         6.588       2.974        5.552       2.026  # Groupwise-weighted statistics... fmean(d, f, w) #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa         5.035771    3.447914     1.453903   0.2653375 #> versicolor     5.930956    2.795360     4.255436   1.3213342 #> virginica      6.578510    2.990917     5.543057   2.0419757  # Simple Transformations... head(fmode(d, TRA = \"replace\"))    # Replacing values with the mode #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1            5           3          1.5         0.2 #> 2            5           3          1.5         0.2 #> 3            5           3          1.5         0.2 #> 4            5           3          1.5         0.2 #> 5            5           3          1.5         0.2 #> 6            5           3          1.5         0.2 head(fmedian(d, TRA = \"-\"))        # Subtracting the median #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1         -0.7         0.5        -2.95        -1.1 #> 2         -0.9         0.0        -2.95        -1.1 #> 3         -1.1         0.2        -3.05        -1.1 #> 4         -1.2         0.1        -2.85        -1.1 #> 5         -0.8         0.6        -2.95        -1.1 #> 6         -0.4         0.9        -2.65        -0.9 head(fsum(d, TRA = \"%\"))           # Computing percentages #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1    0.5818597   0.7631923    0.2483591   0.1111729 #> 2    0.5590416   0.6541648    0.2483591   0.1111729 #> 3    0.5362236   0.6977758    0.2306191   0.1111729 #> 4    0.5248146   0.6759703    0.2660990   0.1111729 #> 5    0.5704507   0.7849978    0.2483591   0.1111729 #> 6    0.6160867   0.8504143    0.3015789   0.2223457 head(fsd(d, TRA = \"/\"))            # Dividing by the standard-deviation (scaling), etc... #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     6.158928    8.029986    0.7930671   0.2623854 #> 2     5.917402    6.882845    0.7930671   0.2623854 #> 3     5.675875    7.341701    0.7364195   0.2623854 #> 4     5.555112    7.112273    0.8497148   0.2623854 #> 5     6.038165    8.259414    0.7930671   0.2623854 #> 6     6.521218    8.947698    0.9630101   0.5247707  # Weighted Transformations... head(fnth(d, 0.75, w = w, TRA = \"replace\"))  # Replacing by the weighted 3rd quartile #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1          6.4         3.4          5.1         1.8 #> 2          6.4         3.4          5.1         1.8 #> 3          6.4         3.4          5.1         1.8 #> 4          6.4         3.4          5.1         1.8 #> 5          6.4         3.4          5.1         1.8 #> 6          6.4         3.4          5.1         1.8  # Grouped Transformations... head(fvar(d, f, TRA = \"replace\"))  # Replacing values with the group variance #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     0.124249   0.1436898   0.03015918  0.01110612 #> 2     0.124249   0.1436898   0.03015918  0.01110612 #> 3     0.124249   0.1436898   0.03015918  0.01110612 #> 4     0.124249   0.1436898   0.03015918  0.01110612 #> 5     0.124249   0.1436898   0.03015918  0.01110612 #> 6     0.124249   0.1436898   0.03015918  0.01110612 head(fsd(d, f, TRA = \"/\"))         # Grouped scaling #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     14.46851    9.233260     8.061544    1.897793 #> 2     13.90112    7.914223     8.061544    1.897793 #> 3     13.33372    8.441838     7.485720    1.897793 #> 4     13.05003    8.178031     8.637369    1.897793 #> 5     14.18481    9.497068     8.061544    1.897793 #> 6     15.31960   10.288490     9.789018    3.795585 head(fmin(d, f, TRA = \"-\"))        # Setting the minimum value in each species to 0 #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1          0.8         1.2          0.4         0.1 #> 2          0.6         0.7          0.4         0.1 #> 3          0.4         0.9          0.3         0.1 #> 4          0.3         0.8          0.5         0.1 #> 5          0.7         1.3          0.4         0.1 #> 6          1.1         1.6          0.7         0.3 head(fsum(d, f, TRA = \"/\"))        # Dividing by the sum (proportions) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   0.02037555  0.02042007   0.01915185  0.01626016 #> 2   0.01957651  0.01750292   0.01915185  0.01626016 #> 3   0.01877747  0.01866978   0.01778386  0.01626016 #> 4   0.01837795  0.01808635   0.02051984  0.01626016 #> 5   0.01997603  0.02100350   0.01915185  0.01626016 #> 6   0.02157411  0.02275379   0.02325581  0.03252033 head(fmedian(d, f, TRA = \"-\"))     # Groupwise de-median #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1          0.1         0.1         -0.1         0.0 #> 2         -0.1        -0.4         -0.1         0.0 #> 3         -0.3        -0.2         -0.2         0.0 #> 4         -0.4        -0.3          0.0         0.0 #> 5          0.0         0.2         -0.1         0.0 #> 6          0.4         0.5          0.2         0.2 head(ffirst(d, f, TRA = \"%%\"))     # Taking modulus of first group-value, etc. ... #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1          0.0         0.0          0.0           0 #> 2          4.9         3.0          0.0           0 #> 3          4.7         3.2          1.3           0 #> 4          4.6         3.1          0.1           0 #> 5          5.0         0.1          0.0           0 #> 6          0.3         0.4          0.3           0  # Grouped and weighted transformations... head(fsd(d, f, w, \"/\"), 3)         # weighted scaling #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     14.24438    9.301969     8.285370    1.776243 #> 2     13.68577    7.973117     8.285370    1.776243 #> 3     13.12717    8.504658     7.693558    1.776243 head(fmedian(d, f, w, \"-\"), 3)     # subtracting the weighted group-median #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1          0.1         0.1         -0.1           0 #> 2         -0.1        -0.4         -0.1           0 #> 3         -0.3        -0.2         -0.2           0 head(fmode(d, f, w, \"replace\"), 3) # replace with weighted statistical mode #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1            5         3.4          1.4         0.2 #> 2            5         3.4          1.4         0.2 #> 3            5         3.4          1.4         0.2  ## Some more advanced transformations... head(fbetween(d))                             # Averaging (faster t.: fmean(d, TRA = \"replace\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.843333    3.057333        3.758    1.199333 #> 2     5.843333    3.057333        3.758    1.199333 #> 3     5.843333    3.057333        3.758    1.199333 #> 4     5.843333    3.057333        3.758    1.199333 #> 5     5.843333    3.057333        3.758    1.199333 #> 6     5.843333    3.057333        3.758    1.199333 head(fwithin(d))                              # Centering (faster than: fmean(d, TRA = \"-\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   -0.7433333  0.44266667       -2.358  -0.9993333 #> 2   -0.9433333 -0.05733333       -2.358  -0.9993333 #> 3   -1.1433333  0.14266667       -2.458  -0.9993333 #> 4   -1.2433333  0.04266667       -2.258  -0.9993333 #> 5   -0.8433333  0.54266667       -2.358  -0.9993333 #> 6   -0.4433333  0.84266667       -2.058  -0.7993333 head(fwithin(d, f, w))                        # Grouped and weighted (same as fmean(d, f, w, \"-\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   0.06422856  0.05208646  -0.05390312 -0.06533746 #> 2  -0.13577144 -0.44791354  -0.05390312 -0.06533746 #> 3  -0.33577144 -0.24791354  -0.15390312 -0.06533746 #> 4  -0.43577144 -0.34791354   0.04609688 -0.06533746 #> 5  -0.03577144  0.15208646  -0.05390312 -0.06533746 #> 6   0.36422856  0.45208646   0.24609688  0.13466254 head(fwithin(d, f, w, mean = 5))              # Setting a custom mean #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.064229    5.052086     4.946097    4.934663 #> 2     4.864229    4.552086     4.946097    4.934663 #> 3     4.664229    4.752086     4.846097    4.934663 #> 4     4.564229    4.652086     5.046097    4.934663 #> 5     4.964229    5.152086     4.946097    4.934663 #> 6     5.364229    5.452086     5.246097    5.134663 head(fwithin(d, f, w, theta = 0.76))          # Quasi-centering i.e. d - theta*fbetween(d, f, w) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1    1.2728137   0.8795857    0.2950336 -0.00165647 #> 2    1.0728137   0.3795857    0.2950336 -0.00165647 #> 3    0.8728137   0.5795857    0.1950336 -0.00165647 #> 4    0.7728137   0.4795857    0.3950336 -0.00165647 #> 5    1.1728137   0.9795857    0.2950336 -0.00165647 #> 6    1.5728137   1.2795857    0.5950336  0.19834353 head(fwithin(d, f, w, mean = \"overall.mean\")) # Preserving the overall mean of the data #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.903831    3.138117     3.667614    1.133719 #> 2     5.703831    2.638117     3.667614    1.133719 #> 3     5.503831    2.838117     3.567614    1.133719 #> 4     5.403831    2.738117     3.767614    1.133719 #> 5     5.803831    3.238117     3.667614    1.133719 #> 6     6.203831    3.538117     3.967614    1.333719 head(fscale(d))                               # Scaling and centering #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   -0.8976739  1.01560199    -1.335752   -1.311052 #> 2   -1.1392005 -0.13153881    -1.335752   -1.311052 #> 3   -1.3807271  0.32731751    -1.392399   -1.311052 #> 4   -1.5014904  0.09788935    -1.279104   -1.311052 #> 5   -1.0184372  1.24503015    -1.335752   -1.311052 #> 6   -0.5353840  1.93331463    -1.165809   -1.048667 head(fscale(d, mean = 5, sd = 3))             # Custom scaling and centering #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1    2.3069784    8.046806    0.9927451    1.066844 #> 2    1.5823985    4.605384    0.9927451    1.066844 #> 3    0.8578187    5.981953    0.8228021    1.066844 #> 4    0.4955288    5.293668    1.1626881    1.066844 #> 5    1.9446885    8.735090    0.9927451    1.066844 #> 6    3.3938481   10.799944    1.5025740    1.854000 head(fscale(d, mean = FALSE, sd = 3))         # Mean preserving scaling #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     3.150312    6.104139  -0.24925490   -2.733823 #> 2     2.425732    2.662717  -0.24925490   -2.733823 #> 3     1.701152    4.039286  -0.41919786   -2.733823 #> 4     1.338862    3.351001  -0.07931195   -2.733823 #> 5     2.788022    6.792424  -0.24925490   -2.733823 #> 6     4.237181    8.857277   0.26057397   -1.946667 head(fscale(d, f, w))                         # Grouped and weighted scaling and centering #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1   0.17939134   0.1384305   -0.3190052   -0.580276 #> 2  -0.37921170  -1.1904223   -0.3190052   -0.580276 #> 3  -0.93781474  -0.6588812   -0.9108174   -0.580276 #> 4  -1.21711626  -0.9246517    0.2728070   -0.580276 #> 5  -0.09991018   0.4042010   -0.3190052   -0.580276 #> 6   1.01729590   1.2015127    1.4564313    1.195967 head(fscale(d, f, w, mean = 5, sd = 3))       # Custom grouped and weighted scaling and centering #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.538174    5.415291     4.042984    3.259172 #> 2     3.862365    1.428733     4.042984    3.259172 #> 3     2.186556    3.023356     2.267548    3.259172 #> 4     1.348651    2.226045     5.818421    3.259172 #> 5     4.700269    6.212603     4.042984    3.259172 #> 6     8.051888    8.604538     9.369294    8.587901 head(fscale(d, f, w, mean = FALSE,            # Preserving group means             sd = \"within.sd\"))                # and setting group-sd to fsd(fwithin(d, f, w), w = w) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.124988    3.495163     1.323723   0.1488517 #> 2     4.847178    3.041600     1.323723   0.1488517 #> 3     4.569368    3.223025     1.082215   0.1488517 #> 4     4.430462    3.132312     1.565231   0.1488517 #> 5     4.986083    3.585875     1.323723   0.1488517 #> 6     5.541704    3.858013     2.048247   0.5054182 head(fscale(d, f, w, mean = \"overall.mean\",   # Full harmonization of group means and variances,             sd = \"within.sd\"))                # while preserving the level and scale of the data. #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.928819    3.133280     3.591337    1.082571 #> 2     5.651009    2.679717     3.591337    1.082571 #> 3     5.373199    2.861142     3.349829    1.082571 #> 4     5.234294    2.770429     3.832845    1.082571 #> 5     5.789914    3.223992     3.591337    1.082571 #> 6     6.345535    3.496130     4.315861    1.439137  head(get_vars(iris, 1:2))                      # Use get_vars for fast selecting, gv is shortcut #>   Sepal.Length Sepal.Width #> 1          5.1         3.5 #> 2          4.9         3.0 #> 3          4.7         3.2 #> 4          4.6         3.1 #> 5          5.0         3.6 #> 6          5.4         3.9 head(fhdbetween(gv(iris, 1:2), gv(iris, 3:5))) # Linear prediction with factors and covariates #>   Sepal.Length Sepal.Width #> 1     4.950107    3.389732 #> 2     4.950107    3.389732 #> 3     4.859513    3.374264 #> 4     5.040702    3.405199 #> 5     4.950107    3.389732 #> 6     5.220692    3.560823 head(fhdwithin(gv(iris, 1:2), gv(iris, 3:5)))  # Linear partialling out factors and covariates #>   Sepal.Length Sepal.Width #> 1   0.14989286   0.1102684 #> 2  -0.05010714  -0.3897316 #> 3  -0.15951256  -0.1742640 #> 4  -0.44070173  -0.3051992 #> 5   0.04989286   0.2102684 #> 6   0.17930818   0.3391766 ss(iris, 1:10, 1:2)                            # Similarly fsubset/ss for fast subsetting rows #>    Sepal.Length Sepal.Width #> 1           5.1         3.5 #> 2           4.9         3.0 #> 3           4.7         3.2 #> 4           4.6         3.1 #> 5           5.0         3.6 #> 6           5.4         3.9 #> 7           4.6         3.4 #> 8           5.0         3.4 #> 9           4.4         2.9 #> 10          4.9         3.1  # Simple Time-Computations.. head(flag(AirPassengers, -1:3))                # One lead and three lags #>           F1  --  L1  L2  L3 #> Jan 1949 118 112  NA  NA  NA #> Feb 1949 132 118 112  NA  NA #> Mar 1949 129 132 118 112  NA #> Apr 1949 121 129 132 118 112 #> May 1949 135 121 129 132 118 #> Jun 1949 148 135 121 129 132 head(fdiff(EuStockMarkets,                     # Suitably lagged first and second differences       c(1, frequency(EuStockMarkets)), diff = 1:2)) #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>          D1.DAX D2.DAX L260D1.DAX L260D2.DAX D1.SMI D2.SMI L260D1.SMI #> 1991.496     NA     NA         NA         NA     NA     NA         NA #> 1991.500 -15.12     NA         NA         NA   10.4     NA         NA #> 1991.504  -7.12   8.00         NA         NA   -9.9  -20.3         NA #> 1991.508  14.53  21.65         NA         NA    5.5   15.4         NA #>          L260D2.SMI D1.CAC D2.CAC L260D1.CAC L260D2.CAC D1.FTSE D2.FTSE #> 1991.496         NA     NA     NA         NA         NA      NA      NA #> 1991.500         NA  -22.3     NA         NA         NA    16.6      NA #> 1991.504         NA  -32.5  -10.2         NA         NA   -12.0   -28.6 #> 1991.508         NA   -9.9   22.6         NA         NA    22.2    34.2 #>          L260D1.FTSE L260D2.FTSE #> 1991.496          NA          NA #> 1991.500          NA          NA #> 1991.504          NA          NA #> 1991.508          NA          NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] head(fdiff(EuStockMarkets, rho = 0.87))        # Quasi-differences (x_t - rho*x_t-1) #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>               DAX     SMI     CAC    FTSE #> 1991.496       NA      NA      NA      NA #> 1991.500 196.6175 228.553 208.164 334.268 #> 1991.504 202.6519 209.605 195.065 307.826 #> 1991.508 223.3763 223.718 213.440 340.466 #> 1991.512 207.8552 221.433 237.053 335.452 #> 1991.515 202.8108 204.258 215.203 305.111 head(fdiff(EuStockMarkets, log = TRUE))        # Log-differences #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>                   DAX          SMI          CAC         FTSE #> 1991.496           NA           NA           NA           NA #> 1991.500 -0.009326550  0.006178360 -0.012658756  0.006770286 #> 1991.504 -0.004422175 -0.005880448 -0.018740638 -0.004889587 #> 1991.508  0.009003794  0.003271184 -0.005779182  0.009027020 #> 1991.512 -0.001778217  0.001483372  0.008743353  0.005771847 #> 1991.515 -0.004676712 -0.008933417 -0.005120160 -0.007230164 head(fgrowth(EuStockMarkets))                  # Exact growth rates (percentage change) #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>                 DAX        SMI        CAC       FTSE #> 1991.496         NA         NA         NA         NA #> 1991.500 -0.9283193  0.6197485 -1.2578971  0.6793256 #> 1991.504 -0.4412412 -0.5863192 -1.8566124 -0.4877652 #> 1991.508  0.9044450  0.3276540 -0.5762515  0.9067887 #> 1991.512 -0.1776637  0.1484472  0.8781687  0.5788536 #> 1991.515 -0.4665793 -0.8893632 -0.5107074 -0.7204089 head(fgrowth(EuStockMarkets, logdiff = TRUE))  # Log-difference growth rates (percentage change) #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>                 DAX        SMI        CAC       FTSE #> 1991.496         NA         NA         NA         NA #> 1991.500 -0.9326550  0.6178360 -1.2658756  0.6770286 #> 1991.504 -0.4422175 -0.5880448 -1.8740638 -0.4889587 #> 1991.508  0.9003794  0.3271184 -0.5779182  0.9027020 #> 1991.512 -0.1778217  0.1483372  0.8743353  0.5771847 #> 1991.515 -0.4676712 -0.8933417 -0.5120160 -0.7230164 # Note that it is not necessary to use factors for grouping. fmean(gv(mtcars, -c(2,8:9)), mtcars$cyl) # Can also use vector (internally converted using qF()) #>        mpg     disp        hp     drat       wt     qsec     gear     carb #> 4 26.66364 105.1364  82.63636 4.070909 2.285727 19.13727 4.090909 1.545455 #> 6 19.74286 183.3143 122.28571 3.585714 3.117143 17.97714 3.857143 3.428571 #> 8 15.10000 353.1000 209.21429 3.229286 3.999214 16.77214 3.285714 3.500000 fmean(gv(mtcars, -c(2,8:9)),       gv(mtcars, c(2,8:9)))              # or a list of vector (internally grouped using GRP()) #>            mpg     disp        hp     drat       wt     qsec     gear     carb #> 4.0.1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 2.000000 #> 4.1.0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 1.666667 #> 4.1.1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 1.428571 #> 6.0.1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667 #> 6.1.0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000 #> 8.0.0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333 #> 8.0.1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000 g <- GRP(mtcars, ~ cyl + vs + am)        # It is also possible to create grouping objects print(g)                                 # These are instructive to learn about the grouping, #> collapse grouping object of length 32 with 7 ordered groups #>  #> Call: GRP.default(X = mtcars, by = ~cyl + vs + am), X is unsorted #>  #> Distribution of group sizes:  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   2.500   3.000   4.571   5.500  12.000  #>  #> Groups with sizes:  #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>     1     3     7     3     4    12     2  plot(g)                                  # and are directly handed down to C++ code  fmean(gv(mtcars, -c(2,8:9)), g)          # This can speed up multiple computations over same groups #>            mpg     disp        hp     drat       wt     qsec     gear     carb #> 4.0.1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 2.000000 #> 4.1.0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 1.666667 #> 4.1.1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 1.428571 #> 6.0.1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667 #> 6.1.0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000 #> 8.0.0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333 #> 8.0.1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000 fsd(gv(mtcars, -c(2,8:9)), g) #>             mpg      disp       hp      drat        wt       qsec      gear #> 4.0.1        NA        NA       NA        NA        NA         NA        NA #> 4.1.0 1.4525839 13.969371 19.65536 0.1300000 0.4075230 1.67143651 0.5773503 #> 4.1.1 4.7577005 18.802128 24.14441 0.3783926 0.4400840 0.94546285 0.3779645 #> 6.0.1 0.7505553  8.660254 37.52777 0.1616581 0.1281601 0.76872188 0.5773503 #> 6.1.0 1.6317169 44.742634  9.17878 0.5919459 0.1162164 0.81590441 0.5773503 #> 8.0.0 2.7743959 71.823494 33.35984 0.2302749 0.7683069 0.80164745 0.0000000 #> 8.0.1 0.5656854 35.355339 50.20458 0.4808326 0.2828427 0.07071068 0.0000000 #>            carb #> 4.0.1        NA #> 4.1.0 0.5773503 #> 4.1.1 0.5345225 #> 6.0.1 1.1547005 #> 6.1.0 1.7320508 #> 8.0.0 0.9003366 #> 8.0.1 2.8284271  # Factors can efficiently be created using qF() f1 <- qF(mtcars$cyl)                     # Unlike GRP objects, factors are checked for NA's f2 <- qF(mtcars$cyl, na.exclude = FALSE) # This can however be avoided through this option class(f2)                                # Note the added class #> [1] \"factor\"      \"na.included\"  library(microbenchmark) microbenchmark(fmean(mtcars, f1), fmean(mtcars, f2)) # A minor difference, larger on larger data #> Unit: microseconds #>               expr   min    lq    mean median    uq    max neval #>  fmean(mtcars, f1) 3.813 4.018 4.61701 4.0795 4.182 53.751   100 #>  fmean(mtcars, f2) 3.813 3.895 4.01349 3.9360 4.018  9.061   100  with(mtcars, finteraction(cyl, vs, am))  # Efficient interactions of vectors and/or factors #>  [1] 6.0.1 6.0.1 4.1.1 6.1.0 8.0.0 6.1.0 8.0.0 4.1.0 4.1.0 6.1.0 6.1.0 8.0.0 #> [13] 8.0.0 8.0.0 8.0.0 8.0.0 8.0.0 4.1.1 4.1.1 4.1.1 4.1.0 8.0.0 8.0.0 8.0.0 #> [25] 8.0.0 4.1.1 4.0.1 4.1.1 8.0.1 6.0.1 8.0.1 4.1.1 #> Levels: 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1 finteraction(gv(mtcars, c(2,8:9)))       # .. or lists of vectors/factors #>  [1] 6.0.1 6.0.1 4.1.1 6.1.0 8.0.0 6.1.0 8.0.0 4.1.0 4.1.0 6.1.0 6.1.0 8.0.0 #> [13] 8.0.0 8.0.0 8.0.0 8.0.0 8.0.0 4.1.1 4.1.1 4.1.1 4.1.0 8.0.0 8.0.0 8.0.0 #> [25] 8.0.0 4.1.1 4.0.1 4.1.1 8.0.1 6.0.1 8.0.1 4.1.1 #> Levels: 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  # Simple row- or column-wise computations on matrices or data frames with dapply() dapply(mtcars, quantile)                 # column quantiles #>         mpg cyl    disp    hp  drat      wt    qsec vs am gear carb #> 0%   10.400   4  71.100  52.0 2.760 1.51300 14.5000  0  0    3    1 #> 25%  15.425   4 120.825  96.5 3.080 2.58125 16.8925  0  0    3    2 #> 50%  19.200   6 196.300 123.0 3.695 3.32500 17.7100  0  0    4    2 #> 75%  22.800   8 326.000 180.0 3.920 3.61000 18.9000  1  1    4    4 #> 100% 33.900   8 472.000 335.0 4.930 5.42400 22.9000  1  1    5    8 dapply(mtcars, quantile, MARGIN = 1)     # Row-quantiles #>                   0%    25%   50%    75%  100% #> Mazda RX4          0 3.2600 4.000 18.730 160.0 #> Mazda RX4 Wag      0 3.3875 4.000 19.010 160.0 #> Datsun 710         1 1.6600 4.000 20.705 108.0 #> Hornet 4 Drive     0 2.0000 3.215 20.420 258.0 #> Hornet Sportabout  0 2.5000 3.440 17.860 360.0 #> Valiant            0 1.8800 3.460 19.160 225.0 #> Duster 360         0 3.1050 4.000 15.070 360.0 #> Merc 240D          0 2.5950 4.000 22.200 146.7 #> Merc 230           0 2.5750 4.000 22.850 140.8 #> Merc 280           0 3.6800 4.000 18.750 167.6 #> Merc 280C          0 3.6800 4.000 18.350 167.6 #> Merc 450SE         0 3.0000 4.070 16.900 275.8 #> Merc 450SL         0 3.0000 3.730 17.450 275.8 #> Merc 450SLC        0 3.0000 3.780 16.600 275.8 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 18 rows ]   # dapply preserves the data structure of any matrices / data frames passed   # Some fast matrix row/column functions are also provided by the matrixStats package # Similarly, BY performs grouped comptations BY(mtcars, f2, quantile) #>         mpg cyl   disp    hp  drat     wt  qsec vs  am gear carb #> 4.0%   21.4   4  71.10  52.0 3.690 1.5130 16.70  0 0.0    3    1 #> 4.25%  22.8   4  78.85  65.5 3.810 1.8850 18.56  1 0.5    4    1 #> 4.50%  26.0   4 108.00  91.0 4.080 2.2000 18.90  1 1.0    4    2 #> 4.75%  30.4   4 120.65  96.0 4.165 2.6225 19.95  1 1.0    4    2 #> 4.100% 33.9   4 146.70 113.0 4.930 3.1900 22.90  1 1.0    5    2 #> 6.0%   17.8   6 145.00 105.0 2.760 2.6200 15.50  0 0.0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ] BY(mtcars, f2, quantile, expand.wide = TRUE) #>   mpg.0% mpg.25% mpg.50% mpg.75% mpg.100% cyl.0% cyl.25% cyl.50% cyl.75% #> 4   21.4    22.8      26    30.4     33.9      4       4       4       4 #>   cyl.100% disp.0% disp.25% disp.50% disp.75% disp.100% hp.0% hp.25% hp.50% #> 4        4    71.1    78.85      108   120.65     146.7    52   65.5     91 #>   hp.75% hp.100% drat.0% drat.25% drat.50% drat.75% drat.100% wt.0% wt.25% #> 4     96     113    3.69     3.81     4.08    4.165      4.93 1.513  1.885 #>   wt.50% wt.75% wt.100% qsec.0% qsec.25% qsec.50% qsec.75% qsec.100% vs.0% #> 4    2.2 2.6225    3.19    16.7    18.56     18.9    19.95      22.9     0 #>   vs.25% vs.50% vs.75% vs.100% am.0% am.25% am.50% am.75% am.100% gear.0% #> 4      1      1      1       1     0    0.5      1      1       1       3 #>   gear.25% gear.50% gear.75% gear.100% carb.0% carb.25% carb.50% carb.75% #> 4        4        4        4         5       1        1        2        2 #>   carb.100% #> 4         2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] # For efficient (grouped) replacing and sweeping out computed statistics, use TRA() sds <- fsd(mtcars) head(TRA(mtcars, sds, \"/\"))     # Simple scaling (if sd's not needed, use fsd(mtcars, TRA = \"/\")) #>                        mpg     cyl      disp       hp     drat       wt #> Mazda RX4         3.484351 3.35961 1.2909608 1.604367 7.294100 2.677684 #> Mazda RX4 Wag     3.484351 3.35961 1.2909608 1.604367 7.294100 2.938298 #> Datsun 710        3.783009 2.23974 0.8713986 1.356419 7.200586 2.371079 #> Hornet 4 Drive    3.550719 3.35961 2.0816744 1.604367 5.760468 3.285784 #> Hornet Sportabout 3.102731 4.47948 2.9046619 2.552402 5.891388 3.515738 #> Valiant           3.003178 3.35961 1.8154137 1.531441 5.161978 3.536178 #>                        qsec       vs       am     gear      carb #> Mazda RX4          9.211261 0.000000 2.004044 5.421494 2.4764735 #> Mazda RX4 Wag      9.524645 0.000000 2.004044 5.421494 2.4764735 #> Datsun 710        10.414433 1.984063 2.004044 5.421494 0.6191184 #> Hornet 4 Drive    10.878913 1.984063 0.000000 4.066120 0.6191184 #> Hornet Sportabout  9.524645 0.000000 0.000000 4.066120 1.2382368 #> Valiant           11.315413 1.984063 0.000000 4.066120 0.6191184  microbenchmark(TRA(mtcars, sds, \"/\"), sweep(mtcars, 2, sds, \"/\")) # A remarkable performance gain.. #> Unit: microseconds #>                        expr     min      lq      mean   median       uq     max #>       TRA(mtcars, sds, \"/\")   2.050   2.624   3.28369   3.0135   3.5260  12.915 #>  sweep(mtcars, 2, sds, \"/\") 289.173 295.651 307.49180 300.0175 310.8005 473.919 #>  neval #>    100 #>    100  sds <- fsd(mtcars, f2) head(TRA(mtcars, sds, \"/\", f2)) # Groupd scaling (if sd's not needed: fsd(mtcars, f2, TRA = \"/\")) #>                         mpg cyl     disp       hp      drat       wt      qsec #> Mazda RX4         14.447218 Inf 3.849628 4.534121  8.192327 7.352414  9.643407 #> Mazda RX4 Wag     14.447218 Inf 3.849628 4.534121  8.192327 8.068012  9.971493 #> Datsun 710         5.055626 Inf 4.019114 4.442421 10.534350 4.073293 11.061282 #> Hornet 4 Drive    14.722403 Inf 6.207525 4.534121  6.469838 9.022142 11.389297 #> Hornet Sportabout  7.304550 Inf 5.311981 3.432928  8.459515 4.529864 14.230606 #> Valiant           12.452126 Inf 5.413539 4.328025  5.797647 9.709677 11.846275 #>                         vs       am     gear      carb #> Mazda RX4         0.000000 1.870829 5.796551 2.2067091 #> Mazda RX4 Wag     0.000000 1.870829 5.796551 2.2067091 #> Datsun 710        3.316625 2.140872 7.416198 1.9148542 #> Hornet 4 Drive    1.870829 0.000000 4.347413 0.5516773 #> Hornet Sportabout      NaN 0.000000 4.130678 1.2848321 #> Valiant           1.870829 0.000000 4.347413 0.5516773  # All functions above perserve the structure of matrices / data frames # If conversions are required, use these efficient functions: mtcarsM <- qM(mtcars)                      # Matrix from data.frame head(qDF(mtcarsM))                         # data.frame from matrix columns #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 head(mrtl(mtcarsM, TRUE, \"data.frame\"))    # data.frame from matrix rows, etc.. #>     Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive Hornet Sportabout Valiant #> mpg        21            21       22.8           21.4              18.7    18.1 #> cyl         6             6        4.0            6.0               8.0     6.0 #>     Duster 360 Merc 240D Merc 230 Merc 280 Merc 280C Merc 450SE Merc 450SL #> mpg       14.3      24.4     22.8     19.2      17.8       16.4       17.3 #> cyl        8.0       4.0      4.0      6.0       6.0        8.0        8.0 #>     Merc 450SLC Cadillac Fleetwood Lincoln Continental Chrysler Imperial #> mpg        15.2               10.4                10.4              14.7 #> cyl         8.0                8.0                 8.0               8.0 #>     Fiat 128 Honda Civic Toyota Corolla Toyota Corona Dodge Challenger #> mpg     32.4        30.4           33.9          21.5             15.5 #> cyl      4.0         4.0            4.0           4.0              8.0 #>     AMC Javelin Camaro Z28 Pontiac Firebird Fiat X1-9 Porsche 914-2 #> mpg        15.2       13.3             19.2      27.3            26 #> cyl         8.0        8.0              8.0       4.0             4 #>     Lotus Europa Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E #> mpg         30.4           15.8         19.7            15       21.4 #> cyl          4.0            8.0          6.0             8        4.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] head(qDT(mtcarsM, \"cars\"))                 # Saving row.names when converting matrix to data.table #>                 cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am #>               <char> <num> <num> <num> <num> <num> <num> <num> <num> <num> #> 1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1 #> 2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1 #> 3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1 #> 4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0 #>     gear  carb #>    <num> <num> #> 1:     4     4 #> 2:     4     4 #> 3:     4     1 #> 4:     3     1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] head(qDT(mtcars, \"cars\"))                  # Same use a data.frame #>                 cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am #>               <char> <num> <num> <num> <num> <num> <num> <num> <num> <num> #> 1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1 #> 2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1 #> 3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1 #> 4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0 #>     gear  carb #>    <num> <num> #> 1:     4     4 #> 2:     4     4 #> 3:     4     1 #> 4:     3     1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]   ## Now let's get some real data and see how we can use this power for data manipulation head(wlddev) # World Bank World Development Data: 216 countries, 61 years, 5 series (columns 9-13) #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # Starting with some discriptive tools... namlab(wlddev, class = TRUE)           # Show variable names, labels and classes #>    Variable     Class #> 1   country character #> 2     iso3c    factor #> 3      date      Date #> 4      year   integer #> 5    decade   integer #> 6    region    factor #> 7    income    factor #> 8      OECD   logical #> 9     PCGDP   numeric #> 10   LIFEEX   numeric #> 11     GINI   numeric #> 12      ODA   numeric #> 13      POP   numeric #>                                                                                Label #> 1                                                                       Country Name #> 2                                                                       Country Code #> 3                                                         Date Recorded (Fictitious) #> 4                                                                               Year #> 5                                                                             Decade #> 6                                                                             Region #> 7                                                                       Income Level #> 8                                                            Is OECD Member Country? #> 9                                                 GDP per capita (constant 2010 US$) #> 10                                           Life expectancy at birth, total (years) #> 11                                                  Gini index (World Bank estimate) #> 12 Net official development assistance and official aid received (constant 2018 US$) #> 13                                                                 Population, total fnobs(wlddev)                          # Observation count #> country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX  #>   13176   13176   13176   13176   13176   13176   13176   13176    9470   11670  #>    GINI     ODA     POP  #>    1744    8608   12919  pwnobs(wlddev)                         # Pairwise observation count #>         country iso3c  date  year decade region income  OECD PCGDP LIFEEX GINI #> country   13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 #> iso3c     13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 #> date      13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 #> year      13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 #> decade    13176 13176 13176 13176  13176  13176  13176 13176  9470  11670 1744 #>          ODA   POP #> country 8608 12919 #> iso3c   8608 12919 #> date    8608 12919 #> year    8608 12919 #> decade  8608 12919 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ] head(fnobs(wlddev, wlddev$country))    # Grouped observation count #>                country iso3c date year decade region income OECD PCGDP LIFEEX #> Afghanistan         61    61   61   61     61     61     61   61    18     60 #> Albania             61    61   61   61     61     61     61   61    40     60 #> Algeria             61    61   61   61     61     61     61   61    60     60 #> American Samoa      61    61   61   61     61     61     61   61    17      0 #> Andorra             61    61   61   61     61     61     61   61    50      0 #>                GINI ODA POP #> Afghanistan       0  60  60 #> Albania           9  32  60 #> Algeria           3  60  60 #> American Samoa    0   0  60 #> Andorra           0   0  60 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fndistinct(wlddev)                     # Distinct values #> country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX  #>     216     216      61      61       7       7       4       2    9470   10548  #>    GINI     ODA     POP  #>     368    7832   12877  descr(wlddev)                          # Describe data #> Dataset: wlddev, 13 Variables, N = 13176 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics #>       N  Ndist #>   13176    216 #> Table #>                       Freq   Perc #> Afghanistan             61   0.46 #> Albania                 61   0.46 #> Algeria                 61   0.46 #> American Samoa          61   0.46 #> Andorra                 61   0.46 #> Angola                  61   0.46 #> Antigua and Barbuda     61   0.46 #> Argentina               61   0.46 #> Armenia                 61   0.46 #> Aruba                   61   0.46 #> Australia               61   0.46 #> Austria                 61   0.46 #> Azerbaijan              61   0.46 #> Bahamas, The            61   0.46 #> ... 202 Others       12322  93.52 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      61      61      61      61      61      61  #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics #>       N  Ndist #>   13176    216 #> Table #>                  Freq   Perc #> ABW                61   0.46 #> AFG                61   0.46 #> AGO                61   0.46 #> ALB                61   0.46 #> AND                61   0.46 #> ARE                61   0.46 #> ARG                61   0.46 #> ARM                61   0.46 #> ASM                61   0.46 #> ATG                61   0.46 #> AUS                61   0.46 #> AUT                61   0.46 #> AZE                61   0.46 #> BDI                61   0.46 #> ... 202 Others  12322  93.52 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      61      61      61      61      61      61  #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics #>          N       Ndist         Min         Max   #>      13176          61  1961-01-01  2021-01-01   #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics #>       N  Ndist  Mean     SD   Min   Max  Skew  Kurt #>   13176     61  1990  17.61  1960  2020    -0   1.8 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1963  1966  1975  1990  2005  2014  2017  2020 #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics #>       N  Ndist     Mean     SD   Min   Max  Skew  Kurt #>   13176      7  1985.57  17.51  1960  2020  0.03  1.79 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1960  1960  1970  1990  2000  2010  2010  2020 #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics #>       N  Ndist #>   13176      7 #> Table #>                             Freq   Perc #> Europe & Central Asia       3538  26.85 #> Sub-Saharan Africa          2928  22.22 #> Latin America & Caribbean   2562  19.44 #> East Asia & Pacific         2196  16.67 #> Middle East & North Africa  1281   9.72 #> South Asia                   488   3.70 #> North America                183   1.39 #> -------------------------------------------------------------------------------- #> income (factor): Income Level #> Statistics #>       N  Ndist #>   13176      4 #> Table #>                      Freq   Perc #> High income          4819  36.57 #> Upper middle income  3660  27.78 #> Lower middle income  2867  21.76 #> Low income           1830  13.89 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics #>       N  Ndist #>   13176      2 #> Table #>         Freq   Perc #> FALSE  10980  83.33 #> TRUE    2196  16.67 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (28.13% NAs) #>      N  Ndist      Mean        SD     Min        Max  Skew   Kurt #>   9470   9470  12048.78  19077.64  132.08  196061.42  3.13  17.12 #> Quantiles #>       1%      5%     10%      25%      50%       75%       90%       95% #>   227.71  399.62  555.55  1303.19  3767.16  14787.03  35646.02  48507.84 #>        99% #>   92340.28 #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (11.43% NAs) #>       N  Ndist  Mean     SD    Min    Max   Skew  Kurt #>   11670  10548  64.3  11.48  18.91  85.42  -0.67  2.67 #> Quantiles #>      1%     5%    10%    25%    50%    75%    90%    95%    99% #>   35.83  42.77  46.83  56.36  67.44  72.95  77.08  79.34  82.36 #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (86.76% NAs) #>      N  Ndist   Mean   SD   Min   Max  Skew  Kurt #>   1744    368  38.53  9.2  20.7  65.8   0.6  2.53 #> Quantiles #>     1%    5%   10%   25%   50%  75%   90%    95%   99% #>   24.6  26.3  27.6  31.5  36.4   45  52.6  55.98  60.5 #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (34.67% NAs) #>      N  Ndist        Mean          SD          Min             Max  Skew #>   8608   7832  454'720131  868'712654  -997'679993  2.56715605e+10  6.98 #>     Kurt #>   114.89 #> Quantiles #>             1%           5%          10%          25%         50%         75% #>   -12'593999.7  1'363500.01  8'347000.31  44'887499.8  165'970001  495'042503 #>              90%             95%             99% #>   1.18400697e+09  1.93281696e+09  3.73380782e+09 #> -------------------------------------------------------------------------------- #> POP (numeric): Population, total #> Statistics (1.95% NAs) #>       N  Ndist         Mean          SD   Min             Max  Skew    Kurt #>   12919  12877  24'245971.6  102'120674  2833  1.39771500e+09  9.75  108.91 #> Quantiles #>        1%       5%      10%     25%       50%        75%          90% #>   8698.84  31083.3  62268.4  443791  4'072517  12'816178  46'637331.4 #>           95%         99% #>   81'177252.5  308'862641 #> -------------------------------------------------------------------------------- varying(wlddev, ~ country)             # Show which variables vary within countries #>  iso3c   date   year decade region income   OECD  PCGDP LIFEEX   GINI    ODA  #>  FALSE   TRUE   TRUE   TRUE  FALSE  FALSE  FALSE   TRUE   TRUE   TRUE   TRUE  #>    POP  #>   TRUE  qsu(wlddev, pid = ~ country,           # Panel-summarize columns 9 though 12 of this data     cols = 9:12, vlabels = TRUE)       # (between and within countries) #> , , PCGDP: GDP per capita (constant 2010 US$) #>  #>              N/T        Mean          SD          Min         Max #> Overall     9470   12048.778  19077.6416     132.0776  196061.417 #> Between      206  12962.6054  20189.9007     253.1886   141200.38 #> Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254 #>  #> , , LIFEEX: Life expectancy at birth, total (years) #>  #>              N/T     Mean       SD      Min      Max #> Overall    11670  64.2963  11.4764   18.907  85.4171 #> Between      207  64.9537   9.8936  40.9663  85.4171 #> Within   56.3768  64.2963   6.0842  32.9068  84.4198 #>  #> , , GINI: Gini index (World Bank estimate) #>  #>              N/T     Mean      SD      Min      Max #> Overall     1744  38.5341  9.2006     20.7     65.8 #> Between      167  39.4233  8.1356  24.8667  61.7143 #> Within   10.4431  38.5341  2.9277  25.3917  55.3591 #>  #> , , ODA: Net official development assistance and official aid received (constant 2018 US$) #>  #>              N/T        Mean          SD              Min             Max #> Overall     8608  454'720131  868'712654      -997'679993  2.56715605e+10 #> Between      178  439'168412  569'049959       468717.916  3.62337432e+09 #> Within   48.3596  454'720131  650'709624  -2.44379420e+09  2.45610972e+10 #>  qsu(wlddev, ~ region, ~ country,       # Do all of that by region and also compute higher moments     cols = 9:12, higher = TRUE)        # -> returns a 4D array #> , , Overall, PCGDP #>  #>                              N/T        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max    Skew     Kurt #> East Asia & Pacific         71992.1517  1.6392   4.7419 #> Europe & Central Asia       196061.417  2.2022  10.1977 #> Latin America & Caribbean   88391.3331  4.1702  29.3739 #> Middle East & North Africa  116232.753  2.4178   9.7669 #> North America               113236.091   0.938   2.9688 #> South Asia                    8476.564  2.7874  10.3402 #> Sub-Saharan Africa          20532.9523  3.1161  14.4175 #>  #> , , Between, PCGDP #>  #>                             N/T        Mean          SD         Min         Max #> East Asia & Pacific          34  10513.2441   12771.742    444.2899  39722.0077 #> Europe & Central Asia        56  25992.9618   24051.035    809.4753   141200.38 #> Latin America & Caribbean    38   7628.4477   8470.9708   1357.3326  77403.7443 #>                               Skew     Kurt #> East Asia & Pacific         1.1488   2.7089 #> Europe & Central Asia       2.0026   9.0733 #> Latin America & Caribbean   4.4548  32.4956 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 slices ]  qsu(wlddev, ~ region, ~ country, cols = 9:12,     higher = TRUE, array = FALSE) |>                           # Return as a list of matrices.. unlist2d(c(\"Variable\",\"Trans\"), row.names = \"Region\") |> head()# and turn into a tidy data.frame #>   Variable   Trans                     Region    N      Mean        SD #> 1    PCGDP Overall        East Asia & Pacific 1467 10513.244 14383.551 #> 2    PCGDP Overall      Europe & Central Asia 2243 25992.962 26435.132 #> 3    PCGDP Overall  Latin America & Caribbean 1976  7628.448  8818.505 #> 4    PCGDP Overall Middle East & North Africa  842 13878.421 18419.791 #> 5    PCGDP Overall              North America  180 48699.760 24196.285 #> 6    PCGDP Overall                 South Asia  382  1235.926  1611.223 #>          Min        Max      Skew      Kurt #> 1   132.0776  71992.152 1.6392248  4.741856 #> 2   366.9354 196061.417 2.2022472 10.197685 #> 3  1005.4085  88391.333 4.1701769 29.373869 #> 4   578.5996 116232.753 2.4177586  9.766883 #> 5 16405.9053 113236.091 0.9380056  2.968769 #> 6   265.9625   8476.564 2.7873830 10.340176 pwcor(num_vars(wlddev), P = TRUE)                           # Pairwise correlations with p-value #>          year decade  PCGDP LIFEEX   GINI    ODA    POP #> year      1     .99*   .16*   .47*  -.20*   .14*   .06* #> decade   .99*    1     .15*   .46*  -.20*   .14*   .06* #> PCGDP    .16*   .15*    1     .57*  -.44*  -.16*  -.06* #> LIFEEX   .47*   .46*   .57*    1    -.35*  -.02    .03* #> GINI    -.20*  -.20*  -.44*  -.35*    1    -.20*   .04  #> ODA      .14*   .14*  -.16*  -.02   -.20*    1     .31* #> POP      .06*   .06*  -.06*   .03*   .04    .31*    1   pwcor(fmean(num_vars(wlddev), wlddev$country), P = TRUE)    # Correlating country means #> Warning: the standard deviation is zero #>           year  decade   PCGDP  LIFEEX    GINI     ODA     POP #> year       NA      NA      NA      NA      NA      NA      NA  #> decade     NA      1      .00     .00     .00     .00     .00  #> PCGDP      NA     .00      1      .60*   -.42*   -.25*   -.07  #> LIFEEX     NA     .00     .60*     1     -.40*   -.21*   -.02  #> GINI       NA     .00    -.42*   -.40*     1     -.19*   -.04  #> ODA        NA     .00    -.25*   -.21*   -.19*     1      .50* #> POP        NA     .00    -.07    -.02    -.04     .50*     1   pwcor(fwithin(num_vars(wlddev), wlddev$country), P = TRUE)  # Within-country correlations #>          year decade  PCGDP LIFEEX   GINI    ODA    POP #> year      1     .99*   .44*   .84*  -.21*   .19*   .24* #> decade   .99*    1     .44*   .83*  -.19*   .18*   .24* #> PCGDP    .44*   .44*    1     .31*  -.01   -.01    .06* #> LIFEEX   .84*   .83*   .31*    1    -.16*   .17*   .29* #> GINI    -.21*  -.19*  -.01   -.16*    1    -.08*   .01  #> ODA      .19*   .18*  -.01    .17*  -.08*    1    -.11* #> POP      .24*   .24*   .06*   .29*   .01   -.11*    1   psacf(wlddev, ~country, ~year, cols = 9:12)                 # Panel-data Autocorrelation function  pspacf(wlddev, ~country, ~year, cols = 9:12)                # Partial panel-autocorrelations  psmat(wlddev, ~iso3c, ~year, cols = 9:12) |> plot()         # Convert panel to 3D array and plot   ## collapse offers a few very efficent functions for data manipulation: # Fast selecting and replacing columns series <- get_vars(wlddev, 9:12)     # Same as wlddev[9:12] but 2x faster series <- fselect(wlddev, PCGDP:ODA) # Same thing: > 100x faster than dplyr::select get_vars(wlddev, 9:12) <- series     # Replace, 8x faster wlddev[9:12] <- series + replaces names fselect(wlddev, PCGDP:ODA) <- series # Same thing  # Fast subsetting head(fsubset(wlddev, country == \"Ireland\", -country, -iso3c)) #>         date year decade                region      income OECD PCGDP   LIFEEX #> 1 1961-01-01 1960   1960 Europe & Central Asia High income TRUE    NA 69.79651 #> 2 1962-01-01 1961   1960 Europe & Central Asia High income TRUE    NA 69.97827 #> 3 1963-01-01 1962   1960 Europe & Central Asia High income TRUE    NA 70.13407 #> 4 1964-01-01 1963   1960 Europe & Central Asia High income TRUE    NA 70.27293 #> 5 1965-01-01 1964   1960 Europe & Central Asia High income TRUE    NA 70.40129 #> 6 1966-01-01 1965   1960 Europe & Central Asia High income TRUE    NA 70.52315 #>   GINI ODA     POP #> 1   NA  NA 2828600 #> 2   NA  NA 2824400 #> 3   NA  NA 2836050 #> 4   NA  NA 2852650 #> 5   NA  NA 2866550 #> 6   NA  NA 2877300 head(fsubset(wlddev, country == \"Ireland\" & year > 1990, year, PCGDP:ODA)) #>   year    PCGDP   LIFEEX GINI ODA #> 1 1991 24642.11 75.00527   NA  NA #> 2 1992 25292.81 75.18095   NA  NA #> 3 1993 25844.34 75.33612   NA  NA #> 4 1994 27224.37 75.47680 36.9  NA #> 5 1995 29694.65 75.61756 37.0  NA #> 6 1996 31644.89 75.83171 35.6  NA ss(wlddev, 1:10, 1:10) # This is an order of magnitude faster than wlddev[1:10, 1:10] #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #> 6 Afghanistan   AFG 1966-01-01 1965   1960 South Asia Low income FALSE    NA #> 7 Afghanistan   AFG 1967-01-01 1966   1960 South Asia Low income FALSE    NA #>   LIFEEX #> 1 32.446 #> 2 32.962 #> 3 33.471 #> 4 33.971 #> 5 34.463 #> 6 34.948 #> 7 35.430 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]  # Fast transforming head(ftransform(wlddev, ODA_GDP = ODA / PCGDP, ODA_LIFEEX = sqrt(ODA) / LIFEEX)) #> Warning: NaNs produced #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP ODA_GDP ODA_LIFEEX #> 1 32.446   NA 116769997 8996973      NA   333.0462 #> 2 32.962   NA 232080002 9169410      NA   462.1738 #> 3 33.471   NA 112839996 9351441      NA   317.3678 #> 4 33.971   NA 237720001 9543205      NA   453.8627 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] settransform(wlddev, ODA_GDP = ODA / PCGDP, ODA_LIFEEX = sqrt(ODA) / LIFEEX) # by reference #> Warning: NaNs produced head(ftransform(wlddev, PCGDP = NULL, ODA = NULL, GINI_sum = fsum(GINI))) #>       country iso3c       date year decade     region     income  OECD LIFEEX #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE 32.446 #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE 32.962 #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE 33.471 #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE 33.971 #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE 34.463 #>   GINI     POP ODA_GDP ODA_LIFEEX GINI_sum #> 1   NA 8996973      NA   333.0462  67203.5 #> 2   NA 9169410      NA   462.1738  67203.5 #> 3   NA 9351441      NA   317.3678  67203.5 #> 4   NA 9543205      NA   453.8627  67203.5 #> 5   NA 9744781      NA   499.1535  67203.5 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(ftransformv(wlddev, 9:12, log))                   # Can also transform with lists of columns #> Warning: NaNs produced #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>     LIFEEX GINI      ODA     POP ODA_GDP ODA_LIFEEX #> 1 3.479577   NA 18.57572 8996973      NA   333.0462 #> 2 3.495355   NA 19.26259 9169410      NA   462.1738 #> 3 3.510679   NA 18.54148 9351441      NA   317.3678 #> 4 3.525507   NA 19.28660 9543205      NA   453.8627 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] head(ftransformv(wlddev, 9:12, fscale, apply = FALSE)) # apply = FALSE invokes fscale.data.frame #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>      LIFEEX GINI        ODA     POP ODA_GDP ODA_LIFEEX #> 1 -2.775283   NA -0.3890241 8996973      NA   333.0462 #> 2 -2.730321   NA -0.2562874 9169410      NA   462.1738 #> 3 -2.685969   NA -0.3935480 9351441      NA   317.3678 #> 4 -2.642402   NA -0.2497951 9543205      NA   453.8627 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] settransformv(wlddev, 9:12, fscale, apply = FALSE)     # Changing the data by reference ftransform(wlddev) <- fscale(gv(wlddev, 9:12))         # Same thing (using replacement method)  library(magrittr) # Same thing, using magrittr wlddev %<>% ftransformv(9:12, fscale, apply = FALSE) wlddev %>% ftransform(gv(., 9:12) |>              # With compound pipes: Scaling and lagging                         fscale() |> flag(0:2, iso3c, year)) |> head() #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #>      LIFEEX GINI        ODA     POP ODA_GDP ODA_LIFEEX L1.PCGDP L2.PCGDP #> 1 -2.775283   NA -0.3890241 8996973      NA   333.0462       NA       NA #> 2 -2.730321   NA -0.2562874 9169410      NA   462.1738       NA       NA #> 3 -2.685969   NA -0.3935480 9351441      NA   317.3678       NA       NA #>   L1.LIFEEX L2.LIFEEX L1.GINI L2.GINI     L1.ODA     L2.ODA #> 1        NA        NA      NA      NA         NA         NA #> 2 -2.775283        NA      NA      NA -0.3890241         NA #> 3 -2.730321 -2.775283      NA      NA -0.2562874 -0.3890241 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]  # Fast reordering head(roworder(wlddev, -country, year)) #>    country iso3c       date year decade             region              income #> 1 Zimbabwe   ZWE 1961-01-01 1960   1960 Sub-Saharan Africa Lower middle income #> 2 Zimbabwe   ZWE 1962-01-01 1961   1960 Sub-Saharan Africa Lower middle income #> 3 Zimbabwe   ZWE 1963-01-01 1962   1960 Sub-Saharan Africa Lower middle income #> 4 Zimbabwe   ZWE 1964-01-01 1963   1960 Sub-Saharan Africa Lower middle income #>    OECD      PCGDP     LIFEEX GINI        ODA     POP     ODA_GDP ODA_LIFEEX #> 1 FALSE -0.5794259 -0.9826503   NA         NA 3776681          NA         NA #> 2 FALSE -0.5779547 -0.9422195   NA -0.5233262 3905034    97.77415   5.912678 #> 3 FALSE -0.5789920 -0.9018759   NA         NA 4039201          NA         NA #> 4 FALSE -0.5775741 -0.8620551   NA -0.4094221 4178726 96162.61027 182.938198 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] head(colorder(wlddev, country, year)) #>       country year iso3c       date decade     region     income  OECD PCGDP #> 1 Afghanistan 1960   AFG 1961-01-01   1960 South Asia Low income FALSE    NA #> 2 Afghanistan 1961   AFG 1962-01-01   1960 South Asia Low income FALSE    NA #> 3 Afghanistan 1962   AFG 1963-01-01   1960 South Asia Low income FALSE    NA #> 4 Afghanistan 1963   AFG 1964-01-01   1960 South Asia Low income FALSE    NA #>      LIFEEX GINI        ODA     POP ODA_GDP ODA_LIFEEX #> 1 -2.775283   NA -0.3890241 8996973      NA   333.0462 #> 2 -2.730321   NA -0.2562874 9169410      NA   462.1738 #> 3 -2.685969   NA -0.3935480 9351441      NA   317.3678 #> 4 -2.642402   NA -0.2497951 9543205      NA   453.8627 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]  # Fast renaming head(frename(wlddev, country = Ctry, year = Yr)) #>          Ctry iso3c       date   Yr decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>      LIFEEX GINI        ODA     POP ODA_GDP ODA_LIFEEX #> 1 -2.775283   NA -0.3890241 8996973      NA   333.0462 #> 2 -2.730321   NA -0.2562874 9169410      NA   462.1738 #> 3 -2.685969   NA -0.3935480 9351441      NA   317.3678 #> 4 -2.642402   NA -0.2497951 9543205      NA   453.8627 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] setrename(wlddev, country = Ctry, year = Yr)     # By reference head(frename(wlddev, tolower, cols = 9:12)) #>          Ctry iso3c       date   Yr decade     region     income  OECD pcgdp #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>      lifeex gini        oda     POP ODA_GDP ODA_LIFEEX #> 1 -2.775283   NA -0.3890241 8996973      NA   333.0462 #> 2 -2.730321   NA -0.2562874 9169410      NA   462.1738 #> 3 -2.685969   NA -0.3935480 9351441      NA   317.3678 #> 4 -2.642402   NA -0.2497951 9543205      NA   453.8627 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]  # Fast grouping fgroup_by(wlddev, Ctry, decade) |> fgroup_vars() |> head() #>          Ctry decade #> 1 Afghanistan   1960 #> 2 Afghanistan   1960 #> 3 Afghanistan   1960 #> 4 Afghanistan   1960 #> 5 Afghanistan   1960 #> 6 Afghanistan   1960 rm(wlddev)                                       # .. but only works with collapse functions  ## Now lets start putting things together wlddev |> fsubset(year > 1990, region, income, PCGDP:ODA) |>   fgroup_by(region, income) |> fmean()         # Fast aggregation using the mean #>                       region              income      PCGDP   LIFEEX     GINI #> 1        East Asia & Pacific         High income 32671.0522 78.21996 32.95000 #> 2        East Asia & Pacific Lower middle income  1738.5111 65.45647 36.51972 #> 3        East Asia & Pacific Upper middle income  4575.8695 70.87431 40.64815 #> 4      Europe & Central Asia         High income 40814.1215 77.67583 30.94218 #> 5      Europe & Central Asia          Low income   695.7951 65.04128 32.13333 #> 6      Europe & Central Asia Lower middle income  1779.7149 68.79873 30.66176 #> 7      Europe & Central Asia Upper middle income  5182.1800 71.30199 34.85362 #> 8  Latin America & Caribbean         High income 19864.5057 75.55674 48.35714 #> 9  Latin America & Caribbean          Low income  1189.8492 58.97359 41.10000 #> 10 Latin America & Caribbean Lower middle income  1987.5292 69.31612 50.58125 #> 11 Latin America & Caribbean Upper middle income  6224.6674 72.58127 49.88547 #>          ODA #> 1  112194118 #> 2  509965862 #> 3  219146704 #> 4  321907195 #> 5  244539286 #> 6  395371417 #> 7  463652924 #> 8   31307379 #> 9  753747928 #> 10 559540257 #> 11 188305879 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 12 rows ]  # Same thing using dplyr manipulation verbs library(dplyr) wlddev |> filter(year > 1990) |> select(region, income, PCGDP:ODA) |>   group_by(region,income) |> fmean()       # This is already a lot faster than summarize_all(mean) #> # A tibble: 23 × 6 #>    region                    income               PCGDP LIFEEX  GINI        ODA #>    <fct>                     <fct>                <dbl>  <dbl> <dbl>      <dbl> #>  1 East Asia & Pacific       High income         32671.   78.2  33.0 112194118. #>  2 East Asia & Pacific       Lower middle income  1739.   65.5  36.5 509965862. #>  3 East Asia & Pacific       Upper middle income  4576.   70.9  40.6 219146704. #>  4 Europe & Central Asia     High income         40814.   77.7  30.9 321907195. #>  5 Europe & Central Asia     Low income            696.   65.0  32.1 244539286. #>  6 Europe & Central Asia     Lower middle income  1780.   68.8  30.7 395371417. #>  7 Europe & Central Asia     Upper middle income  5182.   71.3  34.9 463652924. #>  8 Latin America & Caribbean High income         19865.   75.6  48.4  31307379. #>  9 Latin America & Caribbean Low income           1190.   59.0  41.1 753747928. #> 10 Latin America & Caribbean Lower middle income  1988.   69.3  50.6 559540257. #> # ℹ 13 more rows  wlddev |> fsubset(year > 1990, region, income, PCGDP:POP) |>   fgroup_by(region, income) |> fmean(POP)     # Weighted group means #>                       region              income     sum.POP      PCGDP #> 1        East Asia & Pacific         High income  6165902760 37889.3406 #> 2        East Asia & Pacific Lower middle income 13784998066  2135.6182 #> 3        East Asia & Pacific Upper middle income 40150644873  3769.4215 #> 4      Europe & Central Asia         High income 13923291507 34583.4203 #> 5      Europe & Central Asia          Low income   203224216   722.7351 #> 6      Europe & Central Asia Lower middle income  2399154808  2145.1352 #> 7      Europe & Central Asia Upper middle income  8919358306  8238.8391 #> 8  Latin America & Caribbean         High income   842939686 13068.1667 #> 9  Latin America & Caribbean          Low income   267125746  1193.5889 #> 10 Latin America & Caribbean Lower middle income   815192217  2011.6260 #>      LIFEEX     GINI        ODA #> 1  80.79250 32.81601  -79785907 #> 2  68.24548 36.40362 1060544119 #> 3  73.07773 40.38496 1229983586 #> 4  78.82923 32.27710 1107199019 #> 5  65.76604 32.22326  261043896 #> 6  68.98050 28.97857  556232624 #> 7  69.78322 38.66475 1187976647 #> 8  76.76809 48.85497   97880105 #> 9  59.34831 41.10000  794781510 #> 10 69.33538 50.52363  571015463 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13 rows ]  wlddev |> fsubset(year > 1990, region, income, PCGDP:POP) |>   fgroup_by(region, income) |> fsd(POP)       # Weighted group standard deviations #>                       region              income     sum.POP       PCGDP #> 1        East Asia & Pacific         High income  6165902760 11619.90339 #> 2        East Asia & Pacific Lower middle income 13784998066  1074.84975 #> 3        East Asia & Pacific Upper middle income 40150644873  2374.39410 #> 4      Europe & Central Asia         High income 13923291507 13593.46879 #> 5      Europe & Central Asia          Low income   203224216   238.47730 #> 6      Europe & Central Asia Lower middle income  2399154808   841.97662 #> 7      Europe & Central Asia Upper middle income  8919358306  3175.38606 #> 8  Latin America & Caribbean         High income   842939686  6273.64310 #> 9  Latin America & Caribbean          Low income   267125746    70.56928 #> 10 Latin America & Caribbean Lower middle income   815192217   580.31064 #>      LIFEEX     GINI        ODA #> 1  2.730868 1.247116  129087430 #> 2  4.230245 4.316708  943798728 #> 3  2.428883 2.458222 1519636778 #> 4  2.895801 2.994940 1131585991 #> 5  4.555972 1.547793  112926454 #> 6  1.688117 4.573107  370416116 #> 7  3.707457 4.227846 1068502287 #> 8  2.421882 5.289102   71662290 #> 9  2.815436 0.000000  566758933 #> 10 4.220554 5.684628  243414533 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13 rows ]  wlddev |> na_omit(cols = \"POP\") |> fgroup_by(region, income) |>   fselect(PCGDP:POP) |> fnth(0.75, POP)       # Weighted group third quartile #>                       region              income     sum.POP     PCGDP   LIFEEX #> 1        East Asia & Pacific         High income 11407808149 42201.079 81.71320 #> 2        East Asia & Pacific Lower middle income 22174820629  2350.332 69.84968 #> 3        East Asia & Pacific Upper middle income 69639871478  3796.016 73.77902 #> 4      Europe & Central Asia         High income 27285316560 37939.285 79.12994 #> 5      Europe & Central Asia          Low income   311485944  1010.399 68.74935 #> 6      Europe & Central Asia Lower middle income  4511786205  3049.405 70.15657 #> 7      Europe & Central Asia Upper middle income 16972478305 10543.823 70.57815 #> 8  Latin America & Caribbean         High income  1466292826 13929.260 77.62527 #> 9  Latin America & Caribbean          Low income   429756890  1426.615 60.19115 #> 10 Latin America & Caribbean Lower middle income  1290800630  2241.730 71.18339 #>        GINI        ODA #> 1  33.50000  744862041 #> 2  39.09699 1828537218 #> 3  42.25373 2498289114 #> 4  34.70000 1665332268 #> 5  33.54169  369881699 #> 6  29.76143  713578745 #> 7  41.20000 1780829097 #> 8  54.82206  156066838 #> 9  41.10000  956301069 #> 10 55.50000  657517519 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13 rows ]  wlddev |> fgroup_by(country) |> fselect(PCGDP:ODA) |>   fwithin() |> head()                         # Within transformation #>   PCGDP    LIFEEX GINI         ODA #> 1    NA -16.75117   NA -1370778502 #> 2    NA -16.23517   NA -1255468497 #> 3    NA -15.72617   NA -1374708502 #> 4    NA -15.22617   NA -1249828497 #> 5    NA -14.73417   NA -1191628485 #> 6    NA -14.24917   NA -1145708502 wlddev |> fgroup_by(country) |> fselect(PCGDP:ODA) |>   fmedian(TRA = \"-\") |> head()                # Grouped centering using the median #>   PCGDP   LIFEEX GINI        ODA #> 1    NA -17.5395   NA -144765007 #> 2    NA -17.0235   NA  -29455002 #> 3    NA -16.5145   NA -148695007 #> 4    NA -16.0145   NA  -23815002 #> 5    NA -15.5225   NA   34385010 #> 6    NA -15.0375   NA   80304993 # Replacing data points by the weighted first quartile: wlddev |> na_omit(cols = \"POP\") |> fgroup_by(country) |>   fselect(country, year, PCGDP:POP) %>%   ftransform(fselect(., -country, -year) |>              fnth(0.25, POP, \"fill\")) |> head() #>       country year    PCGDP   LIFEEX GINI       ODA     POP #> 1 Afghanistan 1960 406.9948 45.86685   NA 237899441 8996973 #> 2 Afghanistan 1961 406.9948 45.86685   NA 237899441 9169410 #> 3 Afghanistan 1962 406.9948 45.86685   NA 237899441 9351441 #> 4 Afghanistan 1963 406.9948 45.86685   NA 237899441 9543205 #> 5 Afghanistan 1964 406.9948 45.86685   NA 237899441 9744781 #> 6 Afghanistan 1965 406.9948 45.86685   NA 237899441 9956320  wlddev |> fgroup_by(country) |> fselect(PCGDP:ODA) |> fscale() |> head() # Standardizing #>   PCGDP    LIFEEX GINI        ODA #> 1    NA -1.653181   NA -0.6498451 #> 2    NA -1.602256   NA -0.5951801 #> 3    NA -1.552023   NA -0.6517082 #> 4    NA -1.502678   NA -0.5925063 #> 5    NA -1.454122   NA -0.5649154 #> 6    NA -1.406257   NA -0.5431461 wlddev |> fgroup_by(country) |> fselect(PCGDP:POP) |>    fscale(POP) |> head()  # Weighted.. #>       POP PCGDP    LIFEEX GINI        ODA #> 1 8996973    NA -2.172769   NA -0.9502811 #> 2 9169410    NA -2.119489   NA -0.9011481 #> 3 9351441    NA -2.066932   NA -0.9519557 #> 4 9543205    NA -2.015304   NA -0.8987449 #> 5 9744781    NA -1.964502   NA -0.8739462 #> 6 9956320    NA -1.914423   NA -0.8543799  wlddev |> fselect(country, year, PCGDP:ODA) |>  # Adding 1 lead and 2 lags of each variable   fgroup_by(country) |> flag(-1:2, year) |> head() #>       country year F1.PCGDP PCGDP L1.PCGDP L2.PCGDP F1.LIFEEX LIFEEX L1.LIFEEX #> 1 Afghanistan 1960       NA    NA       NA       NA    32.962 32.446        NA #> 2 Afghanistan 1961       NA    NA       NA       NA    33.471 32.962    32.446 #> 3 Afghanistan 1962       NA    NA       NA       NA    33.971 33.471    32.962 #>   L2.LIFEEX F1.GINI GINI L1.GINI L2.GINI    F1.ODA       ODA    L1.ODA #> 1        NA      NA   NA      NA      NA 232080002 116769997        NA #> 2        NA      NA   NA      NA      NA 112839996 232080002 116769997 #> 3    32.446      NA   NA      NA      NA 237720001 112839996 232080002 #>      L2.ODA #> 1        NA #> 2        NA #> 3 116769997 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ] wlddev |> fselect(country, year, PCGDP:ODA) |>  # Adding 1 lead and 10-year growth rates   fgroup_by(country) |> fgrowth(c(0:1,10), 1, year) |> head() #>       country year PCGDP G1.PCGDP L10G1.PCGDP LIFEEX G1.LIFEEX L10G1.LIFEEX #> 1 Afghanistan 1960    NA       NA          NA 32.446        NA           NA #> 2 Afghanistan 1961    NA       NA          NA 32.962  1.590335           NA #> 3 Afghanistan 1962    NA       NA          NA 33.471  1.544202           NA #> 4 Afghanistan 1963    NA       NA          NA 33.971  1.493830           NA #> 5 Afghanistan 1964    NA       NA          NA 34.463  1.448294           NA #>   GINI G1.GINI L10G1.GINI       ODA    G1.ODA L10G1.ODA #> 1   NA      NA         NA 116769997        NA        NA #> 2   NA      NA         NA 232080002  98.74969        NA #> 3   NA      NA         NA 112839996 -51.37884        NA #> 4   NA      NA         NA 237720001 110.66998        NA #> 5   NA      NA         NA 295920013  24.48259        NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # etc...  # Aggregation with multiple functions wlddev |> fsubset(year > 1990, region, income, PCGDP:ODA) |>   fgroup_by(region, income) %>% {     add_vars(fgroup_vars(., \"unique\"),              fmedian(., keep.group_vars = FALSE) |> add_stub(\"median_\"),              fmean(., keep.group_vars = FALSE) |> add_stub(\"mean_\"),              fsd(., keep.group_vars = FALSE) |> add_stub(\"sd_\"))   } |> head() #>                  region              income median_PCGDP median_LIFEEX #> 1   East Asia & Pacific         High income   32573.8177      78.54024 #> 2   East Asia & Pacific Lower middle income    1658.3786      66.07200 #> 3   East Asia & Pacific Upper middle income    3583.2189      70.61500 #> 4 Europe & Central Asia         High income   36201.7707      78.16061 #> 5 Europe & Central Asia          Low income     668.9513      66.08000 #>   median_GINI median_ODA mean_PCGDP mean_LIFEEX mean_GINI  mean_ODA   sd_PCGDP #> 1       32.75   11500000 32671.0522    78.21996  32.95000 112194118 13031.1867 #> 2       35.70  257079987  1738.5111    65.45647  36.51972 509965862   904.3004 #> 3       40.35   49730000  4575.8695    70.87431  40.64815 219146704  2489.3795 #> 4       31.10  138889999 40814.1215    77.67583  30.94218 321907195 29485.3091 #> 5       32.45  239055000   695.7951    65.04128  32.13333 244539286   242.4158 #>   sd_LIFEEX  sd_GINI    sd_ODA #> 1  3.825737 1.322624 223580786 #> 2  5.003373 4.779528 686619373 #> 3  3.157915 3.506637 684346804 #> 4  3.810700 3.676878 632730086 #> 5  4.723263 1.713087 116363515 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # Transformation with multiple functions wlddev |> fselect(country, year, PCGDP:ODA) |>   fgroup_by(country) %>% {     add_vars(fdiff(., c(1,10), 1, year) |> flag(0:2, year),  # Sequence of lagged differences              ftransform(., fselect(., PCGDP:ODA) |> fwithin() |> add_stub(\"W.\")) |>                flag(0:2, year, keep.ids = FALSE))            # Sequence of lagged demeaned vars   } |> head() #>       country year D1.PCGDP L1.D1.PCGDP L2.D1.PCGDP L10D1.PCGDP L1.L10D1.PCGDP #> 1 Afghanistan 1960       NA          NA          NA          NA             NA #>   L2.L10D1.PCGDP D1.LIFEEX L1.D1.LIFEEX L2.D1.LIFEEX L10D1.LIFEEX #> 1             NA        NA           NA           NA           NA #>   L1.L10D1.LIFEEX L2.L10D1.LIFEEX D1.GINI L1.D1.GINI L2.D1.GINI L10D1.GINI #> 1              NA              NA      NA         NA         NA         NA #>   L1.L10D1.GINI L2.L10D1.GINI D1.ODA L1.D1.ODA L2.D1.ODA L10D1.ODA L1.L10D1.ODA #> 1            NA            NA     NA        NA        NA        NA           NA #>   L2.L10D1.ODA PCGDP L1.PCGDP L2.PCGDP LIFEEX L1.LIFEEX L2.LIFEEX GINI L1.GINI #> 1           NA    NA       NA       NA 32.446        NA        NA   NA      NA #>   L2.GINI       ODA L1.ODA L2.ODA W.PCGDP L1.W.PCGDP L2.W.PCGDP  W.LIFEEX #> 1      NA 116769997     NA     NA      NA         NA         NA -16.75117 #>   L1.W.LIFEEX L2.W.LIFEEX W.GINI L1.W.GINI L2.W.GINI       W.ODA L1.W.ODA #> 1          NA          NA     NA        NA        NA -1370778502       NA #>   L2.W.ODA #> 1       NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]  # With ftransform, can also easily do one or more grouped mutations on the fly.. settransform(wlddev, median_ODA = fmedian(ODA, list(region, income), TRA = \"fill\"))  settransform(wlddev, sd_ODA = fsd(ODA, list(region, income), TRA = \"fill\"),                      mean_GDP = fmean(PCGDP, country, TRA = \"fill\"))  wlddev %<>% ftransform(fmedian(list(median_ODA = ODA, median_GDP = PCGDP),                                list(region, income), TRA = \"fill\"))  # On a groped data frame it is also possible to grouped transform certain columns # but perform aggregate operatins on others: wlddev |> fgroup_by(region, income) %>%     ftransform(gmedian_GDP = fmedian(PCGDP, GRP(.), TRA = \"replace\"),                omedian_GDP = fmedian(PCGDP, TRA = \"replace\"),  # \"replace\" preserves NA's                omedian_GDP_fill = fmedian(PCGDP)) |> tail() #>        country iso3c       date year decade             region #> 13171 Zimbabwe   ZWE 2016-01-01 2015   2010 Sub-Saharan Africa #> 13172 Zimbabwe   ZWE 2017-01-01 2016   2010 Sub-Saharan Africa #> 13173 Zimbabwe   ZWE 2018-01-01 2017   2010 Sub-Saharan Africa #>                    income  OECD    PCGDP LIFEEX GINI       ODA      POP #> 13171 Lower middle income FALSE 1234.103 59.534   NA 817729980 13814629 #> 13172 Lower middle income FALSE 1224.310 60.294   NA 687659973 14030390 #> 13173 Lower middle income FALSE 1263.321 60.812 44.3 753909973 14236745 #>       median_ODA    sd_ODA mean_GDP median_GDP gmedian_GDP omedian_GDP #> 13171  280630005 694376321 1219.436   1336.053    1336.053    3767.162 #> 13172  280630005 694376321 1219.436   1336.053    1336.053    3767.162 #> 13173  280630005 694376321 1219.436   1336.053    1336.053    3767.162 #>       omedian_GDP_fill #> 13171         3767.162 #> 13172         3767.162 #> 13173         3767.162 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]  rm(wlddev)  ## For multi-type data aggregation, the function collap() offers ease and flexibility # Aggregate this data by country and decade: Numeric columns with mean, categorical with mode head(collap(wlddev, ~ country + decade, fmean, fmode)) #>       country iso3c       date   year decade     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 1964.5   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1971-01-01 1974.5   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 1984.5   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 1991-01-01 1994.5   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2001-01-01 2004.5   2000 South Asia Low income FALSE #>     PCGDP  LIFEEX GINI        ODA      POP #> 1      NA 34.6908   NA  222288999  9886773 #> 2      NA 39.9053   NA  236169998 12451803 #> 3      NA 46.4176   NA   71666001 12291854 #> 4      NA 53.0097   NA  317255000 16931903 #> 5 379.373 58.0881   NA 3054051961 24870022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # taking weighted mean and weighted mode: head(collap(wlddev, ~ country + decade, fmean, fmode, w = ~ POP, wFUN = fsum)) #>       country iso3c       date     year decade     region     income  OECD #> 1 Afghanistan   AFG 1970-01-01 1964.675   1960 South Asia Low income FALSE #> 2 Afghanistan   AFG 1980-01-01 1974.672   1970 South Asia Low income FALSE #> 3 Afghanistan   AFG 1981-01-01 1984.364   1980 South Asia Low income FALSE #> 4 Afghanistan   AFG 2000-01-01 1994.941   1990 South Asia Low income FALSE #> 5 Afghanistan   AFG 2010-01-01 2004.788   2000 South Asia Low income FALSE #>      PCGDP   LIFEEX GINI        ODA       POP #> 1       NA 34.77716   NA  223006447  98867731 #> 2       NA 40.00367   NA  236798314 124518028 #> 3       NA 46.32098   NA   70613923 122918537 #> 4       NA 53.25897   NA  306818649 169319030 #> 5 382.5583 58.23630   NA 3240143310 248700217 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # Multi-function aggregation of certain columns head(collap(wlddev, ~ country + decade,             list(fmean, fmedian, fsd),             list(ffirst, flast), cols = c(3,9:12))) #>       country ffirst.date flast.date decade fmean.PCGDP fmedian.PCGDP fsd.PCGDP #> 1 Afghanistan  1961-01-01 1970-01-01   1960          NA            NA        NA #> 2 Afghanistan  1971-01-01 1980-01-01   1970          NA            NA        NA #> 3 Afghanistan  1981-01-01 1990-01-01   1980          NA            NA        NA #> 4 Afghanistan  1991-01-01 2000-01-01   1990          NA            NA        NA #>   fmean.LIFEEX fmedian.LIFEEX fsd.LIFEEX fmean.GINI fmedian.GINI fsd.GINI #> 1      34.6908        34.7055   1.490964         NA           NA       NA #> 2      39.9053        39.8430   1.738383         NA           NA       NA #> 3      46.4176        46.4005   2.161460         NA           NA       NA #> 4      53.0097        53.1200   1.695424         NA           NA       NA #>   fmean.ODA fmedian.ODA   fsd.ODA #> 1 222288999   234900002  80884369 #> 2 236169998   246509995  34241008 #> 3  71666001    48539999  72958531 #> 4 317255000   285175003 160500141 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]  # Customized Aggregation: Assign columns to functions head(collap(wlddev, ~ country + decade,             custom = list(fmean = 9:10, fsd = 9:12, flast = 3, ffirst = 6:8))) #>       country flast.date decade ffirst.region ffirst.income ffirst.OECD #> 1 Afghanistan 1970-01-01   1960    South Asia    Low income       FALSE #> 2 Afghanistan 1980-01-01   1970    South Asia    Low income       FALSE #> 3 Afghanistan 1990-01-01   1980    South Asia    Low income       FALSE #> 4 Afghanistan 2000-01-01   1990    South Asia    Low income       FALSE #> 5 Afghanistan 2010-01-01   2000    South Asia    Low income       FALSE #>   fmean.PCGDP fsd.PCGDP fmean.LIFEEX fsd.LIFEEX fsd.GINI    fsd.ODA #> 1          NA        NA      34.6908   1.490964       NA   80884369 #> 2          NA        NA      39.9053   1.738383       NA   34241008 #> 3          NA        NA      46.4176   2.161460       NA   72958531 #> 4          NA        NA      53.0097   1.695424       NA  160500141 #> 5     379.373  53.66524      58.0881   1.565630       NA 2013110021 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # For grouped data frames use collapg wlddev |> fsubset(year > 1990, country, region, income, PCGDP:ODA) |>   fgroup_by(country) |> collapg(fmean, ffirst) |>   ftransform(AMGDP = PCGDP > fmedian(PCGDP, list(region, income), TRA = \"fill\"),              AMODA = ODA > fmedian(ODA, income, TRA = \"replace_fill\")) |> head() #>          country                     region              income      PCGDP #> 1    Afghanistan                 South Asia          Low income   483.8351 #> 2        Albania      Europe & Central Asia Upper middle income  3127.1510 #> 3        Algeria Middle East & North Africa Upper middle income  4056.4341 #> 4 American Samoa        East Asia & Pacific Upper middle income 10071.0659 #> 5        Andorra      Europe & Central Asia         High income 40768.8453 #> 6         Angola         Sub-Saharan Africa Lower middle income  2876.5065 #>     LIFEEX     GINI        ODA AMGDP AMODA #> 1 58.32283       NA 2888193791 FALSE  TRUE #> 2 75.19266 31.41111  343797587 FALSE  TRUE #> 3 72.57717 31.45000  287459654 FALSE  TRUE #> 4       NA       NA         NA  TRUE    NA #> 5       NA       NA         NA  TRUE    NA #> 6 51.59572 48.66667  412104483  TRUE FALSE  ## Additional flexibility for data transformation tasks is offerend by tidy transformation operators # Within-transformation (centering on overall mean) head(W(wlddev, ~ country, cols = 9:12, mean = \"overall.mean\")) #>       country W.PCGDP W.LIFEEX W.GINI      W.ODA #> 1 Afghanistan      NA 47.54514     NA -916058371 #> 2 Afghanistan      NA 48.06114     NA -800748366 #> 3 Afghanistan      NA 48.57014     NA -919988371 #> 4 Afghanistan      NA 49.07014     NA -795108366 #> 5 Afghanistan      NA 49.56214     NA -736908354 #> 6 Afghanistan      NA 50.04714     NA -690988371 # Partialling out country and year fixed effects head(HDW(wlddev, PCGDP + LIFEEX ~ qF(country) + qF(year))) #>   HDW.PCGDP HDW.LIFEEX #> 1 1578.6211 -1.3980224 #> 2 1412.8849 -1.1838196 #> 3  917.2033 -1.0547978 #> 4  627.8605 -0.8296048 #> 5  168.0458 -0.6683027 #> 6 -234.9535 -0.4708428 # Same, adding ODA as continuous regressor head(HDW(wlddev, PCGDP + LIFEEX ~ qF(country) + qF(year) + ODA)) #>   HDW.PCGDP HDW.LIFEEX #> 1 -324.3991 -1.1765307 #> 2 -439.5404 -0.9751559 #> 3 -598.9266 -0.7835446 #> 4  100.2175 -0.6186010 #> 5  -70.7664 -0.4966332 #> 6  330.3561 -0.2257800 # Standardizing (scaling and centering) by country head(STD(wlddev, ~ country, cols = 9:12)) #>       country STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA #> 1 Afghanistan        NA  -1.653181       NA -0.6498451 #> 2 Afghanistan        NA  -1.602256       NA -0.5951801 #> 3 Afghanistan        NA  -1.552023       NA -0.6517082 #> 4 Afghanistan        NA  -1.502678       NA -0.5925063 #> 5 Afghanistan        NA  -1.454122       NA -0.5649154 #> 6 Afghanistan        NA  -1.406257       NA -0.5431461 # Computing 1 lead and 3 lags of the 4 series head(L(wlddev, -1:3, ~ country, ~year, cols = 9:12)) #>       country year F1.PCGDP PCGDP L1.PCGDP L2.PCGDP L3.PCGDP F1.LIFEEX LIFEEX #> 1 Afghanistan 1960       NA    NA       NA       NA       NA    32.962 32.446 #> 2 Afghanistan 1961       NA    NA       NA       NA       NA    33.471 32.962 #> 3 Afghanistan 1962       NA    NA       NA       NA       NA    33.971 33.471 #>   L1.LIFEEX L2.LIFEEX L3.LIFEEX F1.GINI GINI L1.GINI L2.GINI L3.GINI    F1.ODA #> 1        NA        NA        NA      NA   NA      NA      NA      NA 232080002 #> 2    32.446        NA        NA      NA   NA      NA      NA      NA 112839996 #> 3    32.962    32.446        NA      NA   NA      NA      NA      NA 237720001 #>         ODA    L1.ODA    L2.ODA L3.ODA #> 1 116769997        NA        NA     NA #> 2 232080002 116769997        NA     NA #> 3 112839996 232080002 116769997     NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ] # Computing the 1- and 10-year first differences head(D(wlddev, c(1,10), 1, ~ country, ~year, cols = 9:12)) #>       country year D1.PCGDP L10D1.PCGDP D1.LIFEEX L10D1.LIFEEX D1.GINI #> 1 Afghanistan 1960       NA          NA        NA           NA      NA #> 2 Afghanistan 1961       NA          NA     0.516           NA      NA #> 3 Afghanistan 1962       NA          NA     0.509           NA      NA #> 4 Afghanistan 1963       NA          NA     0.500           NA      NA #> 5 Afghanistan 1964       NA          NA     0.492           NA      NA #> 6 Afghanistan 1965       NA          NA     0.485           NA      NA #>   L10D1.GINI     D1.ODA L10D1.ODA #> 1         NA         NA        NA #> 2         NA  115310005        NA #> 3         NA -119240005        NA #> 4         NA  124880005        NA #> 5         NA   58200012        NA #> 6         NA   45919983        NA head(D(wlddev, c(1,10), 1:2, ~ country, ~year, cols = 9:12))     # ..first and second differences #>       country year D1.PCGDP D2.PCGDP L10D1.PCGDP L10D2.PCGDP D1.LIFEEX #> 1 Afghanistan 1960       NA       NA          NA          NA        NA #> 2 Afghanistan 1961       NA       NA          NA          NA     0.516 #> 3 Afghanistan 1962       NA       NA          NA          NA     0.509 #>   D2.LIFEEX L10D1.LIFEEX L10D2.LIFEEX D1.GINI D2.GINI L10D1.GINI L10D2.GINI #> 1        NA           NA           NA      NA      NA         NA         NA #> 2        NA           NA           NA      NA      NA         NA         NA #> 3    -0.007           NA           NA      NA      NA         NA         NA #>       D1.ODA     D2.ODA L10D1.ODA L10D2.ODA #> 1         NA         NA        NA        NA #> 2  115310005         NA        NA        NA #> 3 -119240005 -234550011        NA        NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ] # Computing the 1- and 10-year growth rates head(G(wlddev, c(1,10), 1, ~ country, ~year, cols = 9:12)) #>       country year G1.PCGDP L10G1.PCGDP G1.LIFEEX L10G1.LIFEEX G1.GINI #> 1 Afghanistan 1960       NA          NA        NA           NA      NA #> 2 Afghanistan 1961       NA          NA  1.590335           NA      NA #> 3 Afghanistan 1962       NA          NA  1.544202           NA      NA #> 4 Afghanistan 1963       NA          NA  1.493830           NA      NA #> 5 Afghanistan 1964       NA          NA  1.448294           NA      NA #> 6 Afghanistan 1965       NA          NA  1.407306           NA      NA #>   L10G1.GINI    G1.ODA L10G1.ODA #> 1         NA        NA        NA #> 2         NA  98.74969        NA #> 3         NA -51.37884        NA #> 4         NA 110.66998        NA #> 5         NA  24.48259        NA #> 6         NA  15.51770        NA # Adding growth rate variables to dataset add_vars(wlddev) <- G(wlddev, c(1, 10), 1, ~ country, ~year, cols = 9:12, keep.ids = FALSE) get_vars(wlddev, \"G1.\", regex = TRUE) <- NULL # Deleting again  # These operators can conveniently be used in regression formulas: # Using a Mundlak (1978) procedure to estimate the effect of OECD on LIFEEX, controlling for PCGDP lm(LIFEEX ~ log(PCGDP) + OECD + B(log(PCGDP), country),    wlddev |> fselect(country, OECD, PCGDP, LIFEEX) |> na_omit()) #>  #> Call: #> lm(formula = LIFEEX ~ log(PCGDP) + OECD + B(log(PCGDP), country),  #>     data = na_omit(fselect(wlddev, country, OECD, PCGDP, LIFEEX))) #>  #> Coefficients: #>            (Intercept)              log(PCGDP)                OECDTRUE   #>               19.32590                 8.20551                 0.02478   #> B(log(PCGDP), country)   #>               -2.65428   #>   # Adding 10-year lagged life-expectancy to allow for some convergence effects (dynamic panel model) lm(LIFEEX ~ L(LIFEEX, 10, country) + log(PCGDP) + OECD + B(log(PCGDP), country),    wlddev |> fselect(country, OECD, PCGDP, LIFEEX) |> na_omit()) #>  #> Call: #> lm(formula = LIFEEX ~ L(LIFEEX, 10, country) + log(PCGDP) + OECD +  #>     B(log(PCGDP), country), data = na_omit(fselect(wlddev, country,  #>     OECD, PCGDP, LIFEEX))) #>  #> Coefficients: #>            (Intercept)  L(LIFEEX, 10, country)              log(PCGDP)   #>                 9.2756                  0.8656                  0.9229   #>               OECDTRUE  B(log(PCGDP), country)   #>                 0.4158                 -0.6581   #>   # Tranformation functions and operators also support indexed data classes: wldi <- findex_by(wlddev, country, year) head(W(wldi$PCGDP))                      # Country-demeaning #> [1] NA NA NA NA NA NA #>  #> Indexed by:  country [1] | year [6 (61)]  head(W(wldi, cols = 9:12)) #>       country year W.PCGDP  W.LIFEEX W.GINI       W.ODA #> 1 Afghanistan 1960      NA -16.75117     NA -1370778502 #> 2 Afghanistan 1961      NA -16.23517     NA -1255468497 #> 3 Afghanistan 1962      NA -15.72617     NA -1374708502 #> 4 Afghanistan 1963      NA -15.22617     NA -1249828497 #> 5 Afghanistan 1964      NA -14.73417     NA -1191628485 #> 6 Afghanistan 1965      NA -14.24917     NA -1145708502 #>  #> Indexed by:  country [1] | year [6 (61)]  head(W(wldi$PCGDP, effect = 2))          # Time-demeaning #> [1] NA NA NA NA NA NA #>  #> Indexed by:  country [1] | year [6 (61)]  head(W(wldi, effect = 2, cols = 9:12)) #>       country year W.PCGDP  W.LIFEEX W.GINI      W.ODA #> 1 Afghanistan 1960      NA -21.46606     NA -122241092 #> 2 Afghanistan 1961      NA -21.51241     NA  -37552049 #> 3 Afghanistan 1962      NA -21.38618     NA -183366702 #> 4 Afghanistan 1963      NA -21.23172     NA  -54896550 #> 5 Afghanistan 1964      NA -21.20502     NA   -9633789 #> 6 Afghanistan 1965      NA -21.18163     NA    5438669 #>  #> Indexed by:  country [1] | year [6 (61)]  head(HDW(wldi$PCGDP))                    # Country- and time-demeaning #> [1] NA NA NA NA NA NA #>  #> Indexed by:  country [1] | year [6 (61)]  head(HDW(wldi, cols = 9:12)) #>   HDW.PCGDP HDW.LIFEEX HDW.GINI     HDW.ODA #> 1        NA  -6.706423       NA -1093922188 #> 2        NA  -6.688440       NA -1032355993 #> 3        NA  -6.562210       NA -1156945288 #> 4        NA  -6.472079       NA -1046169271 #> 5        NA  -6.445378       NA  -996348510 #> 6        NA  -6.367659       NA  -983277444 #>  #> Indexed by:  country [1] | year [6 (61)]  head(STD(wldi$PCGDP))                    # Standardizing by country #> [1] NA NA NA NA NA NA #>  #> Indexed by:  country [1] | year [6 (61)]  head(STD(wldi, cols = 9:12)) #>       country year STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA #> 1 Afghanistan 1960        NA  -1.653181       NA -0.6498451 #> 2 Afghanistan 1961        NA  -1.602256       NA -0.5951801 #> 3 Afghanistan 1962        NA  -1.552023       NA -0.6517082 #> 4 Afghanistan 1963        NA  -1.502678       NA -0.5925063 #> 5 Afghanistan 1964        NA  -1.454122       NA -0.5649154 #> 6 Afghanistan 1965        NA  -1.406257       NA -0.5431461 #>  #> Indexed by:  country [1] | year [6 (61)]  head(L(wldi$PCGDP, -1:3))                # Panel-lags #>      F1 -- L1 L2 L3 #> [1,] NA NA NA NA NA #> [2,] NA NA NA NA NA #> [3,] NA NA NA NA NA #> [4,] NA NA NA NA NA #> [5,] NA NA NA NA NA #> [6,] NA NA NA NA NA #> attr(,\"class\") #> [1] \"matrix\" \"array\"  #>  #> Indexed by:  country [1] | year [6 (61)]  head(L(wldi, -1:3, 9:12)) #>       country year F1.PCGDP PCGDP L1.PCGDP L2.PCGDP L3.PCGDP F1.LIFEEX LIFEEX #> 1 Afghanistan 1960       NA    NA       NA       NA       NA    32.962 32.446 #> 2 Afghanistan 1961       NA    NA       NA       NA       NA    33.471 32.962 #> 3 Afghanistan 1962       NA    NA       NA       NA       NA    33.971 33.471 #>   L1.LIFEEX L2.LIFEEX L3.LIFEEX F1.GINI GINI L1.GINI L2.GINI L3.GINI    F1.ODA #> 1        NA        NA        NA      NA   NA      NA      NA      NA 232080002 #> 2    32.446        NA        NA      NA   NA      NA      NA      NA 112839996 #> 3    32.962    32.446        NA      NA   NA      NA      NA      NA 237720001 #>         ODA    L1.ODA    L2.ODA L3.ODA #> 1 116769997        NA        NA     NA #> 2 232080002 116769997        NA     NA #> 3 112839996 232080002 116769997     NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ] #>  #> Indexed by:  country [1] | year [6 (61)]  head(G(wldi$PCGDP))                      # Panel-Growth rates #> [1] NA NA NA NA NA NA #>  #> Indexed by:  country [1] | year [6 (61)]  head(G(wldi, 1, 1, 9:12)) #>       country year G1.PCGDP G1.LIFEEX G1.GINI    G1.ODA #> 1 Afghanistan 1960       NA        NA      NA        NA #> 2 Afghanistan 1961       NA  1.590335      NA  98.74969 #> 3 Afghanistan 1962       NA  1.544202      NA -51.37884 #> 4 Afghanistan 1963       NA  1.493830      NA 110.66998 #> 5 Afghanistan 1964       NA  1.448294      NA  24.48259 #> 6 Afghanistan 1965       NA  1.407306      NA  15.51770 #>  #> Indexed by:  country [1] | year [6 (61)]   lm(Dlog(PCGDP) ~ L(Dlog(LIFEEX), 0:3), wldi)   # Panel data regression #>  #> Call: #> lm(formula = Dlog(PCGDP) ~ L(Dlog(LIFEEX), 0:3), data = wldi) #>  #> Coefficients: #>            (Intercept)  L(Dlog(LIFEEX), 0:3)--  L(Dlog(LIFEEX), 0:3)L1   #>                0.01544                -0.12618                 0.38523   #> L(Dlog(LIFEEX), 0:3)L2  L(Dlog(LIFEEX), 0:3)L3   #>                0.54179                -0.16475   #>  rm(wldi)  # Remove all objects used in this example section rm(v, d, w, f, f1, f2, g, mtcarsM, sds, series, wlddev)"},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-renamed.html","id":null,"dir":"Reference","previous_headings":"","what":"Renamed Functions — collapse-renamed","title":"Renamed Functions — collapse-renamed","text":"functions renamed (mostly v1.6.0 update) make namespace consistent.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/collapse-renamed.html","id":"renaming","dir":"Reference","previous_headings":"","what":"Renaming","title":"Renamed Functions — collapse-renamed","text":"","code":"fNobs -> fnobs fNdistinct -> fndistinct fHDwithin -> fhdwithin fHDbetween -> fhdbetween replace_NA -> replace_na replace_Inf -> replace_inf"},{"path":"https://sebkrantz.github.io/collapse/reference/colorder.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Reordering of Data Frame Columns — colorder","title":"Fast Reordering of Data Frame Columns — colorder","text":"Efficiently reorder columns data frame. fully reference see also data.table::setcolorder.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/colorder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Reordering of Data Frame Columns — colorder","text":"","code":"colorder(.X, ..., pos = \"front\")  colorderv(X, neworder = radixorder(names(X)),           pos = \"front\", regex = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/colorder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Reordering of Data Frame Columns — colorder","text":".X, X data frame list. ... colorder: Column names .X new order (can also use sequences .e. col1:coln, newname = colk, ...). colorderv: arguments grep regex = TRUE. neworder vector column names, positive indices, suitable logical vector, function .numeric, vector regular expressions matching column names (regex = TRUE). pos integer character. Different options regarding column arrangement ...length() < ncol(.X) (length(neworder) < ncol(X)). regex logical. TRUE regular expression search column names X using (vector ) regular expression(s) passed neworder. Matching done using grep. Note multiple regular expressions matched order passed, funique applied resulting set indices.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/colorder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Reordering of Data Frame Columns — colorder","text":".X/X columns reordered (deep copies).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/colorder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Reordering of Data Frame Columns — colorder","text":"","code":"head(colorder(mtcars, vs, cyl:hp, am)) #>                   vs cyl disp  hp am  mpg drat    wt  qsec gear carb #> Mazda RX4          0   6  160 110  1 21.0 3.90 2.620 16.46    4    4 #> Mazda RX4 Wag      0   6  160 110  1 21.0 3.90 2.875 17.02    4    4 #> Datsun 710         1   4  108  93  1 22.8 3.85 2.320 18.61    4    1 #> Hornet 4 Drive     1   6  258 110  0 21.4 3.08 3.215 19.44    3    1 #> Hornet Sportabout  0   8  360 175  0 18.7 3.15 3.440 17.02    3    2 #> Valiant            1   6  225 105  0 18.1 2.76 3.460 20.22    3    1 head(colorder(mtcars, vs, cyl:hp, am, pos = \"end\")) #>                    mpg drat    wt  qsec gear carb vs cyl disp  hp am #> Mazda RX4         21.0 3.90 2.620 16.46    4    4  0   6  160 110  1 #> Mazda RX4 Wag     21.0 3.90 2.875 17.02    4    4  0   6  160 110  1 #> Datsun 710        22.8 3.85 2.320 18.61    4    1  1   4  108  93  1 #> Hornet 4 Drive    21.4 3.08 3.215 19.44    3    1  1   6  258 110  0 #> Hornet Sportabout 18.7 3.15 3.440 17.02    3    2  0   8  360 175  0 #> Valiant           18.1 2.76 3.460 20.22    3    1  1   6  225 105  0 head(colorder(mtcars, vs, cyl:hp, am, pos = \"after\")) #>                    mpg drat    wt  qsec vs cyl disp  hp am gear carb #> Mazda RX4         21.0 3.90 2.620 16.46  0   6  160 110  1    4    4 #> Mazda RX4 Wag     21.0 3.90 2.875 17.02  0   6  160 110  1    4    4 #> Datsun 710        22.8 3.85 2.320 18.61  1   4  108  93  1    4    1 #> Hornet 4 Drive    21.4 3.08 3.215 19.44  1   6  258 110  0    3    1 #> Hornet Sportabout 18.7 3.15 3.440 17.02  0   8  360 175  0    3    2 #> Valiant           18.1 2.76 3.460 20.22  1   6  225 105  0    3    1 head(colorder(mtcars, vs, cyl, pos = \"exchange\")) #>                    mpg vs disp  hp drat    wt  qsec cyl am gear carb #> Mazda RX4         21.0  0  160 110 3.90 2.620 16.46   6  1    4    4 #> Mazda RX4 Wag     21.0  0  160 110 3.90 2.875 17.02   6  1    4    4 #> Datsun 710        22.8  1  108  93 3.85 2.320 18.61   4  1    4    1 #> Hornet 4 Drive    21.4  1  258 110 3.08 3.215 19.44   6  0    3    1 #> Hornet Sportabout 18.7  0  360 175 3.15 3.440 17.02   8  0    3    2 #> Valiant           18.1  1  225 105 2.76 3.460 20.22   6  0    3    1 head(colorder(mtcars, vs, cyl:hp, new = am))    # renaming #>                   vs cyl disp  hp new  mpg drat    wt  qsec gear carb #> Mazda RX4          0   6  160 110   1 21.0 3.90 2.620 16.46    4    4 #> Mazda RX4 Wag      0   6  160 110   1 21.0 3.90 2.875 17.02    4    4 #> Datsun 710         1   4  108  93   1 22.8 3.85 2.320 18.61    4    1 #> Hornet 4 Drive     1   6  258 110   0 21.4 3.08 3.215 19.44    3    1 #> Hornet Sportabout  0   8  360 175   0 18.7 3.15 3.440 17.02    3    2 #> Valiant            1   6  225 105   0 18.1 2.76 3.460 20.22    3    1  ## Same in standard evaluation head(colorderv(mtcars, c(8, 2:4, 9))) #>                   vs cyl disp  hp am  mpg drat    wt  qsec gear carb #> Mazda RX4          0   6  160 110  1 21.0 3.90 2.620 16.46    4    4 #> Mazda RX4 Wag      0   6  160 110  1 21.0 3.90 2.875 17.02    4    4 #> Datsun 710         1   4  108  93  1 22.8 3.85 2.320 18.61    4    1 #> Hornet 4 Drive     1   6  258 110  0 21.4 3.08 3.215 19.44    3    1 #> Hornet Sportabout  0   8  360 175  0 18.7 3.15 3.440 17.02    3    2 #> Valiant            1   6  225 105  0 18.1 2.76 3.460 20.22    3    1 head(colorderv(mtcars, c(8, 2:4, 9), pos = \"end\")) #>                    mpg drat    wt  qsec gear carb vs cyl disp  hp am #> Mazda RX4         21.0 3.90 2.620 16.46    4    4  0   6  160 110  1 #> Mazda RX4 Wag     21.0 3.90 2.875 17.02    4    4  0   6  160 110  1 #> Datsun 710        22.8 3.85 2.320 18.61    4    1  1   4  108  93  1 #> Hornet 4 Drive    21.4 3.08 3.215 19.44    3    1  1   6  258 110  0 #> Hornet Sportabout 18.7 3.15 3.440 17.02    3    2  0   8  360 175  0 #> Valiant           18.1 2.76 3.460 20.22    3    1  1   6  225 105  0 head(colorderv(mtcars, c(8, 2:4, 9), pos = \"after\")) #>                    mpg drat    wt  qsec vs cyl disp  hp am gear carb #> Mazda RX4         21.0 3.90 2.620 16.46  0   6  160 110  1    4    4 #> Mazda RX4 Wag     21.0 3.90 2.875 17.02  0   6  160 110  1    4    4 #> Datsun 710        22.8 3.85 2.320 18.61  1   4  108  93  1    4    1 #> Hornet 4 Drive    21.4 3.08 3.215 19.44  1   6  258 110  0    3    1 #> Hornet Sportabout 18.7 3.15 3.440 17.02  0   8  360 175  0    3    2 #> Valiant           18.1 2.76 3.460 20.22  1   6  225 105  0    3    1 head(colorderv(mtcars, c(8, 2), pos = \"exchange\")) #>                    mpg vs disp  hp drat    wt  qsec cyl am gear carb #> Mazda RX4         21.0  0  160 110 3.90 2.620 16.46   6  1    4    4 #> Mazda RX4 Wag     21.0  0  160 110 3.90 2.875 17.02   6  1    4    4 #> Datsun 710        22.8  1  108  93 3.85 2.320 18.61   4  1    4    1 #> Hornet 4 Drive    21.4  1  258 110 3.08 3.215 19.44   6  0    3    1 #> Hornet Sportabout 18.7  0  360 175 3.15 3.440 17.02   8  0    3    2 #> Valiant           18.1  1  225 105 2.76 3.460 20.22   6  0    3    1"},{"path":"https://sebkrantz.github.io/collapse/reference/dapply.html","id":null,"dir":"Reference","previous_headings":"","what":"Data Apply — dapply","title":"Data Apply — dapply","text":"dapply efficiently applies functions columns rows matrix-like objects default returns object type attributes (unless result scalar drop = TRUE). Alternatively possible return result plain matrix data.frame. simple parallelism also available.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/dapply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data Apply — dapply","text":"","code":"dapply(X, FUN, ..., MARGIN = 2, parallel = FALSE, mc.cores = 1L,        return = c(\"same\", \"matrix\", \"data.frame\"), drop = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/reference/dapply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data Apply — dapply","text":"X matrix, data frame alike object. FUN function, can scalar- vector-valued. ... arguments FUN. MARGIN integer. margin FUN applied . Default 2 indicates columns 1 indicates rows. See also Details. parallel logical. TRUE implements simple parallel execution internally calling mclapply instead lapply. mc.cores integer. Argument mclapply indicating number cores use parallel execution. Can use detectCores() select available cores. return integer string indicating type object return. default 1 - \"\" returns object type (.e. class attributes retained, just names dimensions adjusted). 2 - \"matrix\" always returns output matrix 3 - \"data.frame\" always returns data frame. drop logical. result one row one column, drop = TRUE drop dimensions return (named) atomic vector.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/dapply.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Data Apply — dapply","text":"dapply efficient command apply functions rows columns data without loosing information (attributes) data changing classes format data. principally efficient wrapper around lapply works follows: Save attributes X. MARGIN = 2 (columns), convert matrices plain lists columns using mctl remove attributes data frames. MARGIN = 1 (rows), convert matrices plain lists rows using mrtl. data frames remove attributes, efficiently convert matrix using .call(cbind, X) also convert list rows using mrtl. Call lapply mclapply plain lists (faster calling lapply object attributes). depending requested output type, use matrix, unlist .call(cbind, ...) convert result back matrix list columns. modify relevant attributes accordingly efficiently attach object (checks). performance gain working plain lists makes dapply much slower calling lapply data frame. conversions involved, row-operations require memory, still faster apply.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/dapply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data Apply — dapply","text":"X FUN applied every row column.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/dapply.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data Apply — dapply","text":"","code":"head(dapply(mtcars, log))                      # Take natural log of each variable #>                        mpg      cyl     disp       hp     drat        wt #> Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 #> Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 #> Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 #> Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 #> Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 #> Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 #>                       qsec   vs   am     gear      carb #> Mazda RX4         2.800933 -Inf    0 1.386294 1.3862944 #> Mazda RX4 Wag     2.834389 -Inf    0 1.386294 1.3862944 #> Datsun 710        2.923699    0    0 1.386294 0.0000000 #> Hornet 4 Drive    2.967333    0 -Inf 1.098612 0.0000000 #> Hornet Sportabout 2.834389 -Inf -Inf 1.098612 0.6931472 #> Valiant           3.006672    0 -Inf 1.098612 0.0000000 head(dapply(mtcars, log, return = \"matrix\"))   # Return as matrix #>                        mpg      cyl     disp       hp     drat        wt #> Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 #> Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 #> Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 #> Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 #> Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 #> Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 #>                       qsec   vs   am     gear      carb #> Mazda RX4         2.800933 -Inf    0 1.386294 1.3862944 #> Mazda RX4 Wag     2.834389 -Inf    0 1.386294 1.3862944 #> Datsun 710        2.923699    0    0 1.386294 0.0000000 #> Hornet 4 Drive    2.967333    0 -Inf 1.098612 0.0000000 #> Hornet Sportabout 2.834389 -Inf -Inf 1.098612 0.6931472 #> Valiant           3.006672    0 -Inf 1.098612 0.0000000 m <- as.matrix(mtcars) head(dapply(m, log))                           # Same thing #>                        mpg      cyl     disp       hp     drat        wt #> Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 #> Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 #> Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 #> Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 #> Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 #> Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 #>                       qsec   vs   am     gear      carb #> Mazda RX4         2.800933 -Inf    0 1.386294 1.3862944 #> Mazda RX4 Wag     2.834389 -Inf    0 1.386294 1.3862944 #> Datsun 710        2.923699    0    0 1.386294 0.0000000 #> Hornet 4 Drive    2.967333    0 -Inf 1.098612 0.0000000 #> Hornet Sportabout 2.834389 -Inf -Inf 1.098612 0.6931472 #> Valiant           3.006672    0 -Inf 1.098612 0.0000000 head(dapply(m, log, return = \"data.frame\"))    # Return data frame from matrix #>                        mpg      cyl     disp       hp     drat        wt #> Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 #> Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 #> Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 #> Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 #> Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 #> Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 #>                       qsec   vs   am     gear      carb #> Mazda RX4         2.800933 -Inf    0 1.386294 1.3862944 #> Mazda RX4 Wag     2.834389 -Inf    0 1.386294 1.3862944 #> Datsun 710        2.923699    0    0 1.386294 0.0000000 #> Hornet 4 Drive    2.967333    0 -Inf 1.098612 0.0000000 #> Hornet Sportabout 2.834389 -Inf -Inf 1.098612 0.6931472 #> Valiant           3.006672    0 -Inf 1.098612 0.0000000 dapply(mtcars, sum); dapply(m, sum)            # Computing sum of each column, return as vector #>      mpg      cyl     disp       hp     drat       wt     qsec       vs  #>  642.900  198.000 7383.100 4694.000  115.090  102.952  571.160   14.000  #>       am     gear     carb  #>   13.000  118.000   90.000  #>      mpg      cyl     disp       hp     drat       wt     qsec       vs  #>  642.900  198.000 7383.100 4694.000  115.090  102.952  571.160   14.000  #>       am     gear     carb  #>   13.000  118.000   90.000  dapply(mtcars, sum, drop = FALSE)              # This returns a data frame of 1 row #>     mpg cyl   disp   hp   drat      wt   qsec vs am gear carb #> 1 642.9 198 7383.1 4694 115.09 102.952 571.16 14 13  118   90 dapply(mtcars, sum, MARGIN = 1)                # Compute row-sum of each column, return as vector #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>             328.980             329.795             259.580             426.135  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>             590.310             385.540             656.920             270.980  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>             299.570             350.460             349.660             510.740  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>             511.500             509.850             728.560             726.644  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>             725.695             213.850             195.165             206.955  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>             273.775             519.650             506.085             646.280  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>             631.175             208.215             272.570             273.683  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>             670.690             379.590             694.710             288.890  dapply(m, sum, MARGIN = 1)                     # Same thing for matrices, faster t. apply(m, 1, sum) #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>             328.980             329.795             259.580             426.135  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>             590.310             385.540             656.920             270.980  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>             299.570             350.460             349.660             510.740  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>             511.500             509.850             728.560             726.644  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>             725.695             213.850             195.165             206.955  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>             273.775             519.650             506.085             646.280  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>             631.175             208.215             272.570             273.683  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>             670.690             379.590             694.710             288.890  head(dapply(m, sum, MARGIN = 1, drop = FALSE)) # Gives matrix with one column #>                       sum #> Mazda RX4         328.980 #> Mazda RX4 Wag     329.795 #> Datsun 710        259.580 #> Hornet 4 Drive    426.135 #> Hornet Sportabout 590.310 #> Valiant           385.540 head(dapply(m, quantile, MARGIN = 1))          # Compute row-quantiles #>                   0%    25%   50%    75% 100% #> Mazda RX4          0 3.2600 4.000 18.730  160 #> Mazda RX4 Wag      0 3.3875 4.000 19.010  160 #> Datsun 710         1 1.6600 4.000 20.705  108 #> Hornet 4 Drive     0 2.0000 3.215 20.420  258 #> Hornet Sportabout  0 2.5000 3.440 17.860  360 #> Valiant            0 1.8800 3.460 19.160  225 dapply(m, quantile)                            # Column-quantiles #>         mpg cyl    disp    hp  drat      wt    qsec vs am gear carb #> 0%   10.400   4  71.100  52.0 2.760 1.51300 14.5000  0  0    3    1 #> 25%  15.425   4 120.825  96.5 3.080 2.58125 16.8925  0  0    3    2 #> 50%  19.200   6 196.300 123.0 3.695 3.32500 17.7100  0  0    4    2 #> 75%  22.800   8 326.000 180.0 3.920 3.61000 18.9000  1  1    4    4 #> 100% 33.900   8 472.000 335.0 4.930 5.42400 22.9000  1  1    5    8 head(dapply(mtcars, quantile, MARGIN = 1))     # Same for data frames, output is also a data.frame #>                   0%    25%   50%    75% 100% #> Mazda RX4          0 3.2600 4.000 18.730  160 #> Mazda RX4 Wag      0 3.3875 4.000 19.010  160 #> Datsun 710         1 1.6600 4.000 20.705  108 #> Hornet 4 Drive     0 2.0000 3.215 20.420  258 #> Hornet Sportabout  0 2.5000 3.440 17.860  360 #> Valiant            0 1.8800 3.460 19.160  225 dapply(mtcars, quantile) #>         mpg cyl    disp    hp  drat      wt    qsec vs am gear carb #> 0%   10.400   4  71.100  52.0 2.760 1.51300 14.5000  0  0    3    1 #> 25%  15.425   4 120.825  96.5 3.080 2.58125 16.8925  0  0    3    2 #> 50%  19.200   6 196.300 123.0 3.695 3.32500 17.7100  0  0    4    2 #> 75%  22.800   8 326.000 180.0 3.920 3.61000 18.9000  1  1    4    4 #> 100% 33.900   8 472.000 335.0 4.930 5.42400 22.9000  1  1    5    8  # With classed objects, we have to be a bit careful if (FALSE) { # \\dontrun{ dapply(EuStockMarkets, quantile)  # This gives an error because the tsp attribute is misspecified } # } dapply(EuStockMarkets, quantile, return = \"matrix\")    # These both work fine.. #>           DAX      SMI     CAC     FTSE #> 0%   1402.340 1587.400 1611.00 2281.000 #> 25%  1744.102 2165.625 1875.15 2843.150 #> 50%  2140.565 2796.350 1992.30 3246.600 #> 75%  2722.367 3812.425 2274.35 3993.575 #> 100% 6186.090 8412.000 4388.50 6179.000 dapply(EuStockMarkets, quantile, return = \"data.frame\") #>           DAX      SMI     CAC     FTSE #> 0%   1402.340 1587.400 1611.00 2281.000 #> 25%  1744.102 2165.625 1875.15 2843.150 #> 50%  2140.565 2796.350 1992.30 3246.600 #> 75%  2722.367 3812.425 2274.35 3993.575 #> 100% 6186.090 8412.000 4388.50 6179.000   # Similarly for grouped tibbles and other data frame based classes library(dplyr) gmtcars <- group_by(mtcars,cyl,vs,am) head(dapply(gmtcars, log))               # Still gives a grouped tibble back #> # A tibble: 6 × 11 #> # Groups:   cyl, vs, am [4] #>     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1  3.04  1.79  5.08  4.70  1.36 0.963  2.80  -Inf     0  1.39 1.39  #> 2  3.04  1.79  5.08  4.70  1.36 1.06   2.83  -Inf     0  1.39 1.39  #> 3  3.13  1.39  4.68  4.53  1.35 0.842  2.92     0     0  1.39 0     #> 4  3.06  1.79  5.55  4.70  1.12 1.17   2.97     0  -Inf  1.10 0     #> 5  2.93  2.08  5.89  5.16  1.15 1.24   2.83  -Inf  -Inf  1.10 0.693 #> 6  2.90  1.79  5.42  4.65  1.02 1.24   3.01     0  -Inf  1.10 0     dapply(gmtcars, quantile, MARGIN = 1)    # Here it makes sense to keep the groups attribute #> # A tibble: 32 × 5 #> # Groups:   cyl, vs, am [7] #>     `0%` `25%` `50%` `75%` `100%` #>  * <dbl> <dbl> <dbl> <dbl>  <dbl> #>  1     0  3.26  4     18.7   160  #>  2     0  3.39  4     19.0   160  #>  3     1  1.66  4     20.7   108  #>  4     0  2     3.22  20.4   258  #>  5     0  2.5   3.44  17.9   360  #>  6     0  1.88  3.46  19.2   225  #>  7     0  3.10  4     15.1   360  #>  8     0  2.60  4     22.2   147. #>  9     0  2.58  4     22.8   141. #> 10     0  3.68  4     18.8   168. #> # ℹ 22 more rows dapply(gmtcars, quantile)                # This does not make much sense, ... #> # A tibble: 5 × 11 #> # Groups:   cyl, vs, am [7] #>     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #> * <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1  10.4     4  71.1  52    2.76  1.51  14.5     0     0     3     1 #> 2  15.4     4 121.   96.5  3.08  2.58  16.9     0     0     3     2 #> 3  19.2     6 196.  123    3.70  3.32  17.7     0     0     4     2 #> 4  22.8     8 326   180    3.92  3.61  18.9     1     1     4     4 #> 5  33.9     8 472   335    4.93  5.42  22.9     1     1     5     8 dapply(gmtcars, quantile,                # better convert to plain data.frame:        return = \"data.frame\") #>         mpg cyl    disp    hp  drat      wt    qsec vs am gear carb #> 0%   10.400   4  71.100  52.0 2.760 1.51300 14.5000  0  0    3    1 #> 25%  15.425   4 120.825  96.5 3.080 2.58125 16.8925  0  0    3    2 #> 50%  19.200   6 196.300 123.0 3.695 3.32500 17.7100  0  0    4    2 #> 75%  22.800   8 326.000 180.0 3.920 3.61000 18.9000  1  1    4    4 #> 100% 33.900   8 472.000 335.0 4.930 5.42400 22.9000  1  1    5    8"},{"path":"https://sebkrantz.github.io/collapse/reference/data-transformations.html","id":null,"dir":"Reference","previous_headings":"","what":"Data Transformations — data-transformations","title":"Data Transformations — data-transformations","text":"collapse provides ensemble functions perform common data transformations efficiently user friendly: dapply applies functions rows columns matrices data frames, preserving data format. S3 generic efficient Split-Apply-Combine computing, similar dapply. set arithmetic operators facilitates row-wise %rr%, %r+%, %r-%, %r*%, %r/% column-wise %cr%, %c+%, %c-%, %c*%, %c/% replacing sweeping operations involving vector matrix data frame / list. Since v1.7, operators %+=%, %-=%, %*=% %/=% column- element- wise math reference, function setop can also perform sweeping rows reference. (set)TRA advanced S3 generic efficiently perform (groupwise) replacing sweeping statistics, either creating copy data reference.  Supported operations : collapse's Fast Statistical Functions built-TRA argument faster access (.e. can compute (groupwise) statistics use transform data single function call). fscale/STD S3 generic perform (groupwise / weighted) scaling / standardizing data orders magnitude faster scale. fwithin/W S3 generic efficiently perform (groupwise / weighted) within-transformations / demeaning / centering data. Similarly fbetween/B computes (groupwise / weighted) -transformations / averages (also lot faster ave). fhdwithin/HDW, shorthand 'higher-dimensional within transform', S3 generic efficiently center data multiple groups partial-linear models (possibly involving many levels fixed effects interactions). words, fhdwithin/HDW efficiently computes residuals linear models. Similarly fhdbetween/HDB, shorthand 'higher-dimensional transformation', computes corresponding means fitted values. flag/L/F, fdiff/D/Dlog fgrowth/G S3 generics compute sequences lags / leads suitably lagged iterated (quasi-, log-) differences growth rates time series panel data. fcumsum flexibly computes (grouped, ordered) cumulative sums. Time Series Panel Series. STD, W, B, HDW, HDB, L, D, Dlog G parsimonious wrappers around f- functions representing corresponding transformation 'operators'. additional capabilities applied data-frames (.e. variable selection, formula input, auto-renaming id-variable preservation), easier employ regression formulas, otherwise identical functionality.","code":""},{"path":[]},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/descr.html","id":null,"dir":"Reference","previous_headings":"","what":"Detailed Statistical Description of Data Frame — descr","title":"Detailed Statistical Description of Data Frame — descr","text":"descr offers fast detailed description variable data frame. Since v1.9.0 fully supports grouped weighted computations.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/descr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detailed Statistical Description of Data Frame — descr","text":"","code":"descr(X, ...)  # Default S3 method descr(X, by = NULL, w = NULL, cols = NULL,       Ndistinct = TRUE, higher = TRUE, table = TRUE, sort.table = \"freq\",       Qprobs = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), Qtype = 7L,       label.attr = \"label\", stepwise = FALSE, ...)  # S3 method for class 'grouped_df' descr(X, w = NULL,       Ndistinct = TRUE, higher = TRUE, table = TRUE, sort.table = \"freq\",       Qprobs = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), Qtype = 7L,       label.attr = \"label\", stepwise = FALSE, ...)  # S3 method for class 'descr' as.data.frame(x, ..., gid = \"Group\")  # S3 method for class 'descr' print(x, n = 14, perc = TRUE, digits = .op[[\"digits\"]], t.table = TRUE, total = TRUE,       compact = FALSE, summary = !compact, reverse = FALSE, stepwise = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/descr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detailed Statistical Description of Data Frame — descr","text":"X (grouped) data frame list atomic vectors. Atomic vectors, matrices arrays can passed first coerced data frame using qDF. factor, GRP object, atomic vector / list vectors (internally grouped GRP), one- two-sided formula e.g. ~ group1 var1 + var2 ~ group1 + group2 group X. See Examples. w numeric vector (non-negative) weights. default method also supports one-sided formulas .e. ~ weightcol ~ log(weightcol). grouped_df method supports lazy-expressions (without ~). See Examples. cols select columns describe using column names, indices logical vector selector function (e.g. .numeric). Note: cols ignored two-sided formula passed . Ndistinct logical. TRUE (default) computes number distinct values variables using fndistinct. higher logical. Argument passed qsu: TRUE (default) computes skewness kurtosis. table logical. TRUE (default) computes (sorted) frequency table categorical variables (excluding Date variables). sort.table integer character string specifying frequency table presented:  Qprobs double. Probabilities quantiles compute numeric variables, passed .quantile. something non-numeric passed (.e. NULL, FALSE, NA, \"\" etc.), quantiles computed. Qtype integer. Quantile types 5-9 following Hyndman Fan (1996) recommended type 8, default 7 quantile. label.attr character. name label attribute display variable (variables labeled). ... descr: arguments passed qsu.default. [.descr: variable names indices passed [.list. argument unused print .data.frame methods. x object class 'descr'. n integer. maximum number table elements print categorical variables. number distinct elements <= n, whole table printed. Otherwise remaining items summed '... %s Others' category. perc logical. TRUE (default) adds percentages frequencies table categorical variables, , !.null(), percentage observations group. digits integer. number decimals print statistics, quantiles percentage tables. t.table logical. TRUE (default) prints transposed table. total logical. TRUE (default) adds 'Total' column grouped tables (using argument). compact logical. TRUE combines statistics quantiles generate compact printout. Especially useful groups (). summary logical. TRUE (default) computes displays summary frequencies, size table categorical variable exceeds n. reverse logical. TRUE prints contents reverse order, starting last column, dataset can analyzed scrolling console calling descr. stepwise logical. TRUE prints one variable time. user needs press [enter] see printout next variable. called descr, computation also done one variable time, finished 'descr' object returned invisibly. gid character. Name assigned group-id column, describing data groups.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/descr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detailed Statistical Description of Data Frame — descr","text":"descr heavily inspired Hmisc::describe, much faster advanced statistical capabilities. principally wrapper around qsu, fquantile (.quantile), fndistinct numeric variables, computes frequency tables categorical variables using qtab. Date variables summarized fnobs, fndistinct frange. Since v1.9.0 grouped weighted computations fully supported. use sampling weights produce weighted mean, sd, skewness kurtosis, weighted quantiles numeric data. categorical data, tables display sum weights instead frequencies, percentage tables well percentage missing values indicated next 'Statistics' print, relative total sum weights. can done groups. Grouped (weighted) quantiles computed using . larger datasets, calling stepwise option directly descr() recommended, precomputing statistics variables digesting results can time consuming. list-object returned descr can efficiently converted tidy data frame using .data.frame method. representation include frequency tables computed categorical variables.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/descr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detailed Statistical Description of Data Frame — descr","text":"2-level nested list-based object class 'descr'. list size dataset, contains statistics computed variable, stored list containing class, label, basic statistics quantiles / tables computed variable (matrix form). object attributes attached providing 'name' dataset, number rows dataset ('N'), attribute 'arstat' indicating whether arrays statistics generated passing arguments (e.g. pid) qsu.default, attribute 'table' indicating whether table = TRUE (.e. object contain tables categorical variables), attributes 'groups' /'weights' providing GRP object /weight vector grouped /weighted data descriptions.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/descr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detailed Statistical Description of Data Frame — descr","text":"","code":"## Simple Use descr(iris) #> Dataset: iris, 5 Variables, N = 150 #> -------------------------------------------------------------------------------- #> Sepal.Length (numeric):  #> Statistics #>     N  Ndist  Mean    SD  Min  Max  Skew  Kurt #>   150     35  5.84  0.83  4.3  7.9  0.31  2.43 #> Quantiles #>    1%   5%  10%  25%  50%  75%  90%   95%  99% #>   4.4  4.6  4.8  5.1  5.8  6.4  6.9  7.25  7.7 #> -------------------------------------------------------------------------------- #> Sepal.Width (numeric):  #> Statistics #>     N  Ndist  Mean    SD  Min  Max  Skew  Kurt #>   150     23  3.06  0.44    2  4.4  0.32  3.18 #> Quantiles #>    1%    5%  10%  25%  50%  75%   90%  95%   99% #>   2.2  2.34  2.5  2.8    3  3.3  3.61  3.8  4.15 #> -------------------------------------------------------------------------------- #> Petal.Length (numeric):  #> Statistics #>     N  Ndist  Mean    SD  Min  Max   Skew  Kurt #>   150     43  3.76  1.77    1  6.9  -0.27   1.6 #> Quantiles #>     1%   5%  10%  25%   50%  75%  90%  95%  99% #>   1.15  1.3  1.4  1.6  4.35  5.1  5.8  6.1  6.7 #> -------------------------------------------------------------------------------- #> Petal.Width (numeric):  #> Statistics #>     N  Ndist  Mean    SD  Min  Max  Skew  Kurt #>   150     22   1.2  0.76  0.1  2.5  -0.1  1.66 #> Quantiles #>    1%   5%  10%  25%  50%  75%  90%  95%  99% #>   0.1  0.2  0.2  0.3  1.3  1.8  2.2  2.3  2.5 #> -------------------------------------------------------------------------------- #> Species (factor):  #> Statistics #>     N  Ndist #>   150      3 #> Table #>             Freq   Perc #> setosa        50  33.33 #> versicolor    50  33.33 #> virginica     50  33.33 #> -------------------------------------------------------------------------------- descr(wlddev) #> Dataset: wlddev, 13 Variables, N = 13176 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics #>       N  Ndist #>   13176    216 #> Table #>                       Freq   Perc #> Afghanistan             61   0.46 #> Albania                 61   0.46 #> Algeria                 61   0.46 #> American Samoa          61   0.46 #> Andorra                 61   0.46 #> Angola                  61   0.46 #> Antigua and Barbuda     61   0.46 #> Argentina               61   0.46 #> Armenia                 61   0.46 #> Aruba                   61   0.46 #> Australia               61   0.46 #> Austria                 61   0.46 #> Azerbaijan              61   0.46 #> Bahamas, The            61   0.46 #> ... 202 Others       12322  93.52 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      61      61      61      61      61      61  #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics #>       N  Ndist #>   13176    216 #> Table #>                  Freq   Perc #> ABW                61   0.46 #> AFG                61   0.46 #> AGO                61   0.46 #> ALB                61   0.46 #> AND                61   0.46 #> ARE                61   0.46 #> ARG                61   0.46 #> ARM                61   0.46 #> ASM                61   0.46 #> ATG                61   0.46 #> AUS                61   0.46 #> AUT                61   0.46 #> AZE                61   0.46 #> BDI                61   0.46 #> ... 202 Others  12322  93.52 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      61      61      61      61      61      61  #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics #>          N       Ndist         Min         Max   #>      13176          61  1961-01-01  2021-01-01   #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics #>       N  Ndist  Mean     SD   Min   Max  Skew  Kurt #>   13176     61  1990  17.61  1960  2020    -0   1.8 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1963  1966  1975  1990  2005  2014  2017  2020 #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics #>       N  Ndist     Mean     SD   Min   Max  Skew  Kurt #>   13176      7  1985.57  17.51  1960  2020  0.03  1.79 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1960  1960  1970  1990  2000  2010  2010  2020 #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics #>       N  Ndist #>   13176      7 #> Table #>                             Freq   Perc #> Europe & Central Asia       3538  26.85 #> Sub-Saharan Africa          2928  22.22 #> Latin America & Caribbean   2562  19.44 #> East Asia & Pacific         2196  16.67 #> Middle East & North Africa  1281   9.72 #> South Asia                   488   3.70 #> North America                183   1.39 #> -------------------------------------------------------------------------------- #> income (factor): Income Level #> Statistics #>       N  Ndist #>   13176      4 #> Table #>                      Freq   Perc #> High income          4819  36.57 #> Upper middle income  3660  27.78 #> Lower middle income  2867  21.76 #> Low income           1830  13.89 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics #>       N  Ndist #>   13176      2 #> Table #>         Freq   Perc #> FALSE  10980  83.33 #> TRUE    2196  16.67 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (28.13% NAs) #>      N  Ndist      Mean        SD     Min        Max  Skew   Kurt #>   9470   9470  12048.78  19077.64  132.08  196061.42  3.13  17.12 #> Quantiles #>       1%      5%     10%      25%      50%       75%       90%       95% #>   227.71  399.62  555.55  1303.19  3767.16  14787.03  35646.02  48507.84 #>        99% #>   92340.28 #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (11.43% NAs) #>       N  Ndist  Mean     SD    Min    Max   Skew  Kurt #>   11670  10548  64.3  11.48  18.91  85.42  -0.67  2.67 #> Quantiles #>      1%     5%    10%    25%    50%    75%    90%    95%    99% #>   35.83  42.77  46.83  56.36  67.44  72.95  77.08  79.34  82.36 #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (86.76% NAs) #>      N  Ndist   Mean   SD   Min   Max  Skew  Kurt #>   1744    368  38.53  9.2  20.7  65.8   0.6  2.53 #> Quantiles #>     1%    5%   10%   25%   50%  75%   90%    95%   99% #>   24.6  26.3  27.6  31.5  36.4   45  52.6  55.98  60.5 #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (34.67% NAs) #>      N  Ndist        Mean          SD          Min             Max  Skew #>   8608   7832  454'720131  868'712654  -997'679993  2.56715605e+10  6.98 #>     Kurt #>   114.89 #> Quantiles #>             1%           5%          10%          25%         50%         75% #>   -12'593999.7  1'363500.01  8'347000.31  44'887499.8  165'970001  495'042503 #>              90%             95%             99% #>   1.18400697e+09  1.93281696e+09  3.73380782e+09 #> -------------------------------------------------------------------------------- #> POP (numeric): Population, total #> Statistics (1.95% NAs) #>       N  Ndist         Mean          SD   Min             Max  Skew    Kurt #>   12919  12877  24'245971.6  102'120674  2833  1.39771500e+09  9.75  108.91 #> Quantiles #>        1%       5%      10%     25%       50%        75%          90% #>   8698.84  31083.3  62268.4  443791  4'072517  12'816178  46'637331.4 #>           95%         99% #>   81'177252.5  308'862641 #> -------------------------------------------------------------------------------- descr(GGDC10S) #> Dataset: GGDC10S, 16 Variables, N = 5027 #> -------------------------------------------------------------------------------- #> Country (character): Country #> Statistics #>      N  Ndist #>   5027     43 #> Table #>                Freq   Perc #> USA             129   2.57 #> EGY             129   2.57 #> MOR             128   2.55 #> IDN             126   2.51 #> PHL             126   2.51 #> TWN             126   2.51 #> DNK             126   2.51 #> ESP             126   2.51 #> FRA             126   2.51 #> GBR             126   2.51 #> ITA             126   2.51 #> NLD             126   2.51 #> SWE             126   2.51 #> CHN             125   2.49 #> ... 29 Others  3256  64.77 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       4     105     124     117     126     129  #> -------------------------------------------------------------------------------- #> Regioncode (character): Region code #> Statistics #>      N  Ndist #>   5027      6 #> Table #>       Freq   Perc #> ASI   1372  27.29 #> SSA   1148  22.84 #> LAM   1117  22.22 #> EUR   1004  19.97 #> MENA   257   5.11 #> NAM    129   2.57 #> -------------------------------------------------------------------------------- #> Region (character): Region #> Statistics #>      N  Ndist #>   5027      6 #> Table #>                               Freq   Perc #> Asia                          1372  27.29 #> Sub-saharan Africa            1148  22.84 #> Latin America                 1117  22.22 #> Europe                        1004  19.97 #> Middle East and North Africa   257   5.11 #> North America                  129   2.57 #> -------------------------------------------------------------------------------- #> Variable (character): Variable #> Statistics #>      N  Ndist #>   5027      2 #> Table #>      Freq   Perc #> EMP  2516  50.05 #> VA   2511  49.95 #> -------------------------------------------------------------------------------- #> Year (numeric): Year #> Statistics #>      N  Ndist     Mean     SD   Min   Max   Skew  Kurt #>   5027     67  1981.58  17.57  1947  2013  -0.05  1.86 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1950  1953  1957  1967  1982  1997  2006  2009  2011 #> -------------------------------------------------------------------------------- #> AGR (numeric): Agriculture  #> Statistics (13.19% NAs) #>      N  Ndist        Mean           SD  Min             Max   Skew    Kurt #>   4364   4353  2'526696.5  37'129098.1    0  1.19187778e+09  23.95  642.16 #> Quantiles #>     1%     5%     10%    25%      50%       75%     90%          95% #>   0.09  23.18  144.56  930.7  4394.52  29781.04  315403  2'393977.49 #>           99% #>   24'932575.1 #> -------------------------------------------------------------------------------- #> MIN (numeric): Mining #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min             Max   Skew    Kurt #>   4355   4224  1'867908.95  32'334251.7    0  1.10344053e+09  25.27  712.33 #> Quantiles #>     1%   5%   10%    25%     50%      75%       90%        95%          99% #>   0.02  1.2  3.78  38.95  173.22  4841.26  64810.08  713420.36  14'309891.9 #> -------------------------------------------------------------------------------- #> MAN (numeric): Manufacturing #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min             Max   Skew   Kurt #>   4355   4353  5'538491.36  63'090998.4    0  1.86843541e+09  20.71  498.7 #> Quantiles #>     1%     5%     10%     25%     50%       75%        90%          95% #>   0.05  27.31  103.84  620.44  3718.1  52805.35  516077.08  2'978846.99 #>          99% #>   108'499037 #> -------------------------------------------------------------------------------- #> PU (numeric): Utilities #> Statistics (13.39% NAs) #>      N  Ndist       Mean           SD  Min          Max  Skew    Kurt #>   4354   4237  335679.47  2'576027.41    0  65'324543.8  13.5  244.29 #> Quantiles #>   1%    5%  10%    25%     50%      75%       90%        95%          99% #>    0  2.16  6.3  25.74  167.95  4892.25  63004.56  291356.48  11'866259.3 #> -------------------------------------------------------------------------------- #> CON (numeric): Construction #> Statistics (13.37% NAs) #>      N  Ndist         Mean         SD  Min         Max   Skew    Kurt #>   4355   4339  1'801597.63  24'382598    0  860'638677  26.15  774.73 #> Quantiles #>     1%     5%    10%     25%      50%       75%        90%        95% #>   0.02  15.03  43.37  215.57  1473.45  13514.84  132609.51  829361.57 #>           99% #>   37'430603.6 #> -------------------------------------------------------------------------------- #> WRT (numeric): Trade, restaurants and hotels #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min             Max   Skew    Kurt #>   4355   4344  3'392909.52  36'950812.9    0  1.15497404e+09  21.19  530.06 #> Quantiles #>     1%     5%    10%     25%      50%       75%        90%          95% #>   0.03  25.07  96.85  650.38  3773.64  41648.17  475116.68  2'646521.57 #>           99% #>   79'618054.2 #> -------------------------------------------------------------------------------- #> TRA (numeric): Transport, storage and communication #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min         Max   Skew    Kurt #>   4355   4334  1'473269.72  16'815143.2    0  547'047040  22.77  604.58 #> Quantiles #>     1%     5%    10%     25%     50%       75%        90%          95% #>   0.05  12.28  37.35  205.79  1174.8  18927.21  195055.31  1'059843.16 #>           99% #>   31'750009.1 #> -------------------------------------------------------------------------------- #> FIRE (numeric): Finance, insurance, real estate and business services #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD       Min         Max   Skew    Kurt #>   4355   4349  1'657114.84  13'709981.9  -2848.81  387'997506  16.48  356.43 #> Quantiles #>   1%    5%   10%     25%     50%      75%        90%          95%        99% #>    0  3.87  14.3  128.18  960.13  13460.4  252299.08  1'599086.92  55'536957 #> -------------------------------------------------------------------------------- #> GOV (numeric): Government services #> Statistics (30.73% NAs) #>      N  Ndist         Mean           SD  Min         Max   Skew    Kurt #>   3482   3470  1'712300.28  16'967383.7    0  485'535400  18.67  430.18 #> Quantiles #>     1%     5%     10%     25%      50%       75%        90%          95% #>   0.02  48.14  121.87  723.98  3928.51  37689.12  331990.24  1'400263.37 #>           99% #>   56'340246.3 #> -------------------------------------------------------------------------------- #> OTH (numeric): Community, social and personal services #> Statistics (15.5% NAs) #>      N  Ndist         Mean           SD  Min         Max   Skew    Kurt #>   4248   4238  1'684527.32  15'613923.6    0  402'671182  14.93  273.79 #> Quantiles #>     1%     5%    10%     25%      50%       75%        90%        95% #>   0.02  15.92  49.56  310.09  1433.17  13321.29  107230.29  605013.39 #>           99% #>   42'264477.4 #> -------------------------------------------------------------------------------- #> SUM (numeric): Summation of sector GDP #> Statistics (13.19% NAs) #>      N  Ndist         Mean          SD  Min             Max   Skew    Kurt #>   4364   4364  21'566436.8  251'812500    0  8.06794210e+09  22.53  589.58 #> Quantiles #>     1%      5%      10%      25%       50%        75%          90%          95% #>   0.38  269.63  1242.98  4803.94  23186.19  284646.08  2'644610.11  15'030223.5 #>          99% #>   435'513356 #> --------------------------------------------------------------------------------  # Some useful print options (also try stepwise argument) print(descr(GGDC10S), reverse = TRUE, t.table = FALSE) #> SUM (numeric): Summation of sector GDP #> Statistics (13.19% NAs) #>      N  Ndist         Mean          SD  Min             Max   Skew    Kurt #>   4364   4364  21'566436.8  251'812500    0  8.06794210e+09  22.53  589.58 #> Quantiles #>     1%      5%      10%      25%       50%        75%          90%          95% #>   0.38  269.63  1242.98  4803.94  23186.19  284646.08  2'644610.11  15'030223.5 #>          99% #>   435'513356 #> -------------------------------------------------------------------------------- #> OTH (numeric): Community, social and personal services #> Statistics (15.5% NAs) #>      N  Ndist         Mean           SD  Min         Max   Skew    Kurt #>   4248   4238  1'684527.32  15'613923.6    0  402'671182  14.93  273.79 #> Quantiles #>     1%     5%    10%     25%      50%       75%        90%        95% #>   0.02  15.92  49.56  310.09  1433.17  13321.29  107230.29  605013.39 #>           99% #>   42'264477.4 #> -------------------------------------------------------------------------------- #> GOV (numeric): Government services #> Statistics (30.73% NAs) #>      N  Ndist         Mean           SD  Min         Max   Skew    Kurt #>   3482   3470  1'712300.28  16'967383.7    0  485'535400  18.67  430.18 #> Quantiles #>     1%     5%     10%     25%      50%       75%        90%          95% #>   0.02  48.14  121.87  723.98  3928.51  37689.12  331990.24  1'400263.37 #>           99% #>   56'340246.3 #> -------------------------------------------------------------------------------- #> FIRE (numeric): Finance, insurance, real estate and business services #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD       Min         Max   Skew    Kurt #>   4355   4349  1'657114.84  13'709981.9  -2848.81  387'997506  16.48  356.43 #> Quantiles #>   1%    5%   10%     25%     50%      75%        90%          95%        99% #>    0  3.87  14.3  128.18  960.13  13460.4  252299.08  1'599086.92  55'536957 #> -------------------------------------------------------------------------------- #> TRA (numeric): Transport, storage and communication #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min         Max   Skew    Kurt #>   4355   4334  1'473269.72  16'815143.2    0  547'047040  22.77  604.58 #> Quantiles #>     1%     5%    10%     25%     50%       75%        90%          95% #>   0.05  12.28  37.35  205.79  1174.8  18927.21  195055.31  1'059843.16 #>           99% #>   31'750009.1 #> -------------------------------------------------------------------------------- #> WRT (numeric): Trade, restaurants and hotels #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min             Max   Skew    Kurt #>   4355   4344  3'392909.52  36'950812.9    0  1.15497404e+09  21.19  530.06 #> Quantiles #>     1%     5%    10%     25%      50%       75%        90%          95% #>   0.03  25.07  96.85  650.38  3773.64  41648.17  475116.68  2'646521.57 #>           99% #>   79'618054.2 #> -------------------------------------------------------------------------------- #> CON (numeric): Construction #> Statistics (13.37% NAs) #>      N  Ndist         Mean         SD  Min         Max   Skew    Kurt #>   4355   4339  1'801597.63  24'382598    0  860'638677  26.15  774.73 #> Quantiles #>     1%     5%    10%     25%      50%       75%        90%        95% #>   0.02  15.03  43.37  215.57  1473.45  13514.84  132609.51  829361.57 #>           99% #>   37'430603.6 #> -------------------------------------------------------------------------------- #> PU (numeric): Utilities #> Statistics (13.39% NAs) #>      N  Ndist       Mean           SD  Min          Max  Skew    Kurt #>   4354   4237  335679.47  2'576027.41    0  65'324543.8  13.5  244.29 #> Quantiles #>   1%    5%  10%    25%     50%      75%       90%        95%          99% #>    0  2.16  6.3  25.74  167.95  4892.25  63004.56  291356.48  11'866259.3 #> -------------------------------------------------------------------------------- #> MAN (numeric): Manufacturing #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min             Max   Skew   Kurt #>   4355   4353  5'538491.36  63'090998.4    0  1.86843541e+09  20.71  498.7 #> Quantiles #>     1%     5%     10%     25%     50%       75%        90%          95% #>   0.05  27.31  103.84  620.44  3718.1  52805.35  516077.08  2'978846.99 #>          99% #>   108'499037 #> -------------------------------------------------------------------------------- #> MIN (numeric): Mining #> Statistics (13.37% NAs) #>      N  Ndist         Mean           SD  Min             Max   Skew    Kurt #>   4355   4224  1'867908.95  32'334251.7    0  1.10344053e+09  25.27  712.33 #> Quantiles #>     1%   5%   10%    25%     50%      75%       90%        95%          99% #>   0.02  1.2  3.78  38.95  173.22  4841.26  64810.08  713420.36  14'309891.9 #> -------------------------------------------------------------------------------- #> AGR (numeric): Agriculture  #> Statistics (13.19% NAs) #>      N  Ndist        Mean           SD  Min             Max   Skew    Kurt #>   4364   4353  2'526696.5  37'129098.1    0  1.19187778e+09  23.95  642.16 #> Quantiles #>     1%     5%     10%    25%      50%       75%     90%          95% #>   0.09  23.18  144.56  930.7  4394.52  29781.04  315403  2'393977.49 #>           99% #>   24'932575.1 #> -------------------------------------------------------------------------------- #> Year (numeric): Year #> Statistics #>      N  Ndist     Mean     SD   Min   Max   Skew  Kurt #>   5027     67  1981.58  17.57  1947  2013  -0.05  1.86 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1950  1953  1957  1967  1982  1997  2006  2009  2011 #> -------------------------------------------------------------------------------- #> Variable (character): Variable #> Statistics #>      N  Ndist #>   5027      2 #> Table #>         EMP     VA #> Freq   2516   2511 #> Perc  50.05  49.95 #> -------------------------------------------------------------------------------- #> Region (character): Region #> Statistics #>      N  Ndist #>   5027      6 #> Table #>        Asia  Sub-saharan Africa  Latin America  Europe #> Freq   1372                1148           1117    1004 #> Perc  27.29               22.84          22.22   19.97 #>       Middle East and North Africa  North America #> Freq                           257            129 #> Perc                          5.11           2.57 #> -------------------------------------------------------------------------------- #> Regioncode (character): Region code #> Statistics #>      N  Ndist #>   5027      6 #> Table #>         ASI    SSA    LAM    EUR  MENA   NAM #> Freq   1372   1148   1117   1004   257   129 #> Perc  27.29  22.84  22.22  19.97  5.11  2.57 #> -------------------------------------------------------------------------------- #> Country (character): Country #> Statistics #>      N  Ndist #>   5027     43 #> Table #>        USA   EGY   MOR   IDN   PHL   TWN   DNK   ESP   FRA   GBR   ITA   NLD #> Freq   129   129   128   126   126   126   126   126   126   126   126   126 #> Perc  2.57  2.57  2.55  2.51  2.51  2.51  2.51  2.51  2.51  2.51  2.51  2.51 #>        SWE   CHN  ... 29 Others #> Freq   126   125           3256 #> Perc  2.51  2.49          64.77 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       4     105     124     117     126     129  #> -------------------------------------------------------------------------------- #> Dataset: GGDC10S, 16 Variables, N = 5027 # For bigger data consider: descr(big_data, stepwise = TRUE)  # Generating a data frame as.data.frame(descr(wlddev, table = FALSE)) #>   Variable     Class                      Label     N Ndist   Min   Max Mean SD #> 1  country character               Country Name 13176   216    NA    NA   NA NA #> 2    iso3c    factor               Country Code 13176   216    NA    NA   NA NA #> 3     date      Date Date Recorded (Fictitious) 13176    61 -3287 18628   NA NA #>   Skew Kurt 1% 5% 10% 25% 50% 75% 90% 95% 99% #> 1   NA   NA NA NA  NA  NA  NA  NA  NA  NA  NA #> 2   NA   NA NA NA  NA  NA  NA  NA  NA  NA  NA #> 3   NA   NA NA NA  NA  NA  NA  NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 rows ]  ## Weighted Desciptions descr(wlddev, w = ~ replace_na(POP)) # replacing NA's with 0's for fquantile() #> Dataset: wlddev, 12 Variables, N = 13176, WeightSum = 313233706778 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics #>        WeightSum  Ndist #>   3.13233707e+11    216 #> Table #>                        WeightSum   Perc #> China                65272180000  20.84 #> India                52835203044  16.87 #> United States        15226426293   4.86 #> Indonesia            10681870259   3.41 #> Brazil                8711884458   2.78 #> Russian Federation    8388319293   2.68 #> Japan                 7088669911   2.26 #> Pakistan              6865420747   2.19 #> Bangladesh            6217567789   1.98 #> Nigeria               6191168112   1.98 #> Mexico                4948012523   1.58 #> Germany               4773054666   1.52 #> Vietnam               3955178878   1.26 #> Philippines           3805345278   1.21 #> ... 202 Others      108273405527  34.57 #>  #> Summary of Table WeightSums #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #> 5.0e+05 3.1e+07 2.6e+08 1.5e+09 8.3e+08 6.5e+10  #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics #>        WeightSum  Ndist #>   3.13233707e+11    216 #> Table #>                    WeightSum   Perc #> CHN              65272180000  20.84 #> IND              52835203044  16.87 #> USA              15226426293   4.86 #> IDN              10681870259   3.41 #> BRA               8711884458   2.78 #> RUS               8388319293   2.68 #> JPN               7088669911   2.26 #> PAK               6865420747   2.19 #> BGD               6217567789   1.98 #> NGA               6191168112   1.98 #> MEX               4948012523   1.58 #> DEU               4773054666   1.52 #> VNM               3955178878   1.26 #> PHL               3805345278   1.21 #> ... 202 Others  108273405527  34.57 #>  #> Summary of Table WeightSums #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #> 5.0e+05 3.1e+07 2.6e+08 1.5e+09 8.3e+08 6.5e+10  #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics #>          N       Ndist         Min         Max   #>      13176          61  1961-01-01  2021-01-01   #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics (1.95% NAs) #>       N  Ndist       WeightSum    Mean     SD   Min   Max   Skew  Kurt #>   12919     61  3.13233707e+11  1994.1  16.75  1960  2019  -0.32  1.97 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1961  1965  1969  1981  1996  2009  2015  2017  2019 #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics (1.95% NAs) #>       N  Ndist       WeightSum     Mean     SD   Min   Max   Skew  Kurt #>   12919      7  3.13233707e+11  1989.47  16.52  1960  2010  -0.33  1.91 #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1960  1960  1980  1990  2000  2010  2010  2010 #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics #>        WeightSum  Ndist #>   3.13233707e+11      7 #> Table #>                                WeightSum   Perc #> East Asia & Pacific         103222500256  32.95 #> South Asia                   69206603131  22.09 #> Europe & Central Asia        49081067014  15.67 #> Sub-Saharan Africa           33308413094  10.63 #> Latin America & Caribbean    26135854126   8.34 #> North America                16881058226   5.39 #> Middle East & North Africa   15398210931   4.92 #> -------------------------------------------------------------------------------- #> income (factor): Income Level #> Statistics #>        WeightSum  Ndist #>   3.13233707e+11      4 #> Table #>                         WeightSum   Perc #> Upper middle income  119606023798  38.18 #> Lower middle income  113837684528  36.34 #> High income           58840837058  18.78 #> Low income            20949161394   6.69 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics #>        WeightSum  Ndist #>   3.13233707e+11      2 #> Table #>           WeightSum  Perc #> FALSE  249344473835  79.6 #> TRUE    63889232943  20.4 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (28.13% NAs) #>      N  Ndist       WeightSum     Mean        SD     Min        Max  Skew  Kurt #>   9470   9470  2.95445830e+11  7956.24  12984.91  132.08  196061.42  2.19  7.22 #> Quantiles #>       1%      5%     10%     25%      50%      75%       90%       95% #>   164.32  263.48  370.57  711.93  1875.79  7786.69  29257.66  41278.55 #>        99% #>   51732.26 #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (11.51% NAs) #>       N  Ndist       WeightSum   Mean    SD    Min    Max   Skew  Kurt #>   11659  10548  3.12878084e+11  65.88  9.75  18.91  85.42  -0.73  2.96 #> Quantiles #>      1%     5%    10%    25%    50%    75%    90%    95%   99% #>   41.73  46.11  50.79  60.14  68.29  72.98  76.58  78.69  82.5 #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (86.76% NAs) #>      N  Ndist       WeightSum   Mean    SD   Min   Max  Skew  Kurt #>   1744    368  8.26010770e+10  39.52  7.61  20.7  65.8  0.88  3.63 #> Quantiles #>   1%    5%   10%   25%    50%   75%    90%   95%    99% #>   26  29.2  31.3  34.3  39.16  42.2  52.03  55.6  59.98 #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (34.75% NAs) #>      N  Ndist       WeightSum            Mean              SD          Min #>   8597   7832  2.31451603e+11  1.61325042e+09  1.63323654e+09  -997'679993 #>              Max  Skew   Kurt #>   2.56715605e+10  1.75  11.89 #> Quantiles #>            1%           5%          10%         25%             50% #>   -907'107452  -327'497633  56'138226.8  332'261483  1.34542532e+09 #>              75%             90%             95%             99% #>   2.51256879e+09  3.52051981e+09  4.48714786e+09  8.06805848e+09 #> --------------------------------------------------------------------------------  ## Grouped Desciptions descr(GGDC10S, ~ Variable) #> Dataset: GGDC10S, 15 Variables, N = 5027 #> Grouped by: Variable [2] #>         N   Perc #> EMP  2516  50.05 #> VA   2511  49.95 #> -------------------------------------------------------------------------------- #> Country (character): Country #> Statistics (N = 5027) #>         N   Perc  Ndist #> EMP  2516  50.05     42 #> VA   2511  49.95     43 #>  #> Table (Freq Perc) #>                      EMP         VA      Total #> USA              64  2.5    65  2.6   129  2.6 #> EGY              65  2.6    64  2.5   129  2.6 #> MOR              65  2.6    63  2.5   128  2.5 #> IDN              63  2.5    63  2.5   126  2.5 #> PHL              63  2.5    63  2.5   126  2.5 #> TWN              63  2.5    63  2.5   126  2.5 #> DNK              64  2.5    62  2.5   126  2.5 #> ESP              64  2.5    62  2.5   126  2.5 #> FRA              64  2.5    62  2.5   126  2.5 #> GBR              64  2.5    62  2.5   126  2.5 #> ITA              64  2.5    62  2.5   126  2.5 #> NLD              64  2.5    62  2.5   126  2.5 #> SWE              64  2.5    62  2.5   126  2.5 #> CHN              62  2.5    63  2.5   125  2.5 #> ... 29 Others  1623 64.5  1633 65.0  3256 64.8 #>  #> Summary of Table Frequencies #>       EMP              VA           Total       #>  Min.   : 0.00   Min.   : 4.0   Min.   :  4.0   #>  1st Qu.:52.00   1st Qu.:53.0   1st Qu.:105.0   #>  Median :62.00   Median :62.0   Median :124.0   #>  Mean   :58.51   Mean   :58.4   Mean   :116.9   #>  3rd Qu.:63.00   3rd Qu.:62.0   3rd Qu.:126.0   #>  Max.   :65.00   Max.   :65.0   Max.   :129.0   #> -------------------------------------------------------------------------------- #> Regioncode (character): Region code #> Statistics (N = 5027) #>         N   Perc  Ndist #> EMP  2516  50.05      6 #> VA   2511  49.95      6 #>  #> Table (Freq Perc) #>            EMP        VA      Total #> ASI   684 27.2  688 27.4  1372 27.3 #> SSA   571 22.7  577 23.0  1148 22.8 #> LAM   558 22.2  559 22.3  1117 22.2 #> EUR   509 20.2  495 19.7  1004 20.0 #> MENA  130  5.2  127  5.1   257  5.1 #> NAM    64  2.5   65  2.6   129  2.6 #> -------------------------------------------------------------------------------- #> Region (character): Region #> Statistics (N = 5027) #>         N   Perc  Ndist #> EMP  2516  50.05      6 #> VA   2511  49.95      6 #>  #> Table (Freq Perc) #>                                    EMP        VA      Total #> Asia                          684 27.2  688 27.4  1372 27.3 #> Sub-saharan Africa            571 22.7  577 23.0  1148 22.8 #> Latin America                 558 22.2  559 22.3  1117 22.2 #> Europe                        509 20.2  495 19.7  1004 20.0 #> Middle East and North Africa  130  5.2  127  5.1   257  5.1 #> North America                  64  2.5   65  2.6   129  2.6 #> -------------------------------------------------------------------------------- #> Year (numeric): Year #> Statistics (N = 5027) #>         N   Perc  Ndist     Mean     SD   Min   Max   Skew  Kurt #> EMP  2516  50.05     66  1981.38  17.61  1947  2012  -0.05  1.86 #> VA   2511  49.95     67  1981.78  17.53  1947  2013  -0.05  1.85 #>  #> Quantiles #>        1%    5%   10%   25%   50%   75%   90%   95%   99% #> EMP  1950  1953  1957  1967  1982  1997  2006  2009  2011 #> VA   1950  1953  1958  1967  1982  1997  2006  2009  2011 #> -------------------------------------------------------------------------------- #> AGR (numeric): Agriculture  #> Statistics (N = 4364, 13.19% NAs) #>         N   Perc  Ndist         Mean           SD   Min             Max   Skew #> EMP  2225  50.99   2219     16746.43     55644.84  5.24          390980   4.58 #> VA   2139  49.01   2135  5'137560.88  52'913681.8     0  1.19187778e+09  16.74 #>        Kurt #> EMP   23.76 #> VA   314.45 #>  #> Quantiles #>        1%     5%     10%     25%       50%        75%          90%          95% #> EMP  7.67  67.33  187.94  752.73   2168.96    5762.23     18285.84      48898.1 #> VA      0   3.11   72.85  1976.5  21040.45  156589.16  2'563619.99  8'693829.74 #>              99% #> EMP     310671.2 #> VA   47'607622.9 #> -------------------------------------------------------------------------------- #> MIN (numeric): Mining #> Statistics (N = 4355, 13.37% NAs) #>         N   Perc  Ndist         Mean           SD   Min             Max   Skew #> EMP  2216  50.88   2153       359.61      1295.29  0.11        12908.36   6.64 #> VA   2139  49.12   2072  3'802686.57  46'062895.7     0  1.10344053e+09  17.68 #>        Kurt #> EMP   50.77 #> VA   349.32 #>  #> Quantiles #>        1%    5%    10%     25%    50%       75%       90%          95% #> EMP  0.21  1.19   2.37   18.05  56.44    144.22    675.54      1145.26 #> VA      0  1.49  12.29  327.55   4642  35481.94  746910.9  1'843491.16 #>              99% #> EMP      8839.77 #> VA   35'804017.4 #> -------------------------------------------------------------------------------- #> MAN (numeric): Manufacturing #> Statistics (N = 4355, 13.37% NAs) #>         N   Perc  Ndist         Mean           SD   Min             Max  Skew #> EMP  2216  50.88   2214      5204.33     13924.82  1.04        145898.4  6.18 #> VA   2139  49.12   2139  11'270966.4  89'674720.3     0  1.86843541e+09  14.5 #>        Kurt #> EMP   48.12 #> VA   245.16 #>  #> Quantiles #>         1%     5%     10%      25%       50%        75%         90%        95% #> EMP  10.85  63.22  114.55   439.01   1188.59    4235.75    11914.54   18920.24 #> VA       0      3   51.07  3220.02  48182.47  267410.31  3'034481.4  24'608297 #>             99% #> EMP    84869.43 #> VA   220'767719 #> -------------------------------------------------------------------------------- #> PU (numeric): Utilities #> Statistics (N = 4354, 13.39% NAs) #>         N   Perc  Ndist       Mean           SD   Min          Max  Skew #> EMP  2215  50.87   2141     153.42       365.12  0.12      3903.81  6.47 #> VA   2139  49.13   2097  683126.98  3'643270.26     0  65'324543.8  9.43 #>        Kurt #> EMP    54.8 #> VA   120.69 #>  #> Quantiles #>        1%    5%   10%     25%      50%       75%        90%          95% #> EMP  1.28  3.97  6.19   14.85    40.69    144.25     356.06        589.6 #> VA      0  0.37   7.7  329.13  5185.74  34756.69  305247.84  1'843310.67 #>              99% #> EMP      1661.89 #> VA   17'121707.3 #> -------------------------------------------------------------------------------- #> CON (numeric): Construction #> Statistics (N = 4355, 13.37% NAs) #>         N   Perc  Ndist         Mean         SD   Min         Max   Skew #> EMP  2216  50.88   2209      1793.61    5114.13  1.71    69887.56   7.17 #> VA   2139  49.12   2130  3'666191.22  34'696912     0  860'638677  18.32 #>        Kurt #> EMP   63.74 #> VA   380.89 #>  #> Quantiles #>         1%     5%    10%     25%       50%       75%        90%         95% #> EMP  11.65  32.58  45.26  140.32    450.36   1664.01    3991.46     5910.12 #> VA       0   1.34  30.97  964.91  12628.28  80096.67  871467.04  7'884387.9 #>              99% #> EMP     29914.57 #> VA   58'732994.5 #> -------------------------------------------------------------------------------- #> WRT (numeric): Trade, restaurants and hotels #> Statistics (N = 4355, 13.37% NAs) #>         N   Perc  Ndist        Mean           SD   Min             Max   Skew #> EMP  2216  50.88   2212     4368.38      8616.85  1.64        84165.11   4.29 #> VA   2139  49.12   2132  6'903431.8  52'500538.5     0  1.15497404e+09  14.85 #>        Kurt #> EMP   25.93 #> VA   260.94 #>  #> Quantiles #>         1%     5%     10%      25%       50%        75%          90% #> EMP  15.15  58.31  111.22   459.61   1447.36     4228.6     11405.12 #> VA       0    3.6   65.03  2955.05  39897.72  256568.47  2'712856.32 #>              95%          99% #> EMP     18215.36     49580.74 #> VA   18'710060.6  94'112882.1 #> -------------------------------------------------------------------------------- #> TRA (numeric): Transport, storage and communication #> Statistics (N = 4355, 13.37% NAs) #>         N   Perc  Ndist         Mean           SD   Min         Max   Skew #> EMP  2216  50.88   2203      1442.44      3289.42  1.73    31222.74   5.31 #> VA   2139  49.12   2131  2'998080.02  23'900671.1     0  547'047040  15.96 #>        Kurt #> EMP   36.86 #> VA   297.62 #>  #> Quantiles #>        1%     5%    10%      25%      50%        75%          90%         95% #> EMP  5.53  22.39  38.99   135.87   406.94     1178.6      3545.03     5644.88 #> VA      0   1.72  28.86  1374.15  18552.4  113658.26  1'098376.42  7'637158.2 #>              99% #> EMP        20475 #> VA   53'478514.1 #> -------------------------------------------------------------------------------- #> FIRE (numeric): Finance, insurance, real estate and business services #> Statistics (N = 4355, 13.37% NAs) #>         N   Perc  Ndist         Mean         SD       Min         Max   Skew #> EMP  2216  50.88   2216      1330.68    3113.74      0.78    28092.69   5.07 #> VA   2139  49.12   2133  3'372504.14  19'416463  -2848.81  387'997506  11.55 #>        Kurt #> EMP   35.06 #> VA   176.34 #>  #> Quantiles #>          1%    5%    10%     25%       50%        75%          90%          95% #> EMP    2.56  7.03  14.42   68.77     298.8    1034.59      3356.25      6749.82 #> VA   -48.78  0.14  13.28  894.75  12776.55  143622.56  1'627030.46  11'657805.1 #>              99% #> EMP     18330.87 #> VA   67'958580.7 #> -------------------------------------------------------------------------------- #> GOV (numeric): Government services #> Statistics (N = 3482, 30.73% NAs) #>         N   Perc  Ndist         Mean           SD  Min         Max   Skew #> EMP  1780  51.12   1772      4196.81      7278.04    0    44817.34   3.14 #> VA   1702  48.88   1698  3'498683.46  24'143501.2    0  485'535400  13.04 #>        Kurt #> EMP   13.76 #> VA   210.91 #>  #> Quantiles #>        1%     5%     10%      25%       50%      75%          90%          95% #> EMP  20.8  57.72  120.85    409.9   1413.06  4413.38     10919.28     20592.43 #> VA      0  13.88  129.17  3029.88  37694.55   232193  1'448746.76  4'846006.68 #>            99% #> EMP   38132.65 #> VA   81'906146 #> -------------------------------------------------------------------------------- #> OTH (numeric): Community, social and personal services #> Statistics (N = 4248, 15.5% NAs) #>         N   Perc  Ndist         Mean           SD   Min         Max   Skew #> EMP  2109  49.65   2106      2268.11      8022.24  4.07   104517.87   9.48 #> VA   2139  50.35   2132  3'343192.42  21'880087.7     0  402'671182  10.55 #>        Kurt #> EMP  102.76 #> VA   137.84 #>  #> Quantiles #>         1%     5%    10%     25%       50%       75%        90%          95% #> EMP  20.32  34.82  84.19  233.75    699.45    1672.3    4121.24      7461.79 #> VA       0    2.1  20.09  787.54  10963.92  65040.92  598083.51  8'741638.08 #>              99% #> EMP     23123.63 #> VA   91'548932.3 #> -------------------------------------------------------------------------------- #> SUM (numeric): Summation of sector GDP #> Statistics (N = 4364, 13.19% NAs) #>         N   Perc  Ndist         Mean          SD     Min             Max   Skew #> EMP  2225  50.99   2225     36846.87    96318.65  173.88          764200   5.02 #> VA   2139  49.01   2139  43'961639.1  358'350627       0  8.06794210e+09  15.77 #>        Kurt #> EMP   30.98 #> VA   289.46 #>  #> Quantiles #>          1%      5%      10%      25%        50%          75%          90% #> EMP  256.12  599.38  1599.27  3555.62    9593.98      24801.5     66975.01 #> VA        0   25.01   444.54    21302  243186.47  1'396139.11  15'926968.3 #>             95%         99% #> EMP   152402.28    550909.6 #> VA   104'405351  692'993893 #> -------------------------------------------------------------------------------- descr(wlddev, ~ income) #> Dataset: wlddev, 12 Variables, N = 13176 #> Grouped by: income [4] #>                         N   Perc #> High income          4819  36.57 #> Low income           1830  13.89 #> Lower middle income  2867  21.76 #> Upper middle income  3660  27.78 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57     79 #> Low income           1830  13.89     30 #> Lower middle income  2867  21.76     47 #> Upper middle income  3660  27.78     60 #>  #> Table (Freq Perc) #>                      High income  Low income  Lower middle income #> Afghanistan              0  0.00    61  3.33              0  0.00 #> Albania                  0  0.00     0  0.00              0  0.00 #> Algeria                  0  0.00     0  0.00              0  0.00 #> American Samoa           0  0.00     0  0.00              0  0.00 #> Andorra                 61  1.27     0  0.00              0  0.00 #> Angola                   0  0.00     0  0.00             61  2.13 #> Antigua and Barbuda     61  1.27     0  0.00              0  0.00 #> Argentina                0  0.00     0  0.00              0  0.00 #> Armenia                  0  0.00     0  0.00              0  0.00 #> Aruba                   61  1.27     0  0.00              0  0.00 #> Australia               61  1.27     0  0.00              0  0.00 #> Austria                 61  1.27     0  0.00              0  0.00 #> Azerbaijan               0  0.00     0  0.00              0  0.00 #> Bahamas, The            61  1.27     0  0.00              0  0.00 #>                      Upper middle income        Total #> Afghanistan                      0  0.00     61  0.46 #> Albania                         61  1.67     61  0.46 #> Algeria                         61  1.67     61  0.46 #> American Samoa                  61  1.67     61  0.46 #> Andorra                          0  0.00     61  0.46 #> Angola                           0  0.00     61  0.46 #> Antigua and Barbuda              0  0.00     61  0.46 #> Argentina                       61  1.67     61  0.46 #> Armenia                         61  1.67     61  0.46 #> Aruba                            0  0.00     61  0.46 #> Australia                        0  0.00     61  0.46 #> Austria                          0  0.00     61  0.46 #> Azerbaijan                      61  1.67     61  0.46 #> Bahamas, The                     0  0.00     61  0.46 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #>  #> Summary of Table Frequencies #>   High income      Low income     Lower middle income Upper middle income #>  Min.   : 0.00   Min.   : 0.000   Min.   : 0.00       Min.   : 0.00       #>  1st Qu.: 0.00   1st Qu.: 0.000   1st Qu.: 0.00       1st Qu.: 0.00       #>  Median : 0.00   Median : 0.000   Median : 0.00       Median : 0.00       #>  Mean   :22.31   Mean   : 8.472   Mean   :13.27       Mean   :16.94       #>  3rd Qu.:61.00   3rd Qu.: 0.000   3rd Qu.: 0.00       3rd Qu.:61.00       #>  Max.   :61.00   Max.   :61.000   Max.   :61.00       Max.   :61.00       #>      Total    #>  Min.   :61   #>  1st Qu.:61   #>  Median :61   #>  Mean   :61   #>  3rd Qu.:61   #>  Max.   :61   #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57     79 #> Low income           1830  13.89     30 #> Lower middle income  2867  21.76     47 #> Upper middle income  3660  27.78     60 #>  #> Table (Freq Perc) #>                 High income  Low income  Lower middle income #> ABW                61  1.27     0  0.00              0  0.00 #> AFG                 0  0.00    61  3.33              0  0.00 #> AGO                 0  0.00     0  0.00             61  2.13 #> ALB                 0  0.00     0  0.00              0  0.00 #> AND                61  1.27     0  0.00              0  0.00 #> ARE                61  1.27     0  0.00              0  0.00 #> ARG                 0  0.00     0  0.00              0  0.00 #> ARM                 0  0.00     0  0.00              0  0.00 #> ASM                 0  0.00     0  0.00              0  0.00 #> ATG                61  1.27     0  0.00              0  0.00 #> AUS                61  1.27     0  0.00              0  0.00 #> AUT                61  1.27     0  0.00              0  0.00 #> AZE                 0  0.00     0  0.00              0  0.00 #> BDI                 0  0.00    61  3.33              0  0.00 #>                 Upper middle income        Total #> ABW                         0  0.00     61  0.46 #> AFG                         0  0.00     61  0.46 #> AGO                         0  0.00     61  0.46 #> ALB                        61  1.67     61  0.46 #> AND                         0  0.00     61  0.46 #> ARE                         0  0.00     61  0.46 #> ARG                        61  1.67     61  0.46 #> ARM                        61  1.67     61  0.46 #> ASM                        61  1.67     61  0.46 #> ATG                         0  0.00     61  0.46 #> AUS                         0  0.00     61  0.46 #> AUT                         0  0.00     61  0.46 #> AZE                        61  1.67     61  0.46 #> BDI                         0  0.00     61  0.46 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #>  #> Summary of Table Frequencies #>   High income      Low income     Lower middle income Upper middle income #>  Min.   : 0.00   Min.   : 0.000   Min.   : 0.00       Min.   : 0.00       #>  1st Qu.: 0.00   1st Qu.: 0.000   1st Qu.: 0.00       1st Qu.: 0.00       #>  Median : 0.00   Median : 0.000   Median : 0.00       Median : 0.00       #>  Mean   :22.31   Mean   : 8.472   Mean   :13.27       Mean   :16.94       #>  3rd Qu.:61.00   3rd Qu.: 0.000   3rd Qu.: 0.00       3rd Qu.:61.00       #>  Max.   :61.00   Max.   :61.000   Max.   :61.00       Max.   :61.00       #>      Total    #>  Min.   :61   #>  1st Qu.:61   #>  Median :61   #>  Mean   :61   #>  3rd Qu.:61   #>  Max.   :61   #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics (N = 13176) #>                         N   Perc  Ndist         Min         Max #> High income          4819  36.57     61  1961-01-01  2021-01-01 #> Low income           1830  13.89     61  1961-01-01  2021-01-01 #> Lower middle income  2867  21.76     61  1961-01-01  2021-01-01 #> Upper middle income  3660  27.78     61  1961-01-01  2021-01-01 #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics (N = 13176) #>                         N   Perc  Ndist  Mean     SD   Min   Max  Skew  Kurt #> High income          4819  36.57     61  1990  17.61  1960  2020     0   1.8 #> Low income           1830  13.89     61  1990  17.61  1960  2020    -0   1.8 #> Lower middle income  2867  21.76     61  1990  17.61  1960  2020    -0   1.8 #> Upper middle income  3660  27.78     61  1990  17.61  1960  2020     0   1.8 #>  #> Quantiles #>                        1%    5%   10%   25%   50%   75%   90%   95%   99% #> High income          1960  1963  1966  1975  1990  2005  2014  2017  2020 #> Low income           1960  1963  1966  1975  1990  2005  2014  2017  2020 #> Lower middle income  1960  1963  1966  1975  1990  2005  2014  2017  2020 #> Upper middle income  1960  1963  1966  1975  1990  2005  2014  2017  2020 #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics (N = 13176) #>                         N   Perc  Ndist     Mean     SD   Min   Max  Skew  Kurt #> High income          4819  36.57      7  1985.57  17.51  1960  2020  0.03  1.79 #> Low income           1830  13.89      7  1985.57  17.52  1960  2020  0.03  1.79 #> Lower middle income  2867  21.76      7  1985.57  17.51  1960  2020  0.03  1.79 #> Upper middle income  3660  27.78      7  1985.57  17.51  1960  2020  0.03  1.79 #>  #> Quantiles #>                        1%    5%   10%   25%   50%   75%   90%   95%   99% #> High income          1960  1960  1960  1970  1990  2000  2010  2010  2020 #> Low income           1960  1960  1960  1970  1990  2000  2010  2010  2020 #> Lower middle income  1960  1960  1960  1970  1990  2000  2010  2010  2020 #> Upper middle income  1960  1960  1960  1970  1990  2000  2010  2010  2020 #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57      6 #> Low income           1830  13.89      5 #> Lower middle income  2867  21.76      6 #> Upper middle income  3660  27.78      6 #>  #> Table (Freq Perc) #>                             High income  Low income  Lower middle income #> Europe & Central Asia         2257 46.8     61  3.3             244  8.5 #> Sub-Saharan Africa              61  1.3   1464 80.0            1037 36.2 #> Latin America & Caribbean     1037 21.5     61  3.3             244  8.5 #> East Asia & Pacific            793 16.5      0  0.0             793 27.7 #> Middle East & North Africa     488 10.1    122  6.7             305 10.6 #> South Asia                       0  0.0    122  6.7             244  8.5 #> North America                  183  3.8      0  0.0               0  0.0 #>                             Upper middle income      Total #> Europe & Central Asia                  976 26.7  3538 26.9 #> Sub-Saharan Africa                     366 10.0  2928 22.2 #> Latin America & Caribbean             1220 33.3  2562 19.4 #> East Asia & Pacific                    610 16.7  2196 16.7 #> Middle East & North Africa             366 10.0  1281  9.7 #> South Asia                             122  3.3   488  3.7 #> North America                            0  0.0   183  1.4 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57      2 #> Low income           1830  13.89      1 #> Lower middle income  2867  21.76      1 #> Upper middle income  3660  27.78      2 #>  #> Table (Freq Perc) #>        High income  Low income  Lower middle income  Upper middle income #> FALSE   2745  57.0  1830 100.0           2867 100.0           3538  96.7 #> TRUE    2074  43.0     0   0.0              0   0.0            122   3.3 #>              Total #> FALSE  10980  83.3 #> TRUE    2196  16.7 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (N = 9470, 28.13% NAs) #>                         N   Perc  Ndist      Mean        SD     Min        Max #> High income          3179  33.57   3179  30280.73  23847.05  932.04  196061.42 #> Low income           1311  13.84   1311    597.41    288.44  164.34    1864.79 #> Lower middle income  2246  23.72   2246   1574.25    858.72  144.99    4818.19 #> Upper middle income  2734  28.87   2734   4945.33   2979.56  132.08   20532.95 #>                      Skew   Kurt #> High income          2.17  10.34 #> Low income           1.24   4.71 #> Lower middle income  0.91   3.72 #> Upper middle income  1.23   4.94 #>  #> Quantiles #>                           1%       5%      10%       25%       50%       75% #> High income          3053.83  5395.18  7768.74  14369.61  24745.65  38936.22 #> Low income            191.73   234.77   289.48    396.52    535.96    745.29 #> Lower middle income   194.43   398.88   585.24    961.12   1437.78   1987.89 #> Upper middle income   466.58  1248.19  1835.98   2864.47   4219.97   6452.07 #>                          90%       95%        99% #> High income            57259   75529.1  116493.28 #> Low income            985.45   1180.37    1513.83 #> Lower middle income  2829.09   3192.75     4191.8 #> Upper middle income  8966.02  10867.95   14416.71 #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (N = 11670, 11.43% NAs) #>                         N   Perc  Ndist   Mean    SD    Min    Max   Skew  Kurt #> High income          3831  32.83   3566  73.62  5.67  42.67  85.42  -1.01  5.56 #> Low income           1800  15.42   1751  49.73  9.09  26.17  74.43   0.27  2.67 #> Lower middle income  2790  23.91   2694  58.15  9.31  18.91   76.7  -0.34  2.68 #> Upper middle income  3249  27.84   3083  66.65  7.54  36.53  80.28   -1.1  4.23 #>  #> Quantiles #>                         1%     5%    10%    25%    50%    75%    90%    95% #> High income          55.62  63.95  67.12   70.5  73.93  77.61  80.67  81.79 #> Low income            31.5  35.78   38.2  43.52  49.04  56.06  61.98  65.87 #> Lower middle income  36.62  42.66  45.73   51.5  58.53  65.79     70  71.72 #> Upper middle income  42.66  51.22  55.93   62.8  68.36  71.95  74.66  75.94 #>                        99% #> High income          83.22 #> Low income           71.71 #> Lower middle income  74.91 #> Upper middle income  78.42 #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (N = 1744, 86.76% NAs) #>                        N   Perc  Ndist   Mean    SD   Min   Max  Skew  Kurt #> High income          680  38.99    213   33.3  6.79  20.7  58.9  1.49  5.68 #> Low income           107   6.14     88  41.13  6.58  29.5  65.8  0.75  4.24 #> Lower middle income  369  21.16    219  40.05   9.3    24  63.2  0.44  2.22 #> Upper middle income  588  33.72    280  43.16  8.95  25.2  64.8  0.08  2.35 #>  #> Quantiles #>                         1%     5%    10%    25%    50%   75%    90%    95% #> High income          23.42   25.2  26.39  28.48  32.35  35.5  41.01  48.72 #> Low income           29.81  32.35  33.26  35.65   41.1  44.9  48.26  51.61 #> Lower middle income  24.77  26.84  28.88   32.7   38.7  46.6  54.52  56.94 #> Upper middle income  26.21  27.87   30.6  36.77  42.45  49.5  54.83  58.56 #>                        99% #> High income          56.62 #> Low income           60.99 #> Lower middle income  59.82 #> Upper middle income     63 #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (N = 8608, 34.67% NAs) #>                         N   Perc  Ndist        Mean              SD #> High income          1575   18.3   1407  153'663194      425'918409 #> Low income           1692  19.66   1678  631'660165      941'498380 #> Lower middle income  2544  29.55   2503  692'072692  1.02452490e+09 #> Upper middle income  2797  32.49   2700  301'326218      765'116131 #>                              Min             Max   Skew    Kurt #> High income          -464'709991  4.34612988e+09   5.25   36.27 #> Low income               -500000  1.04032100e+10   4.46   32.13 #> Lower middle income  -605'969971  1.18790801e+10   3.79   25.24 #> Upper middle income  -997'679993  2.56715605e+10  16.31  464.86 #>  #> Quantiles #>                                1%           5%          10%         25% #> High income          -54'802401.1   -755999.99       264000  4'400000.1 #> Low income            1'100000.02  33'997999.8  71'296000.7  151'814999 #> Lower middle income     209999.99  14'721500.3  41'358000.2  100'485003 #> Upper middle income  -73'793201.9  4'558000.18    12'666000   38'000000 #>                              50%         75%             90%             95% #> High income          21'209999.1  104'934998      375'347992      661'426996 #> Low income            332'904999  692'777496  1.47914895e+09  2.14049348e+09 #> Lower middle income   336'494995  810'707520  1.84614302e+09  2.59226945e+09 #> Upper middle income   105'139999  311'519989      714'823975  1.18504797e+09 #>                                 99% #> High income          2.31632209e+09 #> Low income           4.82899863e+09 #> Lower middle income  4.69573516e+09 #> Upper middle income  2.98750435e+09 #> -------------------------------------------------------------------------------- #> POP (numeric): Population, total #> Statistics (N = 12919, 1.95% NAs) #>                         N   Perc  Ndist         Mean           SD     Min #> High income          4737  36.67   4712  12'421540.4  34'160829.5    2833 #> Low income           1792  13.87   1791  11'690380.2  13'942313.8  365047 #> Lower middle income  2790   21.6   2790  40'802037.5   137'302296   41202 #> Upper middle income  3600  27.87   3596  33'223895.5   143'647992    4375 #>                                 Max  Skew   Kurt #> High income              328'239523   5.5  39.75 #> Low income               112'078730  3.22  16.57 #> Lower middle income  1.36641775e+09   6.7   52.4 #> Upper middle income  1.39771500e+09  7.53  61.78 #>  #> Quantiles #>                             1%           5%         10%          25% #> High income            7467.08      18432.4       30517        84449 #> Low income           594755.89  1'206251.55  1'919035.6  3'842838.75 #> Lower middle income   58452.97    105620.75    224651.9     1'188469 #> Upper middle income    7357.28     47202.15     93885.2    609166.25 #>                             50%          75%          90%          95% #> High income            1'632114     8'336605  37'508393.4  58'933084.2 #> Low income             7'181772  13'579920.8  25'964845.3  36'596246.7 #> Lower middle income  5'914923.5  25'966431.8    81'157844   146'949299 #> Upper middle income    3'763490  16'347212.5  47'556659.8   104'771148 #>                                 99% #> High income              209'091400 #> Low income              76'539171.5 #> Lower middle income      893'256928 #> Upper middle income  1.02344515e+09 #> -------------------------------------------------------------------------------- print(descr(wlddev, ~ income), compact = TRUE) #> Dataset: wlddev, 12 Variables, N = 13176 #> Grouped by: income [4] #>                         N   Perc #> High income          4819  36.57 #> Low income           1830  13.89 #> Lower middle income  2867  21.76 #> Upper middle income  3660  27.78 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57     79 #> Low income           1830  13.89     30 #> Lower middle income  2867  21.76     47 #> Upper middle income  3660  27.78     60 #>  #> Table (Freq Perc) #>                      High income  Low income  Lower middle income #> Afghanistan              0  0.00    61  3.33              0  0.00 #> Albania                  0  0.00     0  0.00              0  0.00 #> Algeria                  0  0.00     0  0.00              0  0.00 #> American Samoa           0  0.00     0  0.00              0  0.00 #> Andorra                 61  1.27     0  0.00              0  0.00 #> Angola                   0  0.00     0  0.00             61  2.13 #> Antigua and Barbuda     61  1.27     0  0.00              0  0.00 #> Argentina                0  0.00     0  0.00              0  0.00 #> Armenia                  0  0.00     0  0.00              0  0.00 #> Aruba                   61  1.27     0  0.00              0  0.00 #> Australia               61  1.27     0  0.00              0  0.00 #> Austria                 61  1.27     0  0.00              0  0.00 #> Azerbaijan               0  0.00     0  0.00              0  0.00 #> Bahamas, The            61  1.27     0  0.00              0  0.00 #>                      Upper middle income        Total #> Afghanistan                      0  0.00     61  0.46 #> Albania                         61  1.67     61  0.46 #> Algeria                         61  1.67     61  0.46 #> American Samoa                  61  1.67     61  0.46 #> Andorra                          0  0.00     61  0.46 #> Angola                           0  0.00     61  0.46 #> Antigua and Barbuda              0  0.00     61  0.46 #> Argentina                       61  1.67     61  0.46 #> Armenia                         61  1.67     61  0.46 #> Aruba                            0  0.00     61  0.46 #> Australia                        0  0.00     61  0.46 #> Austria                          0  0.00     61  0.46 #> Azerbaijan                      61  1.67     61  0.46 #> Bahamas, The                     0  0.00     61  0.46 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57     79 #> Low income           1830  13.89     30 #> Lower middle income  2867  21.76     47 #> Upper middle income  3660  27.78     60 #>  #> Table (Freq Perc) #>                 High income  Low income  Lower middle income #> ABW                61  1.27     0  0.00              0  0.00 #> AFG                 0  0.00    61  3.33              0  0.00 #> AGO                 0  0.00     0  0.00             61  2.13 #> ALB                 0  0.00     0  0.00              0  0.00 #> AND                61  1.27     0  0.00              0  0.00 #> ARE                61  1.27     0  0.00              0  0.00 #> ARG                 0  0.00     0  0.00              0  0.00 #> ARM                 0  0.00     0  0.00              0  0.00 #> ASM                 0  0.00     0  0.00              0  0.00 #> ATG                61  1.27     0  0.00              0  0.00 #> AUS                61  1.27     0  0.00              0  0.00 #> AUT                61  1.27     0  0.00              0  0.00 #> AZE                 0  0.00     0  0.00              0  0.00 #> BDI                 0  0.00    61  3.33              0  0.00 #>                 Upper middle income        Total #> ABW                         0  0.00     61  0.46 #> AFG                         0  0.00     61  0.46 #> AGO                         0  0.00     61  0.46 #> ALB                        61  1.67     61  0.46 #> AND                         0  0.00     61  0.46 #> ARE                         0  0.00     61  0.46 #> ARG                        61  1.67     61  0.46 #> ARM                        61  1.67     61  0.46 #> ASM                        61  1.67     61  0.46 #> ATG                         0  0.00     61  0.46 #> AUS                         0  0.00     61  0.46 #> AUT                         0  0.00     61  0.46 #> AZE                        61  1.67     61  0.46 #> BDI                         0  0.00     61  0.46 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics (N = 13176) #>                         N   Perc  Ndist         Min         Max #> High income          4819  36.57     61  1961-01-01  2021-01-01 #> Low income           1830  13.89     61  1961-01-01  2021-01-01 #> Lower middle income  2867  21.76     61  1961-01-01  2021-01-01 #> Upper middle income  3660  27.78     61  1961-01-01  2021-01-01 #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics (N = 13176) #>                         N   Perc  Ndist  Mean     SD   Min   Max  Skew  Kurt #> High income          4819  36.57     61  1990  17.61  1960  2020     0   1.8 #> Low income           1830  13.89     61  1990  17.61  1960  2020    -0   1.8 #> Lower middle income  2867  21.76     61  1990  17.61  1960  2020    -0   1.8 #>                        1%    5%   10%   25%   50%   75%   90%   95%   99% #> High income          1960  1963  1966  1975  1990  2005  2014  2017  2020 #> Low income           1960  1963  1966  1975  1990  2005  2014  2017  2020 #> Lower middle income  1960  1963  1966  1975  1990  2005  2014  2017  2020 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics (N = 13176) #>                         N   Perc  Ndist     Mean     SD   Min   Max  Skew  Kurt #> High income          4819  36.57      7  1985.57  17.51  1960  2020  0.03  1.79 #> Low income           1830  13.89      7  1985.57  17.52  1960  2020  0.03  1.79 #> Lower middle income  2867  21.76      7  1985.57  17.51  1960  2020  0.03  1.79 #>                        1%    5%   10%   25%   50%   75%   90%   95%   99% #> High income          1960  1960  1960  1970  1990  2000  2010  2010  2020 #> Low income           1960  1960  1960  1970  1990  2000  2010  2010  2020 #> Lower middle income  1960  1960  1960  1970  1990  2000  2010  2010  2020 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57      6 #> Low income           1830  13.89      5 #> Lower middle income  2867  21.76      6 #> Upper middle income  3660  27.78      6 #>  #> Table (Freq Perc) #>                             High income  Low income  Lower middle income #> Europe & Central Asia         2257 46.8     61  3.3             244  8.5 #> Sub-Saharan Africa              61  1.3   1464 80.0            1037 36.2 #> Latin America & Caribbean     1037 21.5     61  3.3             244  8.5 #> East Asia & Pacific            793 16.5      0  0.0             793 27.7 #> Middle East & North Africa     488 10.1    122  6.7             305 10.6 #> South Asia                       0  0.0    122  6.7             244  8.5 #> North America                  183  3.8      0  0.0               0  0.0 #>                             Upper middle income      Total #> Europe & Central Asia                  976 26.7  3538 26.9 #> Sub-Saharan Africa                     366 10.0  2928 22.2 #> Latin America & Caribbean             1220 33.3  2562 19.4 #> East Asia & Pacific                    610 16.7  2196 16.7 #> Middle East & North Africa             366 10.0  1281  9.7 #> South Asia                             122  3.3   488  3.7 #> North America                            0  0.0   183  1.4 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics (N = 13176) #>                         N   Perc  Ndist #> High income          4819  36.57      2 #> Low income           1830  13.89      1 #> Lower middle income  2867  21.76      1 #> Upper middle income  3660  27.78      2 #>  #> Table (Freq Perc) #>        High income  Low income  Lower middle income  Upper middle income #> FALSE   2745  57.0  1830 100.0           2867 100.0           3538  96.7 #> TRUE    2074  43.0     0   0.0              0   0.0            122   3.3 #>              Total #> FALSE  10980  83.3 #> TRUE    2196  16.7 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (N = 9470, 28.13% NAs) #>                         N   Perc  Ndist      Mean        SD     Min        Max #> High income          3179  33.57   3179  30280.73  23847.05  932.04  196061.42 #> Low income           1311  13.84   1311    597.41    288.44  164.34    1864.79 #> Lower middle income  2246  23.72   2246   1574.25    858.72  144.99    4818.19 #>                      Skew   Kurt       1%       5%      10%       25%       50% #> High income          2.17  10.34  3053.83  5395.18  7768.74  14369.61  24745.65 #> Low income           1.24   4.71   191.73   234.77   289.48    396.52    535.96 #> Lower middle income  0.91   3.72   194.43   398.88   585.24    961.12   1437.78 #>                           75%      90%       95%        99% #> High income          38936.22    57259   75529.1  116493.28 #> Low income             745.29   985.45   1180.37    1513.83 #> Lower middle income   1987.89  2829.09   3192.75     4191.8 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (N = 11670, 11.43% NAs) #>                         N   Perc  Ndist   Mean    SD    Min    Max   Skew  Kurt #> High income          3831  32.83   3566  73.62  5.67  42.67  85.42  -1.01  5.56 #> Low income           1800  15.42   1751  49.73  9.09  26.17  74.43   0.27  2.67 #> Lower middle income  2790  23.91   2694  58.15  9.31  18.91   76.7  -0.34  2.68 #>                         1%     5%    10%    25%    50%    75%    90%    95% #> High income          55.62  63.95  67.12   70.5  73.93  77.61  80.67  81.79 #> Low income            31.5  35.78   38.2  43.52  49.04  56.06  61.98  65.87 #> Lower middle income  36.62  42.66  45.73   51.5  58.53  65.79     70  71.72 #>                        99% #> High income          83.22 #> Low income           71.71 #> Lower middle income  74.91 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (N = 1744, 86.76% NAs) #>                        N   Perc  Ndist   Mean    SD   Min   Max  Skew  Kurt #> High income          680  38.99    213   33.3  6.79  20.7  58.9  1.49  5.68 #> Low income           107   6.14     88  41.13  6.58  29.5  65.8  0.75  4.24 #> Lower middle income  369  21.16    219  40.05   9.3    24  63.2  0.44  2.22 #>                         1%     5%    10%    25%    50%   75%    90%    95% #> High income          23.42   25.2  26.39  28.48  32.35  35.5  41.01  48.72 #> Low income           29.81  32.35  33.26  35.65   41.1  44.9  48.26  51.61 #> Lower middle income  24.77  26.84  28.88   32.7   38.7  46.6  54.52  56.94 #>                        99% #> High income          56.62 #> Low income           60.99 #> Lower middle income  59.82 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (N = 8608, 34.67% NAs) #>                         N   Perc  Ndist        Mean              SD #> High income          1575   18.3   1407  153'663194      425'918409 #> Low income           1692  19.66   1678  631'660165      941'498380 #> Lower middle income  2544  29.55   2503  692'072692  1.02452490e+09 #>                              Min             Max   Skew    Kurt            1% #> High income          -464'709991  4.34612988e+09   5.25   36.27  -54'802401.1 #> Low income               -500000  1.04032100e+10   4.46   32.13   1'100000.02 #> Lower middle income  -605'969971  1.18790801e+10   3.79   25.24     209999.99 #>                               5%          10%         25%          50% #> High income           -755999.99       264000  4'400000.1  21'209999.1 #> Low income           33'997999.8  71'296000.7  151'814999   332'904999 #> Lower middle income  14'721500.3  41'358000.2  100'485003   336'494995 #>                             75%             90%             95%             99% #> High income          104'934998      375'347992      661'426996  2.31632209e+09 #> Low income           692'777496  1.47914895e+09  2.14049348e+09  4.82899863e+09 #> Lower middle income  810'707520  1.84614302e+09  2.59226945e+09  4.69573516e+09 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> -------------------------------------------------------------------------------- #> POP (numeric): Population, total #> Statistics (N = 12919, 1.95% NAs) #>                         N   Perc  Ndist         Mean           SD     Min #> High income          4737  36.67   4712  12'421540.4  34'160829.5    2833 #> Low income           1792  13.87   1791  11'690380.2  13'942313.8  365047 #> Lower middle income  2790   21.6   2790  40'802037.5   137'302296   41202 #>                                 Max  Skew   Kurt         1%           5% #> High income              328'239523   5.5  39.75    7467.08      18432.4 #> Low income               112'078730  3.22  16.57  594755.89  1'206251.55 #> Lower middle income  1.36641775e+09   6.7   52.4   58452.97    105620.75 #>                             10%          25%         50%          75% #> High income               30517        84449    1'632114     8'336605 #> Low income           1'919035.6  3'842838.75    7'181772  13'579920.8 #> Lower middle income    224651.9     1'188469  5'914923.5  25'966431.8 #>                              90%          95%             99% #> High income          37'508393.4  58'933084.2      209'091400 #> Low income           25'964845.3  36'596246.7     76'539171.5 #> Lower middle income    81'157844   146'949299      893'256928 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #> --------------------------------------------------------------------------------  ## Grouped & Weighted Desciptions descr(wlddev, ~ income, w = ~ replace_na(POP)) #> Dataset: wlddev, 11 Variables, N = 13176, WeightSum = 313233706778 #> Grouped by: income [4] #>                         N   Perc       WeightSum   Perc #> High income          4819  36.57  5.88408371e+10  18.78 #> Low income           1830  13.89  2.09491614e+10   6.69 #> Lower middle income  2867  21.76  1.13837685e+11  36.34 #> Upper middle income  3660  27.78  1.19606024e+11  38.18 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics (WeightSum = 313233706778) #>                           WeightSum   Perc  Ndist #> High income          5.88408371e+10  18.78     79 #> Low income           2.09491614e+10   6.69     30 #> Lower middle income  1.13837685e+11  36.34     47 #> Upper middle income  1.19606024e+11  38.18     60 #>  #> Table (WeightSum Perc) #>                           High income         Low income  Lower middle income #> China                         0   0.0            0   0.0              0   0.0 #> India                         0   0.0            0   0.0    52835203044  46.4 #> United States       15226426293  25.9            0   0.0              0   0.0 #> Indonesia                     0   0.0            0   0.0    10681870259   9.4 #> Brazil                        0   0.0            0   0.0              0   0.0 #> Russian Federation            0   0.0            0   0.0              0   0.0 #> Japan                7088669911  12.0            0   0.0              0   0.0 #> Pakistan                      0   0.0            0   0.0     6865420747   6.0 #> Bangladesh                    0   0.0            0   0.0     6217567789   5.5 #> Nigeria                       0   0.0            0   0.0     6191168112   5.4 #> Mexico                        0   0.0            0   0.0              0   0.0 #> Germany              4773054666   8.1            0   0.0              0   0.0 #> Vietnam                       0   0.0            0   0.0     3955178878   3.5 #> Philippines                   0   0.0            0   0.0     3805345278   3.3 #>                     Upper middle income               Total #> China                 65272180000  54.6   65272180000  20.8 #> India                           0   0.0   52835203044  16.9 #> United States                   0   0.0   15226426293   4.9 #> Indonesia                       0   0.0   10681870259   3.4 #> Brazil                 8711884458   7.3    8711884458   2.8 #> Russian Federation     8388319293   7.0    8388319293   2.7 #> Japan                           0   0.0    7088669911   2.3 #> Pakistan                        0   0.0    6865420747   2.2 #> Bangladesh                      0   0.0    6217567789   2.0 #> Nigeria                         0   0.0    6191168112   2.0 #> Mexico                 4948012523   4.1    4948012523   1.6 #> Germany                         0   0.0    4773054666   1.5 #> Vietnam                         0   0.0    3955178878   1.3 #> Philippines                     0   0.0    3805345278   1.2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #>  #> Summary of Table WeightSums #>   High income          Low income        Lower middle income #>  Min.   :0.000e+00   Min.   :0.000e+00   Min.   :0.000e+00   #>  1st Qu.:0.000e+00   1st Qu.:0.000e+00   1st Qu.:0.000e+00   #>  Median :0.000e+00   Median :0.000e+00   Median :0.000e+00   #>  Mean   :2.724e+08   Mean   :9.699e+07   Mean   :5.270e+08   #>  3rd Qu.:1.083e+07   3rd Qu.:0.000e+00   3rd Qu.:0.000e+00   #>  Max.   :1.523e+10   Max.   :3.280e+09   Max.   :5.284e+10   #>  Upper middle income     Total           #>  Min.   :0.000e+00   Min.   :5.006e+05   #>  1st Qu.:0.000e+00   1st Qu.:3.067e+07   #>  Median :0.000e+00   Median :2.553e+08   #>  Mean   :5.537e+08   Mean   :1.450e+09   #>  3rd Qu.:5.643e+06   3rd Qu.:8.257e+08   #>  Max.   :6.527e+10   Max.   :6.527e+10   #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics (WeightSum = 313233706778) #>                           WeightSum   Perc  Ndist #> High income          5.88408371e+10  18.78     79 #> Low income           2.09491614e+10   6.69     30 #> Lower middle income  1.13837685e+11  36.34     47 #> Upper middle income  1.19606024e+11  38.18     60 #>  #> Table (WeightSum Perc) #>                       High income         Low income  Lower middle income #> CHN                       0   0.0            0   0.0              0   0.0 #> IND                       0   0.0            0   0.0    52835203044  46.4 #> USA             15226426293  25.9            0   0.0              0   0.0 #> IDN                       0   0.0            0   0.0    10681870259   9.4 #> BRA                       0   0.0            0   0.0              0   0.0 #> RUS                       0   0.0            0   0.0              0   0.0 #> JPN              7088669911  12.0            0   0.0              0   0.0 #> PAK                       0   0.0            0   0.0     6865420747   6.0 #> BGD                       0   0.0            0   0.0     6217567789   5.5 #> NGA                       0   0.0            0   0.0     6191168112   5.4 #> MEX                       0   0.0            0   0.0              0   0.0 #> DEU              4773054666   8.1            0   0.0              0   0.0 #> VNM                       0   0.0            0   0.0     3955178878   3.5 #> PHL                       0   0.0            0   0.0     3805345278   3.3 #>                 Upper middle income               Total #> CHN               65272180000  54.6   65272180000  20.8 #> IND                         0   0.0   52835203044  16.9 #> USA                         0   0.0   15226426293   4.9 #> IDN                         0   0.0   10681870259   3.4 #> BRA                8711884458   7.3    8711884458   2.8 #> RUS                8388319293   7.0    8388319293   2.7 #> JPN                         0   0.0    7088669911   2.3 #> PAK                         0   0.0    6865420747   2.2 #> BGD                         0   0.0    6217567789   2.0 #> NGA                         0   0.0    6191168112   2.0 #> MEX                4948012523   4.1    4948012523   1.6 #> DEU                         0   0.0    4773054666   1.5 #> VNM                         0   0.0    3955178878   1.3 #> PHL                         0   0.0    3805345278   1.2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] #>  #> Summary of Table WeightSums #>   High income          Low income        Lower middle income #>  Min.   :0.000e+00   Min.   :0.000e+00   Min.   :0.000e+00   #>  1st Qu.:0.000e+00   1st Qu.:0.000e+00   1st Qu.:0.000e+00   #>  Median :0.000e+00   Median :0.000e+00   Median :0.000e+00   #>  Mean   :2.724e+08   Mean   :9.699e+07   Mean   :5.270e+08   #>  3rd Qu.:1.083e+07   3rd Qu.:0.000e+00   3rd Qu.:0.000e+00   #>  Max.   :1.523e+10   Max.   :3.280e+09   Max.   :5.284e+10   #>  Upper middle income     Total           #>  Min.   :0.000e+00   Min.   :5.006e+05   #>  1st Qu.:0.000e+00   1st Qu.:3.067e+07   #>  Median :0.000e+00   Median :2.553e+08   #>  Mean   :5.537e+08   Mean   :1.450e+09   #>  3rd Qu.:5.643e+06   3rd Qu.:8.257e+08   #>  Max.   :6.527e+10   Max.   :6.527e+10   #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics (N = 13176) #>                         N   Perc  Ndist         Min         Max #> High income          4819  36.57     61  1961-01-01  2021-01-01 #> Low income           1830  13.89     61  1961-01-01  2021-01-01 #> Lower middle income  2867  21.76     61  1961-01-01  2021-01-01 #> Upper middle income  3660  27.78     61  1961-01-01  2021-01-01 #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics (N = 12919, 1.95% NAs) #>                         N  Ndist       WeightSum   Perc     Mean     SD   Min #> High income          4737     61  5.88408371e+10  18.78  1991.75  17.15  1960 #> Low income           1792     61  2.09491614e+10   6.69  1997.27  16.31  1960 #> Lower middle income  2790     61  1.13837685e+11  36.34  1995.41  16.47  1960 #> Upper middle income  3600     61  1.19606024e+11  38.18  1993.44  16.69  1960 #>                       Max   Skew  Kurt #> High income          2019  -0.15  1.84 #> Low income           2019  -0.56  2.23 #> Lower middle income  2019  -0.42  2.08 #> Upper middle income  2019  -0.27  1.95 #>  #> Quantiles #>                        1%    5%   10%   25%   50%   75%   90%   95%   99% #> High income          1960  1963  1967  1977  1993  2007  2015  2017  2019 #> Low income           1961  1966  1972  1985  2001  2011  2016  2018  2019 #> Lower middle income  1961  1965  1970  1983  1998  2010  2016  2018  2019 #> Upper middle income  1961  1964  1969  1980  1995  2008  2015  2017  2019 #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics (N = 12919, 1.95% NAs) #>                         N  Ndist       WeightSum   Perc     Mean     SD   Min #> High income          4737      7  5.88408371e+10  18.78  1987.19  16.92  1960 #> Low income           1792      7  2.09491614e+10   6.69  1992.55  16.05  1960 #> Lower middle income  2790      7  1.13837685e+11  36.34  1990.76  16.25  1960 #> Upper middle income  3600      7  1.19606024e+11  38.18  1988.84  16.48  1960 #>                       Max   Skew  Kurt #> High income          2010  -0.16  1.77 #> Low income           2010  -0.59  2.18 #> Lower middle income  2010  -0.44  2.02 #> Upper middle income  2010  -0.28  1.88 #>  #> Quantiles #>                        1%    5%   10%   25%   50%   75%   90%   95%   99% #> High income          1960  1960  1960  1970  1990  2000  2010  2010  2010 #> Low income           1960  1960  1970  1980  2000  2010  2010  2010  2010 #> Lower middle income  1960  1960  1970  1980  1990  2010  2010  2010  2010 #> Upper middle income  1960  1960  1960  1980  1990  2000  2010  2010  2010 #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics (WeightSum = 313233706778) #>                           WeightSum   Perc  Ndist #> High income          5.88408371e+10  18.78      6 #> Low income           2.09491614e+10   6.69      5 #> Lower middle income  1.13837685e+11  36.34      6 #> Upper middle income  1.19606024e+11  38.18      6 #>  #> Table (WeightSum Perc) #>                                     High income           Low income #> East Asia & Pacific         11407808149 19.3876            0  0.0000 #> South Asia                            0  0.0000   2252259680 10.7511 #> Europe & Central Asia       27285316560 46.3714    311485944  1.4869 #> Sub-Saharan Africa              4222055  0.0072  16384603068 78.2113 #> Latin America & Caribbean    1466292826  2.4920    429756890  2.0514 #> North America               16881058226 28.6894            0  0.0000 #> Middle East & North Africa   1796139242  3.0525   1571055812  7.4994 #>                             Lower middle income  Upper middle income #> East Asia & Pacific         22174820629 19.4793  69639871478 58.2244 #> South Asia                  65947845945 57.9315   1006497506  0.8415 #> Europe & Central Asia        4511786205  3.9634  16972478305 14.1903 #> Sub-Saharan Africa          14399976836 12.6496   2519611135  2.1066 #> Latin America & Caribbean    1290800630  1.1339  22949003780 19.1872 #> North America                         0  0.0000            0  0.0000 #> Middle East & North Africa   5512454283  4.8424   6518561594  5.4500 #>                                            Total #> East Asia & Pacific         103222500256 32.9538 #> South Asia                   69206603131 22.0942 #> Europe & Central Asia        49081067014 15.6692 #> Sub-Saharan Africa           33308413094 10.6337 #> Latin America & Caribbean    26135854126  8.3439 #> North America                16881058226  5.3893 #> Middle East & North Africa   15398210931  4.9159 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics (WeightSum = 313233706778) #>                           WeightSum   Perc  Ndist #> High income          5.88408371e+10  18.78      2 #> Low income           2.09491614e+10   6.69      1 #> Lower middle income  1.13837685e+11  36.34      1 #> Upper middle income  1.19606024e+11  38.18      2 #>  #> Table (WeightSum Perc) #>              High income         Low income  Lower middle income #> FALSE   3113623360   5.3  20949161394 100.0   113837684528 100.0 #> TRUE   55727213698  94.7            0   0.0              0   0.0 #>        Upper middle income               Total #> FALSE   111444004553  93.2  249344473835  79.6 #> TRUE      8162019245   6.8   63889232943  20.4 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (N = 9470, 28.13% NAs) #>                         N  Ndist       WeightSum   Perc      Mean       SD #> High income          3179   3179  5.55288564e+10  18.79  31284.74  13807.6 #> Low income           1311   1311  1.69031453e+10   5.72    557.14   279.41 #> Lower middle income  2246   2246  1.10267107e+11  37.32   1238.83   823.89 #> Upper middle income  2734   2734  1.12746722e+11  38.16   4145.68  3515.97 #>                         Min        Max  Skew  Kurt #> High income          932.04  196061.42   0.2  3.21 #> Low income           164.34    1864.79  1.07  4.08 #> Lower middle income  144.99    4818.19  1.25  4.57 #> Upper middle income  132.08   20532.95  0.66  2.52 #>  #> Quantiles #>                           1%      5%      10%       25%       50%       75% #> High income          3268.18  8837.9  13369.8  20951.55  31079.21  41791.15 #> Low income            179.91     205   234.62    356.02    495.11    710.27 #> Lower middle income   236.75  365.72   396.21    580.69   1039.84   1665.26 #> Upper middle income    141.8  196.78   260.78    781.64   3543.37   6803.24 #>                           90%       95%       99% #> High income          48866.19  52164.71   60264.2 #> Low income              945.3   1128.55   1396.37 #> Lower middle income   2337.98   2885.88   3973.91 #> Upper middle income   9014.51  10737.23  12916.96 #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (N = 11659, 11.51% NAs) #>                         N  Ndist       WeightSum   Perc   Mean    SD    Min #> High income          3828   3566  5.87959699e+10  18.79  75.69  4.53  42.67 #> Low income           1792   1751  2.09491614e+10    6.7  53.51  8.87  26.17 #> Lower middle income  2790   2694  1.13837685e+11  36.38  60.59  8.36  18.91 #> Upper middle income  3249   3083  1.19295269e+11  38.13  68.27  7.19  36.53 #>                        Max   Skew  Kurt #> High income          85.42  -0.73  4.94 #> Low income           74.43  -0.01  2.47 #> Lower middle income   76.7  -0.56  2.52 #> Upper middle income  80.28  -1.42  4.95 #>  #> Quantiles #>                         1%     5%    10%    25%    50%    75%    90%    95% #> High income          62.63  69.38  70.21   72.5  76.03  78.74  81.44  82.57 #> Low income           33.82  39.38  42.65  46.95     53  60.36  65.27   67.1 #> Lower middle income  41.43  45.37  47.75  54.69   62.3  67.54  69.77  71.27 #> Upper middle income  44.11  51.87   58.2  65.86   69.5  73.51  75.62  76.45 #>                        99% #> High income          83.79 #> Low income           72.68 #> Lower middle income  74.91 #> Upper middle income  76.91 #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (N = 1744, 86.76% NAs) #>                        N  Ndist       WeightSum   Perc   Mean    SD   Min   Max #> High income          680    213  2.07396836e+10  25.11  36.03  4.93  20.7  58.9 #> Low income           107     88  1.90256783e+09    2.3  39.76  5.99  29.5  65.8 #> Lower middle income  369    219  2.16883977e+10  26.26  35.16  5.52    24  63.2 #> Upper middle income  588    280  3.82704279e+10  46.33  43.88  7.52  25.2  64.8 #>                      Skew  Kurt #> High income          0.18  3.61 #> Low income           0.58  4.23 #> Lower middle income  1.44  6.24 #> Upper middle income  0.68  2.86 #>  #> Quantiles #>                         1%     5%    10%    25%    50%    75%    90%    95% #> High income          25.38  28.18  29.87   32.3  35.35  40.47   41.1   41.4 #> Low income           29.83  30.14  32.84     35  40.28  43.55  46.13   48.5 #> Lower middle income  24.95  28.63   29.8  31.77   34.4     37   41.6   46.5 #> Upper middle income  28.35   34.4   36.6   38.7     42   48.7   55.6  58.93 #>                        99% #> High income          47.26 #> Low income           53.98 #> Lower middle income   55.5 #> Upper middle income  61.32 #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (N = 8597, 34.75% NAs) #>                         N  Ndist       WeightSum   Perc            Mean #> High income          1572   1407  5.60343429e+09   2.42      469'519277 #> Low income           1684   1678  2.05403995e+10   8.87  1.27976069e+09 #> Lower middle income  2544   2503  1.11067918e+11  47.99  2.23495473e+09 #> Upper middle income  2797   2700  9.42398516e+10  40.72  1.02122309e+09 #>                                  SD          Min             Max  Skew   Kurt #> High income              823'186883  -464'709991  4.34612988e+09  2.18    8.3 #> Low income           1.36723776e+09      -500000  1.04032100e+10  2.17   9.67 #> Lower middle income  1.71188944e+09  -605'969971  1.18790801e+10  1.48   6.42 #> Upper middle income  1.31969536e+09  -997'679993  2.56715605e+10  2.58  38.51 #>  #> Quantiles #>                                1%           5%          10%          25% #> High income           -211'221010  -111'039383   -51'563267  12'609999.7 #> Low income            18'581027.3   102'699417   170'565775   340'775627 #> Lower middle income  -1'186156.61   169'613235   351'778004   955'472807 #> Upper middle income   -951'313387  -674'066685  -464'822389   127'570913 #>                                 50%             75%             90% #> High income             73'609429.8      636'421870  1.67398341e+09 #> Low income               792'634263  1.73644025e+09  3.15899287e+09 #> Lower middle income  2.02836117e+09  2.90852841e+09  4.33204816e+09 #> Upper middle income      653'751768  1.89074430e+09  2.85101616e+09 #>                                 95%             99% #> High income          1.97758747e+09  4.03283599e+09 #> Low income           4.28987656e+09  6.46379868e+09 #> Lower middle income  5.37706240e+09  8.56557908e+09 #> Upper middle income  3.48410000e+09  3.84146040e+09 #> --------------------------------------------------------------------------------  ## Passing Arguments down to qsu.default: for Panel Data Statistics descr(iris, pid = iris$Species) #> Dataset: iris, 5 Variables, N = 150 #> -------------------------------------------------------------------------------- #> Sepal.Length (numeric):  #> Statistics #>          N/T  Mean    SD   Min   Max   Skew  Kurt #> Overall  150  5.84  0.83   4.3   7.9   0.31  2.43 #> Between    3  5.84   0.8  5.01  6.59  -0.21   1.5 #> Within    50  5.84  0.51  4.16  7.16   0.12  3.26 #>  #> Quantiles #>    1%   5%  10%  25%  50%  75%  90%   95%  99% #>   4.4  4.6  4.8  5.1  5.8  6.4  6.9  7.25  7.7 #> -------------------------------------------------------------------------------- #> Sepal.Width (numeric):  #> Statistics #>          N/T  Mean    SD   Min   Max  Skew  Kurt #> Overall  150  3.06  0.44     2   4.4  0.32  3.18 #> Between    3  3.06  0.34  2.77  3.43  0.43   1.5 #> Within    50  3.06  0.34  1.93  4.03  0.03  3.51 #>  #> Quantiles #>    1%    5%  10%  25%  50%  75%   90%  95%   99% #>   2.2  2.34  2.5  2.8    3  3.3  3.61  3.8  4.15 #> -------------------------------------------------------------------------------- #> Petal.Length (numeric):  #> Statistics #>          N/T  Mean    SD   Min   Max   Skew  Kurt #> Overall  150  3.76  1.77     1   6.9  -0.27   1.6 #> Between    3  3.76  2.09  1.46  5.55  -0.42   1.5 #> Within    50  3.76  0.43   2.5  5.11   0.12  3.89 #>  #> Quantiles #>     1%   5%  10%  25%   50%  75%  90%  95%  99% #>   1.15  1.3  1.4  1.6  4.35  5.1  5.8  6.1  6.7 #> -------------------------------------------------------------------------------- #> Petal.Width (numeric):  #> Statistics #>          N/T  Mean    SD   Min   Max   Skew  Kurt #> Overall  150   1.2  0.76   0.1   2.5   -0.1  1.66 #> Between    3   1.2   0.9  0.25  2.03  -0.25   1.5 #> Within    50   1.2   0.2  0.57  1.67  -0.05  3.36 #>  #> Quantiles #>    1%   5%  10%  25%  50%  75%  90%  95%  99% #>   0.1  0.2  0.2  0.3  1.3  1.8  2.2  2.3  2.5 #> -------------------------------------------------------------------------------- #> Species (factor):  #> Statistics #>     N  Ndist #>   150      3 #>  #> Table #>             Freq   Perc #> setosa        50  33.33 #> versicolor    50  33.33 #> virginica     50  33.33 #> -------------------------------------------------------------------------------- descr(wlddev, pid = wlddev$iso3c) #> Dataset: wlddev, 13 Variables, N = 13176 #> -------------------------------------------------------------------------------- #> country (character): Country Name #> Statistics #>       N  Ndist #>   13176    216 #>  #> Table #>                       Freq   Perc #> Afghanistan             61   0.46 #> Albania                 61   0.46 #> Algeria                 61   0.46 #> American Samoa          61   0.46 #> Andorra                 61   0.46 #> Angola                  61   0.46 #> Antigua and Barbuda     61   0.46 #> Argentina               61   0.46 #> Armenia                 61   0.46 #> Aruba                   61   0.46 #> Australia               61   0.46 #> Austria                 61   0.46 #> Azerbaijan              61   0.46 #> Bahamas, The            61   0.46 #> ... 202 Others       12322  93.52 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      61      61      61      61      61      61  #> -------------------------------------------------------------------------------- #> iso3c (factor): Country Code #> Statistics #>       N  Ndist #>   13176    216 #>  #> Table #>                  Freq   Perc #> ABW                61   0.46 #> AFG                61   0.46 #> AGO                61   0.46 #> ALB                61   0.46 #> AND                61   0.46 #> ARE                61   0.46 #> ARG                61   0.46 #> ARM                61   0.46 #> ASM                61   0.46 #> ATG                61   0.46 #> AUS                61   0.46 #> AUT                61   0.46 #> AZE                61   0.46 #> BDI                61   0.46 #> ... 202 Others  12322  93.52 #>  #> Summary of Table Frequencies #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>      61      61      61      61      61      61  #> -------------------------------------------------------------------------------- #> date (Date): Date Recorded (Fictitious) #> Statistics #>          N       Ndist         Min         Max   #>      13176          61  1961-01-01  2021-01-01   #> -------------------------------------------------------------------------------- #> year (integer): Year #> Statistics #>            N/T  Mean     SD   Min   Max  Skew  Kurt #> Overall  13176  1990  17.61  1960  2020    -0   1.8 #> Between    216  1990      0  1990  1990     -     - #> Within      61  1990  17.61  1960  2020    -0   1.8 #>  #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1963  1966  1975  1990  2005  2014  2017  2020 #> -------------------------------------------------------------------------------- #> decade (integer): Decade #> Statistics #>            N/T     Mean     SD      Min      Max  Skew  Kurt #> Overall  13176  1985.57  17.51     1960     2020  0.03  1.79 #> Between    216  1985.57      0  1985.57  1985.57     -     - #> Within      61  1985.57  17.51     1960     2020  0.03  1.79 #>  #> Quantiles #>     1%    5%   10%   25%   50%   75%   90%   95%   99% #>   1960  1960  1960  1970  1990  2000  2010  2010  2020 #> -------------------------------------------------------------------------------- #> region (factor): Region #> Statistics #>       N  Ndist #>   13176      7 #>  #> Table #>                             Freq   Perc #> Europe & Central Asia       3538  26.85 #> Sub-Saharan Africa          2928  22.22 #> Latin America & Caribbean   2562  19.44 #> East Asia & Pacific         2196  16.67 #> Middle East & North Africa  1281   9.72 #> South Asia                   488   3.70 #> North America                183   1.39 #> -------------------------------------------------------------------------------- #> income (factor): Income Level #> Statistics #>       N  Ndist #>   13176      4 #>  #> Table #>                      Freq   Perc #> High income          4819  36.57 #> Upper middle income  3660  27.78 #> Lower middle income  2867  21.76 #> Low income           1830  13.89 #> -------------------------------------------------------------------------------- #> OECD (logical): Is OECD Member Country? #> Statistics #>       N  Ndist #>   13176      2 #>  #> Table #>         Freq   Perc #> FALSE  10980  83.33 #> TRUE    2196  16.67 #> -------------------------------------------------------------------------------- #> PCGDP (numeric): GDP per capita (constant 2010 US$) #> Statistics (28.13% NAs) #>            N/T      Mean        SD        Min        Max  Skew   Kurt #> Overall   9470  12048.78  19077.64     132.08  196061.42  3.13  17.12 #> Between    206  12962.61   20189.9     253.19  141200.38  3.13  16.23 #> Within   45.97  12048.78   6723.68  -33504.87   76767.53  0.66   17.2 #>  #> Quantiles #>       1%      5%     10%      25%      50%       75%       90%       95% #>   227.71  399.62  555.55  1303.19  3767.16  14787.03  35646.02  48507.84 #>        99% #>   92340.28 #> -------------------------------------------------------------------------------- #> LIFEEX (numeric): Life expectancy at birth, total (years) #> Statistics (11.43% NAs) #>            N/T   Mean     SD    Min    Max   Skew  Kurt #> Overall  11670   64.3  11.48  18.91  85.42  -0.67  2.67 #> Between    207  64.95   9.89  40.97  85.42   -0.5  2.17 #> Within   56.38   64.3   6.08  32.91  84.42  -0.26   3.7 #>  #> Quantiles #>      1%     5%    10%    25%    50%    75%    90%    95%    99% #>   35.83  42.77  46.83  56.36  67.44  72.95  77.08  79.34  82.36 #> -------------------------------------------------------------------------------- #> GINI (numeric): Gini index (World Bank estimate) #> Statistics (86.76% NAs) #>            N/T   Mean    SD    Min    Max  Skew  Kurt #> Overall   1744  38.53   9.2   20.7   65.8   0.6  2.53 #> Between    167  39.42  8.14  24.87  61.71  0.58  2.83 #> Within   10.44  38.53  2.93  25.39  55.36  0.33  5.34 #>  #> Quantiles #>     1%    5%   10%   25%   50%  75%   90%    95%   99% #>   24.6  26.3  27.6  31.5  36.4   45  52.6  55.98  60.5 #> -------------------------------------------------------------------------------- #> ODA (numeric): Net official development assistance and official aid received (constant 2018 US$) #> Statistics (34.67% NAs) #>            N/T        Mean          SD              Min             Max  Skew #> Overall   8608  454'720131  868'712654      -997'679993  2.56715605e+10  6.98 #> Between    178  439'168412  569'049959        468717.92  3.62337432e+09  2.36 #> Within   48.36  454'720131  650'709624  -2.44379420e+09  2.45610972e+10   9.6 #>            Kurt #> Overall  114.89 #> Between    9.95 #> Within   263.37 #>  #> Quantiles #>             1%           5%          10%          25%         50%         75% #>   -12'593999.7  1'363500.01  8'347000.31  44'887499.8  165'970001  495'042503 #>              90%             95%             99% #>   1.18400697e+09  1.93281696e+09  3.73380782e+09 #> -------------------------------------------------------------------------------- #> POP (numeric): Population, total #> Statistics (1.95% NAs) #>            N/T         Mean           SD          Min             Max   Skew #> Overall  12919  24'245971.6   102'120674         2833  1.39771500e+09   9.75 #> Between    216    24'178573  98'616506.7      8343.33  1.08786967e+09      9 #> Within   59.81  24'245971.6  26'803077.4  -405'793067      510'077008  -0.41 #>            Kurt #> Overall  108.91 #> Between   90.02 #> Within   149.24 #>  #> Quantiles #>        1%       5%      10%     25%       50%        75%          90% #>   8698.84  31083.3  62268.4  443791  4'072517  12'816178  46'637331.4 #>           95%         99% #>   81'177252.5  308'862641 #> --------------------------------------------------------------------------------"},{"path":"https://sebkrantz.github.io/collapse/reference/efficient-programming.html","id":null,"dir":"Reference","previous_headings":"","what":"Small Functions to Make R Programming More Efficient — efficient-programming","title":"Small Functions to Make R Programming More Efficient — efficient-programming","text":"small set functions address common inefficiencies R, creation logical vectors compare quantities, unnecessary copies objects elementary mathematical sub-assignment operations, obtaining information objects (esp. data frames), dealing missing values.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/efficient-programming.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Small Functions to Make R Programming More Efficient — efficient-programming","text":"","code":"anyv(x, value)              # Faster than any(x == value). See also kit::panyv() allv(x, value)              # Faster than all(x == value). See also kit::pallv() allNA(x)                    # Faster than all(is.na(x)). See also kit::pallNA() whichv(x, value,            # Faster than which(x == value)        invert = FALSE)      # or which(x != value). See also Note (3) whichNA(x, invert = FALSE)  # Faster than which((!)is.na(x)) x %==% value                # Infix for whichv(v, value, FALSE), use e.g. in fsubset() x %!=% value                # Infix for whichv(v, value, TRUE). See also Note (3) alloc(value, n,             # Fast rep_len(value, n) or replicate(n, value).       simplify = TRUE)      # simplify only works if length(value) == 1. See Details. copyv(X, v, R, ..., invert  # Fast replace(X, v, R), replace(X, X (!/=)= v, R) or     = FALSE, vind1 = FALSE, # replace(X, (!)v, R[(!)v]). See Details and Note (4).     xlist = FALSE)          # For multi-replacement see also kit::vswitch() setv(X, v, R, ..., invert   # Same for X[v] <- r, X[x (!/=)= v] <- r or     = FALSE, vind1 = FALSE, # x[(!)v] <- r[(!)v]. Modifies X by reference, fastest.     xlist = FALSE)          # X/R/V can also be lists/DFs. See Details and Examples. setop(X, op, V, ...,        # Faster than X <- X +\\-\\*\\/ V (modifies by reference)       rowwise = FALSE)      # optionally can also add v to rows of a matrix or list X %+=% V                    # Infix for setop(X, \"+\", V). See also Note (2) X %-=% V                    # Infix for setop(X, \"-\", V). See also Note (2) X %*=% V                    # Infix for setop(X, \"*\", V). See also Note (2) X %/=% V                    # Infix for setop(X, \"/\", V). See also Note (2) na_rm(x)                    # Fast: if(anyNA(x)) x[!is.na(x)] else x, last na_locf(x, set = FALSE)     # obs. carried forward and first obs. carried back. na_focb(x, set = FALSE)     # (by reference). These also support lists (NULL/empty) na_omit(X, cols = NULL,     # Faster na.omit for matrices and data frames,         na.attr = FALSE,    # can use selected columns to check, attach indices,         prop = 0, ...)      # and remove cases with a proportion of values missing na_insert(X, prop = 0.1,    # Insert missing values at random (by reference)     value = NA, set = FALSE) missing_cases(X, cols=NULL, # The opposite of complete.cases(), faster for DF's.   prop = 0, count = FALSE)  # See also kit::panyNA(), kit::pallNA(), kit::pcountNA() vlengths(X, use.names=TRUE) # Faster lengths() and nchar() (in C, no method dispatch) vtypes(X, use.names = TRUE) # Get data storage types (faster vapply(X, typeof, ...)) vgcd(x)                     # Greatest common divisor of positive integers or doubles fnlevels(x)                 # Faster version of nlevels(x) (for factors) fnrow(X)                    # Faster nrow for data frames (not faster for matrices) fncol(X)                    # Faster ncol for data frames (not faster for matrices) fdim(X)                     # Faster dim for data frames (not faster for matrices) seq_row(X)                  # Fast integer sequences along rows of X seq_col(X)                  # Fast integer sequences along columns of X vec(X)                      # Vectorization (stacking) of matrix or data frame/list cinv(x)                     # Choleski (fast) inverse of symmetric PD matrix, e.g. X'X"},{"path":"https://sebkrantz.github.io/collapse/reference/efficient-programming.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Small Functions to Make R Programming More Efficient — efficient-programming","text":"X, V, R vector, matrix data frame. x, v (atomic) vector matrix (na_rm/locf/focb also support lists). value single value (atomic) vector type. whichv can also length(x) vector. invert logical. TRUE considers elements x != value. set logical. TRUE transforms x reference. simplify logical. value length-1 vector, alloc() simplify = TRUE returns length-n vector type. simplify = FALSE, result always list. vind1 logical. length(v) == 1L, setting vind1 = TRUE interpret v index, rather value search replace. xlist logical. X list, default treat like data frame replace rows. Setting xlist = TRUE treat X replacement R like 1-dimensional list vectors. op integer character string indicating operation perform.  rowwise logical. TRUE performs operation V row X. cols select columns check missing values using column names, indices, logical vector function (e.g. .numeric). default check columns, inefficient. n integer. length vector allocate value. na.attr logical. TRUE adds attribute containing removed cases. compatibility reasons exactly format na.omit .e. attribute called \"na.action\" class \"omit\". prop double. na_insert: proportion observations randomly replaced NA. missing_cases na_omit: proportion values missing case considered missing (within cols specified). matrices implemented R rowSums(.na(X)) >= max(.integer(prop * ncol(X)), 1L). C code data frames works equivalently, skips list- raw-columns (ncol(X) adjusted downwards). count logical. TRUE returns row-wise missing value count (within cols). ignores prop. use.names logical. Preserve names X list. ... na_omit: arguments passed [ vectors matrices. indexed data also possible specify drop.index.levels argument, see indexing. copyv, setv setop, argument unused, serves placeholder possible future arguments.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/efficient-programming.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Small Functions to Make R Programming More Efficient — efficient-programming","text":"alloc fusion rep_len replicate faster cases. value length one vector simplify = TRUE, functionality rep_len(value, n) .e. output length n vector value. Otherwise, equivalent replicate(n, value, simplify = FALSE), .e., output length-n list objects. efficiency reasons object copied (pointer object replicated). copyv setv designed optimize operations require replacing data objects broadest sense. difference copyv first deep-copies X replacements whereas setv modifies X place returns result invisibly. 3 ways functions can used: replace single value, setv(X, v, R) efficient alternative X[X == v] <- R,  copyv(X, v, R) efficient replace(X, X == v, R). can inverted using setv(X, v, R, invert = TRUE), equivalent X[X != v] <- R. standard replacement integer logical indices .e. X[v] <- R efficient using setv(X, v, R), , v logical, setv(X, v, R, invert = TRUE) efficient X[!v] <- R. distinguish use case (1) length(v) == 1, argument vind1 = TRUE can set ensure v always interpreted index. copy values objects equal size .e. setv(X, v, R) faster X[v] <- R[v], setv(X, v, R, invert = TRUE) faster X[!v] <- R[!v]. X R can atomic data frames / lists. X list, default behavior interpret like data frame, apply setv/copyv element/column X. R also list, done using mapply. Thus setv/copyv can also used replace elements rows data frames, copy rows equally sized frames. Note replacing subsets data frames set data.table provides convenient interface (also copy just want deep-copy object without modifications ). X interpreted like data frame, setting xlist = TRUE interpret like 1D list-vector analogous atomic vectors, except use case (1) permitted .e. value comparisons list elements.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/efficient-programming.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Small Functions to Make R Programming More Efficient — efficient-programming","text":"None functions (apart alloc) currently support complex vectors. setop operators %+=%, %-=%, %*=% %/=% also work integer data, perform integer related checks. R's integers bounded +-2,147,483,647 NA_integer_ stored value -2,147,483,648. Thus computations resulting values exceeding +-2,147,483,647 result integer overflows, NA_integer_ occur either side setop call. programmers functions meant provide efficient math possible responsible users. possible compare factors levels (e.g. iris$Species %==% \"setosa\")) using integers (iris$Species %==% 1L). latter slightly efficient. Nothing special implemented objects apart basic types, e.g. dates (stored doubles) need generate date object .e. wlddev$date %==% .Date(\"2019-01-01\"). Using wlddev$date %==% \"2019-01-01\" give integer(0). setv/copyv allow positive integer indices passed v, , efficiency reasons, check first last index. Thus indices middle fall outside data range terminate R.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/efficient-programming.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Small Functions to Make R Programming More Efficient — efficient-programming","text":"","code":"oldopts <- options(max.print = 70) ## Which value whichNA(wlddev$PCGDP)                # Same as which(is.na(wlddev$PCGDP)) #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 #> [20]  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38 #> [39]  39  40  41  42  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75 #> [58]  76  77  78  79  80  81 122 183 184 185 186 187 188 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3636 entries ] whichNA(wlddev$PCGDP, invert = TRUE) # Same as which(!is.na(wlddev$PCGDP)) #>  [1]  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  82 #> [20]  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 #> [39] 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 #> [58] 121 123 124 125 126 127 128 129 130 131 132 133 134 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9400 entries ] whichv(wlddev$country, \"Chad\")       # Same as which(wlddev$county == \"Chad\") #>  [1] 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 #> [16] 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 #> [31] 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 #> [46] 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 #> [61] 2379 wlddev$country %==% \"Chad\"           # Same thing #>  [1] 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 #> [16] 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 #> [31] 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 #> [46] 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 #> [61] 2379 whichv(wlddev$country, \"Chad\", TRUE) # Same as which(wlddev$county != \"Chad\") #>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #> [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #> [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13045 entries ] wlddev$country %!=% \"Chad\"           # Same thing #>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #> [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #> [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13045 entries ] lvec <- wlddev$country == \"Chad\"     # If we already have a logical vector... whichv(lvec, FALSE)                  # is fastver than which(!lvec) #>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #> [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #> [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13045 entries ] rm(lvec)  # Using the %==% operator can yield tangible performance gains fsubset(wlddev, iso3c %==% \"DEU\") # 3x faster than: #>   country iso3c       date year decade                region      income OECD #> 1 Germany   DEU 1961-01-01 1960   1960 Europe & Central Asia High income TRUE #> 2 Germany   DEU 1962-01-01 1961   1960 Europe & Central Asia High income TRUE #> 3 Germany   DEU 1963-01-01 1962   1960 Europe & Central Asia High income TRUE #> 4 Germany   DEU 1964-01-01 1963   1960 Europe & Central Asia High income TRUE #> 5 Germany   DEU 1965-01-01 1964   1960 Europe & Central Asia High income TRUE #>   PCGDP   LIFEEX GINI ODA      POP #> 1    NA 69.31002   NA  NA 72814900 #> 2    NA 69.50800   NA  NA 73377632 #> 3    NA 69.69154   NA  NA 74025784 #> 4    NA 69.85961   NA  NA 74714353 #> 5    NA 70.01371   NA  NA 75318337 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 56 rows ] fsubset(wlddev, iso3c == \"DEU\") #>   country iso3c       date year decade                region      income OECD #> 1 Germany   DEU 1961-01-01 1960   1960 Europe & Central Asia High income TRUE #> 2 Germany   DEU 1962-01-01 1961   1960 Europe & Central Asia High income TRUE #> 3 Germany   DEU 1963-01-01 1962   1960 Europe & Central Asia High income TRUE #> 4 Germany   DEU 1964-01-01 1963   1960 Europe & Central Asia High income TRUE #> 5 Germany   DEU 1965-01-01 1964   1960 Europe & Central Asia High income TRUE #>   PCGDP   LIFEEX GINI ODA      POP #> 1    NA 69.31002   NA  NA 72814900 #> 2    NA 69.50800   NA  NA 73377632 #> 3    NA 69.69154   NA  NA 74025784 #> 4    NA 69.85961   NA  NA 74714353 #> 5    NA 70.01371   NA  NA 75318337 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 56 rows ]  # With multiple categories we can use %iin% fsubset(wlddev, iso3c %iin% c(\"DEU\", \"ITA\", \"FRA\")) #>   country iso3c       date year decade                region      income OECD #> 1  France   FRA 1961-01-01 1960   1960 Europe & Central Asia High income TRUE #> 2  France   FRA 1962-01-01 1961   1960 Europe & Central Asia High income TRUE #> 3  France   FRA 1963-01-01 1962   1960 Europe & Central Asia High income TRUE #> 4  France   FRA 1964-01-01 1963   1960 Europe & Central Asia High income TRUE #> 5  France   FRA 1965-01-01 1964   1960 Europe & Central Asia High income TRUE #>      PCGDP   LIFEEX GINI ODA      POP #> 1 12743.93 69.86829   NA  NA 46621669 #> 2 13203.32 70.11707   NA  NA 47240543 #> 3 13911.26 70.31463   NA  NA 47904877 #> 4 14572.28 70.51463   NA  NA 48582611 #> 5 15337.08 70.66341   NA  NA 49230595 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 178 rows ]  ## Math by reference: permissible types of operations x <- alloc(1.0, 1e5) # Vector x %+=% 1 x %+=% 1:1e5 xm <- matrix(alloc(1.0, 1e5), ncol = 100) # Matrix xm %+=% 1 xm %+=% 1:1e3 setop(xm, \"+\", 1:100, rowwise = TRUE) xm %+=% xm xm %+=% 1:1e5 xd <- qDF(replicate(100, alloc(1.0, 1e3), simplify = FALSE)) # Data Frame xd %+=% 1 xd %+=% 1:1e3 setop(xd, \"+\", 1:100, rowwise = TRUE) xd %+=% xd rm(x, xm, xd)  ## setv() and copyv() x <- rnorm(100) y <- sample.int(10, 100, replace = TRUE) setv(y, 5, 0)            # Faster than y[y == 5] <- 0 setv(y, 4, x)            # Faster than y[y == 4] <- x[y == 4] #> Warning: Type of R (double) is larger than X (integer) and thus coerced. This incurs loss of information, such as digits of real numbers being truncated upon coercion to integer. To avoid this, make sure X has a larger type than R: character > double > integer > logical. setv(y, 20:30, y[40:50]) # Faster than y[20:30] <- y[40:50] setv(y, 20:30, x)        # Faster than y[20:30] <- x[20:30] #> Warning: Type of R (double) is larger than X (integer) and thus coerced. This incurs loss of information, such as digits of real numbers being truncated upon coercion to integer. To avoid this, make sure X has a larger type than R: character > double > integer > logical. rm(x, y)  # Working with data frames, here returning copies of the frame copyv(mtcars, 20:30, ss(mtcars, 10:20)) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] copyv(mtcars, 20:30, fscale(mtcars)) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] ftransform(mtcars, new = copyv(cyl, 4, vs)) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb new #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   6 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   6 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   6 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   8 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ] # Column-wise: copyv(mtcars, 2:3, fscale(mtcars), xlist = TRUE) #>                    mpg        cyl        disp  hp drat    wt  qsec vs am gear #> Mazda RX4         21.0 -0.1049878 -0.57061982 110 3.90 2.620 16.46  0  1    4 #> Mazda RX4 Wag     21.0 -0.1049878 -0.57061982 110 3.90 2.875 17.02  0  1    4 #> Datsun 710        22.8 -1.2248578 -0.99018209  93 3.85 2.320 18.61  1  1    4 #> Hornet 4 Drive    21.4 -0.1049878  0.22009369 110 3.08 3.215 19.44  1  0    3 #> Hornet Sportabout 18.7  1.0148821  1.04308123 175 3.15 3.440 17.02  0  0    3 #> Valiant           18.1 -0.1049878 -0.04616698 105 2.76 3.460 20.22  1  0    3 #>                   carb #> Mazda RX4            4 #> Mazda RX4 Wag        4 #> Datsun 710           1 #> Hornet 4 Drive       1 #> Hornet Sportabout    2 #> Valiant              1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] copyv(mtcars, 2:3, mtcars[4:5], xlist = TRUE) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0 110 3.90 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0 110 3.90 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8  93 3.85  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4 110 3.08 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7 175 3.15 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1 105 2.76 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  ## Missing values mtc_na <- na_insert(mtcars, 0.15)    # Set 15% of values missing at random fnobs(mtc_na)                        # See observation count #>  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb  #>   28   28   28   28   28   28   28   28   28   28   28  missing_cases(mtc_na)                # Fast equivalent to !complete.cases(mtc_na) #>  [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE #> [13]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE #> [25] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE missing_cases(mtc_na, cols = 3:4)    # Missing cases on certain columns? #>  [1] FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE #> [13] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE #> [25] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE missing_cases(mtc_na, count = TRUE)  # Missing case count #>  [1] 3 2 1 0 1 0 2 2 6 3 1 2 1 0 1 0 2 0 1 4 2 0 2 1 0 0 2 0 2 1 2 0 missing_cases(mtc_na, prop = 0.8)    # Cases with 80% or more missing #>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE missing_cases(mtc_na, cols = 3:4, prop = 1)     # Cases mssing columns 3 and 4 #>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE missing_cases(mtc_na, cols = 3:4, count = TRUE) # Missing case count on columns 3 and 4 #>  [1] 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0  na_omit(mtc_na)                      # 12x faster than na.omit(mtc_na) #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] na_omit(mtc_na, prop = 0.8)          # Only remove cases missing 80% or more #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46 NA  1   NA   NA #> Mazda RX4 Wag     21.0  NA  160 110 3.90 2.875 17.02  0 NA    4    4 #> Datsun 710        22.8   4   NA  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8   NA 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] na_omit(mtc_na, na.attr = TRUE)      # Adds attribute with removed cases, like na.omit #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] na_omit(mtc_na, cols = .c(vs, am))   # Removes only cases missing vs or am #>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Datsun 710        22.8   4    NA  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8    NA 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Duster 360        14.3   8 360.0 245   NA 3.570 15.84  0  0   NA    4 #> Merc 280            NA   6 167.6  NA 3.92 3.440 18.30  1  0   NA    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 18 rows ] na_omit(qM(mtc_na))                  # Also works for matrices #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] na_omit(mtc_na$vs, na.attr = TRUE)   # Also works with vectors #>  [1] 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 #> attr(,\"na.action\") #> [1]  1  8  9 20 #> attr(,\"class\") #> [1] \"omit\" na_rm(mtc_na$vs)                     # For vectors na_rm is faster ... #>  [1] 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 rm(mtc_na)  ## Efficient vectorization head(vec(EuStockMarkets)) # Atomic objects: no copy at all #> [1] 1628.75 1613.63 1606.51 1621.04 1618.16 1610.61 head(vec(mtcars))         # Lists: directly in C #> [1] 21.0 21.0 22.8 21.4 18.7 18.1  options(oldopts)"},{"path":"https://sebkrantz.github.io/collapse/reference/extract_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Find and Extract / Subset List Elements — get_elem","title":"Find and Extract / Subset List Elements — get_elem","text":"suite functions subset extract (potentially complex) lists list-like structures. Subsetting may occur according certain data types, using identifier functions, element names regular expressions search list certain objects. atomic_elem list_elem non-recursive functions extract replace atomic sub-list elements top-level list tree. reg_elem recursive equivalent atomic_elem returns 'regular' part list - atomic elements final nodes. irreg_elem returns non-regular elements (.e. call terms objects, formulas, etc...). See Examples. get_elem returns part list responding either identifier function, regular expression, exact element names indices applied final objects. has_elem checks existence element returns TRUE match found. See Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/extract_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find and Extract / Subset List Elements — get_elem","text":"","code":"## Non-recursive (top-level) subsetting and replacing atomic_elem(l, return = \"sublist\", keep.class = FALSE) atomic_elem(l) <- valueatomic_elem(l) <- value list_elem(l, return = \"sublist\", keep.class = FALSE) list_elem(l) <- valuelist_elem(l) <- value  ## Recursive separation of regular (atomic) and irregular (non-atomic) parts reg_elem(l, recursive = TRUE, keep.tree = FALSE, keep.class = FALSE) irreg_elem(l, recursive = TRUE, keep.tree = FALSE, keep.class = FALSE)  ## Extract elements / subset list tree get_elem(l, elem, recursive = TRUE, DF.as.list = FALSE, keep.tree = FALSE,          keep.class = FALSE, regex = FALSE, invert = FALSE, ...)  ## Check for the existence of elements has_elem(l, elem, recursive = TRUE, DF.as.list = FALSE, regex = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/extract_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find and Extract / Subset List Elements — get_elem","text":"l list. value list length extracted subset l. elem function returning TRUE FALSE applied elements l, character vector element names regular expressions (regex = TRUE). get_elem also supports vector indices used subset final objects. return integer string specifying selector function return. options : Note: replacement functions replace data, names replaced together data. recursive logical. list search recursive (.e. go though elements), just top-level? DF..list logical. TRUE treats data frames like (sub-)lists; FALSE like atomic elements. keep.tree logical. TRUE always returns entire list tree leading matched results, FALSE drops top-level part tree possible. keep.class logical. list-based objects: class retained? works objects [ method retains class. regex logical. regular expression search used list names, exact matches? invert logical. Invert search .e. exclude matched elements list? ... arguments grep (regex = TRUE).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/extract_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find and Extract / Subset List Elements — get_elem","text":"lack better terminology, collapse defines 'regular' R objects objects either atomic list. reg_elem recursive = TRUE extracts subset list tree leading atomic elements final nodes. part list tree unlistable - calling is_unlistable(reg_elem(l)) TRUE lists l. Conversely, elements left behind reg_elem picked irreg_elem. Thus is_unlistable(irreg_elem(l)) always FALSE lists irregular elements (otherwise irreg_elem returns empty list). keep.tree = TRUE, reg_elem, irreg_elem get_elem always return entire list tree, cut branches leading desired result. keep.tree = FALSE, top-level parts tree omitted far possible. example nested list three levels one data-matrix one final branches, get_elem(l, .matrix, keep.tree = TRUE) return list (lres) depth 3, matrix can accessed lres[[1]][[1]][[1]]. however make much sense. get_elem(l, .matrix, keep.tree = FALSE) therefore figgure can drop entire tree return just matrix. keep.tree = FALSE makes additional optimizations matching elements far-apart corners nested structure, preserving hierarchy elements branch. Thus list l <- list(list(2,list(\"\",1)),list(1,list(\"b\",2))) calling get_elem(l, .character) just return list(\"\",\"b\").","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/extract_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find and Extract / Subset List Elements — get_elem","text":"","code":"m <- qM(mtcars) get_elem(list(list(list(m))), is.matrix) #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] get_elem(list(list(list(m))), is.matrix, keep.tree = TRUE) #> [[1]] #> [[1]][[1]] #> [[1]][[1]][[1]] #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #>  #>   l <- list(list(2,list(\"a\",1)),list(1,list(\"b\",2))) has_elem(l, is.logical) #> [1] FALSE has_elem(l, is.numeric) #> [1] TRUE get_elem(l, is.character) #> [[1]] #> [1] \"a\" #>  #> [[2]] #> [1] \"b\" #>  get_elem(l, is.character, keep.tree = TRUE) #> [[1]] #> [[1]][[1]] #> [[1]][[1]][[1]] #> [1] \"a\" #>  #>  #>  #> [[2]] #> [[2]][[1]] #> [[2]][[1]][[1]] #> [1] \"b\" #>  #>  #>   l <- lm(mpg ~ cyl + vs, data = mtcars) str(reg_elem(l)) #> List of 9 #>  $ coefficients : Named num [1:3] 39.625 -3.091 -0.939 #>   ..- attr(*, \"names\")= chr [1:3] \"(Intercept)\" \"cyl\" \"vs\" #>  $ residuals    : Named num [1:32] -0.081 -0.081 -3.523 1.258 3.8 ... #>   ..- attr(*, \"names\")= chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ... #>  $ effects      : Named num [1:32] -113.65 -28.6 1.54 2.39 3.75 ... #>   ..- attr(*, \"names\")= chr [1:32] \"(Intercept)\" \"cyl\" \"vs\" \"\" ... #>  $ rank         : int 3 #>  $ fitted.values: Named num [1:32] 21.1 21.1 26.3 20.1 14.9 ... #>   ..- attr(*, \"names\")= chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ... #>  $ assign       : int [1:3] 0 1 2 #>  $ qr           :List of 5 #>   ..$ qr   : num [1:32, 1:3] -5.657 0.177 0.177 0.177 0.177 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ... #>   .. .. ..$ : chr [1:3] \"(Intercept)\" \"cyl\" \"vs\" #>   .. ..- attr(*, \"assign\")= int [1:3] 0 1 2 #>   ..$ qraux: num [1:3] 1.18 1.02 1.13 #>   ..$ pivot: int [1:3] 1 2 3 #>   ..$ tol  : num 1e-07 #>   ..$ rank : int 3 #>  $ df.residual  : int 29 #>  $ model        :'data.frame':\t32 obs. of  3 variables: #>   ..$ mpg: num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #>   ..$ cyl: num [1:32] 6 6 4 6 8 6 8 4 4 6 ... #>   ..$ vs : num [1:32] 0 0 1 1 0 1 0 1 1 1 ... #>   ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ cyl + vs #>   .. .. ..- attr(*, \"variables\")= language list(mpg, cyl, vs) #>   .. .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1 #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:3] \"mpg\" \"cyl\" \"vs\" #>   .. .. .. .. ..$ : chr [1:2] \"cyl\" \"vs\" #>   .. .. ..- attr(*, \"term.labels\")= chr [1:2] \"cyl\" \"vs\" #>   .. .. ..- attr(*, \"order\")= int [1:2] 1 1 #>   .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. ..- attr(*, \"response\")= int 1 #>   .. .. ..- attr(*, \".Environment\")=<environment: 0x1207ec1f0>  #>   .. .. ..- attr(*, \"predvars\")= language list(mpg, cyl, vs) #>   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. ..- attr(*, \"names\")= chr [1:3] \"mpg\" \"cyl\" \"vs\" str(irreg_elem(l)) #> List of 2 #>  $ call : language lm(formula = mpg ~ cyl + vs, data = mtcars) #>  $ terms:Classes 'terms', 'formula'  language mpg ~ cyl + vs #>   .. ..- attr(*, \"variables\")= language list(mpg, cyl, vs) #>   .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1 #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. ..$ : chr [1:3] \"mpg\" \"cyl\" \"vs\" #>   .. .. .. ..$ : chr [1:2] \"cyl\" \"vs\" #>   .. ..- attr(*, \"term.labels\")= chr [1:2] \"cyl\" \"vs\" #>   .. ..- attr(*, \"order\")= int [1:2] 1 1 #>   .. ..- attr(*, \"intercept\")= int 1 #>   .. ..- attr(*, \"response\")= int 1 #>   .. ..- attr(*, \".Environment\")=<environment: 0x1207ec1f0>  #>   .. ..- attr(*, \"predvars\")= language list(mpg, cyl, vs) #>   .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\" #>   .. .. ..- attr(*, \"names\")= chr [1:3] \"mpg\" \"cyl\" \"vs\" get_elem(l, is.matrix) #>                     (Intercept)          cyl          vs #> Mazda RX4            -5.6568542 -35.00178567 -2.47487373 #> Mazda RX4 Wag         0.1767767   9.94359090 -2.27533496 #> Datsun 710            0.1767767   0.21715832 -1.64251357 #> Hornet 4 Drive        0.1767767   0.01602374  0.36419832 #> Hornet Sportabout     0.1767767  -0.18511084 -0.01520019 #> Valiant               0.1767767   0.01602374  0.36419832 #> Duster 360            0.1767767  -0.18511084 -0.01520019 #> Merc 240D             0.1767767   0.21715832  0.13477385 #> Merc 230              0.1767767   0.21715832  0.13477385 #> Merc 280              0.1767767   0.01602374  0.36419832 #> Merc 280C             0.1767767   0.01602374  0.36419832 #> Merc 450SE            0.1767767  -0.18511084 -0.01520019 #> Merc 450SL            0.1767767  -0.18511084 -0.01520019 #> Merc 450SLC           0.1767767  -0.18511084 -0.01520019 #> Cadillac Fleetwood    0.1767767  -0.18511084 -0.01520019 #> Lincoln Continental   0.1767767  -0.18511084 -0.01520019 #> Chrysler Imperial     0.1767767  -0.18511084 -0.01520019 #> Fiat 128              0.1767767   0.21715832  0.13477385 #> Honda Civic           0.1767767   0.21715832  0.13477385 #> Toyota Corolla        0.1767767   0.21715832  0.13477385 #> Toyota Corona         0.1767767   0.21715832  0.13477385 #> Dodge Challenger      0.1767767  -0.18511084 -0.01520019 #> AMC Javelin           0.1767767  -0.18511084 -0.01520019 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ] #> attr(,\"assign\") #> [1] 0 1 2 get_elem(l, \"residuals\") #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>          -0.0809747          -0.0809747          -3.5232427           1.2581068  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>           3.8003749          -2.0418932          -0.5996251          -1.9232427  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>          -3.5232427          -0.9418932          -2.3418932           1.5003749  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>           2.4003749           0.3003749          -4.4996251          -4.4996251  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>          -0.1996251           6.0767573           4.0767573           7.5767573  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>          -4.8232427           0.6003749           0.3003749          -1.5996251  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>           4.3003749           0.9767573          -1.2623243           4.0767573  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>           0.9003749          -1.3809747           0.1003749          -4.9232427  get_elem(l, \"fit\", regex = TRUE) #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>            21.08097            21.08097            26.32324            20.14189  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>            14.89963            20.14189            14.89963            26.32324  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>            26.32324            20.14189            20.14189            14.89963  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>            14.89963            14.89963            14.89963            14.89963  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>            14.89963            26.32324            26.32324            26.32324  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>            26.32324            14.89963            14.89963            14.89963  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>            14.89963            26.32324            27.26232            26.32324  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>            14.89963            21.08097            14.89963            26.32324  has_elem(l, \"tol\") #> [1] TRUE get_elem(l, \"tol\") #> [1] 1e-07"},{"path":"https://sebkrantz.github.io/collapse/reference/fFtest.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","title":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","text":"fFtest computes R-squared based F-test exclusion variables exc, full (unrestricted) model defined variables supplied exc X. test efficient designed cases exc X may contain multiple factors continuous variables. also efficient 2-part formula method.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fFtest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","text":"","code":"fFtest(...) # Internal method dispatch: formula if is.call(..1) || is.call(..2)  # Default S3 method fFtest(y, exc, X = NULL, w = NULL, full.df = TRUE, ...)  # S3 method for class 'formula' fFtest(formula, data = NULL, weights = NULL, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fFtest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","text":"y numeric vector: dependent variable. exc numeric vector, factor, numeric matrix list / data frame numeric vectors /factors: variables test / exclude. X numeric vector, factor, numeric matrix list / data frame numeric vectors /factors: covariates include restricted (without exc) unrestricted model. left empty (X = NULL), test amounts F-test regression y exc. w numeric. vector (frequency) weights. formula 2-part formula: y ~ exc | X, exc X expressions connected +, X can omitted. Note operators (:, *, ^, -, etc.) supported, can interact variables using standard functions like finteraction/itn magrittr::multiply_by inside formula e.g. log(y) ~ x1 + itn(x2, x3) | x4 log(y) ~ x1 + multiply_by(x2, x3) | x4. data named list data frame. weights weights vector expression results vector evaluated data environment. full.df logical. TRUE (default), degrees freedom calculated restricted unrestricted models estimated using lm() (.e. factors expanded matrices dummies). FALSE uses one degree freedom per factor. ... arguments passed fFtest.default fhdwithin. Sensible options might lm.method argument control parameters fixest::demean, workhorse function underlying fhdwithin higher-order centering tasks.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fFtest.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","text":"Factors continuous regressors efficiently projected using fhdwithin, option full.df regulates whether degree freedom subtracted used factor level (equivalent dummy-variable estimator / expanding factors), one degree freedom per factor (treating factors variables). test automatically removes missing values considers complete cases y, exc X. Unused factor levels exc X dropped. Note intercept always added fhdwithin, necessary include intercept data supplied exc / X.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fFtest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","text":"5 x 3 numeric matrix statistics. columns contain statistics: R-squared model numerator degrees freedom .e. number variables (k) used factor levels full.df = TRUE denominator degrees freedom: N - k - 1. F-statistic corresponding P-value rows show statistics : Full (unrestricted) Model (y ~ exc + X) Restricted Model (y ~ X) Exclusion Restriction exc. R-squared shown simply difference full restricted R-Squared's, R-Squared model y ~ exc. X = NULL, vector 5 statistics testing model (y ~ exc) shown.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fFtest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Weighted) F-test for Linear Models (with Factors) — fFtest","text":"","code":"## We could use fFtest as a simple seasonality test: fFtest(AirPassengers, qF(cycle(AirPassengers)))         # Testing for level-seasonality #>   R-Sq.     DF1     DF2 F-Stat. P-value  #>   0.106      11     132   1.424   0.169  fFtest(AirPassengers, qF(cycle(AirPassengers)),         # Seasonality test around a cubic trend         poly(seq_along(AirPassengers), 3)) #>                    R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model         0.965  14 129 250.585   0.000 #> Restricted Model   0.862   3 140 291.593   0.000 #> Exclusion Rest.    0.102  11 129  33.890   0.000 fFtest(fdiff(AirPassengers), qF(cycle(AirPassengers)))  # Seasonality in first-difference #>   R-Sq.     DF1     DF2 F-Stat. P-value  #>   0.749      11     131  35.487   0.000   ## A more classical example with only continuous variables fFtest(mpg ~ cyl + vs | hp + carb, mtcars) #>                   R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model        0.750   4  27  20.261   0.000 #> Restricted Model  0.605   2  29  22.175   0.000 #> Exclusion Rest.   0.145   2  27   7.858   0.002 fFtest(mtcars$mpg, mtcars[c(\"cyl\",\"vs\")], mtcars[c(\"hp\",\"carb\")]) #>                   R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model        0.750   4  27  20.261   0.000 #> Restricted Model  0.605   2  29  22.175   0.000 #> Exclusion Rest.   0.145   2  27   7.858   0.002   ## Now encoding cyl and vs as factors fFtest(mpg ~ qF(cyl) + qF(vs) | hp + carb, mtcars) #>                   R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model        0.756   5  26  16.140   0.000 #> Restricted Model  0.605   2  29  22.175   0.000 #> Exclusion Rest.   0.152   3  26   5.395   0.005 fFtest(mtcars$mpg, lapply(mtcars[c(\"cyl\",\"vs\")], qF), mtcars[c(\"hp\",\"carb\")]) #>                   R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model        0.756   5  26  16.140   0.000 #> Restricted Model  0.605   2  29  22.175   0.000 #> Exclusion Rest.   0.152   3  26   5.395   0.005  ## Using iris data: A factor and a continuous variable excluded fFtest(Sepal.Length ~ Petal.Width + Species | Sepal.Width + Petal.Length, iris) #>                    R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model         0.867   5 144 188.251   0.000 #> Restricted Model   0.840   2 147 386.386   0.000 #> Exclusion Rest.    0.027   3 144   9.816   0.000 fFtest(iris$Sepal.Length, iris[4:5], iris[2:3]) #>                    R-Sq. DF1 DF2 F-Stat. P-Value #> Full Model         0.867   5 144 188.251   0.000 #> Restricted Model   0.840   2 147 386.386   0.000 #> Exclusion Rest.    0.027   3 144   9.816   0.000  ## Testing the significance of country-FE in regression of GDP on life expectancy fFtest(log(PCGDP) ~ iso3c | LIFEEX, wlddev) #>                      R-Sq.   DF1   DF2   F-Stat.   P-Value #> Full Model           0.955   199  8822   943.424     0.000 #> Restricted Model     0.602     1  9020 13653.865     0.000 #> Exclusion Rest.      0.353   198  8822   350.373     0.000 fFtest(log(wlddev$PCGDP), wlddev$iso3c, wlddev$LIFEEX) #>                      R-Sq.   DF1   DF2   F-Stat.   P-Value #> Full Model           0.955   199  8822   943.424     0.000 #> Restricted Model     0.602     1  9020 13653.865     0.000 #> Exclusion Rest.      0.353   198  8822   350.373     0.000   ## Ok, country-FE are significant, what about adding time-FE fFtest(log(PCGDP) ~ qF(year) | iso3c + LIFEEX, wlddev) #>                     R-Sq.  DF1  DF2  F-Stat.  P-Value #> Full Model          0.963  258 8763  876.312    0.000 #> Restricted Model    0.955  199 8822  943.424    0.000 #> Exclusion Rest.     0.008   59 8763   30.126    0.000 fFtest(log(wlddev$PCGDP), qF(wlddev$year), wlddev[c(\"iso3c\",\"LIFEEX\")]) #>                     R-Sq.  DF1  DF2  F-Stat.  P-Value #> Full Model          0.963  258 8763  876.312    0.000 #> Restricted Model    0.955  199 8822  943.424    0.000 #> Exclusion Rest.     0.008   59 8763   30.126    0.000  # Same test done using lm: data <- na_omit(get_vars(wlddev, c(\"iso3c\",\"year\",\"PCGDP\",\"LIFEEX\"))) full <- lm(PCGDP ~ LIFEEX + iso3c + qF(year), data) rest <- lm(PCGDP ~ LIFEEX + iso3c, data) anova(rest, full) #> Analysis of Variance Table #>  #> Model 1: PCGDP ~ LIFEEX + iso3c #> Model 2: PCGDP ~ LIFEEX + iso3c + qF(year) #>   Res.Df        RSS Df  Sum of Sq     F    Pr(>F)     #> 1   8822 3.0044e+11                                   #> 2   8763 2.5097e+11 59 4.9475e+10 29.28 < 2.2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://sebkrantz.github.io/collapse/reference/fast-data-manipulation.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Data Manipulation — fast-data-manipulation","title":"Fast Data Manipulation — fast-data-manipulation","text":"collapse provides following functions fast manipulation (mostly) data frames. fselect much faster alternative dplyr::select select columns using expressions involving column names. get_vars versatile programmer friendly function efficiently select replace columns names, indices, logical vectors, regular expressions, using functions identify columns. num_vars, cat_vars, char_vars, fact_vars, logi_vars date_vars convenience functions efficiently select replace columns data type. add_vars efficiently adds new columns position within data frame (default end). can done vie replacement (.e. add_vars(data) <- newdata) returning appended data, e.g., add_vars(data, newdata1, newdata2, ...). thus also efficient alternative cbind.data.frame. rowbind efficiently combines data frames / lists row-wise. implementation derived data.table::rbindlist, also fast alternative rbind.data.frame. join provides fast, class-agnostic, verbose table joins. pivot efficiently reshapes data, supporting longer, wider recast pivoting, well multi-column-pivots pivots taking along variable labels. fsubset much faster version subset efficiently subset vectors, matrices data frames. non-standard evaluation offered fsubset needed, function ss much faster secure alternative [.data.frame. fslice(v) much faster alternative dplyr::slice_[head|tail|min|max] filtering/deduplicating matrix-like objects (groups). fsummarise much faster version dplyr::summarise, especially used together Fast Statistical Functions fgroup_by. fmutate much faster version dplyr::mutate, especially used together Fast Statistical Functions, fast Data Transformation Functions, fgroup_by. ftransform(v) much faster version transform, also supports list input nested pipelines. settransform(v) reference, .e. assigns calling environment. fcompute(v) similar ftransform(v) returns modified/computed columns. roworder fast substitute dplyr::arrange, syntax inspired data.table::setorder. colorder efficiently reorders columns data frame, see also data.table::setcolorder. frename fast substitute dplyr::rename, efficiently rename various objects. setrename renames objects reference. relabel setrelabel thing variable labels (see also vlabels).","code":""},{"path":[]},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fast-grouping-ordering.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Grouping and Ordering — fast-grouping-ordering","title":"Fast Grouping and Ordering — fast-grouping-ordering","text":"collapse provides following functions efficiently group order data: radixorder(v), provides fast radix-ordering direct access method order(..., method = \"radix\"), well possibility return attributes useful grouping data finding unique elements. function roworder(v) efficiently reorders data frame. group(v) provides fast grouping first-appearance order rows, based hashing algorithm C. Objects class 'qG', see . GRP creates collapse grouping objects class 'GRP' based radixorder group. 'GRP' objects form central building block grouped operations programming collapse efficient inputs collapse functions supporting grouped operations. fgroup_by provides fast replacement dplyr::group_by, creating grouped data frame (data.table / tibble etc.) 'GRP' object attached. grouped frame can used grouped operations using collapse's fast functions. fmatch fast alternative match, also supports matching data frame rows. funique faster version unique. data frame method also allows selecting unique rows according subset columns. fnunique efficiently calculates number unique values/rows. fduplicated fast alternative duplicated. any_duplicated simpler faster alternative anyDuplicated. fcount(v) computes group counts based subset columns data, fast replacement dplyr::count. qF, shorthand 'quick-factor' implements fast factor generation atomic vectors using either radix ordering method = \"radix\" hashing method = \"hash\". Factors can also used efficient grouped programming collapse functions, especially generated using qF(x, na.exclude = FALSE) assigns level missing values attaches class 'na.included' ensuring additional missing value checks executed collapse functions. qG, shorthand 'quick-group', generates kind factor-light without levels attribute instead attribute providing number levels. Optionally levels / groups can attached, without converting character. Objects class 'qG', also recognized collapse ecosystem. fdroplevels substantially faster replacement droplevels. finteraction fast alternative interaction implemented wrapper around as_factor_GRP(GRP(...)). can used generate factor multiple vectors, factors list vectors / factors. Unused factor levels always dropped. groupid generalization data.table::rleid providing run-length type group-id atomic vectors. generalization also supports passing ordering vector skipping missing values. example qF qG method = \"radix\" essentially implemented using groupid(x, radixorder(x)). seqid specialized function creates group-id sequences integer values. regular panel dataset groupid(id, order(id, time)) seqid(time, order(id, time)) provide id variable. seqid especially useful identifying discontinuities time-sequences. timeid specialized function convert integer double vectors representing time ('Date', 'POSIXct' etc.) factor 'qG' object based greatest common divisor elements (thus preserving gaps time intervals).","code":""},{"path":[]},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs fndistinct, collapse presents coherent set extremely fast flexible statistical functions (S3 generics) perform column-wise, grouped weighted computations vectors, matrices data frames, special support grouped data frames / tibbles (dplyr) data.table's.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"","code":"## All functions (FUN) follow a common syntax in 4 methods: FUN(x, ...)  ## Default S3 method: FUN(x, g = NULL, [w = NULL,] TRA = NULL, [na.rm = TRUE,]     use.g.names = TRUE, [nthreads = 1L,] ...)  ## S3 method for class 'matrix' FUN(x, g = NULL, [w = NULL,] TRA = NULL, [na.rm = TRUE,]     use.g.names = TRUE, drop = TRUE, [nthreads = 1L,] ...)  ## S3 method for class 'data.frame' FUN(x, g = NULL, [w = NULL,] TRA = NULL, [na.rm = TRUE,]     use.g.names = TRUE, drop = TRUE, [nthreads = 1L,] ...)  ## S3 method for class 'grouped_df' FUN(x, [w = NULL,] TRA = NULL, [na.rm = TRUE,]     use.g.names = FALSE, keep.group_vars = TRUE,     [keep.w = TRUE,] [stub = TRUE,] [nthreads = 1L,] ...)"},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"Please see documentation individual functions.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"x suitably aggregated transformed. Data frame column-attributes overall attributes generally preserved output data type.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":"related-functionality","dir":"Reference","previous_headings":"","what":"Related Functionality","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"Functions fquantile frange atomic vectors. Panel-decomposed (.e. within) statistics well grouped weighted skewness kurtosis implemented qsu. vector-valued functions operators fcumsum, fscale/STD, fbetween/B, fhdbetween/HDB, fwithin/W, fhdwithin/HDW, flag/L/F, fdiff/D/Dlog fgrowth/G grouped Data Transformations Time Series Panel Series. functions also support indexed data (plm).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"","code":"## default vector method mpg <- mtcars$mpg fsum(mpg)                         # Simple sum fsum(mpg, TRA = \"/\")              # Simple transformation: divide all values by the sum fsum(mpg, mtcars$cyl)             # Grouped sum fmean(mpg, mtcars$cyl)            # Grouped mean fmean(mpg, w = mtcars$hp)         # Weighted mean, weighted by hp fmean(mpg, mtcars$cyl, mtcars$hp) # Grouped mean, weighted by hp fsum(mpg, mtcars$cyl, TRA = \"/\")  # Proportions / division by group sums fmean(mpg, mtcars$cyl, mtcars$hp, # Subtract weighted group means, see also ?fwithin       TRA = \"-\")  ## data.frame method fsum(mtcars) fsum(mtcars, TRA = \"%\")                  # This computes percentages fsum(mtcars, mtcars[c(2,8:9)])           # Grouped column sum g <- GRP(mtcars, ~ cyl + vs + am)        # Here precomputing the groups! fsum(mtcars, g)                          # Faster !! fmean(mtcars, g, mtcars$hp) fmean(mtcars, g, mtcars$hp, \"-\")         # Demeaning by weighted group means.. fmean(fgroup_by(mtcars, cyl, vs, am), hp, \"-\")  # Another way of doing it..   fmode(wlddev, drop = FALSE)              # Compute statistical modes of variables in this data fmode(wlddev, wlddev$income)             # Grouped statistical modes ..  ## matrix method m <- qM(mtcars) fsum(m) fsum(m, g) # ..  ## method for grouped data frames - created with dplyr::group_by or fgroup_by library(dplyr) mtcars |> group_by(cyl,vs,am) |> select(mpg,carb) |> fsum() mtcars |> fgroup_by(cyl,vs,am) |> fselect(mpg,carb) |> fsum() # equivalent and faster !! mtcars |> fgroup_by(cyl,vs,am) |> fsum(TRA = \"%\") mtcars |> fgroup_by(cyl,vs,am) |> fmean(hp)         # weighted grouped mean, save sum of weights mtcars |> fgroup_by(cyl,vs,am) |> fmean(hp, keep.group_vars = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html","id":"benchmark","dir":"Reference","previous_headings":"","what":"Benchmark","title":"Fast (Grouped, Weighted) Statistical Functions for Matrix-Like Objects — fast-statistical-functions","text":"","code":"## This compares fsum with data.table (2 threads) and base::rowsum # Starting with small data mtcDT <- qDT(mtcars) f <- qF(mtcars$cyl)  library(microbenchmark) microbenchmark(mtcDT[, lapply(.SD, sum), by = f],                rowsum(mtcDT, f, reorder = FALSE),                fsum(mtcDT, f, na.rm = FALSE), unit = \"relative\")  #                              expr        min         lq      mean    median        uq       max neval cld # mtcDT[, lapply(.SD, sum), by = f] 145.436928 123.542134 88.681111 98.336378 71.880479 85.217726   100   c # rowsum(mtcDT, f, reorder = FALSE)   2.833333   2.798203  2.489064  2.937889  2.425724  2.181173   100  b #     fsum(mtcDT, f, na.rm = FALSE)   1.000000   1.000000  1.000000  1.000000  1.000000  1.000000   100 a  # Now larger data tdata <- qDT(replicate(100, rnorm(1e5), simplify = FALSE)) # 100 columns with 100.000 obs f <- qF(sample.int(1e4, 1e5, TRUE))                        # A factor with 10.000 groups  microbenchmark(tdata[, lapply(.SD, sum), by = f],                rowsum(tdata, f, reorder = FALSE),                fsum(tdata, f, na.rm = FALSE), unit = \"relative\")  #                              expr      min       lq     mean   median       uq       max neval cld # tdata[, lapply(.SD, sum), by = f] 2.646992 2.975489 2.834771 3.081313 3.120070 1.2766475   100   c # rowsum(tdata, f, reorder = FALSE) 1.747567 1.753313 1.629036 1.758043 1.839348 0.2720937   100  b #     fsum(tdata, f, na.rm = FALSE) 1.000000 1.000000 1.000000 1.000000 1.000000 1.0000000   100 a"},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"fbetween fwithin S3 generics efficiently obtain -transformed (averaged) (quasi-)within-transformed (demeaned) data. operations can performed groupwise /weighted. B W wrappers around fbetween fwithin representing '-operator' 'within-operator'. (B / W provide flexibility fbetween / fwithin applied data frames (.e.  column subsetting, formula input, auto-renaming id-variable-preservation capabilities...), otherwise identical.)","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"","code":"fbetween(x, ...)  fwithin(x, ...)        B(x, ...)        W(x, ...)  # Default S3 method fbetween(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # Default S3 method fwithin(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...) # Default S3 method B(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # Default S3 method W(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...)  # S3 method for class 'matrix' fbetween(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # S3 method for class 'matrix' fwithin(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...) # S3 method for class 'matrix' B(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, stub = .op[[\"stub\"]], ...) # S3 method for class 'matrix' W(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1,   stub = .op[[\"stub\"]], ...)  # S3 method for class 'data.frame' fbetween(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # S3 method for class 'data.frame' fwithin(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...) # S3 method for class 'data.frame' B(x, by = NULL, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],   fill = FALSE, stub = .op[[\"stub\"]], keep.by = TRUE, keep.w = TRUE, ...) # S3 method for class 'data.frame' W(x, by = NULL, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],   mean = 0, theta = 1, stub = .op[[\"stub\"]], keep.by = TRUE, keep.w = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fbetween(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # S3 method for class 'pseries' fwithin(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...) # S3 method for class 'pseries' B(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # S3 method for class 'pseries' W(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...)  # S3 method for class 'pdata.frame' fbetween(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, ...) # S3 method for class 'pdata.frame' fwithin(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1, ...) # S3 method for class 'pdata.frame' B(x, effect = 1L, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],   fill = FALSE, stub = .op[[\"stub\"]], keep.ids = TRUE, keep.w = TRUE, ...) # S3 method for class 'pdata.frame' W(x, effect = 1L, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],   mean = 0, theta = 1, stub = .op[[\"stub\"]], keep.ids = TRUE, keep.w = TRUE, ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' fbetween(x, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE,          keep.group_vars = TRUE, keep.w = TRUE, ...) # S3 method for class 'grouped_df' fwithin(x, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1,         keep.group_vars = TRUE, keep.w = TRUE, ...) # S3 method for class 'grouped_df' B(x, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE,   stub = .op[[\"stub\"]], keep.group_vars = TRUE, keep.w = TRUE, ...) # S3 method for class 'grouped_df' W(x, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, theta = 1,   stub = .op[[\"stub\"]], keep.group_vars = TRUE, keep.w = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"x numeric vector, matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. B W data.frame method: g, also allows one- two-sided formulas .e. ~ group1 var1 + var2 ~ group1 + group2. See Examples. w numeric vector (non-negative) weights. B/W data frame pdata.frame methods also allow one-sided formula .e. ~ weightcol. grouped_df (dplyr) method supports lazy-evaluation. See Examples. cols B/W (p)data.frame methods: Select columns scale using function, column names, indices logical vector. Default: numeric columns. Note: cols ignored two-sided formula passed . na.rm logical. Skip missing values x w computing averages. na.rm = FALSE NA NaN encountered, average group NA, data points belonging group output vector also NA. effect plm methods: Select panel identifier used grouping variable. 1L takes first variable index, 2L second etc. Index variables can also called name using character string. one variable supplied, corresponding index-factors interacted. stub character. prefix/stub add names transformed columns. TRUE (default) uses \"W.\"/\"B.\", FALSE rename columns. fill option fbetween/B: Logical. TRUE overwrite missing values x respective average. default missing values x preserved. mean option fwithin/W: mean center , default 0, different mean can supplied added data centering performed. special option performing grouped centering mean = \"overall.mean\". case overall mean data added subtracting group means. theta option fwithin/W: Double. optional scalar parameter quasi-demeaning .e. x - theta * xi.. useful variance components ('random-effects') estimators. see Details. keep., keep.ids, keep.group_vars B W data.frame, pdata.frame grouped_df methods: Logical. Retain grouping / panel-identifier columns output. data frames works grouping variables passed formula. keep.w B W data.frame, pdata.frame grouped_df methods: Logical. Retain column containing weights output. works w passed formula / lazy-expression. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"Without groups, fbetween/B replaces data points x mean weighted mean (w supplied). Similarly fwithin/W subtracts (weighted) mean data points .e. centers data mean. groups supplied g, replacement / centering performed fbetween/B | fwithin/W becomes groupwise. terms panel data notation: x vector panel dataset, xit denotes single data-point belonging group time-period t (t need time-period). xi. denotes x, averaged t. fbetween/B now returns xi. fwithin/W returns x - xi.. Thus data x grouping vector g: B(x,g) + W(x,g) = xi. + x - xi. = x. terms variance, fbetween/B retains variance group averages, fwithin/W, subtracting group means, retains variance within groups. data replacement performed fbetween/B can keep (default) overwrite missing values (option fill = TRUE) x. fwithin/W can center data simply (default), add back mean centering (option mean = value), add overall mean groupwise computations (option mean = \"overall.mean\"). Let x.. denote overall mean x, fwithin/W mean = \"overall.mean\" returns x - xi. + x.. instead x - xi.. useful get rid group-differences preserve overall level data. regression analysis, centering mean = \"overall.mean\" change constant term. See Examples. theta != 1, fwithin/W performs quasi-demeaning x - theta * xi.. mean = \"overall.mean\", x - theta * xi. + theta * x.. returned, mean partially demeaned data still equal overall data mean x... numeric value passed mean simply added back quasi-demeaned data .e. x - theta * xi. + mean. Now case linear panel model \\(y_{} = \\beta_0 + \\beta_1 X_{} + u_{}\\) \\(u_{} = \\alpha_i + \\epsilon_{}\\). \\(\\alpha_i \\neq \\alpha = const.\\) (exists individual heterogeneity), pooled OLS least inefficient inference \\(\\beta_1\\) invalid. \\(E[\\alpha_i|X_{}] = 0\\) (mean independence individual heterogeneity \\(\\alpha_i\\)), variance components 'random-effects' estimator provides asymptotically efficient FGLS solution estimating transformed model \\(y_{}-\\theta y_{.}  = \\beta_0 + \\beta_1 (X_{} - \\theta X_{.}) + (u_{} - \\theta u_{.}\\)), \\(\\theta = 1 - \\frac{\\sigma_\\alpha}{\\sqrt(\\sigma^2_\\alpha + T \\sigma^2_\\epsilon)}\\). estimate \\(\\theta\\) can obtained estimate \\(\\hat{u}_{}\\) (residuals pooled model). \\(E[\\alpha_i|X_{}] \\neq 0\\), pooled OLS biased inconsistent, taking \\(\\theta = 1\\) gives unbiased consistent fixed-effects estimator \\(\\beta_1\\). See Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"fbetween/B returns x every element replaced (groupwise) mean (xi.). Missing values preserved fill = FALSE (default). fwithin/W returns x every element subtracted (groupwise) mean (x - theta * xi. + mean , mean = \"overall.mean\", x - theta * xi. + theta * x..). See Details.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"Mundlak, Yair. 1978. Pooling Time Series Cross Section Data. Econometrica 46 (1): 69-85.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fbetween_fwithin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Between (Averaging) and (Quasi-)Within (Centering) Transformations — fbetween-fwithin","text":"","code":"## Simple centering and averaging head(fbetween(mtcars)) #>                        mpg    cyl     disp       hp     drat      wt     qsec #> Mazda RX4         20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Mazda RX4 Wag     20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Datsun 710        20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Hornet 4 Drive    20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Hornet Sportabout 20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Valiant           20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #>                       vs      am   gear   carb #> Mazda RX4         0.4375 0.40625 3.6875 2.8125 #> Mazda RX4 Wag     0.4375 0.40625 3.6875 2.8125 #> Datsun 710        0.4375 0.40625 3.6875 2.8125 #> Hornet 4 Drive    0.4375 0.40625 3.6875 2.8125 #> Hornet Sportabout 0.4375 0.40625 3.6875 2.8125 #> Valiant           0.4375 0.40625 3.6875 2.8125 head(B(mtcars)) #>                      B.mpg  B.cyl   B.disp     B.hp   B.drat    B.wt   B.qsec #> Mazda RX4         20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Mazda RX4 Wag     20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Datsun 710        20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Hornet 4 Drive    20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Hornet Sportabout 20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #> Valiant           20.09062 6.1875 230.7219 146.6875 3.596562 3.21725 17.84875 #>                     B.vs    B.am B.gear B.carb #> Mazda RX4         0.4375 0.40625 3.6875 2.8125 #> Mazda RX4 Wag     0.4375 0.40625 3.6875 2.8125 #> Datsun 710        0.4375 0.40625 3.6875 2.8125 #> Hornet 4 Drive    0.4375 0.40625 3.6875 2.8125 #> Hornet Sportabout 0.4375 0.40625 3.6875 2.8125 #> Valiant           0.4375 0.40625 3.6875 2.8125 head(fwithin(mtcars)) #>                         mpg     cyl        disp       hp       drat       wt #> Mazda RX4          0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.59725 #> Mazda RX4 Wag      0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.34225 #> Datsun 710         2.709375 -2.1875 -122.721875 -53.6875  0.2534375 -0.89725 #> Hornet 4 Drive     1.309375 -0.1875   27.278125 -36.6875 -0.5165625 -0.00225 #> Hornet Sportabout -1.390625  1.8125  129.278125  28.3125 -0.4465625  0.22275 #> Valiant           -1.990625 -0.1875   -5.721875 -41.6875 -0.8365625  0.24275 #>                       qsec      vs       am    gear    carb #> Mazda RX4         -1.38875 -0.4375  0.59375  0.3125  1.1875 #> Mazda RX4 Wag     -0.82875 -0.4375  0.59375  0.3125  1.1875 #> Datsun 710         0.76125  0.5625  0.59375  0.3125 -1.8125 #> Hornet 4 Drive     1.59125  0.5625 -0.40625 -0.6875 -1.8125 #> Hornet Sportabout -0.82875 -0.4375 -0.40625 -0.6875 -0.8125 #> Valiant            2.37125  0.5625 -0.40625 -0.6875 -1.8125 head(W(mtcars)) #>                       W.mpg   W.cyl      W.disp     W.hp     W.drat     W.wt #> Mazda RX4          0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.59725 #> Mazda RX4 Wag      0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.34225 #> Datsun 710         2.709375 -2.1875 -122.721875 -53.6875  0.2534375 -0.89725 #> Hornet 4 Drive     1.309375 -0.1875   27.278125 -36.6875 -0.5165625 -0.00225 #> Hornet Sportabout -1.390625  1.8125  129.278125  28.3125 -0.4465625  0.22275 #> Valiant           -1.990625 -0.1875   -5.721875 -41.6875 -0.8365625  0.24275 #>                     W.qsec    W.vs     W.am  W.gear  W.carb #> Mazda RX4         -1.38875 -0.4375  0.59375  0.3125  1.1875 #> Mazda RX4 Wag     -0.82875 -0.4375  0.59375  0.3125  1.1875 #> Datsun 710         0.76125  0.5625  0.59375  0.3125 -1.8125 #> Hornet 4 Drive     1.59125  0.5625 -0.40625 -0.6875 -1.8125 #> Hornet Sportabout -0.82875 -0.4375 -0.40625 -0.6875 -0.8125 #> Valiant            2.37125  0.5625 -0.40625 -0.6875 -1.8125 all.equal(fbetween(mtcars) + fwithin(mtcars), mtcars) #> [1] TRUE  ## Groupwise centering and averaging head(fbetween(mtcars, mtcars$cyl)) #>                        mpg cyl     disp        hp     drat       wt     qsec #> Mazda RX4         19.74286   6 183.3143 122.28571 3.585714 3.117143 17.97714 #> Mazda RX4 Wag     19.74286   6 183.3143 122.28571 3.585714 3.117143 17.97714 #> Datsun 710        26.66364   4 105.1364  82.63636 4.070909 2.285727 19.13727 #> Hornet 4 Drive    19.74286   6 183.3143 122.28571 3.585714 3.117143 17.97714 #> Hornet Sportabout 15.10000   8 353.1000 209.21429 3.229286 3.999214 16.77214 #> Valiant           19.74286   6 183.3143 122.28571 3.585714 3.117143 17.97714 #>                          vs        am     gear     carb #> Mazda RX4         0.5714286 0.4285714 3.857143 3.428571 #> Mazda RX4 Wag     0.5714286 0.4285714 3.857143 3.428571 #> Datsun 710        0.9090909 0.7272727 4.090909 1.545455 #> Hornet 4 Drive    0.5714286 0.4285714 3.857143 3.428571 #> Hornet Sportabout 0.0000000 0.1428571 3.285714 3.500000 #> Valiant           0.5714286 0.4285714 3.857143 3.428571 head(fwithin(mtcars, mtcars$cyl)) #>                         mpg cyl       disp        hp        drat          wt #> Mazda RX4          1.257143   0 -23.314286 -12.28571  0.31428571 -0.49714286 #> Mazda RX4 Wag      1.257143   0 -23.314286 -12.28571  0.31428571 -0.24214286 #> Datsun 710        -3.863636   0   2.863636  10.36364 -0.22090909  0.03427273 #> Hornet 4 Drive     1.657143   0  74.685714 -12.28571 -0.50571429  0.09785714 #> Hornet Sportabout  3.600000   0   6.900000 -34.21429 -0.07928571 -0.55921429 #> Valiant           -1.642857   0  41.685714 -17.28571 -0.82571429  0.34285714 #>                         qsec          vs         am        gear       carb #> Mazda RX4         -1.5171429 -0.57142857  0.5714286  0.14285714  0.5714286 #> Mazda RX4 Wag     -0.9571429 -0.57142857  0.5714286  0.14285714  0.5714286 #> Datsun 710        -0.5272727  0.09090909  0.2727273 -0.09090909 -0.5454545 #> Hornet 4 Drive     1.4628571  0.42857143 -0.4285714 -0.85714286 -2.4285714 #> Hornet Sportabout  0.2478571  0.00000000 -0.1428571 -0.28571429 -1.5000000 #> Valiant            2.2428571  0.42857143 -0.4285714 -0.85714286 -2.4285714 all.equal(fbetween(mtcars, mtcars$cyl) + fwithin(mtcars, mtcars$cyl), mtcars) #> [1] TRUE  head(W(wlddev, ~ iso3c, cols = 9:13))    # Center the 5 series in this dataset by country #>   iso3c W.PCGDP  W.LIFEEX W.GINI       W.ODA    W.POP #> 1   AFG      NA -16.75117     NA -1370778502 -9365285 #> 2   AFG      NA -16.23517     NA -1255468497 -9192848 #> 3   AFG      NA -15.72617     NA -1374708502 -9010817 #> 4   AFG      NA -15.22617     NA -1249828497 -8819053 #> 5   AFG      NA -14.73417     NA -1191628485 -8617477 #> 6   AFG      NA -14.24917     NA -1145708502 -8405938 head(cbind(get_vars(wlddev,\"iso3c\"),     # Same thing done manually using fwithin..       add_stub(fwithin(get_vars(wlddev,9:13), wlddev$iso3c), \"W.\"))) #>   iso3c W.PCGDP  W.LIFEEX W.GINI       W.ODA    W.POP #> 1   AFG      NA -16.75117     NA -1370778502 -9365285 #> 2   AFG      NA -16.23517     NA -1255468497 -9192848 #> 3   AFG      NA -15.72617     NA -1374708502 -9010817 #> 4   AFG      NA -15.22617     NA -1249828497 -8819053 #> 5   AFG      NA -14.73417     NA -1191628485 -8617477 #> 6   AFG      NA -14.24917     NA -1145708502 -8405938  ## Using B() and W() for fixed-effects regressions:  # Several ways of running the same regression with cyl-fixed effects lm(W(mpg,cyl) ~ W(carb,cyl), data = mtcars)                     # Centering each individually #>  #> Call: #> lm(formula = W(mpg, cyl) ~ W(carb, cyl), data = mtcars) #>  #> Coefficients: #>  (Intercept)  W(carb, cyl)   #>   -2.822e-16    -4.655e-01   #>  lm(mpg ~ carb, data = W(mtcars, ~ cyl, stub = FALSE))           # Centering the entire data #>  #> Call: #> lm(formula = mpg ~ carb, data = W(mtcars, ~cyl, stub = FALSE)) #>  #> Coefficients: #> (Intercept)         carb   #>  -2.822e-16   -4.655e-01   #>  lm(mpg ~ carb, data = W(mtcars, ~ cyl, stub = FALSE,            # Here only the intercept changes                         mean = \"overall.mean\")) #>  #> Call: #> lm(formula = mpg ~ carb, data = W(mtcars, ~cyl, stub = FALSE,  #>     mean = \"overall.mean\")) #>  #> Coefficients: #> (Intercept)         carb   #>     21.3999      -0.4655   #>  lm(mpg ~ carb + B(carb,cyl), data = mtcars)                     # Procedure suggested by #>  #> Call: #> lm(formula = mpg ~ carb + B(carb, cyl), data = mtcars) #>  #> Coefficients: #>  (Intercept)          carb  B(carb, cyl)   #>      34.8297       -0.4655       -4.7750   #>  # ..Mundlak (1978) - partialling out group averages amounts to the same as demeaning the data plm::plm(mpg ~ carb, mtcars, index = \"cyl\", model = \"within\")   # \"Proof\".. #>  #> Model Formula: mpg ~ carb #> <environment: 0x124926108> #>  #> Coefficients: #>     carb  #> -0.46551  #>   # This takes the interaction of cyl, vs and am as fixed effects lm(W(mpg) ~ W(carb), data = iby(mtcars, id = finteraction(cyl, vs, am))) #>  #> Call: #> lm(formula = W(mpg) ~ W(carb), data = iby(mtcars, id = finteraction(cyl,  #>     vs, am))) #>  #> Coefficients: #> (Intercept)      W(carb)   #>  -1.306e-15   -9.413e-01   #>  lm(mpg ~ carb, data = W(mtcars, ~ cyl + vs + am, stub = FALSE)) #>  #> Call: #> lm(formula = mpg ~ carb, data = W(mtcars, ~cyl + vs + am, stub = FALSE)) #>  #> Coefficients: #> (Intercept)         carb   #>  -1.306e-15   -9.413e-01   #>  lm(mpg ~ carb + B(carb,list(cyl,vs,am)), data = mtcars) #>  #> Call: #> lm(formula = mpg ~ carb + B(carb, list(cyl, vs, am)), data = mtcars) #>  #> Coefficients: #>                (Intercept)                        carb   #>                    27.8168                     -0.9413   #> B(carb, list(cyl, vs, am))   #>                    -1.8057   #>   # Now with cyl fixed effects weighted by hp: lm(W(mpg,cyl,hp) ~ W(carb,cyl,hp), data = mtcars) #>  #> Call: #> lm(formula = W(mpg, cyl, hp) ~ W(carb, cyl, hp), data = mtcars) #>  #> Coefficients: #>      (Intercept)  W(carb, cyl, hp)   #>           0.1747           -0.4469   #>  lm(mpg ~ carb, data = W(mtcars, ~ cyl, ~ hp, stub = FALSE)) #>  #> Call: #> lm(formula = mpg ~ carb, data = W(mtcars, ~cyl, ~hp, stub = FALSE)) #>  #> Coefficients: #> (Intercept)         carb   #>      0.1747      -0.4469   #>  lm(mpg ~ carb + B(carb,cyl,hp), data = mtcars)       # WRONG ! Gives a different coefficient!! #>  #> Call: #> lm(formula = mpg ~ carb + B(carb, cyl, hp), data = mtcars) #>  #> Coefficients: #>      (Intercept)              carb  B(carb, cyl, hp)   #>          34.1833           -0.4383           -4.2638   #>   ## Manual variance components (random-effects) estimation res <- HDW(mtcars, mpg ~ carb)[[1]]  # Get residuals from pooled OLS sig2_u <- fvar(res) sig2_e <- fvar(fwithin(res, mtcars$cyl)) T <- length(res) / fndistinct(mtcars$cyl) sig2_alpha <- sig2_u - sig2_e theta <- 1 - sqrt(sig2_alpha) / sqrt(sig2_alpha + T * sig2_e) lm(mpg ~ carb, data = W(mtcars, ~ cyl, theta = theta, mean = \"overall.mean\", stub = FALSE)) #>  #> Call: #> lm(formula = mpg ~ carb, data = W(mtcars, ~cyl, theta = theta,  #>     mean = \"overall.mean\", stub = FALSE)) #>  #> Coefficients: #> (Intercept)         carb   #>     21.8727      -0.6336   #>   # A slightly different method to obtain theta... plm::plm(mpg ~ carb, mtcars, index = \"cyl\", model = \"random\") #>  #> Model Formula: mpg ~ carb #> <environment: 0x124926108> #>  #> Coefficients: #> (Intercept)        carb  #>    22.40631    -0.68522  #>"},{"path":"https://sebkrantz.github.io/collapse/reference/fcount.html","id":null,"dir":"Reference","previous_headings":"","what":"Efficiently Count Observations by Group — fcount","title":"Efficiently Count Observations by Group — fcount","text":"much faster replacement dplyr::count.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fcount.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Efficiently Count Observations by Group — fcount","text":"","code":"fcount(x, ..., w = NULL, name = \"N\", add = FALSE,       sort = FALSE, decreasing = FALSE)  fcountv(x, cols = NULL, w = NULL, name = \"N\", add = FALSE,         sort = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fcount.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Efficiently Count Observations by Group — fcount","text":"x data frame list-like object, including 'grouped_df' 'indexed_frame'. Atomic vectors matrices can also passed, sent qDF. ... fcount: names sequences columns count cases - passed fselect. fcountv: arguments passed GRP (decreasing, na.last, method, effect etc.). Leaving empty count columns. cols select columns count cases , using column names, indices, logical vector selector function (e.g. is_categorical). w numeric vector weights, may contain missing values. fcount can also (unquoted) name column data frame. fcountv also supports single character name. Note corresponding argument dplyr::count called wt, collapse global default weights arguments called w. name character. name column containing count sum weights. dplyr::count called \"n\", \"N\" consistent rest collapse data.table. add TRUE adds count column x. Alternatively add = \"group_vars\" (add = \"gv\" parsimony) can used retain variables selected counting x count. sort, decreasing arguments passed GRP affecting order rows output (add = FALSE), algorithm used counting. general, sort = FALSE faster unless data already sorted columns used counting.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fcount.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Efficiently Count Observations by Group — fcount","text":"x list, object type x column (name) added end giving count. Otherwise, x atomic, data frame returned qDF(x) count column added. default (add = FALSE) unique rows x columns used counting returned.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fcount.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Efficiently Count Observations by Group — fcount","text":"","code":"fcount(mtcars, cyl, vs, am) #>   cyl vs am  N #> 1   6  0  1  3 #> 2   4  1  1  7 #> 3   6  1  0  4 #> 4   8  0  0 12 #> 5   4  1  0  3 #> 6   4  0  1  1 #> 7   8  0  1  2 fcountv(mtcars, cols = .c(cyl, vs, am)) #>   cyl vs am  N #> 1   6  0  1  3 #> 2   4  1  1  7 #> 3   6  1  0  4 #> 4   8  0  0 12 #> 5   4  1  0  3 #> 6   4  0  1  1 #> 7   8  0  1  2 fcount(mtcars, cyl, vs, am, sort = TRUE) #>   cyl vs am  N #> 1   4  0  1  1 #> 2   4  1  0  3 #> 3   4  1  1  7 #> 4   6  0  1  3 #> 5   6  1  0  4 #> 6   8  0  0 12 #> 7   8  0  1  2 fcount(mtcars, cyl, vs, am, add = TRUE) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb  N #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  3 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  3 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  7 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  4 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 12 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ] fcount(mtcars, cyl, vs, am, add = \"group_vars\") #>                     cyl vs am  N #> Mazda RX4             6  0  1  3 #> Mazda RX4 Wag         6  0  1  3 #> Datsun 710            4  1  1  7 #> Hornet 4 Drive        6  1  0  4 #> Hornet Sportabout     8  0  0 12 #> Valiant               6  1  0  4 #> Duster 360            8  0  0 12 #> Merc 240D             4  1  0  3 #> Merc 230              4  1  0  3 #> Merc 280              6  1  0  4 #> Merc 280C             6  1  0  4 #> Merc 450SE            8  0  0 12 #> Merc 450SL            8  0  0 12 #> Merc 450SLC           8  0  0 12 #> Cadillac Fleetwood    8  0  0 12 #> Lincoln Continental   8  0  0 12 #> Chrysler Imperial     8  0  0 12 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 15 rows ]  ## With grouped data mtcars |> fgroup_by(cyl, vs, am) |> fcount() #>   cyl vs am  N #> 1   4  0  1  1 #> 2   4  1  0  3 #> 3   4  1  1  7 #> 4   6  0  1  3 #> 5   6  1  0  4 #> 6   8  0  0 12 #> 7   8  0  1  2 mtcars |> fgroup_by(cyl, vs, am) |> fcount(add = TRUE) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb  N #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  3 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  3 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  7 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  4 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 12 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl, vs, am) |> fcount(add = \"group_vars\") #>                     cyl vs am  N #> Mazda RX4             6  0  1  3 #> Mazda RX4 Wag         6  0  1  3 #> Datsun 710            4  1  1  7 #> Hornet 4 Drive        6  1  0  4 #> Hornet Sportabout     8  0  0 12 #> Valiant               6  1  0  4 #> Duster 360            8  0  0 12 #> Merc 240D             4  1  0  3 #> Merc 230              4  1  0  3 #> Merc 280              6  1  0  4 #> Merc 280C             6  1  0  4 #> Merc 450SE            8  0  0 12 #> Merc 450SL            8  0  0 12 #> Merc 450SLC           8  0  0 12 #> Cadillac Fleetwood    8  0  0 12 #> Lincoln Continental   8  0  0 12 #> Chrysler Imperial     8  0  0 12 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 15 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]   ## With indexed data: by default counting on the first index variable wlddev |> findex_by(country, year) |> fcount() #>                   country  N #> 1             Afghanistan 61 #> 2                 Albania 61 #> 3                 Algeria 61 #> 4          American Samoa 61 #> 5                 Andorra 61 #> 6                  Angola 61 #> 7     Antigua and Barbuda 61 #> 8               Argentina 61 #> 9                 Armenia 61 #> 10                  Aruba 61 #> 11              Australia 61 #> 12                Austria 61 #> 13             Azerbaijan 61 #> 14           Bahamas, The 61 #> 15                Bahrain 61 #> 16             Bangladesh 61 #> 17               Barbados 61 #> 18                Belarus 61 #> 19                Belgium 61 #> 20                 Belize 61 #> 21                  Benin 61 #> 22                Bermuda 61 #> 23                 Bhutan 61 #> 24                Bolivia 61 #> 25 Bosnia and Herzegovina 61 #> 26               Botswana 61 #> 27                 Brazil 61 #> 28 British Virgin Islands 61 #> 29      Brunei Darussalam 61 #> 30               Bulgaria 61 #> 31           Burkina Faso 61 #> 32                Burundi 61 #> 33             Cabo Verde 61 #> 34               Cambodia 61 #> 35               Cameroon 61 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 181 rows ] wlddev |> findex_by(country, year) |> fcount(add = TRUE) #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP  N #> 1 32.446   NA 116769997 8996973 61 #> 2 32.962   NA 232080002 9169410 61 #> 3 33.471   NA 112839996 9351441 61 #> 4 33.971   NA 237720001 9543205 61 #> 5 34.463   NA 295920013 9744781 61 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13171 rows ] #>  #> Indexed by:  country [216] | year [61]  # Use fcountv to pass additional arguments to GRP.pdata.frame, # here using the effect argument to choose a different index variable wlddev |> findex_by(country, year) |> fcountv(effect = \"year\") #>    year   N #> 1  1960 216 #> 2  1961 216 #> 3  1962 216 #> 4  1963 216 #> 5  1964 216 #> 6  1965 216 #> 7  1966 216 #> 8  1967 216 #> 9  1968 216 #> 10 1969 216 #> 11 1970 216 #> 12 1971 216 #> 13 1972 216 #> 14 1973 216 #> 15 1974 216 #> 16 1975 216 #> 17 1976 216 #> 18 1977 216 #> 19 1978 216 #> 20 1979 216 #> 21 1980 216 #> 22 1981 216 #> 23 1982 216 #> 24 1983 216 #> 25 1984 216 #> 26 1985 216 #> 27 1986 216 #> 28 1987 216 #> 29 1988 216 #> 30 1989 216 #> 31 1990 216 #> 32 1991 216 #> 33 1992 216 #> 34 1993 216 #> 35 1994 216 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] wlddev |> findex_by(country, year) |> fcountv(add = \"group_vars\", effect = \"year\") #>    year   N #> 1  1960 216 #> 2  1961 216 #> 3  1962 216 #> 4  1963 216 #> 5  1964 216 #> 6  1965 216 #> 7  1966 216 #> 8  1967 216 #> 9  1968 216 #> 10 1969 216 #> 11 1970 216 #> 12 1971 216 #> 13 1972 216 #> 14 1973 216 #> 15 1974 216 #> 16 1975 216 #> 17 1976 216 #> 18 1977 216 #> 19 1978 216 #> 20 1979 216 #> 21 1980 216 #> 22 1981 216 #> 23 1982 216 #> 24 1983 216 #> 25 1984 216 #> 26 1985 216 #> 27 1986 216 #> 28 1987 216 #> 29 1988 216 #> 30 1989 216 #> 31 1990 216 #> 32 1991 216 #> 33 1992 216 #> 34 1993 216 #> 35 1994 216 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13141 rows ] #>  #> Indexed by:  country [216] | year [61]"},{"path":"https://sebkrantz.github.io/collapse/reference/fcumsum.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","title":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","text":"fcumsum generic function computes (column-wise) cumulative sum x, (optionally) grouped g /ordered o. Several options deal missing values provided.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fcumsum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","text":"","code":"fcumsum(x, ...)  # Default S3 method fcumsum(x, g = NULL, o = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, check.o = TRUE, ...)  # S3 method for class 'matrix' fcumsum(x, g = NULL, o = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, check.o = TRUE, ...)  # S3 method for class 'data.frame' fcumsum(x, g = NULL, o = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, check.o = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fcumsum(x, na.rm = .op[[\"na.rm\"]], fill = FALSE, shift = \"time\", ...)  # S3 method for class 'pdata.frame' fcumsum(x, na.rm = .op[[\"na.rm\"]], fill = FALSE, shift = \"time\", ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' fcumsum(x, o = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, check.o = TRUE,         keep.ids = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fcumsum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","text":"x numeric vector / time series, (time series) matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. o vector list vectors providing order elements x cumulatively summed. passed radixorderv unless check.o = FALSE. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. fill na.rm = TRUE, setting fill = TRUE overwrite missing values previous value cumulative sum, starting 0. check.o logical. Programmers option: FALSE prevents passing o radixorderv, requiring o valid ordering vector integer typed element range [1, length(x)]. gives extra speed, terminate R element o large small. shift pseries / pdata.frame methods: character. \"time\" \"row\". See flag details. argument control 'shifting' data rather order elements summed. keep.ids pdata.frame / grouped_df methods: Logical. Drop identifiers output (includes grouping variables variables passed o). Note: grouped / panel data frames identifiers dropped, \"groups\" / \"index\" attributes kept. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fcumsum.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","text":"na.rm = FALSE, fcumsum works like cumsum propagates missing values. default na.rm = TRUE skips missing values computes cumulative sum non-missing values. Missing values kept. fill = TRUE, missing values replaced previous value cumulative sum (starting 0), computed non-missing values. default cumulative sum computed order elements appear x. o provided, cumulative sum computed order given radixorderv(o), without need first sort x. applies well groups used (g), case cumulative sum computed separately group. pseries pdata.frame methods assume last factor index time-variable rest grouping variables. time-variable passed radixorderv used ordered computation, cumulative sums accurately computed regardless whether panel-data ordered balanced. fcumsum explicitly supports integers. Integers R bounded bounded +-2,147,483,647, integer overflow error provided cumulative sum (within group) exceeds +-2,147,483,647. case data converted double beforehand.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fcumsum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","text":"cumulative sum values x, (optionally) grouped g /ordered o. See Details Examples.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fcumsum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Ordered) Cumulative Sum for Matrix-Like Objects — fcumsum","text":"","code":"## Non-grouped fcumsum(AirPassengers) #>        Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec #> 1949   112   230   362   491   612   747   895  1043  1179  1298  1402  1520 #> 1950  1635  1761  1902  2037  2162  2311  2481  2651  2809  2942  3056  3196 #> 1951  3341  3491  3669  3832  4004  4182  4381  4580  4764  4926  5072  5238 #> 1952  5409  5589  5782  5963  6146  6364  6594  6836  7045  7236  7408  7602 #> 1953  7798  7994  8230  8465  8694  8937  9201  9473  9710  9921 10101 10302 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] head(fcumsum(EuStockMarkets)) #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>              DAX     SMI     CAC    FTSE #> 1991.496 1628.75  1678.1  1772.8  2443.6 #> 1991.500 3242.38  3366.6  3523.3  4903.8 #> 1991.504 4848.89  5045.2  5241.3  7352.0 #> 1991.508 6469.93  6729.3  6949.4  9822.4 #> 1991.512 8088.09  8415.9  8672.5 12307.1 #> 1991.515 9698.70 10087.5 10386.8 14773.9 fcumsum(mtcars) #>                     mpg cyl disp  hp  drat     wt   qsec vs am gear carb #> Mazda RX4          21.0   6  160 110  3.90  2.620  16.46  0  1    4    4 #> Mazda RX4 Wag      42.0  12  320 220  7.80  5.495  33.48  0  2    8    8 #> Datsun 710         64.8  16  428 313 11.65  7.815  52.09  1  3   12    9 #> Hornet 4 Drive     86.2  22  686 423 14.73 11.030  71.53  2  3   15   10 #> Hornet Sportabout 104.9  30 1046 598 17.88 14.470  88.55  2  3   18   12 #> Valiant           123.0  36 1271 703 20.64 17.930 108.77  3  3   21   13 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  # Non-grouped but ordered o <- order(rnorm(nrow(EuStockMarkets))) all.equal(copyAttrib(fcumsum(EuStockMarkets[o, ], o = o)[order(o), ], EuStockMarkets),           fcumsum(EuStockMarkets)) #> [1] TRUE  ## Grouped head(with(wlddev, fcumsum(PCGDP, iso3c))) #> [1] NA NA NA NA NA NA  ## Grouped and ordered head(with(wlddev, fcumsum(PCGDP, iso3c, year))) #> [1] NA NA NA NA NA NA head(with(wlddev, fcumsum(PCGDP, iso3c, year, fill = TRUE))) #> [1] 0 0 0 0 0 0"},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":null,"dir":"Reference","previous_headings":"","what":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"fdiff S3 generic compute (sequences ) suitably lagged / leaded iterated differences, quasi-differences (quasi-)log-differences. difference log-difference operators D Dlog also exists parsimonious wrappers around fdiff, providing flexibility fdiff applied data frames.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"","code":"fdiff(x, n = 1, diff = 1, ...)       D(x, n = 1, diff = 1, ...)    Dlog(x, n = 1, diff = 1, ...)  # Default S3 method fdiff(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, log = FALSE, rho = 1,       stubs = TRUE, ...) # Default S3 method D(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, rho = 1,   stubs = .op[[\"stub\"]], ...) # Default S3 method Dlog(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, rho = 1, stubs = .op[[\"stub\"]],      ...)  # S3 method for class 'matrix' fdiff(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, log = FALSE, rho = 1,       stubs = length(n) + length(diff) > 2L, ...) # S3 method for class 'matrix' D(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, rho = 1,   stubs = .op[[\"stub\"]], ...) # S3 method for class 'matrix' Dlog(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, rho = 1, stubs = .op[[\"stub\"]],      ...)  # S3 method for class 'data.frame' fdiff(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, log = FALSE, rho = 1,       stubs = length(n) + length(diff) > 2L, ...) # S3 method for class 'data.frame' D(x, n = 1, diff = 1, by = NULL, t = NULL, cols = is.numeric,   fill = NA, rho = 1, stubs = .op[[\"stub\"]], keep.ids = TRUE, ...) # S3 method for class 'data.frame' Dlog(x, n = 1, diff = 1, by = NULL, t = NULL, cols = is.numeric,      fill = NA, rho = 1, stubs = .op[[\"stub\"]], keep.ids = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fdiff(x, n = 1, diff = 1, fill = NA, log = FALSE, rho = 1,       stubs = length(n) + length(diff) > 2L, shift = \"time\", ...) # S3 method for class 'pseries' D(x, n = 1, diff = 1, fill = NA, rho = 1, stubs = .op[[\"stub\"]], shift = \"time\", ...) # S3 method for class 'pseries' Dlog(x, n = 1, diff = 1, fill = NA, rho = 1, stubs = .op[[\"stub\"]], shift = \"time\", ...)  # S3 method for class 'pdata.frame' fdiff(x, n = 1, diff = 1, fill = NA, log = FALSE, rho = 1,       stubs = length(n) + length(diff) > 2L, shift = \"time\", ...) # S3 method for class 'pdata.frame' D(x, n = 1, diff = 1, cols = is.numeric, fill = NA, rho = 1, stubs = .op[[\"stub\"]],   shift = \"time\", keep.ids = TRUE, ...) # S3 method for class 'pdata.frame' Dlog(x, n = 1, diff = 1, cols = is.numeric, fill = NA, rho = 1, stubs = .op[[\"stub\"]],      shift = \"time\", keep.ids = TRUE, ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' fdiff(x, n = 1, diff = 1, t = NULL, fill = NA, log = FALSE, rho = 1,       stubs = length(n) + length(diff) > 2L, keep.ids = TRUE, ...) # S3 method for class 'grouped_df' D(x, n = 1, diff = 1, t = NULL, fill = NA, rho = 1, stubs = .op[[\"stub\"]],   keep.ids = TRUE, ...) # S3 method for class 'grouped_df' Dlog(x, n = 1, diff = 1, t = NULL, fill = NA, rho = 1, stubs = .op[[\"stub\"]],      keep.ids = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"x numeric vector / time series, (time series) matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). n integer. vector indicating number lags leads. diff integer. vector integers > 1 indicating order differencing / log-differencing. g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. Note without t, values group need consecutive right order. See Details flag. data.frame method: g, also allows one- two-sided formulas .e. ~ group1 var1 + var2 ~ group1 + group2. See Examples. t time vector list vectors. See flag. cols data.frame method: Select columns difference using function, column names, indices logical vector. Default: numeric variables. Note: cols ignored two-sided formula passed . fill value insert vectors shifted. Default NA. log logical. TRUE computes log-differences. See Details. rho double. Autocorrelation parameter. Set value 0 1 quasi-differencing. numeric value can supplied. stubs logical. TRUE (default) rename differenced columns adding prefixes \"LnDdiff.\" / \"FnDdiff.\" differences \"LnDlogdiff.\" / \"FnDlogdiff.\" log-differences replacing \"D\" / \"Dlog\" \"QD\" / \"QDlog\" quasi-differences. shift pseries / pdata.frame methods: character. \"time\" \"row\". See flag details. keep.ids data.frame / pdata.frame / grouped_df methods: Logical. Drop identifiers output (includes variables passed t using formulas). Note: 'grouped_df' / 'pdata.frame' identifiers dropped, \"groups\" / \"index\" attributes kept. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"default, fdiff/D/Dlog return x columns differenced / log-differenced. Differences computed repeat(diff) x[] - rho*x[-n], log-differences log(x[]) - rho*log(x[-n]) diff = 1 repeat(diff-1) x[] - rho*x[-n] used compute subsequent differences (usually diff = 1 log-differencing). rho < 1, becomes quasi- (partial) differencing, technique suggested Cochrane Orcutt (1949) deal serial correlation regression models, rho typically estimated running regression model residuals lagged residuals.  also possible compute forward differences passing negative n values. n also supports arbitrary vectors integers (lags), diff supports positive sequences integers (differences): one value passed n /diff, data expanded-wide follows: x atomic vector time series, (time series) matrix returned columns ordered first lag, difference. x matrix data frame, column expanded like manor output ncol(x)*length(n)*length(diff) columns ordered first column name, lag, difference. computational details efficiency considerations see help page flag.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"x differenced diff times using lags n . Quasi log-differences toggled rho log arguments Dlog operator. Computations can grouped g//ordered t. See Details Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"Cochrane, D.; Orcutt, G. H. (1949). Application Least Squares Regression Relationships Containing Auto-Correlated Error Terms. Journal American Statistical Association. 44 (245): 32-61. Prais, S. J. & Winsten, C. B. (1954). Trend Estimators Serial Correlation. Cowles Commission Discussion Paper . 383. Chicago.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fdiff.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":" Fast (Quasi-, Log-) Differences for Time Series and Panel Data — fdiff","text":"","code":"## Simple Time Series: AirPassengers D(AirPassengers)                      # 1st difference, same as fdiff(AirPassengers) #>       Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec #> 1949   NA    6   14   -3   -8   14   13    0  -12  -17  -15   14 #> 1950   -3   11   15   -6  -10   24   21    0  -12  -25  -19   26 #> 1951    5    5   28  -15    9    6   21    0  -15  -22  -16   20 #> 1952    5    9   13  -12    2   35   12   12  -33  -18  -19   22 #> 1953    2    0   40   -1   -6   14   21    8  -35  -26  -31   21 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] D(AirPassengers, -1)                  # Forward difference #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949  -6 -14   3   8 -14 -13   0  12  17  15 -14   3 #> 1950 -11 -15   6  10 -24 -21   0  12  25  19 -26  -5 #> 1951  -5 -28  15  -9  -6 -21   0  15  22  16 -20  -5 #> 1952  -9 -13  12  -2 -35 -12 -12  33  18  19 -22  -2 #> 1953   0 -40   1   6 -14 -21  -8  35  26  31 -21  -3 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] Dlog(AirPassengers)                   # Log-difference #>               Jan          Feb          Mar          Apr          May #> 1949           NA  0.052185753  0.112117298 -0.022989518 -0.064021859 #> 1950 -0.025752496  0.091349779  0.112477983 -0.043485112 -0.076961041 #> 1951  0.035091320  0.033901552  0.171148256 -0.088033349  0.053744276 #> 1952  0.029675768  0.051293294  0.069733338 -0.064193158  0.010989122 #> 1953  0.010256500  0.000000000  0.185717146 -0.004246291 -0.025863511 #>               Jun          Jul          Aug          Sep          Oct #> 1949  0.109484233  0.091937495  0.000000000 -0.084557388 -0.133531393 #> 1950  0.175632569  0.131852131  0.000000000 -0.073203404 -0.172245905 #> 1951  0.034289073  0.111521274  0.000000000 -0.078369067 -0.127339422 #> 1952  0.175008910  0.053584246  0.050858417 -0.146603474 -0.090060824 #> 1953  0.059339440  0.082887660  0.029852963 -0.137741925 -0.116202008 #>               Nov          Dec #> 1949 -0.134732594  0.126293725 #> 1950 -0.154150680  0.205443974 #> 1951 -0.103989714  0.128381167 #> 1952 -0.104778951  0.120363682 #> 1953 -0.158901283  0.110348057 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] D(AirPassengers, 1, 2)                # Second difference #>       Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec #> 1949   NA   NA    8  -17   -5   22   -1  -13  -12   -5    2   29 #> 1950  -17   14    4  -21   -4   34   -3  -21  -12  -13    6   45 #> 1951  -21    0   23  -43   24   -3   15  -21  -15   -7    6   36 #> 1952  -15    4    4  -25   14   33  -23    0  -45   15   -1   41 #> 1953  -20   -2   40  -41   -5   20    7  -13  -43    9   -5   52 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] Dlog(AirPassengers, 1, 2)             # Second log-difference #>                Jan           Feb           Mar           Apr           May #> 1949            NA            NA  0.0599315450 -0.1351068163 -0.0410323405 #> 1950 -0.1520462214  0.1171022747  0.0211282048 -0.1559630954 -0.0334759292 #> 1951 -0.1703526544 -0.0011897681  0.1372467045 -0.2591816057  0.1417776255 #> 1952 -0.0987053985  0.0216175262  0.0184400436 -0.1339264957  0.0751822792 #> 1953 -0.1101071821 -0.0102565002  0.1857171458 -0.1899634367 -0.0216172197 #>                Jun           Jul           Aug           Sep           Oct #> 1949  0.1735060916 -0.0175467375 -0.0919374953 -0.0845573880 -0.0489740046 #> 1950  0.2525936098 -0.0437804375 -0.1318521311 -0.0732034040 -0.0990425008 #> 1951 -0.0194552025  0.0772322010 -0.1115212744 -0.0783690671 -0.0489703553 #> 1952  0.1640197884 -0.1214246638 -0.0027258289 -0.1974618914  0.0565426503 #> 1953  0.0852029504  0.0235482200 -0.0530346967 -0.1675948883  0.0215399175 #>                Nov           Dec #> 1949 -0.0012012013  0.2610263193 #> 1950  0.0180952250  0.3595946540 #> 1951  0.0233497089  0.2323708802 #> 1952 -0.0147181273  0.2251426335 #> 1953 -0.0426992749  0.2692493398 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] D(AirPassengers, 12)                  # Seasonal difference (data is monthly) #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA #> 1950   3   8   9   6   4  14  22  22  22  14  10  22 #> 1951  30  24  37  28  47  29  29  29  26  29  32  26 #> 1952  26  30  15  18  11  40  31  43  25  29  26  28 #> 1953  25  16  43  54  46  25  34  30  28  20   8   7 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] D(AirPassengers,                      # Quasi-difference, see a better example below   rho = pwcor(AirPassengers, L(AirPassengers))) #>              Jan         Feb         Mar         Apr         May         Jun #> 1949          NA  10.4581994  18.6970315   2.2543065  -2.8651096  18.8164476 #> 1950   1.6970315  15.5776155  20.0154743  -0.3874454  -4.6262775  28.9756690 #> 1951  10.5727493  10.7717760  33.9708028  -7.9146474  15.4882724  12.8465205 #> 1952  11.6076884  15.8067152  20.1649634  -4.3175671   9.2047687  42.2843794 #> 1953   9.7222383   7.8018490  47.8018490   8.3940631   3.3542577  23.1154256 #>              Jul         Aug         Sep         Oct         Nov         Dec #> 1949  18.3737225   5.8911921  -6.1088079 -11.5864721 -10.2631631  18.1397566 #> 1950  26.9309974   6.7669098  -5.2330902 -18.7107544 -13.7058882  30.5378101 #> 1951  28.0853526   7.9212650  -7.0787350 -14.6758152  -9.5515330  25.8115814 #> 1952  20.6775667  21.1552309 -23.3671048  -9.6806814 -11.3971778  28.8465205 #> 1953  30.6727005  18.5086129 -24.1729443 -16.5661316 -22.6010707  28.1649634 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ]  head(D(AirPassengers, -2:2, 1:3))     # Sequence of leaded/lagged and iterated differences #>          F2D1 F2D2 F2D3 FD1 FD2 FD3  -- D1  D2  D3 L2D1 L2D2 L2D3 #> Jan 1949  -20  -31  -69  -6   8  25 112 NA  NA  NA   NA   NA   NA #> Feb 1949  -11   -5  -12 -14 -17 -12 118  6  NA  NA   NA   NA   NA #> Mar 1949   11   38   77   3  -5 -27 132 14   8  NA   20   NA   NA #> Apr 1949   -6    7   49   8  22  23 129 -3 -17 -25   11   NA   NA #> May 1949  -27  -39  -19 -14  -1  12 121 -8  -5  12  -11  -31   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]  # let's do some visual analysis plot(AirPassengers)                   # Plot the series - seasonal pattern is evident  plot(stl(AirPassengers, \"periodic\"))  # Seasonal decomposition  plot(D(AirPassengers,c(1,12),1:2))    # Plotting ordinary and seasonal first and second differences  plot(stl(window(D(AirPassengers,12),  # Taking seasonal differences removes most seasonal variation                 1950), \"periodic\"))    ## Time Series Matrix of 4 EU Stock Market Indicators, recorded 260 days per year plot(D(EuStockMarkets, c(0, 260)))                      # Plot series and annual differnces  mod <- lm(DAX ~., L(EuStockMarkets, c(0, 260)))         # Regressing the DAX on its annual lag summary(mod)                                            # and the levels and annual lags others #>  #> Call: #> lm(formula = DAX ~ ., data = L(EuStockMarkets, c(0, 260))) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -224.33  -57.02  -12.40   51.51  359.96  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept) -123.26123   59.74149  -2.063   0.0393 *   #> L260.DAX      -0.02126    0.02151  -0.988   0.3232     #> SMI            0.37415    0.01356  27.589   <2e-16 *** #> L260.SMI       0.28186    0.01901  14.826   <2e-16 *** #> CAC            0.52973    0.01544  34.305   <2e-16 *** #> L260.CAC      -0.23401    0.02145 -10.911   <2e-16 *** #> FTSE          -0.03944    0.01780  -2.215   0.0269 *   #> L260.FTSE      0.02888    0.02182   1.324   0.1858     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 84.02 on 1592 degrees of freedom #>   (260 observations deleted due to missingness) #> Multiple R-squared:  0.9943,\tAdjusted R-squared:  0.9942  #> F-statistic: 3.94e+04 on 7 and 1592 DF,  p-value: < 2.2e-16 #>  r <- residuals(mod)                                     # Obtain residuals pwcor(r, L(r))                                          # Residual Autocorrelation #> [1] .97 fFtest(r, L(r))                                         # F-test of residual autocorrelation #>     R-Sq.       DF1       DF2   F-Stat.   P-value  #>     0.937         1      1597 23690.699     0.000                                                          # (better use lmtest :: bgtest) modCO <- lm(QD1.DAX ~., D(L(EuStockMarkets, c(0, 260)), # Cochrane-Orcutt (1949) estimation                         rho = pwcor(r, L(r)))) summary(modCO) #>  #> Call: #> lm(formula = QD1.DAX ~ ., data = D(L(EuStockMarkets, c(0, 260)),  #>     rho = pwcor(r, L(r)))) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -87.131  -9.079  -0.439   9.228 119.993  #>  #> Coefficients: #>                 Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   -17.979391   2.094867  -8.583   <2e-16 *** #> QD1.L260.DAX    0.048116   0.034403   1.399    0.162     #> QD1.SMI         0.343808   0.013902  24.731   <2e-16 *** #> QD1.L260.SMI    0.014331   0.022530   0.636    0.525     #> QD1.CAC         0.459655   0.024406  18.834   <2e-16 *** #> QD1.L260.CAC   -0.031068   0.030598  -1.015    0.310     #> QD1.FTSE        0.220516   0.020682  10.662   <2e-16 *** #> QD1.L260.FTSE   0.007577   0.025948   0.292    0.770     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 19.06 on 1591 degrees of freedom #>   (261 observations deleted due to missingness) #> Multiple R-squared:  0.8582,\tAdjusted R-squared:  0.8576  #> F-statistic:  1376 on 7 and 1591 DF,  p-value: < 2.2e-16 #>  rCO <- residuals(modCO) fFtest(rCO, L(rCO))                                     # No more autocorrelation #>    R-Sq.      DF1      DF2  F-Stat.  P-value  #>    0.001        1     1596    2.326    0.127   ## World Development Panel Data head(fdiff(num_vars(wlddev), 1, 1,                      # Computes differences of numeric variables              wlddev$country, wlddev$year))              # fdiff requires external inputs.. #>   year decade PCGDP LIFEEX GINI        ODA    POP #> 1   NA     NA    NA     NA   NA         NA     NA #> 2    1      0    NA  0.516   NA  115310005 172437 #> 3    1      0    NA  0.509   NA -119240005 182031 #> 4    1      0    NA  0.500   NA  124880005 191764 #> 5    1      0    NA  0.492   NA   58200012 201576 #> 6    1      0    NA  0.485   NA   45919983 211539 head(D(wlddev, 1, 1, ~country, ~year))                  # Differences of numeric variables #>       country year D1.decade D1.PCGDP D1.LIFEEX D1.GINI     D1.ODA D1.POP #> 1 Afghanistan 1960        NA       NA        NA      NA         NA     NA #> 2 Afghanistan 1961         0       NA     0.516      NA  115310005 172437 #> 3 Afghanistan 1962         0       NA     0.509      NA -119240005 182031 #> 4 Afghanistan 1963         0       NA     0.500      NA  124880005 191764 #> 5 Afghanistan 1964         0       NA     0.492      NA   58200012 201576 #> 6 Afghanistan 1965         0       NA     0.485      NA   45919983 211539 head(D(wlddev, 1, 1, ~country))                         # Without t: Works because data is ordered #>       country D1.year D1.decade D1.PCGDP D1.LIFEEX D1.GINI     D1.ODA D1.POP #> 1 Afghanistan      NA        NA       NA        NA      NA         NA     NA #> 2 Afghanistan       1         0       NA     0.516      NA  115310005 172437 #> 3 Afghanistan       1         0       NA     0.509      NA -119240005 182031 #> 4 Afghanistan       1         0       NA     0.500      NA  124880005 191764 #> 5 Afghanistan       1         0       NA     0.492      NA   58200012 201576 #> 6 Afghanistan       1         0       NA     0.485      NA   45919983 211539 head(D(wlddev, 1, 1, PCGDP + LIFEEX ~ country, ~year))  # Difference of GDP & Life Expectancy #>       country year D1.PCGDP D1.LIFEEX #> 1 Afghanistan 1960       NA        NA #> 2 Afghanistan 1961       NA     0.516 #> 3 Afghanistan 1962       NA     0.509 #> 4 Afghanistan 1963       NA     0.500 #> 5 Afghanistan 1964       NA     0.492 #> 6 Afghanistan 1965       NA     0.485 head(D(wlddev, 0:1, 1, ~ country, ~year, cols = 9:10))  # Same, also retaining original series #>       country year PCGDP D1.PCGDP LIFEEX D1.LIFEEX #> 1 Afghanistan 1960    NA       NA 32.446        NA #> 2 Afghanistan 1961    NA       NA 32.962     0.516 #> 3 Afghanistan 1962    NA       NA 33.471     0.509 #> 4 Afghanistan 1963    NA       NA 33.971     0.500 #> 5 Afghanistan 1964    NA       NA 34.463     0.492 #> 6 Afghanistan 1965    NA       NA 34.948     0.485 head(D(wlddev, 0:1, 1, ~ country, ~year, 9:10,          # Dropping id columns        keep.ids = FALSE)) #>   PCGDP D1.PCGDP LIFEEX D1.LIFEEX #> 1    NA       NA 32.446        NA #> 2    NA       NA 32.962     0.516 #> 3    NA       NA 33.471     0.509 #> 4    NA       NA 33.971     0.500 #> 5    NA       NA 34.463     0.492 #> 6    NA       NA 34.948     0.485  ## Indexed computations: wldi <- findex_by(wlddev, iso3c, year)  # Dynamic Panel Data Models: summary(lm(D(PCGDP) ~ L(PCGDP) + D(LIFEEX), data = wldi))            # Simple case #>  #> Call: #> lm(formula = D(PCGDP) ~ L(PCGDP) + D(LIFEEX), data = wldi) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -16807.5   -107.5    -51.7    109.4  12574.6  #>  #> Indexed by:  iso3c [5] | year [4 (61)]  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  9.179e+01  1.340e+01   6.852 7.76e-12 *** #> L(PCGDP)     8.893e-03  5.577e-04  15.945  < 2e-16 *** #> D(LIFEEX)   -2.928e+01  2.393e+01  -1.224    0.221     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 828.2 on 8803 degrees of freedom #>   (4370 observations deleted due to missingness) #> Multiple R-squared:  0.02932,\tAdjusted R-squared:  0.0291  #> F-statistic: 132.9 on 2 and 8803 DF,  p-value: < 2.2e-16 #>  summary(lm(Dlog(PCGDP) ~ L(log(PCGDP)) + Dlog(LIFEEX), data = wldi)) # In log-differneces #>  #> Call: #> lm(formula = Dlog(PCGDP) ~ L(log(PCGDP)) + Dlog(LIFEEX), data = wldi) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -1.07097 -0.02055  0.00291  0.02570  0.85699  #>  #> Indexed by:  iso3c [5] | year [5 (61)]  #>  #> Coefficients: #>                Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   0.0095557  0.0037921   2.520   0.0118 *   #> L(log(PCGDP)) 0.0007847  0.0004352   1.803   0.0714 .   #> Dlog(LIFEEX)  0.5462310  0.0883588   6.182 6.61e-10 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 0.059 on 8803 degrees of freedom #>   (4370 observations deleted due to missingness) #> Multiple R-squared:  0.004326,\tAdjusted R-squared:  0.0041  #> F-statistic: 19.12 on 2 and 8803 DF,  p-value: 5.158e-09 #>  # Adding a lagged difference... summary(lm(D(PCGDP) ~ L(D(PCGDP, 0:1)) + L(D(LIFEEX), 0:1), data = wldi)) #>  #> Call: #> lm(formula = D(PCGDP) ~ L(D(PCGDP, 0:1)) + L(D(LIFEEX), 0:1),  #>     data = wldi) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -17019.6    -92.5    -41.8     93.5  12148.9  #>  #> Indexed by:  iso3c [4] | year [5 (61)]  #>  #> Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)            76.347493  13.265922   5.755 8.95e-09 *** #> L(D(PCGDP, 0:1))L1.--   0.004958   0.000550   9.014  < 2e-16 *** #> L(D(PCGDP, 0:1))L1.D1   0.320853   0.010302  31.144  < 2e-16 *** #> L(D(LIFEEX), 0:1)--   -46.694289  35.322935  -1.322    0.186     #> L(D(LIFEEX), 0:1)L1    19.980467  35.083154   0.570    0.569     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 789.4 on 8610 degrees of freedom #>   (4561 observations deleted due to missingness) #> Multiple R-squared:  0.1266,\tAdjusted R-squared:  0.1262  #> F-statistic:   312 on 4 and 8610 DF,  p-value: < 2.2e-16 #>  summary(lm(Dlog(PCGDP) ~ L(Dlog(PCGDP, 0:1)) + L(Dlog(LIFEEX), 0:1), data = wldi)) #>  #> Call: #> lm(formula = Dlog(PCGDP) ~ L(Dlog(PCGDP, 0:1)) + L(Dlog(LIFEEX),  #>     0:1), data = wldi) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -1.18359 -0.01869  0.00317  0.02277  1.04980  #>  #> Indexed by:  iso3c [5] | year [5 (61)]  #>  #> Coefficients: #>                               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)                  1.085e-02  3.656e-03   2.969 0.002998 **  #> L(Dlog(PCGDP, 0:1))L1.--     7.837e-05  4.190e-04   0.187 0.851614     #> L(Dlog(PCGDP, 0:1))L1.Dlog1  2.703e-01  1.017e-02  26.570  < 2e-16 *** #> L(Dlog(LIFEEX), 0:1)--      -9.415e-02  1.626e-01  -0.579 0.562630     #> L(Dlog(LIFEEX), 0:1)L1       5.539e-01  1.616e-01   3.428 0.000611 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 0.05602 on 8610 degrees of freedom #>   (4561 observations deleted due to missingness) #> Multiple R-squared:  0.08161,\tAdjusted R-squared:  0.08118  #> F-statistic: 191.3 on 4 and 8610 DF,  p-value: < 2.2e-16 #>  # Same thing: summary(lm(D1.PCGDP ~., data = L(D(wldi,0:1,1,9:10),0:1,keep.ids = FALSE)[,-1])) #>  #> Call: #> lm(formula = D1.PCGDP ~ ., data = L(D(wldi, 0:1, 1, 9:10), 0:1,  #>     keep.ids = FALSE)[, -1]) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -16776.5   -102.2    -17.2     91.5  12277.1  #>  #> Indexed by:  iso3c [4] | year [5 (61)]  #>  #> Coefficients: (1 not defined because of singularities) #>                Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  -3.339e+02  6.105e+01  -5.470 4.62e-08 *** #> L1.PCGDP      2.522e-03  6.527e-04   3.864 0.000112 *** #> L1.D1.PCGDP   3.171e-01  1.029e-02  30.815  < 2e-16 *** #> LIFEEX       -1.777e+01  3.548e+01  -0.501 0.616397     #> L1.LIFEEX     2.433e+01  3.538e+01   0.688 0.491595     #> D1.LIFEEX            NA         NA      NA       NA     #> L1.D1.LIFEEX  2.143e+01  3.499e+01   0.612 0.540244     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 787.3 on 8609 degrees of freedom #>   (4561 observations deleted due to missingness) #> Multiple R-squared:  0.1314,\tAdjusted R-squared:  0.1309  #> F-statistic: 260.4 on 5 and 8609 DF,  p-value: < 2.2e-16 #>   ## Grouped data library(magrittr) wlddev |> fgroup_by(country) |>              fselect(PCGDP,LIFEEX) |> fdiff(0:1,1:2)       # Adding a first and second difference #>    PCGDP D1.PCGDP D2.PCGDP LIFEEX D1.LIFEEX D2.LIFEEX #> 1     NA       NA       NA 32.446        NA        NA #> 2     NA       NA       NA 32.962     0.516        NA #> 3     NA       NA       NA 33.471     0.509    -0.007 #> 4     NA       NA       NA 33.971     0.500    -0.009 #> 5     NA       NA       NA 34.463     0.492    -0.008 #> 6     NA       NA       NA 34.948     0.485    -0.007 #> 7     NA       NA       NA 35.430     0.482    -0.003 #> 8     NA       NA       NA 35.914     0.484     0.002 #> 9     NA       NA       NA 36.403     0.489     0.005 #> 10    NA       NA       NA 36.900     0.497     0.008 #> 11    NA       NA       NA 37.409     0.509     0.012 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13165 rows ] #>  #> Grouped by:  country  [216 | 61 (0)]  wlddev |> fgroup_by(country) |>              fselect(year,PCGDP,LIFEEX) |> D(0:1,1:2,year) # Also using t (safer) #>    year PCGDP D1.PCGDP D2.PCGDP LIFEEX D1.LIFEEX D2.LIFEEX #> 1  1960    NA       NA       NA 32.446        NA        NA #> 2  1961    NA       NA       NA 32.962     0.516        NA #> 3  1962    NA       NA       NA 33.471     0.509    -0.007 #> 4  1963    NA       NA       NA 33.971     0.500    -0.009 #> 5  1964    NA       NA       NA 34.463     0.492    -0.008 #> 6  1965    NA       NA       NA 34.948     0.485    -0.007 #> 7  1966    NA       NA       NA 35.430     0.482    -0.003 #> 8  1967    NA       NA       NA 35.914     0.484     0.002 #> 9  1968    NA       NA       NA 36.403     0.489     0.005 #> 10 1969    NA       NA       NA 36.900     0.497     0.008 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13166 rows ] #>  #> Grouped by:  country  [216 | 61 (0)]  wlddev |> fgroup_by(country) |>                            # Dropping id's              fselect(year,PCGDP,LIFEEX) |> D(0:1,1:2,year, keep.ids = FALSE) #>    PCGDP D1.PCGDP D2.PCGDP LIFEEX D1.LIFEEX D2.LIFEEX #> 1     NA       NA       NA 32.446        NA        NA #> 2     NA       NA       NA 32.962     0.516        NA #> 3     NA       NA       NA 33.471     0.509    -0.007 #> 4     NA       NA       NA 33.971     0.500    -0.009 #> 5     NA       NA       NA 34.463     0.492    -0.008 #> 6     NA       NA       NA 34.948     0.485    -0.007 #> 7     NA       NA       NA 35.430     0.482    -0.003 #> 8     NA       NA       NA 35.914     0.484     0.002 #> 9     NA       NA       NA 36.403     0.489     0.005 #> 10    NA       NA       NA 36.900     0.497     0.008 #> 11    NA       NA       NA 37.409     0.509     0.012 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13165 rows ] #>  #> Grouped by:  country  [216 | 61 (0)]"},{"path":"https://sebkrantz.github.io/collapse/reference/fdist.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast and Flexible Distance Computations — fdist","title":"Fast and Flexible Distance Computations — fdist","text":"fast flexible replacement dist, compute euclidean distances.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast and Flexible Distance Computations — fdist","text":"","code":"fdist(x, v = NULL, ..., method = \"euclidean\", nthreads = .op[[\"nthreads\"]])"},{"path":"https://sebkrantz.github.io/collapse/reference/fdist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast and Flexible Distance Computations — fdist","text":"x numeric vector matrix. Data frames/lists can passed converted matrix using qM. Non-numeric (double) inputs coerced. v (optional) numeric (double) vector length(v) == NCOL(x), compute distances (rows ) x. vector types coerced. ... used. placeholder possible future arguments. method integer character string indicating method computing distances.  nthreads integer. number threads use. v = NULL (full distance matrix), multithreading along distance matrix columns (decreasing thread loads matrix lower triangular). v supplied, multithreading sub-column level (across elements).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast and Flexible Distance Computations — fdist","text":"v = NULL, full lower-triangular distance matrix rows x computed returned 'dist' object (methods apply, see dist). Otherwise, numeric vector distances row x v returned. See Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdist.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast and Flexible Distance Computations — fdist","text":"fdist check missing values, NA's result NA distances. kit::topn suitable complimentary function find nearest neighbors. efficient skips missing values default.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fdist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast and Flexible Distance Computations — fdist","text":"","code":"# Distance matrix m = as.matrix(mtcars) str(fdist(m)) # Same as dist(m) #>  'dist' num [1:496] 0.615 54.909 98.113 210.337 65.472 ... #>  - attr(*, \"Size\")= int 32 #>  - attr(*, \"Labels\")= chr [1:32] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Datsun 710\" \"Hornet 4 Drive\" ... #>  - attr(*, \"Diag\")= logi FALSE #>  - attr(*, \"Upper\")= logi FALSE #>  - attr(*, \"method\")= chr \"euclidean\"  # Distance with vector d = fdist(m, fmean(m)) kit::topn(d, 5)  # Index of 5 nearest neighbours #> [1] 15 16 17 31 19  # Mahalanobis distance m_mahal = t(forwardsolve(t(chol(cov(m))), t(m))) fdist(m_mahal, fmean(m_mahal)) #>  [1] 2.991099 2.878877 2.989507 2.469155 2.330035 2.979523 3.022627 3.167072 #>  [9] 4.753222 3.520384 3.325489 3.078332 2.365275 2.454885 3.346836 2.944842 #> [17] 3.501088 3.013076 3.867089 3.208810 3.665023 2.495443 2.405554 3.417825 #> [25] 2.591927 1.909395 4.284409 3.741747 4.644675 3.339588 4.380911 3.144643 sqrt(unattrib(mahalanobis(m, fmean(m), cov(m)))) #>  [1] 2.991099 2.878877 2.989507 2.469155 2.330035 2.979523 3.022627 3.167072 #>  [9] 4.753222 3.520384 3.325489 3.078332 2.365275 2.454885 3.346836 2.944842 #> [17] 3.501088 3.013076 3.867089 3.208810 3.665023 2.495443 2.405554 3.417825 #> [25] 2.591927 1.909395 4.284409 3.741747 4.644675 3.339588 4.380911 3.144643 # \\donttest{ # Distance of two vectors x <- rnorm(1e6) y <- rnorm(1e6) microbenchmark::microbenchmark(   fdist(x, y),   fdist(x, y, nthreads = 2),   sqrt(sum((x-y)^2)) ) #> Unit: microseconds #>                       expr      min        lq      mean   median        uq #>                fdist(x, y)  926.559  928.0145  935.4503  930.659  936.9935 #>  fdist(x, y, nthreads = 2)  924.796  926.9690  934.2604  928.855  935.4560 #>       sqrt(sum((x - y)^2)) 2632.446 2667.7675 3552.7898 2716.189 2881.2545 #>        max neval #>   1007.534   100 #>   1041.072   100 #>  21224.839   100 # }"},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Removal of Unused Factor Levels — fdroplevels","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"substantially faster replacement droplevels.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"","code":"fdroplevels(x, ...)  # S3 method for class 'factor' fdroplevels(x, ...)  # S3 method for class 'data.frame' fdroplevels(x, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"x factor, data frame / list containing one factors. ... used.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"droplevels passes factor levels dropped factor, first calls unique match drop unused levels. functions internally use hash table, highly inefficient. fdroplevels require mapping values , uses super fast boolean vector method determine levels unused remove levels. addition, unused levels found, x simply returned. missing values found x efficiently skipped process checking replacing levels. attributes x preserved.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"x unused factor levels removed.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"x malformed e.g. levels, function can cause segmentation fault terminating R session, thus use ordinary / proper factors.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fdroplevels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Removal of Unused Factor Levels — fdroplevels","text":"","code":"f <- iris$Species[1:100] fdroplevels(f) #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     setosa     setosa     setosa     #> [37] setosa     setosa     setosa     setosa     setosa     setosa     #> [43] setosa     setosa     setosa     setosa     setosa     setosa     #> [49] setosa     setosa     versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] versicolor versicolor versicolor versicolor #>  [ reached 'max' / getOption(\"max.print\") -- omitted 30 entries ] #> Levels: setosa versicolor identical(fdroplevels(f), droplevels(f)) #> [1] TRUE  fNA <- na_insert(f) fdroplevels(fNA) #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] <NA>       setosa     setosa     <NA>       <NA>       setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     <NA>       setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     <NA>       setosa     setosa     #> [37] setosa     setosa     setosa     setosa     setosa     setosa     #> [43] setosa     setosa     setosa     setosa     setosa     setosa     #> [49] setosa     <NA>       versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor <NA>       versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] <NA>       versicolor versicolor versicolor #>  [ reached 'max' / getOption(\"max.print\") -- omitted 30 entries ] #> Levels: setosa versicolor identical(fdroplevels(fNA), droplevels(fNA)) #> [1] TRUE  identical(fdroplevels(ss(iris, 1:100)), droplevels(ss(iris, 1:100))) #> [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/reference/ffirst_flast.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","title":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","text":"ffirst flast S3 generic functions (column-wise) returns first last values x, (optionally) grouped g. TRA argument can used transform x using (groupwise) first last values.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ffirst_flast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","text":"","code":"ffirst(x, ...) flast(x, ...)  # Default S3 method ffirst(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],        use.g.names = TRUE, ...) # Default S3 method flast(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, ...)  # S3 method for class 'matrix' ffirst(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],        use.g.names = TRUE, drop = TRUE, ...) # S3 method for class 'matrix' flast(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'data.frame' ffirst(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],        use.g.names = TRUE, drop = TRUE, ...) # S3 method for class 'data.frame' flast(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'grouped_df' ffirst(x, TRA = NULL, na.rm = .op[[\"na.rm\"]],        use.g.names = FALSE, keep.group_vars = TRUE, ...) # S3 method for class 'grouped_df' flast(x, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = FALSE, keep.group_vars = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/ffirst_flast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","text":"x vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. TRUE skips missing values returns first / last non-missing value .e. first (1) / last (n) value NA, take second (2) / second--last (n-1) value etc.. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ffirst_flast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","text":"ffirst returns first value x, grouped g, (TRA used) x transformed first value, grouped g. Similarly flast returns last value x, ...","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ffirst_flast.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","text":"functions significantly faster na.rm = FALSE, particularly ffirst can take direct advantage 'group.starts' elements GRP objects.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/ffirst_flast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped) First and Last Value for Matrix-Like Objects — ffirst-flast","text":"","code":"## default vector method ffirst(airquality$Ozone)                   # Simple first value #> [1] 41 ffirst(airquality$Ozone, airquality$Month) # Grouped first value #>   5   6   7   8   9  #>  41  29 135  39  96  ffirst(airquality$Ozone, airquality$Month,        na.rm = FALSE)                      # Grouped first, but without skipping initial NA's #>   5   6   7   8   9  #>  41  NA 135  39  96   ## data.frame method ffirst(airquality) #>   Ozone Solar.R    Wind    Temp   Month     Day  #>    41.0   190.0     7.4    67.0     5.0     1.0  ffirst(airquality, airquality$Month) #>   Ozone Solar.R Wind Temp Month Day #> 5    41     190  7.4   67     5   1 #> 6    29     286  8.6   78     6   1 #> 7   135     269  4.1   84     7   1 #> 8    39      83  6.9   81     8   1 #> 9    96     167  6.9   91     9   1 ffirst(airquality, airquality$Month, na.rm = FALSE) # Again first Ozone measurement in month 6 is NA #>   Ozone Solar.R Wind Temp Month Day #> 5    41     190  7.4   67     5   1 #> 6    NA     286  8.6   78     6   1 #> 7   135     269  4.1   84     7   1 #> 8    39      83  6.9   81     8   1 #> 9    96     167  6.9   91     9   1  ## matrix method aqm <- qM(airquality) ffirst(aqm) #>   Ozone Solar.R    Wind    Temp   Month     Day  #>    41.0   190.0     7.4    67.0     5.0     1.0  ffirst(aqm, airquality$Month) # etc.. #>   Ozone Solar.R Wind Temp Month Day #> 5    41     190  7.4   67     5   1 #> 6    29     286  8.6   78     6   1 #> 7   135     269  4.1   84     7   1 #> 8    39      83  6.9   81     8   1 #> 9    96     167  6.9   91     9   1   ## method for grouped data frames - created with dplyr::group_by or fgroup_by library(dplyr) airquality |> group_by(Month) |> ffirst() #> # A tibble: 5 × 6 #>   Month Ozone Solar.R  Wind  Temp   Day #>   <int> <int>   <int> <dbl> <int> <int> #> 1     5    41     190   7.4    67     1 #> 2     6    29     286   8.6    78     1 #> 3     7   135     269   4.1    84     1 #> 4     8    39      83   6.9    81     1 #> 5     9    96     167   6.9    91     1 airquality |> group_by(Month) |> select(Ozone) |> ffirst(na.rm = FALSE) #> Adding missing grouping variables: `Month` #> # A tibble: 5 × 2 #>   Month Ozone #>   <int> <int> #> 1     5    41 #> 2     6    NA #> 3     7   135 #> 4     8    39 #> 5     9    96  # Note: All examples generalize to flast."},{"path":"https://sebkrantz.github.io/collapse/reference/fgrowth.html","id":null,"dir":"Reference","previous_headings":"","what":" Fast Growth Rates for Time Series and Panel Data — fgrowth","title":" Fast Growth Rates for Time Series and Panel Data — fgrowth","text":"fgrowth S3 generic compute (sequences ) suitably lagged / leaded, iterated compounded growth rates, obtained via exact method computation log differencing. default growth rates provided percentage terms, scale factor can applied. growth operator G parsimonious wrapper around fgrowth, also provides flexibility applied data frames.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fgrowth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":" Fast Growth Rates for Time Series and Panel Data — fgrowth","text":"","code":"fgrowth(x, n = 1, diff = 1, ...)       G(x, n = 1, diff = 1, ...)  # Default S3 method fgrowth(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA,         logdiff = FALSE, scale = 100, power = 1, stubs = TRUE, ...) # Default S3 method G(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, logdiff = FALSE,   scale = 100, power = 1, stubs = .op[[\"stub\"]], ...)  # S3 method for class 'matrix' fgrowth(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA,         logdiff = FALSE, scale = 100, power = 1,         stubs = length(n) + length(diff) > 2L, ...) # S3 method for class 'matrix' G(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA, logdiff = FALSE,   scale = 100, power = 1, stubs = .op[[\"stub\"]], ...)  # S3 method for class 'data.frame' fgrowth(x, n = 1, diff = 1, g = NULL, t = NULL, fill = NA,         logdiff = FALSE, scale = 100, power = 1,         stubs = length(n) + length(diff) > 2L, ...) # S3 method for class 'data.frame' G(x, n = 1, diff = 1, by = NULL, t = NULL, cols = is.numeric,   fill = NA, logdiff = FALSE, scale = 100, power = 1, stubs = .op[[\"stub\"]],   keep.ids = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fgrowth(x, n = 1, diff = 1, fill = NA, logdiff = FALSE, scale = 100,         power = 1, stubs = length(n) + length(diff) > 2L, shift = \"time\", ...) # S3 method for class 'pseries' G(x, n = 1, diff = 1, fill = NA, logdiff = FALSE, scale = 100,   power = 1, stubs = .op[[\"stub\"]], shift = \"time\", ...)  # S3 method for class 'pdata.frame' fgrowth(x, n = 1, diff = 1, fill = NA, logdiff = FALSE, scale = 100,         power = 1, stubs = length(n) + length(diff) > 2L, shift = \"time\", ...) # S3 method for class 'pdata.frame' G(x, n = 1, diff = 1, cols = is.numeric, fill = NA, logdiff = FALSE,   scale = 100, power = 1, stubs = .op[[\"stub\"]], shift = \"time\", keep.ids = TRUE, ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' fgrowth(x, n = 1, diff = 1, t = NULL, fill = NA, logdiff = FALSE,         scale = 100, power = 1, stubs = length(n) + length(diff) > 2L,         keep.ids = TRUE, ...) # S3 method for class 'grouped_df' G(x, n = 1, diff = 1, t = NULL, fill = NA, logdiff = FALSE,   scale = 100, power = 1, stubs = .op[[\"stub\"]], keep.ids = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fgrowth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":" Fast Growth Rates for Time Series and Panel Data — fgrowth","text":"x numeric vector / time series, (time series) matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). n integer. vector indicating number lags leads. diff integer. vector integers > 1 indicating order taking growth rates, e.g. diff = 2 means computing growth rate growth rate. g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. Note without t, values group need consecutive right order. See Details flag. data.frame method: g, also allows one- two-sided formulas .e. ~ group1 var1 + var2 ~ group1 + group2. See Examples. t time vector list vectors. See flag. cols data.frame method: Select columns compute growth rates using function, column names, indices logical vector. Default: numeric variables. Note: cols ignored two-sided formula passed . fill value insert vectors shifted. Default NA. logdiff logical. Compute log-difference growth rates instead exact growth rates. See Details. scale logical. Scale factor post-applied growth rates, default 100 gives growth rates percentage terms. See Details. power numeric. Apply power annualize compound growth rates e.g. fgrowth(AirPassengers, 12, power = 1/12) equivalent ((AirPassengers/flag(AirPassengers, 12))^(1/12)-1)*100. stubs logical. TRUE (default) rename computed columns adding prefix \"LnGdiff.\" / \"FnGdiff.\", \"LnDlogdiff.\" / \"FnDlogdiff.\" logdiff = TRUE. shift pseries / pdata.frame methods: character. \"time\" \"row\". See flag details. keep.ids data.frame / pdata.frame / grouped_df methods: Logical. Drop identifiers output (includes variables passed t using formulas). Note: 'grouped_df' / 'pdata.frame' identifiers dropped, \"groups\" / \"index\" attributes kept. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fgrowth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":" Fast Growth Rates for Time Series and Panel Data — fgrowth","text":"fgrowth/G default computes exact growth rates using repeat(diff) ((x[]/x[-n])^power - 1)*scale, diff > 1 computes growth rate growth rates. logdiff = TRUE, approximate growth rates computed using log(x[]/x[-n])*scale diff = 1 repeat(diff-1) x[] - x[-n] thereafter (usually diff = 1 log-differencing). details see help pages fdiff flag.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fgrowth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":" Fast Growth Rates for Time Series and Panel Data — fgrowth","text":"x growth rate taken diff times using lags n , scaled scale. Computations can grouped g//ordered t. See Details Examples.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fgrowth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":" Fast Growth Rates for Time Series and Panel Data — fgrowth","text":"","code":"## Simple Time Series: AirPassengers G(AirPassengers)                      # Growth rate, same as fgrowth(AirPassengers) #>              Jan         Feb         Mar         Apr         May         Jun #> 1949          NA   5.3571429  11.8644068  -2.2727273  -6.2015504  11.5702479 #> 1950  -2.5423729   9.5652174  11.9047619  -4.2553191  -7.4074074  19.2000000 #> 1951   3.5714286   3.4482759  18.6666667  -8.4269663   5.5214724   3.4883721 #> 1952   3.0120482   5.2631579   7.2222222  -6.2176166   1.1049724  19.1256831 #> 1953   1.0309278   0.0000000  20.4081633  -0.4237288  -2.5531915   6.1135371 #>              Jul         Aug         Sep         Oct         Nov         Dec #> 1949   9.6296296   0.0000000  -8.1081081 -12.5000000 -12.6050420  13.4615385 #> 1950  14.0939597   0.0000000  -7.0588235 -15.8227848 -14.2857143  22.8070175 #> 1951  11.7977528   0.0000000  -7.5376884 -11.9565217  -9.8765432  13.6986301 #> 1952   5.5045872   5.2173913 -13.6363636  -8.6124402  -9.9476440  12.7906977 #> 1953   8.6419753   3.0303030 -12.8676471 -10.9704641 -14.6919431  11.6666667 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] G(AirPassengers, logdiff = TRUE)      # Log-difference #>              Jan         Feb         Mar         Apr         May         Jun #> 1949          NA   5.2185753  11.2117298  -2.2989518  -6.4021859  10.9484233 #> 1950  -2.5752496   9.1349779  11.2477983  -4.3485112  -7.6961041  17.5632569 #> 1951   3.5091320   3.3901552  17.1148256  -8.8033349   5.3744276   3.4289073 #> 1952   2.9675768   5.1293294   6.9733338  -6.4193158   1.0989122  17.5008910 #> 1953   1.0256500   0.0000000  18.5717146  -0.4246291  -2.5863511   5.9339440 #>              Jul         Aug         Sep         Oct         Nov         Dec #> 1949   9.1937495   0.0000000  -8.4557388 -13.3531393 -13.4732594  12.6293725 #> 1950  13.1852131   0.0000000  -7.3203404 -17.2245905 -15.4150680  20.5443974 #> 1951  11.1521274   0.0000000  -7.8369067 -12.7339422 -10.3989714  12.8381167 #> 1952   5.3584246   5.0858417 -14.6603474  -9.0060824 -10.4778951  12.0363682 #> 1953   8.2887660   2.9852963 -13.7741925 -11.6202008 -15.8901283  11.0348057 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] G(AirPassengers, 1, 2)                # Growth rate of growth rate #>                Jan           Feb           Mar           Apr           May #> 1949            NA            NA   121.4689266  -119.1558442   172.8682171 #> 1950  -118.8861985  -476.2318841    24.4588745  -135.7446809    74.0740741 #> 1951   -84.3406593    -3.4482759   441.3333333  -145.1444623  -165.5214724 #> 1952   -78.0120482    74.7368421    37.2222222  -186.0900757  -117.7716390 #> 1953   -91.9400187  -100.0000000           Inf  -102.0762712   502.5531915 #>                Jun           Jul           Aug           Sep           Oct #> 1949  -286.5702479   -16.7724868  -100.0000000          -Inf    54.1666667 #> 1950  -359.2000000   -26.5939597  -100.0000000          -Inf   124.1561181 #> 1951   -36.8217054   238.2022472  -100.0000000          -Inf    58.6231884 #> 1952  1630.8743169   -71.2188729    -5.2173913  -361.3636364   -36.8421053 #> 1953  -339.4468705    41.3580247   -64.9350649  -524.6323529   -14.7438216 #>                Nov           Dec #> 1949     0.8403361  -206.7948718 #> 1950    -9.7142857  -259.6491228 #> 1951   -17.3961841  -238.6986301 #> 1952    15.5031995  -228.5801714 #> 1953    33.9227124  -179.4086022 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] G(AirPassengers, 12)                  # Seasonal growth rate (data is monthly) #>             Jan        Feb        Mar        Apr        May        Jun #> 1949         NA         NA         NA         NA         NA         NA #> 1950  2.6785714  6.7796610  6.8181818  4.6511628  3.3057851 10.3703704 #> 1951 26.0869565 19.0476190 26.2411348 20.7407407 37.6000000 19.4630872 #> 1952 17.9310345 20.0000000  8.4269663 11.0429448  6.3953488 22.4719101 #> 1953 14.6198830  8.8888889 22.2797927 29.8342541 25.1366120 11.4678899 #>             Jul        Aug        Sep        Oct        Nov        Dec #> 1949         NA         NA         NA         NA         NA         NA #> 1950 14.8648649 14.8648649 16.1764706 11.7647059  9.6153846 18.6440678 #> 1951 17.0588235 17.0588235 16.4556962 21.8045113 28.0701754 18.5714286 #> 1952 15.5778894 21.6080402 13.5869565 17.9012346 17.8082192 16.8674699 #> 1953 14.7826087 12.3966942 13.3971292 10.4712042  4.6511628  3.6082474 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ]  head(G(AirPassengers, -2:2, 1:3))     # Sequence of leaded/lagged and iterated growth rates #>                F2G1       F2G2       F2G3        FG1        FG2         FG3  -- #> Jan 1949 -15.151515 -266.66667   77.97753  -5.084746  -52.05811   -90.63805 112 #> Feb 1949  -8.527132   91.86047 -285.94592 -10.606061 -556.06061   757.77959 118 #> Mar 1949   9.090909 -149.83165  -51.15620   2.325581  -64.82558   -60.41293 132 #> Apr 1949  -4.444444  -49.40171  -63.68691   6.611570 -163.75443 -1006.58997 129 #> May 1949 -18.243243 -306.75676  330.09195 -10.370370   18.06268         NaN 121 #>                 G1        G2        G3      L2G1       L2G2 L2G3 #> Jan 1949        NA        NA        NA        NA         NA   NA #> Feb 1949  5.357143        NA        NA        NA         NA   NA #> Mar 1949 11.864407  121.4689        NA 17.857143         NA   NA #> Apr 1949 -2.272727 -119.1558 -198.0957  9.322034         NA   NA #> May 1949 -6.201550  172.8682 -245.0774 -8.333333 -146.66667   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]  # let's do some visual analysis plot(G(AirPassengers, c(0, 1, 12)))  plot(stl(window(G(AirPassengers, 12), # Taking seasonal growth rate removes most seasonal variation                 1950), \"periodic\"))    ## Time Series Matrix of 4 EU Stock Market Indicators, recorded 260 days per year plot(G(EuStockMarkets,c(0,260)))                    # Plot series and annual growth rates  summary(lm(L260G1.DAX ~., G(EuStockMarkets,260)))   # Annual growth rate of DAX regressed on the #>  #> Call: #> lm(formula = L260G1.DAX ~ ., data = G(EuStockMarkets, 260)) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -19.5094  -4.7763   0.4586   5.0337  18.2316  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  4.48795    0.38357   11.70  < 2e-16 *** #> L260G1.SMI   0.37048    0.02635   14.06  < 2e-16 *** #> L260G1.CAC   0.82319    0.02092   39.34  < 2e-16 *** #> L260G1.FTSE -0.25008    0.03883   -6.44 1.58e-10 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 7.817 on 1596 degrees of freedom #>   (260 observations deleted due to missingness) #> Multiple R-squared:  0.8585,\tAdjusted R-squared:  0.8582  #> F-statistic:  3226 on 3 and 1596 DF,  p-value: < 2.2e-16 #>                                                      # growth rates of the other indicators  ## World Development Panel Data head(fgrowth(num_vars(wlddev), 1, 1,                    # Computes growth rates of numeric variables              wlddev$country, wlddev$year))              # fgrowth requires external inputs.. #>         year decade PCGDP   LIFEEX GINI       ODA      POP #> 1         NA     NA    NA       NA   NA        NA       NA #> 2 0.05102041      0    NA 1.590335   NA  98.74969 1.916611 #> 3 0.05099439      0    NA 1.544202   NA -51.37884 1.985199 #> 4 0.05096840      0    NA 1.493830   NA 110.66998 2.050636 #> 5 0.05094244      0    NA 1.448294   NA  24.48259 2.112246 #> 6 0.05091650      0    NA 1.407306   NA  15.51770 2.170793 head(G(wlddev, 1, 1, ~country, ~year))                  # Growth of numeric variables, id's attached #>       country year G1.decade G1.PCGDP G1.LIFEEX G1.GINI    G1.ODA   G1.POP #> 1 Afghanistan 1960        NA       NA        NA      NA        NA       NA #> 2 Afghanistan 1961         0       NA  1.590335      NA  98.74969 1.916611 #> 3 Afghanistan 1962         0       NA  1.544202      NA -51.37884 1.985199 #> 4 Afghanistan 1963         0       NA  1.493830      NA 110.66998 2.050636 #> 5 Afghanistan 1964         0       NA  1.448294      NA  24.48259 2.112246 #> 6 Afghanistan 1965         0       NA  1.407306      NA  15.51770 2.170793 head(G(wlddev, 1, 1, ~country))                         # Without t: Works because data is ordered #>       country    G1.year G1.decade G1.PCGDP G1.LIFEEX G1.GINI    G1.ODA #> 1 Afghanistan         NA        NA       NA        NA      NA        NA #> 2 Afghanistan 0.05102041         0       NA  1.590335      NA  98.74969 #> 3 Afghanistan 0.05099439         0       NA  1.544202      NA -51.37884 #> 4 Afghanistan 0.05096840         0       NA  1.493830      NA 110.66998 #> 5 Afghanistan 0.05094244         0       NA  1.448294      NA  24.48259 #> 6 Afghanistan 0.05091650         0       NA  1.407306      NA  15.51770 #>     G1.POP #> 1       NA #> 2 1.916611 #> 3 1.985199 #> 4 2.050636 #> 5 2.112246 #> 6 2.170793 head(G(wlddev, 1, 1, PCGDP + LIFEEX ~ country, ~year))  # Growth of GDP per Capita & Life Expectancy #>       country year G1.PCGDP G1.LIFEEX #> 1 Afghanistan 1960       NA        NA #> 2 Afghanistan 1961       NA  1.590335 #> 3 Afghanistan 1962       NA  1.544202 #> 4 Afghanistan 1963       NA  1.493830 #> 5 Afghanistan 1964       NA  1.448294 #> 6 Afghanistan 1965       NA  1.407306 head(G(wlddev, 0:1, 1, ~ country, ~year, cols = 9:10))  # Same, also retaining original series #>       country year PCGDP G1.PCGDP LIFEEX G1.LIFEEX #> 1 Afghanistan 1960    NA       NA 32.446        NA #> 2 Afghanistan 1961    NA       NA 32.962  1.590335 #> 3 Afghanistan 1962    NA       NA 33.471  1.544202 #> 4 Afghanistan 1963    NA       NA 33.971  1.493830 #> 5 Afghanistan 1964    NA       NA 34.463  1.448294 #> 6 Afghanistan 1965    NA       NA 34.948  1.407306 head(G(wlddev, 0:1, 1, ~ country, ~year, 9:10,          # Dropping id columns        keep.ids = FALSE)) #>   PCGDP G1.PCGDP LIFEEX G1.LIFEEX #> 1    NA       NA 32.446        NA #> 2    NA       NA 32.962  1.590335 #> 3    NA       NA 33.471  1.544202 #> 4    NA       NA 33.971  1.493830 #> 5    NA       NA 34.463  1.448294 #> 6    NA       NA 34.948  1.407306"},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":null,"dir":"Reference","previous_headings":"","what":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"fhdbetween generalization fbetween efficiently predict multiple factors linear models (.e. predict vectors/factors, matrices, data frames/lists latter may contain multiple factor variables). Similarly, fhdwithin generalization fwithin center multiple factors partial-linear models. corresponding operators HDB HDW additionally allow predict / partial full lm() formulas interactions variables.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"","code":"fhdbetween(x, ...)  fhdwithin(x, ...)        HDB(x, ...)        HDW(x, ...)  # Default S3 method fhdbetween(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, lm.method = \"qr\", ...) # Default S3 method fhdwithin(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, lm.method = \"qr\", ...) # Default S3 method HDB(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, lm.method = \"qr\", ...) # Default S3 method HDW(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, lm.method = \"qr\", ...)  # S3 method for class 'matrix' fhdbetween(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, lm.method = \"qr\", ...) # S3 method for class 'matrix' fhdwithin(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, lm.method = \"qr\", ...) # S3 method for class 'matrix' HDB(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, stub = .op[[\"stub\"]],     lm.method = \"qr\", ...) # S3 method for class 'matrix' HDW(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE, stub = .op[[\"stub\"]],     lm.method = \"qr\", ...)  # S3 method for class 'data.frame' fhdbetween(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE,            variable.wise = FALSE, lm.method = \"qr\", ...) # S3 method for class 'data.frame' fhdwithin(x, fl, w = NULL, na.rm = .op[[\"na.rm\"]], fill = FALSE,           variable.wise = FALSE, lm.method = \"qr\", ...) # S3 method for class 'data.frame' HDB(x, fl, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]], fill = FALSE,     variable.wise = FALSE, stub = .op[[\"stub\"]], lm.method = \"qr\", ...) # S3 method for class 'data.frame' HDW(x, fl, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]], fill = FALSE,     variable.wise = FALSE, stub = .op[[\"stub\"]], lm.method = \"qr\", ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fhdbetween(x, effect = \"all\", w = NULL, na.rm = .op[[\"na.rm\"]], fill = TRUE, ...) # S3 method for class 'pseries' fhdwithin(x, effect = \"all\", w = NULL, na.rm = .op[[\"na.rm\"]], fill = TRUE, ...) # S3 method for class 'pseries' HDB(x, effect = \"all\", w = NULL, na.rm = .op[[\"na.rm\"]], fill = TRUE, ...) # S3 method for class 'pseries' HDW(x, effect = \"all\", w = NULL, na.rm = .op[[\"na.rm\"]], fill = TRUE, ...)  # S3 method for class 'pdata.frame' fhdbetween(x, effect = \"all\", w = NULL, na.rm = .op[[\"na.rm\"]], fill = TRUE,            variable.wise = TRUE, ...) # S3 method for class 'pdata.frame' fhdwithin(x, effect = \"all\", w = NULL, na.rm = .op[[\"na.rm\"]], fill = TRUE,           variable.wise = TRUE, ...) # S3 method for class 'pdata.frame' HDB(x, effect = \"all\", w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],     fill = TRUE, variable.wise = TRUE, stub = .op[[\"stub\"]], ...) # S3 method for class 'pdata.frame' HDW(x, effect = \"all\", w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],     fill = TRUE, variable.wise = TRUE, stub = .op[[\"stub\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"x numeric vector, matrix, data frame, 'indexed_series' ('pseries') 'indexed_frame' ('pdata.frame'). fl numeric vector, factor, matrix, data frame list (may may contain factors). HDW/HDB data frame method fl can also one-two sided lm() formula variables contained x. Interactions (:) full interactions (*) supported. See Examples Note. w vector (non-negative) weights. cols data.frame methods: Select columns center (partial-) predict using column names, indices, logical vector function. Unless specified otherwise numeric columns selected. NULL, columns selected. na.rm remove missing values x fl. default rows missing values x fl removed. case attribute \"na.rm\" attached containing rows removed. fill na.rm = TRUE, fill = TRUE remove rows missing values x fl, fill NA's. variable.wise (p)data.frame methods: Setting variable.wise = TRUE process column individually .e. use non-missing cases column fl (fl checked missing values na.rm = TRUE). lot less efficient uses data available column. effect plm methods: Select panel identifiers used centering. 1L takes first variable index, 2L second etc.. Index variables can also called name using character vector. keyword \"\" uses identifiers. stub character. prefix/stub add names transformed columns. TRUE (default) uses \"HDW.\"/\"HDB.\", FALSE rename columns. lm.method character. linear fitting method. Supported \"chol\" \"qr\". See flm. ... arguments passed fixest::demean (notes im_confident) chol / qr. Possible choices tol set uniform numerical tolerance entire fitting process, nthreads iter govern higher-order centering process.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"fhdbetween/HDB fhdwithin/HDW powerful functions high-dimensional linear prediction problems involving large factors datasets, can just well handle ordinary regression problems. implemented efficient wrappers around fbetween / fwithin, flm C++ code fixest package imported higher-order centering tasks (thus fixest needs installed problems involving one factor). Intended areas use efficiently obtain residuals predicted values data, prepare data complex linear models involving multiple levels fixed effects. models can now fitted using (g)lm() data prepared fhdwithin / HDW (relying bootstrapped SE's inference, implementing appropriate corrections). See Examples. fl vector matrix, result identical lm .e. fhdbetween / HDB returns fitted(lm(x ~ fl)) fhdwithin / HDW residuals(lm(x ~ fl)). fl list containing factors, variables x non-factor variables fl centered factors using either fbetween / fwithin single factor fixest C++ code multiple factors. Afterwards centered data regressed centered predictors. fl just list factors, fhdwithin/HDW returns centered data fhdbetween/HDB corresponding means. Take general example list fl = list(fct1, fct2, ..., var1, var2, ...) fcti factors vari continuous variables. output fhdwithin/HDW | fhdbetween/HDB identical calling resid | fitted lm(x ~ fct1 + fct2 + ... + var1 + var2 + ...). computations performed fhdwithin/HDW fhdbetween/HDB however much faster memory efficient lm factors passed model.matrix expanded matrices dummies projected beforehand. formula interface data.frame method (supported operators HDW | HDB) provides ease use allows additional modeling complexity. example possible project formulas like HDW(data, ~ fct1*var1  + fct2:fct3 + var2:fct2:fct3 + var2:var3 + poly(var5,3)*fct5) containing simple (:) full (*) interactions factors continuous variables polynomials continuous variables, two-three-way interactions factors continuous variables. formula one-sided example (space left (~) left empty), formula applied variables selected cols. specification provided cols (default: numeric variables used formula) can overridden supplying one-dependent variables. example HDW(data, var1 + var2 ~ fct1 + fct2) return data.frame var1 var2 centered fct1 fct2. special methods 'indexed_series' (plm::pseries) 'indexed_frame's (plm::pdata.frame) center panel series variables panel data frame panel-identifiers. default methods fill = TRUE variable.wise = TRUE, missing values kept. change default arguments done ensure coherent framework functions operators applied plm panel data classes.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":"on-the-differences-between-fhdwithin-hdw-and-fwithin-w-","dir":"Reference","previous_headings":"","what":"On the differences between fhdwithin/HDW... and fwithin/W...:","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"fhdwithin/HDW can center data multiple factors also partial continuous variables factor-continuous interactions fwithin/W centers one factor interaction set factors, efficiently. HDW(data, ~ qF(group1) + qF(group2)) simultaneously centers numeric variables data group1 group2, W(data, ~ group1 + group2) centers data interaction group1 group2. equivalent operation HDW : HDW(data, ~ qF(group1):qF(group2)). W always computations variable-wise complete observations (matrices data frames), whereas default HDW removes cases missing either x fl. short, W(data, ~ group1 + group2) actually equivalent HDW(data, ~ qF(group1):qF(group2), variable.wise = TRUE). HDW(data, ~ qF(group1):qF(group2)) remove missing cases. fbetween/B fwithin/W options fill missing cases using group-averages add overall mean back group-demeaned data. options available fhdbetween/HDB fhdwithin/HDW. Since HDB HDW default remove missing cases, also options keep grouping-columns B W.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"HDB returns fitted values regressing x fl. HDW returns residuals. See Details Examples.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fhdbetween_fhdwithin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Higher-Dimensional Centering and Linear Prediction — fhdbetween-fhdwithin","text":"","code":"HDW(mtcars$mpg, mtcars$carb)                   # Simple regression problems #>  [1]  3.3505410  3.3505410 -1.0166151 -2.4166151 -3.0608964 -5.7166151 #>  [7] -3.3494590  2.6391036  1.0391036  1.5505410  0.1505410 -3.3051777 #> [13] -2.4051777 -4.5051777 -7.2494590 -7.2494590 -2.9494590  8.5833849 #> [19]  8.6391036 10.0833849 -2.3166151 -6.2608964 -6.5608964 -4.3494590 #> [25] -2.5608964  3.4833849  4.2391036  8.6391036 -1.8494590  6.1619784 #> [31]  5.5734158 -0.3608964 HDW(mtcars$mpg, mtcars[-1]) #>  [1] -1.599505761 -1.111886079 -3.450644085  0.162595453  1.006565971 #>  [6] -2.283039036 -0.086256253  1.903988115 -1.619089898  0.500970058 #> [11] -1.391654392  2.227837890  1.700426404 -0.542224699 -1.634013415 #> [16] -0.536437711  4.206370638  4.627094192  0.503261089  4.387630904 #> [21] -2.143103442 -1.443053221 -2.532181498 -0.006021976  2.508321011 #> [26] -0.993468693 -0.152953961  2.763727417 -3.070040803  0.006171846 #> [31]  1.058881618 -2.968267683 HDW(mtcars$mpg, qM(mtcars[-1])) #>  [1] -1.599505761 -1.111886079 -3.450644085  0.162595453  1.006565971 #>  [6] -2.283039036 -0.086256253  1.903988115 -1.619089898  0.500970058 #> [11] -1.391654392  2.227837890  1.700426404 -0.542224699 -1.634013415 #> [16] -0.536437711  4.206370638  4.627094192  0.503261089  4.387630904 #> [21] -2.143103442 -1.443053221 -2.532181498 -0.006021976  2.508321011 #> [26] -0.993468693 -0.152953961  2.763727417 -3.070040803  0.006171846 #> [31]  1.058881618 -2.968267683 head(HDW(qM(mtcars[3:4]), mtcars[1:2])) #>                     HDW.disp     HDW.hp #> Mazda RX4         -56.791929 -29.668202 #> Mazda RX4 Wag     -56.791929 -29.668202 #> Datsun 710         -7.001038   6.283636 #> Hornet 4 Drive     43.577448 -28.558294 #> Hornet Sportabout  38.455459 -19.007424 #> Valiant            -8.969914 -42.715033 head(HDW(iris[1:2], iris[3:4]))                # Partialling columns 3 and 4 out of columns 1 and 2 #>   HDW.Sepal.Length HDW.Sepal.Width #> 1       0.21483967       0.2001352 #> 2       0.01483967      -0.2998648 #> 3      -0.13098262      -0.1255786 #> 4      -0.33933805      -0.1741510 #> 5       0.11483967       0.3001352 #> 6       0.41621663       0.6044681 head(HDW(iris[1:2], iris[3:5]))                # Adding the Species factor -> fixed effect #>   HDW.Sepal.Length HDW.Sepal.Width #> 1       0.14989286       0.1102684 #> 2      -0.05010714      -0.3897316 #> 3      -0.15951256      -0.1742640 #> 4      -0.44070173      -0.3051992 #> 5       0.04989286       0.2102684 #> 6       0.17930818       0.3391766  head(HDW(wlddev, PCGDP + LIFEEX ~ iso3c + qF(year))) # Partialling out 2 fixed effects #>   HDW.PCGDP HDW.LIFEEX #> 1 1578.6211 -1.3980224 #> 2 1412.8849 -1.1838196 #> 3  917.2033 -1.0547978 #> 4  627.8605 -0.8296048 #> 5  168.0458 -0.6683027 #> 6 -234.9535 -0.4708428 head(HDW(wlddev, PCGDP + LIFEEX ~ iso3c + qF(year), variable.wise = TRUE)) # Variable-wise #>   HDW.PCGDP HDW.LIFEEX #> 1        NA  -6.706423 #> 2        NA  -6.688440 #> 3        NA  -6.562210 #> 4        NA  -6.472079 #> 5        NA  -6.445378 #> 6        NA  -6.367659 head(HDW(wlddev, PCGDP + LIFEEX ~ iso3c + qF(year) + ODA)) # Adding ODA as a continuous regressor #>   HDW.PCGDP HDW.LIFEEX #> 1 -324.3991 -1.1765307 #> 2 -439.5404 -0.9751559 #> 3 -598.9266 -0.7835446 #> 4  100.2175 -0.6186010 #> 5  -70.7664 -0.4966332 #> 6  330.3561 -0.2257800 head(HDW(wlddev, PCGDP + LIFEEX ~ iso3c:qF(decade) + qF(year) + ODA)) # Country-decade and year FE's #>    HDW.PCGDP  HDW.LIFEEX #> 1  411.79228 -0.55122290 #> 2  231.95880 -0.36367639 #> 3  -73.18195 -0.20459213 #> 4   43.93176 -0.05394933 #> 5 -136.49858  0.06637048 #> 6 -151.30884  0.24440305  head(HDW(wlddev, PCGDP + LIFEEX ~ iso3c*year))          # Country specific time trends #>    HDW.PCGDP    HDW.LIFEEX #> 1  -3.035801 -0.1540994153 #> 2  -7.963841 -0.1544275887 #> 3 -35.533424 -0.1407557620 #> 4 -29.220766 -0.1100839354 #> 5 -38.876368 -0.0614121088 #> 6 -16.317261  0.0002597179 head(HDW(wlddev, PCGDP + LIFEEX ~ iso3c*poly(year, 3))) # Country specific cubic trends #>    HDW.PCGDP   HDW.LIFEEX #> 1   8.885334  0.023614035 #> 2  13.685446  0.006724458 #> 3 -10.597857 -0.011405315 #> 4  -6.279492 -0.023928535 #> 5 -22.048660 -0.025998452 #> 6  -8.561088 -0.018768318  # More complex examples lm(HDW.mpg ~ HDW.hp, data = HDW(mtcars, ~ factor(cyl)*carb + vs + wt:gear + wt:gear:carb)) #>  #> Call: #> lm(formula = HDW.mpg ~ HDW.hp, data = HDW(mtcars, ~factor(cyl) *  #>     carb + vs + wt:gear + wt:gear:carb)) #>  #> Coefficients: #> (Intercept)       HDW.hp   #>  -2.038e-16   -3.265e-02   #>  lm(mpg ~ hp + factor(cyl)*carb + vs + wt:gear + wt:gear:carb, data = mtcars) #>  #> Call: #> lm(formula = mpg ~ hp + factor(cyl) * carb + vs + wt:gear + wt:gear:carb,  #>     data = mtcars) #>  #> Coefficients: #>       (Intercept)                 hp       factor(cyl)6       factor(cyl)8   #>          42.11872           -0.02366           -3.70912           -3.80071   #>              carb                 vs  factor(cyl)6:carb  factor(cyl)8:carb   #>          -1.64558           -0.81529           -0.82919           -1.56964   #>           wt:gear       carb:wt:gear   #>          -1.52766            0.26438   #>   lm(HDW.mpg ~ HDW.hp, data = HDW(mtcars, ~ factor(cyl)*carb + vs + wt:gear)) #>  #> Call: #> lm(formula = HDW.mpg ~ HDW.hp, data = HDW(mtcars, ~factor(cyl) *  #>     carb + vs + wt:gear)) #>  #> Coefficients: #> (Intercept)       HDW.hp   #>  -2.038e-16   -3.265e-02   #>  lm(mpg ~ hp + factor(cyl)*carb + vs + wt:gear, data = mtcars) #>  #> Call: #> lm(formula = mpg ~ hp + factor(cyl) * carb + vs + wt:gear, data = mtcars) #>  #> Coefficients: #>       (Intercept)                 hp       factor(cyl)6       factor(cyl)8   #>           36.4543            -0.0274            -6.2463            -9.4541   #>              carb                 vs  factor(cyl)6:carb  factor(cyl)8:carb   #>            0.2508            -0.3227             0.5897             1.0374   #>           wt:gear   #>           -0.8238   #>   lm(HDW.mpg ~ HDW.hp, data = HDW(mtcars, ~ cyl*carb + vs + wt:gear)) #>  #> Call: #> lm(formula = HDW.mpg ~ HDW.hp, data = HDW(mtcars, ~cyl * carb +  #>     vs + wt:gear)) #>  #> Coefficients: #> (Intercept)       HDW.hp   #>  -3.898e-17   -2.151e-02   #>  lm(mpg ~ hp + cyl*carb + vs + wt:gear, data = mtcars) #>  #> Call: #> lm(formula = mpg ~ hp + cyl * carb + vs + wt:gear, data = mtcars) #>  #> Coefficients: #> (Intercept)           hp          cyl         carb           vs     cyl:carb   #>    48.42617     -0.02151     -2.80751     -1.72418     -1.03254      0.36051   #>     wt:gear   #>    -0.81296   #>   lm(HDW.mpg ~ HDW.hp, data = HDW(mtcars, mpg + hp ~ cyl*carb + factor(cyl)*poly(drat,2))) #>  #> Call: #> lm(formula = HDW.mpg ~ HDW.hp, data = HDW(mtcars, mpg + hp ~  #>     cyl * carb + factor(cyl) * poly(drat, 2))) #>  #> Coefficients: #> (Intercept)       HDW.hp   #>  -9.917e-16   -4.725e-02   #>  lm(mpg ~ hp + cyl*carb + factor(cyl)*poly(drat,2), data = mtcars) #>  #> Call: #> lm(formula = mpg ~ hp + cyl * carb + factor(cyl) * poly(drat,  #>     2), data = mtcars) #>  #> Coefficients: #>                 (Intercept)                           hp   #>                    29.87184                     -0.06227   #>                         cyl                         carb   #>                    -0.32237                     -2.19559   #>                factor(cyl)6                 factor(cyl)8   #>                    -1.60109                           NA   #>              poly(drat, 2)1               poly(drat, 2)2   #>                    27.84148                     -8.41291   #>                    cyl:carb  factor(cyl)6:poly(drat, 2)1   #>                     0.35323                    -49.59226   #> factor(cyl)8:poly(drat, 2)1  factor(cyl)6:poly(drat, 2)2   #>                   -18.35266                    -18.70972   #> factor(cyl)8:poly(drat, 2)2   #>                    -0.56842   #>"},{"path":"https://sebkrantz.github.io/collapse/reference/flag.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Lags and Leads for Time Series and Panel Data — flag","title":"Fast Lags and Leads for Time Series and Panel Data — flag","text":"flag S3 generic compute (sequences ) lags leads. L F wrappers around flag representing lag- lead-operators, L(x,-1) = F(x,1) = F(x) L(x,-3:3) = F(x,3:-3). L F provide flexibility flag applied data frames (.e.  column subsetting, formula input id-variable-preservation capabilities...), otherwise identical. Note: Since v1.9.0, F longer exported, can accessed using collapse:::F, setting options(collapse_export_F = TRUE) loading package. syntax L.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/flag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Lags and Leads for Time Series and Panel Data — flag","text":"","code":"flag(x, n = 1, ...)    L(x, n = 1, ...)  # Default S3 method flag(x, n = 1, g = NULL, t = NULL, fill = NA, stubs = TRUE, ...) # Default S3 method L(x, n = 1, g = NULL, t = NULL, fill = NA, stubs = .op[[\"stub\"]], ...)  # S3 method for class 'matrix' flag(x, n = 1, g = NULL, t = NULL, fill = NA, stubs = length(n) > 1L, ...) # S3 method for class 'matrix' L(x, n = 1, g = NULL, t = NULL, fill = NA, stubs = .op[[\"stub\"]], ...)  # S3 method for class 'data.frame' flag(x, n = 1, g = NULL, t = NULL, fill = NA, stubs = length(n) > 1L, ...) # S3 method for class 'data.frame' L(x, n = 1, by = NULL, t = NULL, cols = is.numeric,   fill = NA, stubs = .op[[\"stub\"]], keep.ids = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' flag(x, n = 1, fill = NA, stubs = length(n) > 1L, shift = \"time\", ...) # S3 method for class 'pseries' L(x, n = 1, fill = NA, stubs = .op[[\"stub\"]], shift = \"time\", ...)  # S3 method for class 'pdata.frame' flag(x, n = 1, fill = NA, stubs = length(n) > 1L, shift = \"time\", ...) # S3 method for class 'pdata.frame' L(x, n = 1, cols = is.numeric, fill = NA, stubs = .op[[\"stub\"]],   shift = \"time\", keep.ids = TRUE, ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' flag(x, n = 1, t = NULL, fill = NA, stubs = length(n) > 1L, keep.ids = TRUE, ...) # S3 method for class 'grouped_df' L(x, n = 1, t = NULL, fill = NA, stubs = .op[[\"stub\"]], keep.ids = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/flag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Lags and Leads for Time Series and Panel Data — flag","text":"x vector / time series, (time series) matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). Data must numeric. n integer. vector indicating lags / leads compute (passing negative integers flag L computes leads, passing negative integers F computes lags). g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. Note without t, values group need consecutive right order. See Details. data.frame method: g, also allows one- two-sided formulas .e. ~ group1 var1 + var2 ~ group1 + group2. See Examples. t time vector list vectors. Data frame methods also allows one-sided formula .e. ~time. grouped_df method supports lazy-evaluation .e. time (quotes). Either support wrapping transformation function e.g. ~timeid(time), qG(time) etc.. See also Details t processed. cols data.frame method: Select columns lag using function, column names, indices logical vector. Default: numeric variables. Note: cols ignored two-sided formula passed . fill value insert vectors shifted. Default NA. stubs logical. TRUE (default) rename lagged / leaded columns adding stub prefix \"Ln.\" / \"Fn.\". shift pseries / pdata.frame methods: character. \"time\" performs fully identified time-lag (index contains time variable), whereas \"row\" performs simple (group) lag, observations shifted based present order rows (group). latter significantly faster, requires time series / panels regularly spaced sorted time within group. keep.ids data.frame / pdata.frame / grouped_df methods: Logical. Drop identifiers output (includes variables passed t using formulas). Note: 'grouped_df' / 'pdata.frame' identifiers dropped, \"groups\" / \"index\" attributes kept. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/flag.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Lags and Leads for Time Series and Panel Data — flag","text":"single integer passed n, g/t left empty, flag/L/F just returns x columns lagged / leaded n. length(n)>1, x atomic vector (time series), flag/L/F returns (time series) matrix lags / leads computed order passed n. instead x matrix / data frame, matrix / data frame ncol(x)*length(n) columns returned columns sorted first variable lag (lags computed variable grouped together). x can standard data type. groups/panel-identifiers supplied g/, flag/L/F efficiently computes panel-lag/lead shifting entire vector(s) inserting fill elements right places. t left empty, data needs ordered values belonging group consecutive right order. necessary groups alphabetically ordered. time-variable supplied t (list time-variables uniquely identifying time-dimension), series / panel fully identified lags / leads can securely computed even data unordered / irregular. Note t argument processed follows: .factor(t) || (.numeric(t) && !.object(t)) (.e. t factor plain numeric vector), assumed represent unit timesteps (e.g. 'year' variable typical dataset), thus coerced integer using .integer(t) directly passed C++ without checks transformations R-level. Otherwise, .object(t) && .numeric(unclass(t)) (.e. t numeric time object, likely 'Date' 'POSIXct'), object passed timeid going C++. Else (e.g. t character), passed qG performs ordered grouping. t list multiple variables, passed finteraction. can customize behavior calling functions (including unclass/.integer) time variable beforehand. C++ level, g/t supplied, flag works follows: Use two initial passes create ordering data accessed. First-pass: Calculate minimum maximum time-value individual. Second-pass: Generate internal ordering vector (o) placing current element index vector slot obtained adding cumulative group size current time-value subtracted individual-minimum together. method computation faster sort-based method delivers optimal performance panel-id supplied g/already factor variable, t integer/factor variable. irregular time/panel series, length(o) > length(x), o represents unobserved 'complete series'. length(o) > 1e7 && length(o) > 3*length(x), warning issued make aware potential performance implications oversized ordering vector. 'indexed_series' ('pseries') 'indexed_frame' ('pdata.frame')  methods automatically utilize identifiers attached objects, already factors, thus lagging quite efficient. However, internal ordering vector still needs computed, thus data known ordered regularly spaced, using shift = \"row\" toggle simple group-lag (utilizing g t methods) can yield significant performance gain.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/flag.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Lags and Leads for Time Series and Panel Data — flag","text":"x lagged / leaded n-times, grouped g/, ordered t. See Details Examples.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/flag.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Lags and Leads for Time Series and Panel Data — flag","text":"","code":"## Simple Time Series: AirPassengers L(AirPassengers)                      # 1 lag #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949  NA 112 118 132 129 121 135 148 148 136 119 104 #> 1950 118 115 126 141 135 125 149 170 170 158 133 114 #> 1951 140 145 150 178 163 172 178 199 199 184 162 146 #> 1952 166 171 180 193 181 183 218 230 242 209 191 172 #> 1953 194 196 196 236 235 229 243 264 272 237 211 180 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] flag(AirPassengers)                   # Same #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949  NA 112 118 132 129 121 135 148 148 136 119 104 #> 1950 118 115 126 141 135 125 149 170 170 158 133 114 #> 1951 140 145 150 178 163 172 178 199 199 184 162 146 #> 1952 166 171 180 193 181 183 218 230 242 209 191 172 #> 1953 194 196 196 236 235 229 243 264 272 237 211 180 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] L(AirPassengers, -1)                  # 1 lead #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949 118 132 129 121 135 148 148 136 119 104 118 115 #> 1950 126 141 135 125 149 170 170 158 133 114 140 145 #> 1951 150 178 163 172 178 199 199 184 162 146 166 171 #> 1952 180 193 181 183 218 230 242 209 191 172 194 196 #> 1953 196 236 235 229 243 264 272 237 211 180 201 204 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ]  head(L(AirPassengers, -1:3))          # 1 lead and 3 lags - output as matrix #>           F1  --  L1  L2  L3 #> Jan 1949 118 112  NA  NA  NA #> Feb 1949 132 118 112  NA  NA #> Mar 1949 129 132 118 112  NA #> Apr 1949 121 129 132 118 112 #> May 1949 135 121 129 132 118 #> Jun 1949 148 135 121 129 132  ## Time Series Matrix of 4 EU Stock Market Indicators, 1991-1998 tsp(EuStockMarkets)                                     # Data is recorded on 260 days per year #> [1] 1991.496 1998.646  260.000 freq <- frequency(EuStockMarkets) plot(stl(EuStockMarkets[,\"DAX\"], freq))                 # There is some obvious seasonality  head(L(EuStockMarkets, -1:3 * freq))                    # 1 annual lead and 3 annual lags #> Time Series: #> Start = c(1991, 130)  #> End = c(1991, 135)  #> Frequency = 260  #>          F260.DAX     DAX L260.DAX L520.DAX L780.DAX F260.SMI    SMI L260.SMI #> 1991.496  1755.98 1628.75       NA       NA       NA   1846.6 1678.1       NA #> 1991.500  1754.95 1613.63       NA       NA       NA   1854.8 1688.5       NA #> 1991.504  1759.90 1606.51       NA       NA       NA   1845.3 1678.6       NA #>          L520.SMI L780.SMI F260.CAC    CAC L260.CAC L520.CAC L780.CAC F260.FTSE #> 1991.496       NA       NA   1907.3 1772.8       NA       NA       NA    2515.8 #> 1991.500       NA       NA   1900.6 1750.5       NA       NA       NA    2521.2 #> 1991.504       NA       NA   1880.9 1718.0       NA       NA       NA    2493.9 #>            FTSE L260.FTSE L520.FTSE L780.FTSE #> 1991.496 2443.6        NA        NA        NA #> 1991.500 2460.2        NA        NA        NA #> 1991.504 2448.2        NA        NA        NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ] summary(lm(DAX ~., data = L(EuStockMarkets,-1:3*freq))) # DAX regressed on its own annual lead, #>  #> Call: #> lm(formula = DAX ~ ., data = L(EuStockMarkets, -1:3 * freq)) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -158.092  -30.174    1.355   28.741  211.844  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept) -1.030e+03  1.016e+02 -10.141  < 2e-16 *** #> F260.DAX     1.037e-01  2.621e-02   3.957 8.25e-05 *** #> L260.DAX    -3.544e-01  4.394e-02  -8.066 2.65e-15 *** #> L520.DAX    -2.232e-01  4.116e-02  -5.423 7.75e-08 *** #> L780.DAX     9.451e-02  4.484e-02   2.107 0.035391 *   #> F260.SMI     4.968e-02  1.554e-02   3.198 0.001441 **  #> SMI          2.616e-01  2.301e-02  11.366  < 2e-16 *** #> L260.SMI     6.138e-02  2.740e-02   2.240 0.025342 *   #> L520.SMI    -2.153e-01  2.707e-02  -7.954 6.15e-15 *** #> L780.SMI    -2.208e-01  3.091e-02  -7.144 2.04e-12 *** #> F260.CAC    -1.392e-01  3.583e-02  -3.884 0.000111 *** #> CAC          7.165e-01  3.189e-02  22.470  < 2e-16 *** #> L260.CAC    -5.482e-02  3.874e-02  -1.415 0.157455     #> L520.CAC     2.326e-01  4.570e-02   5.090 4.46e-07 *** #>  [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ] #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 49.64 on 800 degrees of freedom #>   (1040 observations deleted due to missingness) #> Multiple R-squared:  0.9926,\tAdjusted R-squared:  0.9925  #> F-statistic:  5668 on 19 and 800 DF,  p-value: < 2.2e-16 #>                                                          # lags and the lead/lags of the other series ## World Development Panel Data head(flag(wlddev, 1, wlddev$iso3c, wlddev$year))        # This lags all variables, #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 2 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1     NA   NA        NA      NA #> 2 32.446   NA 116769997 8996973 #> 3 32.962   NA 232080002 9169410 #> 4 33.471   NA 112839996 9351441 #> 5 33.971   NA 237720001 9543205 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(L(wlddev, 1, ~iso3c, ~year))                       # This lags all numeric variables #>   iso3c year L1.decade L1.PCGDP L1.LIFEEX L1.GINI    L1.ODA  L1.POP #> 1   AFG 1960        NA       NA        NA      NA        NA      NA #> 2   AFG 1961      1960       NA    32.446      NA 116769997 8996973 #> 3   AFG 1962      1960       NA    32.962      NA 232080002 9169410 #> 4   AFG 1963      1960       NA    33.471      NA 112839996 9351441 #> 5   AFG 1964      1960       NA    33.971      NA 237720001 9543205 #> 6   AFG 1965      1960       NA    34.463      NA 295920013 9744781 head(L(wlddev, 1, ~iso3c))                              # Without t: Works because data is ordered #>   iso3c L1.year L1.decade L1.PCGDP L1.LIFEEX L1.GINI    L1.ODA  L1.POP #> 1   AFG      NA        NA       NA        NA      NA        NA      NA #> 2   AFG    1960      1960       NA    32.446      NA 116769997 8996973 #> 3   AFG    1961      1960       NA    32.962      NA 232080002 9169410 #> 4   AFG    1962      1960       NA    33.471      NA 112839996 9351441 #> 5   AFG    1963      1960       NA    33.971      NA 237720001 9543205 #> 6   AFG    1964      1960       NA    34.463      NA 295920013 9744781 head(L(wlddev, 1, PCGDP + LIFEEX ~ iso3c, ~year))       # This lags GDP per Capita & Life Expectancy #>   iso3c year L1.PCGDP L1.LIFEEX #> 1   AFG 1960       NA        NA #> 2   AFG 1961       NA    32.446 #> 3   AFG 1962       NA    32.962 #> 4   AFG 1963       NA    33.471 #> 5   AFG 1964       NA    33.971 #> 6   AFG 1965       NA    34.463 head(L(wlddev, 0:2, ~ iso3c, ~year, cols = 9:10))       # Same, also retaining original series #>   iso3c year PCGDP L1.PCGDP L2.PCGDP LIFEEX L1.LIFEEX L2.LIFEEX #> 1   AFG 1960    NA       NA       NA 32.446        NA        NA #> 2   AFG 1961    NA       NA       NA 32.962    32.446        NA #> 3   AFG 1962    NA       NA       NA 33.471    32.962    32.446 #> 4   AFG 1963    NA       NA       NA 33.971    33.471    32.962 #> 5   AFG 1964    NA       NA       NA 34.463    33.971    33.471 #> 6   AFG 1965    NA       NA       NA 34.948    34.463    33.971 head(L(wlddev, 1:2, PCGDP + LIFEEX ~ iso3c, ~year,      # Two lags, dropping id columns        keep.ids = FALSE)) #>   L1.PCGDP L2.PCGDP L1.LIFEEX L2.LIFEEX #> 1       NA       NA        NA        NA #> 2       NA       NA    32.446        NA #> 3       NA       NA    32.962    32.446 #> 4       NA       NA    33.471    32.962 #> 5       NA       NA    33.971    33.471 #> 6       NA       NA    34.463    33.971  # Regressing GDP on its's lags and life-Expectancy and its lags summary(lm(PCGDP ~ ., L(wlddev, 0:2, ~iso3c, ~year, 9:10, keep.ids = FALSE))) #>  #> Call: #> lm(formula = PCGDP ~ ., data = L(wlddev, 0:2, ~iso3c, ~year,  #>     9:10, keep.ids = FALSE)) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -16776.5   -102.2    -17.2     91.5  12277.1  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept) -333.93994   61.04617  -5.470 4.62e-08 *** #> L1.PCGDP       1.31959    0.01021 129.270  < 2e-16 *** #> L2.PCGDP      -0.31707    0.01029 -30.815  < 2e-16 *** #> LIFEEX       -17.77368   35.47772  -0.501    0.616     #> L1.LIFEEX     45.76286   65.87124   0.695    0.487     #> L2.LIFEEX    -21.43005   34.98964  -0.612    0.540     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 787.3 on 8609 degrees of freedom #>   (4561 observations deleted due to missingness) #> Multiple R-squared:  0.9976,\tAdjusted R-squared:  0.9976  #> F-statistic: 7.26e+05 on 5 and 8609 DF,  p-value: < 2.2e-16 #>   ## Indexing the data: facilitates time-based computations wldi <- findex_by(wlddev, iso3c, year) head(L(wldi, 0:2, cols = 9:10))                              # Again 2 lags of GDP and LIFEEX #>   iso3c year PCGDP L1.PCGDP L2.PCGDP LIFEEX L1.LIFEEX L2.LIFEEX #> 1   AFG 1960    NA       NA       NA 32.446        NA        NA #> 2   AFG 1961    NA       NA       NA 32.962    32.446        NA #> 3   AFG 1962    NA       NA       NA 33.471    32.962    32.446 #> 4   AFG 1963    NA       NA       NA 33.971    33.471    32.962 #> 5   AFG 1964    NA       NA       NA 34.463    33.971    33.471 #> 6   AFG 1965    NA       NA       NA 34.948    34.463    33.971 #>  #> Indexed by:  iso3c [1] | year [6 (61)]  head(L(wldi$PCGDP))                                          # Lagging an indexed series #> [1] NA NA NA NA NA NA #>  #> Indexed by:  iso3c [1] | year [6 (61)]  summary(lm(PCGDP ~ L(PCGDP,1:2) + L(LIFEEX,0:2), wldi))      # Running the lm again #>  #> Call: #> lm(formula = PCGDP ~ L(PCGDP, 1:2) + L(LIFEEX, 0:2), data = wldi) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -16776.5   -102.2    -17.2     91.5  12277.1  #>  #> Indexed by:  iso3c [4] | year [5 (61)]  #>  #> Coefficients: #>                    Estimate Std. Error t value Pr(>|t|)     #> (Intercept)      -333.93994   61.04617  -5.470 4.62e-08 *** #> L(PCGDP, 1:2)L1     1.31959    0.01021 129.270  < 2e-16 *** #> L(PCGDP, 1:2)L2    -0.31707    0.01029 -30.815  < 2e-16 *** #> L(LIFEEX, 0:2)--  -17.77368   35.47772  -0.501    0.616     #> L(LIFEEX, 0:2)L1   45.76286   65.87124   0.695    0.487     #> L(LIFEEX, 0:2)L2  -21.43005   34.98964  -0.612    0.540     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 787.3 on 8609 degrees of freedom #>   (4561 observations deleted due to missingness) #> Multiple R-squared:  0.9976,\tAdjusted R-squared:  0.9976  #> F-statistic: 7.26e+05 on 5 and 8609 DF,  p-value: < 2.2e-16 #>  summary(lm(PCGDP ~ ., L(wldi, 0:2, 9:10, keep.ids = FALSE))) # Same thing #>  #> Call: #> lm(formula = PCGDP ~ ., data = L(wldi, 0:2, 9:10, keep.ids = FALSE)) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -16776.5   -102.2    -17.2     91.5  12277.1  #>  #> Indexed by:  iso3c [4] | year [5 (61)]  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept) -333.93994   61.04617  -5.470 4.62e-08 *** #> L1.PCGDP       1.31959    0.01021 129.270  < 2e-16 *** #> L2.PCGDP      -0.31707    0.01029 -30.815  < 2e-16 *** #> LIFEEX       -17.77368   35.47772  -0.501    0.616     #> L1.LIFEEX     45.76286   65.87124   0.695    0.487     #> L2.LIFEEX    -21.43005   34.98964  -0.612    0.540     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 787.3 on 8609 degrees of freedom #>   (4561 observations deleted due to missingness) #> Multiple R-squared:  0.9976,\tAdjusted R-squared:  0.9976  #> F-statistic: 7.26e+05 on 5 and 8609 DF,  p-value: < 2.2e-16 #>   ## Using grouped data: library(magrittr) wlddev |> fgroup_by(iso3c) |> fselect(PCGDP,LIFEEX) |> flag(0:2) #>    PCGDP L1.PCGDP L2.PCGDP LIFEEX L1.LIFEEX L2.LIFEEX #> 1     NA       NA       NA 32.446        NA        NA #> 2     NA       NA       NA 32.962    32.446        NA #> 3     NA       NA       NA 33.471    32.962    32.446 #> 4     NA       NA       NA 33.971    33.471    32.962 #> 5     NA       NA       NA 34.463    33.971    33.471 #> 6     NA       NA       NA 34.948    34.463    33.971 #> 7     NA       NA       NA 35.430    34.948    34.463 #> 8     NA       NA       NA 35.914    35.430    34.948 #> 9     NA       NA       NA 36.403    35.914    35.430 #> 10    NA       NA       NA 36.900    36.403    35.914 #> 11    NA       NA       NA 37.409    36.900    36.403 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13165 rows ] #>  #> Grouped by:  iso3c  [216 | 61 (0)]  wlddev |> fgroup_by(iso3c) |> fselect(year,PCGDP,LIFEEX) |> flag(0:2,year) # Also using t (safer) #>    year PCGDP L1.PCGDP L2.PCGDP LIFEEX L1.LIFEEX L2.LIFEEX #> 1  1960    NA       NA       NA 32.446        NA        NA #> 2  1961    NA       NA       NA 32.962    32.446        NA #> 3  1962    NA       NA       NA 33.471    32.962    32.446 #> 4  1963    NA       NA       NA 33.971    33.471    32.962 #> 5  1964    NA       NA       NA 34.463    33.971    33.471 #> 6  1965    NA       NA       NA 34.948    34.463    33.971 #> 7  1966    NA       NA       NA 35.430    34.948    34.463 #> 8  1967    NA       NA       NA 35.914    35.430    34.948 #> 9  1968    NA       NA       NA 36.403    35.914    35.430 #> 10 1969    NA       NA       NA 36.900    36.403    35.914 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13166 rows ] #>  #> Grouped by:  iso3c  [216 | 61 (0)]"},{"path":"https://sebkrantz.github.io/collapse/reference/flm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Weighted) Linear Model Fitting — flm","title":"Fast (Weighted) Linear Model Fitting — flm","text":"flm fast linear model command (default) returns coefficient matrix. 6 different efficient fitting methods implemented: 4 using base R linear algebra, 2 utilizing RcppArmadillo RcppEigen packages. function overhead 5-10 microseconds, thus well suited bootstrap workhorse.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/flm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Weighted) Linear Model Fitting — flm","text":"","code":"flm(...)  # Internal method dispatch: default if is.atomic(..1)  # Default S3 method flm(y, X, w = NULL, add.icpt = FALSE, return.raw = FALSE,     method = c(\"lm\", \"solve\", \"qr\", \"arma\", \"chol\", \"eigen\"),     eigen.method = 3L, ...)  # S3 method for class 'formula' flm(formula, data = NULL, weights = NULL, add.icpt = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/flm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Weighted) Linear Model Fitting — flm","text":"y response vector matrix. Multiple dependent variables supported methods \"lm\", \"solve\", \"qr\" \"chol\". X matrix regressors. w weight vector. add.icpt logical. TRUE adds intercept column named '(Intercept)' X. formula lm formula, without factors, interaction terms operators (:, *, ^, -, etc.), may include regular transformations e.g. log(var), cbind(y1, y2), magrittr::multiply_by(var1, var2), magrittr::raise_to_power(var, 2). data named list data frame. weights weights vector expression results vector evaluated data environment.  return.raw logical. TRUE returns original output different methods. 'lm', 'arma' 'eigen', includes additional statistics residuals, fitted values standard errors. methods just return coefficients different formats. method integer character string specifying method computation: eigen.method integer. Select method computation used RcppEigen::fastLmPure: See vignette(\"RcppEigen-Introduction\", package = \"RcppEigen\") details methods benchmark results. Run source(system.file(\"examples\", \"lmBenchmark.R\", package = \"RcppEigen\")) re-run benchmark machine. ... arguments passed methods. formula method arguments passed default method. Additional arguments can also passed default method e.g. tol = value set numerical tolerance solution - applicable methods \"lm\", \"solve\" \"qr\" (default 1e-7), LAPACK = TRUE method \"qr\" use LAPACK routines qr decomposition (typically faster LINPACK default).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/flm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Weighted) Linear Model Fitting — flm","text":"return.raw = FALSE, matrix coefficients rows corresponding columns X, otherwise raw results various methods returned.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/flm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast (Weighted) Linear Model Fitting — flm","text":"Method \"qr\" supports sparse matrices, X matrix many dummy variables consider method \"qr\" passing (X, \"dgCMatrix\") instead just X.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/flm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Weighted) Linear Model Fitting — flm","text":"","code":"# Simple usage coef <- flm(mpg ~ hp + carb, mtcars, w = wt)  # Same thing in programming usage flm(mtcars$mpg, qM(mtcars[c(\"hp\",\"carb\")]), mtcars$wt, add.icpt = TRUE) #>                    [,1] #> (Intercept) 28.48401839 #> hp          -0.06834996 #> carb         0.33207257  # Check this is correct lmcoef <- coef(lm(mpg ~ hp + carb, weights = wt, mtcars)) all.equal(drop(coef), lmcoef) #> [1] TRUE  # Multi-dependent variable (only some methods) flm(cbind(mpg, qsec) ~ hp + carb, mtcars, w = wt) #>                     mpg        qsec #> (Intercept) 28.48401839 20.77946948 #> hp          -0.06834996 -0.01409167 #> carb         0.33207257 -0.25468102  # Returning raw results from solver: different for different methods flm(mpg ~ hp + carb, mtcars, return.raw = TRUE) #> $qr #>       (Intercept)            hp          carb #>  [1,]  -5.6568542 -829.78980772 -15.909902577 #>  [2,]   0.1767767  381.74189579   6.743103202 #>  [3,]   0.1767767    0.12620114   5.950257070 #>  [4,]   0.1767767    0.08166844   0.261830398 #>  [5,]   0.1767767   -0.08860368   0.245465211 #>  [6,]   0.1767767    0.09476629   0.250161569 #>  [7,]   0.1767767   -0.27197365   0.072708888 #>  [8,]   0.1767767    0.20740784  -0.018250328 #>  [9,]   0.1767767    0.12096200   0.058763944 #> [10,]   0.1767767    0.04761401  -0.212010544 #> [11,]   0.1767767    0.04761401  -0.212010544 #> [12,]   0.1767767   -0.10170154   0.089074074 #> [13,]   0.1767767   -0.10170154   0.089074074 #> [14,]   0.1767767   -0.10170154   0.089074074 #> [15,]   0.1767767   -0.16719081  -0.020641746 #> [16,]   0.1767767   -0.19338652   0.002695913 #> [17,]   0.1767767   -0.23268009   0.037702400 #> [18,]   0.1767767    0.19692956   0.159144701 #> [19,]   0.1767767    0.23360355  -0.041587987 #> [20,]   0.1767767    0.19954913   0.156810935 #> [21,]   0.1767767    0.11572286   0.231491442 #> [22,]   0.1767767   -0.02311440   0.187121065 #> [23,]   0.1767767   -0.02311440   0.187121065 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ] #>  #> $coefficients #> [1] 30.04025415 -0.07290396  0.26470042 #>  #> $residuals #>  [1] -2.07962064 -2.07962064 -0.72488664 -0.88551939  0.88853735 -4.55003917 #>  [7]  1.06241345 -1.64960970 -0.84377915 -2.93186921 -4.33186921 -1.31164329 #> [13] -0.41164329 -2.51164329 -5.75374480 -5.02470524  0.36885411  6.90670654 #> [19]  3.62135074  8.33380258 -1.73327082 -4.13406156 -4.43406156  0.06241345 #> [25]  1.38853735  1.80670654  2.06460503  8.06849206  3.94758862  0.82973568 #> [31]  7.26496784 -1.22312376 #>  #> $effects #>  [1] -113.64973741  -26.04559222    1.57503553   -0.39549102    1.09761463 #>  [6]   -4.04058135    0.94274199   -1.00142681   -0.32383064   -2.57746205 #> [11]   -3.97746205   -1.15036366   -0.25036366   -2.35036366   -5.71798064 #> [16]   -5.02779999    0.30747100    7.56771408    4.30839253    8.99869601 #> [21]   -1.19272588   -3.82783701   -4.12783701   -0.05725801    1.59761463 #> [26]    2.46771408    2.60009710    8.51849455    3.75408524    0.92534013 #> [31]    6.68209341   -0.75757771 #>  #> $rank #> [1] 3 #>  #> $pivot #> [1] 1 2 3 #>  #> $qraux #> [1] 1.176777 1.081668 1.222156 #>  #> $tol #> [1] 1e-07 #>  #> $pivoted #> [1] FALSE #>  flm(mpg ~ hp + carb, mtcars, method = \"qr\", return.raw = TRUE) #> (Intercept)          hp        carb  #> 30.04025415 -0.07290396  0.26470042    # Test that all methods give the same result all_obj_equal(lapply(1:6, function(i)   flm(mpg ~ hp + carb, mtcars, w = wt, method = i))) #> Registered S3 methods overwritten by 'RcppEigen': #>   method               from          #>   predict.fastLm       RcppArmadillo #>   print.fastLm         RcppArmadillo #>   summary.fastLm       RcppArmadillo #>   print.summary.fastLm RcppArmadillo #> [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/reference/fmatch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Matching — fmatch","title":"Fast Matching — fmatch","text":"Fast matching elements/rows x elements/rows table. much faster replacement match works   atomic vectors data frames / lists equal-length vectors. workhorse function join.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmatch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Matching — fmatch","text":"","code":"fmatch(x, table, nomatch = NA_integer_,        count = FALSE, overid = 1L)  # Check match: throws an informative error for non-matched elements # Default message reflects frequent internal use to check data frame columns ckmatch(x, table, e = \"Unknown columns:\", ...)  # Infix operators based on fmatch(): x %!in% table   # Opposite of %in% x %iin% table   # = which(x %in% table), but more efficient x %!iin% table  # = which(x %!in% table), but more efficient # Use set_collapse(mask = \"%in%\") to replace %in% with # a much faster version based on fmatch()"},{"path":"https://sebkrantz.github.io/collapse/reference/fmatch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Matching — fmatch","text":"x vector, list data frame whose elements matched table. list/data frame, matches found comparing rows, unlike match compares columns. table vector, list data frame match . nomatch integer. Value returned case match found. Default NA_integer_. count logical. Counts number (unique) matches attaches 4 attributes: \"N.nomatch\": number elements x matched = sum(result == nomatch). \"N.groups\": size table  = NROW(table). \"N.distinct\": number unique matches  = fndistinct(result[result != nomatch]). \"class\": \"qG\" class: needed optimized computations results object (e.g. funique(result), needed full join). Note computing attributes requires extra pass matching vector. Also note attributes contain general information whether either x table unique, except two special cases N.groups = N.distinct (table unique) length(result) = N.distinct (x unique). Otherwise use any_duplicated check x/table. overid integer. x/table lists/data frames, fmatch compares rows incrementally, starting first two columns, matching columns necessary (see Details). Overidentification corresponds case subset columns uniquely identify data. case argument controls behavior: 0: Early termination: stop matching additional columns. efficient. 1: Continue matching columns issue warning data overidentified. 2: Continue matching columns without warning.  e error message thrown ckmatch non-matched elements. message followed comma-separated non-matched elements. ... arguments fmatch.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmatch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Matching — fmatch","text":"Integer vector containing positions first matches x table. nomatch returned elements x match table. count = TRUE, result additional attributes class \"qG\".","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmatch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Matching — fmatch","text":"data frames / lists, fmatch compares rows moves data column--column basis (like vectorized hash join algorithm). two columns, first two columns hashed simultaneously speed. columns can added match. likely first 2, 3, 4 etc. columns data frame fully identify data. column fmatch() internally checks whether table rows still eligible matching (eliminating nomatch rows earlier columns) unique. case overid = 0, fmatch() terminates early without considering columns. efficient may give undesirable/wrong results considering columns turn additional elements result vector nomatch values.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fmatch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Matching — fmatch","text":"","code":"x <- c(\"b\", \"c\", \"a\", \"e\", \"f\", \"ff\") fmatch(x, letters) #> [1]  2  3  1  5  6 NA fmatch(x, letters, nomatch = 0) #> [1] 2 3 1 5 6 0 fmatch(x, letters, count = TRUE) #> [1]  2  3  1  5  6 NA #> attr(,\"N.nomatch\") #> [1] 1 #> attr(,\"N.groups\") #> [1] 26 #> attr(,\"N.distinct\") #> [1] 5 #> attr(,\"class\") #> [1] \"qG\"  # Table 1 df1 <- data.frame(   id1 = c(1, 1, 2, 3),   id2 = c(\"a\", \"b\", \"b\", \"c\"),   name = c(\"John\", \"Bob\", \"Jane\", \"Carl\") ) head(df1) #>   id1 id2 name #> 1   1   a John #> 2   1   b  Bob #> 3   2   b Jane #> 4   3   c Carl # Table 2 df2 <- data.frame(   id1 = c(1, 2, 3, 3),   id2 = c(\"a\", \"b\", \"c\", \"e\"),   name = c(\"John\", \"Janne\", \"Carl\", \"Lynne\") ) head(df2) #>   id1 id2  name #> 1   1   a  John #> 2   2   b Janne #> 3   3   c  Carl #> 4   3   e Lynne  # This gives an overidentification warning: columns 1:2 identify the data if(FALSE) fmatch(df1, df2) # This just runs through without warning fmatch(df1, df2, overid = 2) #> [1]  1 NA NA  3 # This terminates computation after first 2 columns fmatch(df1, df2, overid = 0) #> [1]  1 NA  2  3 fmatch(df1[1:2], df2[1:2])  # Same thing! #> [1]  1 NA  2  3 # -> note that here we get an additional match based on the unique ids, # which we didn't get before because \"Jane\" != \"Janne\""},{"path":"https://sebkrantz.github.io/collapse/reference/fmean.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","title":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","text":"fmean generic function computes (column-wise) mean x, (optionally) grouped g /weighted w. TRA argument can used transform x using (grouped, weighted) mean.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","text":"","code":"fmean(x, ...)  # Default S3 method fmean(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'matrix' fmean(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'data.frame' fmean(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'grouped_df' fmean(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = FALSE, keep.group_vars = TRUE,       keep.w = TRUE, stub = .op[[\"stub\"]], nthreads = .op[[\"nthreads\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fmean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","text":"x numeric vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. w numeric vector (non-negative) weights, may contain missing values. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE NA returned encountered. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. nthreads integer. number threads utilize. See Details fsum. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. keep.w grouped_df method: Logical. Retain summed weighting variable computation (contained grouped_df). stub character. keep.w = TRUE stub = TRUE (default), summed weights column prefixed \"sum.\". Users can specify different prefix argument, set FALSE avoid prefixing. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmean.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","text":"weighted mean computed sum(x * w) / sum(w), using single pass C. na.rm = TRUE, missing values removed x w .e. utilizing x[complete.cases(x,w)] w[complete.cases(x,w)]. computational details see fsum, works equivalently.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","text":"(w weighted) mean x, grouped g, (TRA used) x transformed (grouped, weighted) mean.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fmean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Mean for Matrix-Like Objects — fmean","text":"","code":"## default vector method mpg <- mtcars$mpg fmean(mpg)                         # Simple mean #> [1] 20.09062 fmean(mpg, w = mtcars$hp)          # Weighted mean: Weighted by hp #> [1] 17.97245 fmean(mpg, TRA = \"-\")              # Simple transformation: demeaning (See also ?W) #>  [1]  0.909375  0.909375  2.709375  1.309375 -1.390625 -1.990625 -5.790625 #>  [8]  4.309375  2.709375 -0.890625 -2.290625 -3.690625 -2.790625 -4.890625 #> [15] -9.690625 -9.690625 -5.390625 12.309375 10.309375 13.809375  1.409375 #> [22] -4.590625 -4.890625 -6.790625 -0.890625  7.209375  5.909375 10.309375 #> [29] -4.290625 -0.390625 -5.090625  1.309375 fmean(mpg, mtcars$cyl)             # Grouped mean #>        4        6        8  #> 26.66364 19.74286 15.10000  fmean(mpg, mtcars[8:9])            # another grouped mean. #>      0.0      0.1      1.0      1.1  #> 15.05000 19.75000 20.74286 28.37143  g <- GRP(mtcars[c(2,8:9)]) fmean(mpg, g)                      # Pre-computing groups speeds up the computation #>    4.0.1    4.1.0    4.1.1    6.0.1    6.1.0    8.0.0    8.0.1  #> 26.00000 22.90000 28.37143 20.56667 19.12500 15.05000 15.40000  fmean(mpg, g, mtcars$hp)           # Grouped weighted mean #>    4.0.1    4.1.0    4.1.1    6.0.1    6.1.0    8.0.0    8.0.1  #> 26.00000 22.69409 27.68209 20.42405 19.10087 14.82854 15.35259  fmean(mpg, g, TRA = \"-\")           # Demeaning by group #>  [1]  0.4333333  0.4333333 -5.5714286  2.2750000  3.6500000 -1.0250000 #>  [7] -0.7500000  1.5000000 -0.1000000  0.0750000 -1.3250000  1.3500000 #> [13]  2.2500000  0.1500000 -4.6500000 -4.6500000 -0.3500000  4.0285714 #> [19]  2.0285714  5.5285714 -1.4000000  0.4500000  0.1500000 -1.7500000 #> [25]  4.1500000 -1.0714286  0.0000000  2.0285714  0.4000000 -0.8666667 #> [31] -0.4000000 -6.9714286 fmean(mpg, g, mtcars$hp, \"-\")      # Group-demeaning using weighted group means #>  [1]  0.57594937  0.57594937 -4.88209220  2.29913232  3.87145923 -1.00086768 #>  [7] -0.52854077  1.70590551  0.10590551  0.09913232 -1.30086768  1.57145923 #> [13]  2.47145923  0.37145923 -4.42854077 -4.42854077 -0.12854077  4.71790780 #> [19]  2.71790780  6.21790780 -1.19409449  0.67145923  0.37145923 -1.52854077 #> [25]  4.37145923 -0.38209220  0.00000000  2.71790780  0.44741235 -0.72405063 #> [31] -0.35258765 -6.28209220  ## data.frame method fmean(mtcars) #>        mpg        cyl       disp         hp       drat         wt       qsec  #>  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750  #>         vs         am       gear       carb  #>   0.437500   0.406250   3.687500   2.812500  fmean(mtcars, g) #>            mpg cyl     disp        hp     drat       wt     qsec vs am     gear #> 4.0.1 26.00000   4 120.3000  91.00000 4.430000 2.140000 16.70000  0  1 5.000000 #> 4.1.0 22.90000   4 135.8667  84.66667 3.770000 2.935000 20.97000  1  0 3.666667 #> 4.1.1 28.37143   4  89.8000  80.57143 4.148571 2.028286 18.70000  1  1 4.142857 #> 6.0.1 20.56667   6 155.0000 131.66667 3.806667 2.755000 16.32667  0  1 4.333333 #> 6.1.0 19.12500   6 204.5500 115.25000 3.420000 3.388750 19.21500  1  0 3.500000 #> 8.0.0 15.05000   8 357.6167 194.16667 3.120833 4.104083 17.14250  0  0 3.000000 #>           carb #> 4.0.1 2.000000 #> 4.1.0 1.666667 #> 4.1.1 1.428571 #> 6.0.1 4.666667 #> 6.1.0 2.500000 #> 8.0.0 3.083333 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fmean(fgroup_by(mtcars, cyl, vs, am))  # Another way of doing it.. #>   cyl vs am      mpg     disp        hp     drat       wt     qsec     gear #> 1   4  0  1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 #> 2   4  1  0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 #> 3   4  1  1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 #> 4   6  0  1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 #> 5   6  1  0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 #> 6   8  0  0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 #>       carb #> 1 2.000000 #> 2 1.666667 #> 3 1.428571 #> 4 4.666667 #> 5 2.500000 #> 6 3.083333 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(fmean(mtcars, g, TRA = \"-\"))      # etc.. #>                          mpg cyl      disp        hp        drat         wt #> Mazda RX4          0.4333333   0  5.000000 -21.66667  0.09333333 -0.1350000 #> Mazda RX4 Wag      0.4333333   0  5.000000 -21.66667  0.09333333  0.1200000 #> Datsun 710        -5.5714286   0 18.200000  12.42857 -0.29857143  0.2917143 #> Hornet 4 Drive     2.2750000   0 53.450000  -5.25000 -0.34000000 -0.1737500 #> Hornet Sportabout  3.6500000   0  2.383333 -19.16667  0.02916667 -0.6640833 #> Valiant           -1.0250000   0 20.450000 -10.25000 -0.66000000  0.0712500 #>                         qsec vs am       gear       carb #> Mazda RX4          0.1333333  0  0 -0.3333333 -0.6666667 #> Mazda RX4 Wag      0.6933333  0  0 -0.3333333 -0.6666667 #> Datsun 710        -0.0900000  0  0 -0.1428571 -0.4285714 #> Hornet 4 Drive     0.2250000  0  0 -0.5000000 -1.5000000 #> Hornet Sportabout -0.1225000  0  0  0.0000000 -1.0833333 #> Valiant            1.0050000  0  0 -0.5000000 -1.5000000  ## matrix method m <- qM(mtcars) fmean(m) #>        mpg        cyl       disp         hp       drat         wt       qsec  #>  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750  #>         vs         am       gear       carb  #>   0.437500   0.406250   3.687500   2.812500  fmean(m, g) #>            mpg cyl     disp        hp     drat       wt     qsec vs am     gear #> 4.0.1 26.00000   4 120.3000  91.00000 4.430000 2.140000 16.70000  0  1 5.000000 #> 4.1.0 22.90000   4 135.8667  84.66667 3.770000 2.935000 20.97000  1  0 3.666667 #> 4.1.1 28.37143   4  89.8000  80.57143 4.148571 2.028286 18.70000  1  1 4.142857 #> 6.0.1 20.56667   6 155.0000 131.66667 3.806667 2.755000 16.32667  0  1 4.333333 #> 6.1.0 19.12500   6 204.5500 115.25000 3.420000 3.388750 19.21500  1  0 3.500000 #> 8.0.0 15.05000   8 357.6167 194.16667 3.120833 4.104083 17.14250  0  0 3.000000 #>           carb #> 4.0.1 2.000000 #> 4.1.0 1.666667 #> 4.1.1 1.428571 #> 6.0.1 4.666667 #> 6.1.0 2.500000 #> 8.0.0 3.083333 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] head(fmean(m, g, TRA = \"-\")) # etc.. #>                          mpg cyl      disp        hp        drat         wt #> Mazda RX4          0.4333333   0  5.000000 -21.66667  0.09333333 -0.1350000 #> Mazda RX4 Wag      0.4333333   0  5.000000 -21.66667  0.09333333  0.1200000 #> Datsun 710        -5.5714286   0 18.200000  12.42857 -0.29857143  0.2917143 #> Hornet 4 Drive     2.2750000   0 53.450000  -5.25000 -0.34000000 -0.1737500 #> Hornet Sportabout  3.6500000   0  2.383333 -19.16667  0.02916667 -0.6640833 #> Valiant           -1.0250000   0 20.450000 -10.25000 -0.66000000  0.0712500 #>                         qsec vs am       gear       carb #> Mazda RX4          0.1333333  0  0 -0.3333333 -0.6666667 #> Mazda RX4 Wag      0.6933333  0  0 -0.3333333 -0.6666667 #> Datsun 710        -0.0900000  0  0 -0.1428571 -0.4285714 #> Hornet 4 Drive     0.2250000  0  0 -0.5000000 -1.5000000 #> Hornet Sportabout -0.1225000  0  0  0.0000000 -1.0833333 #> Valiant            1.0050000  0  0 -0.5000000 -1.5000000  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fmean()         # Ordinary #>   cyl vs am      mpg     disp        hp     drat       wt     qsec     gear #> 1   4  0  1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 #> 2   4  1  0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 #> 3   4  1  1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 #> 4   6  0  1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 #> 5   6  1  0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 #> 6   8  0  0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 #>       carb #> 1 2.000000 #> 2 1.666667 #> 3 1.428571 #> 4 4.666667 #> 5 2.500000 #> 6 3.083333 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fmean(hp)       # Weighted #>   cyl vs am sum.hp      mpg      disp     drat       wt     qsec     gear #> 1   4  0  1     91 26.00000 120.30000 4.430000 2.140000 16.70000 5.000000 #> 2   4  1  0    254 22.69409 134.33504 3.779843 2.898169 21.08846 3.618110 #> 3   4  1  1    564 27.68209  93.87482 4.080266 2.067223 18.54041 4.200355 #> 4   6  0  1    395 20.42405 153.35443 3.775949 2.757468 16.19063 4.443038 #> 5   6  1  0    461 19.10087 202.24425 3.455358 3.390868 19.16941 3.533623 #> 6   8  0  0   2330 14.82854 363.10815 3.143090 4.158685 17.08489 3.000000 #>       carb #> 1 2.000000 #> 2 1.618110 #> 3 1.485816 #> 4 4.886076 #> 5 2.600868 #> 6 3.210300 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fmean(hp, \"-\")  # Weighted Transform #>                   cyl vs am  hp        mpg      disp         drat          wt #> Mazda RX4           6  0  1 110  0.5759494  6.645570  0.124050633 -0.13746835 #> Mazda RX4 Wag       6  0  1 110  0.5759494  6.645570  0.124050633  0.11753165 #> Datsun 710          4  1  1  93 -4.8820922 14.125177 -0.230265957  0.25277660 #> Hornet 4 Drive      6  1  0 110  2.2991323 55.755748 -0.375357918 -0.17586768 #> Hornet Sportabout   8  0  0 175  3.8714592 -3.108155  0.006909871 -0.71868455 #> Valiant             6  1  0 105 -1.0008677 22.755748 -0.695357918  0.06913232 #>                         qsec       gear       carb #> Mazda RX4          0.2693671 -0.4430380 -0.8860759 #> Mazda RX4 Wag      0.8293671 -0.4430380 -0.8860759 #> Datsun 710         0.0695922 -0.2003546 -0.4858156 #> Hornet 4 Drive     0.2705857 -0.5336226 -1.6008677 #> Hornet Sportabout -0.0648927  0.0000000 -1.2103004 #> Valiant            1.0505857 -0.5336226 -1.6008677 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl,vs,am) |>                fselect(mpg,hp) |> fmean(hp, \"-\")  # Only mpg #>                      hp         mpg #> Mazda RX4           110  0.57594937 #> Mazda RX4 Wag       110  0.57594937 #> Datsun 710           93 -4.88209220 #> Hornet 4 Drive      110  2.29913232 #> Hornet Sportabout   175  3.87145923 #> Valiant             105 -1.00086768 #> Duster 360          245 -0.52854077 #> Merc 240D            62  1.70590551 #> Merc 230             95  0.10590551 #> Merc 280            123  0.09913232 #> Merc 280C           123 -1.30086768 #> Merc 450SE          180  1.57145923 #> Merc 450SL          180  2.47145923 #> Merc 450SLC         180  0.37145923 #> Cadillac Fleetwood  205 -4.42854077 #> Lincoln Continental 215 -4.42854077 #> Chrysler Imperial   230 -0.12854077 #> Fiat 128             66  4.71790780 #> Honda Civic          52  2.71790780 #> Toyota Corolla       65  6.21790780 #> Toyota Corona        97 -1.19409449 #> Dodge Challenger    150  0.67145923 #> AMC Javelin         150  0.37145923 #> Camaro Z28          245 -1.52854077 #> Pontiac Firebird    175  4.37145923 #> Fiat X1-9            66 -0.38209220 #> Porsche 914-2        91  0.00000000 #> Lotus Europa        113  2.71790780 #> Ford Pantera L      264  0.44741235 #> Ferrari Dino        175 -0.72405063 #> Maserati Bora       335 -0.35258765 #> Volvo 142E          109 -6.28209220 #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]"},{"path":"https://sebkrantz.github.io/collapse/reference/fmin_fmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","title":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","text":"fmax fmin generic functions compute (column-wise) maximum minimum value values x, (optionally) grouped g. TRA argument can used transform x using (grouped) maximum minimum value.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmin_fmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","text":"","code":"fmax(x, ...) fmin(x, ...)  # Default S3 method fmax(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, ...) # Default S3 method fmin(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, ...)  # S3 method for class 'matrix' fmax(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, ...) # S3 method for class 'matrix' fmin(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'data.frame' fmax(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, ...) # S3 method for class 'data.frame' fmin(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'grouped_df' fmax(x, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = FALSE, keep.group_vars = TRUE, ...) # S3 method for class 'grouped_df' fmin(x, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = FALSE, keep.group_vars = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fmin_fmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","text":"x numeric vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE NA returned encountered. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmin_fmax.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","text":"Missing-value removal controlled na.rm argument done extra cost since C++ logical comparison involving NA NaN evaluates FALSE. Large performance gains can nevertheless achieved presence missing values na.rm = FALSE, since corresponding computation terminated NA encountered NA returned (unlike max min just run without checks). computational details see fsum.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmin_fmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","text":"fmax returns maximum value x, grouped g, (TRA used) x transformed (grouped) maximum value. Analogous, fmin returns minimum value ...","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fmin_fmax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped) Maxima and Minima for Matrix-Like Objects — fmin-fmax","text":"","code":"## default vector method mpg <- mtcars$mpg fmax(mpg)                         # Maximum value #> [1] 33.9 fmin(mpg)                         # Minimum value (all examples below use fmax but apply to fmin) #> [1] 10.4 fmax(mpg, TRA = \"%\")              # Simple transformation: Take percentage of maximum value #>  [1]  61.94690  61.94690  67.25664  63.12684  55.16224  53.39233  42.18289 #>  [8]  71.97640  67.25664  56.63717  52.50737  48.37758  51.03245  44.83776 #> [15]  30.67847  30.67847  43.36283  95.57522  89.67552 100.00000  63.42183 #> [22]  45.72271  44.83776  39.23304  56.63717  80.53097  76.69617  89.67552 #> [29]  46.60767  58.11209  44.24779  63.12684 fmax(mpg, mtcars$cyl)             # Grouped maximum value #>    4    6    8  #> 33.9 21.4 19.2  fmax(mpg, mtcars[c(2,8:9)])       # More groups.. #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>  26.0  24.4  33.9  21.0  21.4  19.2  15.8  g <- GRP(mtcars, ~ cyl + vs + am) # Precomputing groups gives more speed ! fmax(mpg, g) #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>  26.0  24.4  33.9  21.0  21.4  19.2  15.8  fmax(mpg, g, TRA = \"%\")           # Groupwise percentage of maximum value #>  [1] 100.00000 100.00000  67.25664 100.00000  97.39583  84.57944  74.47917 #>  [8] 100.00000  93.44262  89.71963  83.17757  85.41667  90.10417  79.16667 #> [15]  54.16667  54.16667  76.56250  95.57522  89.67552 100.00000  88.11475 #> [22]  80.72917  79.16667  69.27083 100.00000  80.53097 100.00000  89.67552 #> [29] 100.00000  93.80952  94.93671  63.12684 fmax(mpg, g, TRA = \"replace\")     # Groupwise replace by maximum value #>  [1] 21.0 21.0 33.9 21.4 19.2 21.4 19.2 24.4 24.4 21.4 21.4 19.2 19.2 19.2 19.2 #> [16] 19.2 19.2 33.9 33.9 33.9 24.4 19.2 19.2 19.2 19.2 33.9 26.0 33.9 15.8 21.0 #> [31] 15.8 33.9  ## data.frame method fmax(mtcars) #>     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear  #>  33.900   8.000 472.000 335.000   4.930   5.424  22.900   1.000   1.000   5.000  #>    carb  #>   8.000  head(fmax(mtcars, TRA = \"%\")) #>                        mpg cyl     disp       hp     drat       wt     qsec  vs #> Mazda RX4         61.94690  75 33.89831 32.83582 79.10751 48.30383 71.87773   0 #> Mazda RX4 Wag     61.94690  75 33.89831 32.83582 79.10751 53.00516 74.32314   0 #> Datsun 710        67.25664  50 22.88136 27.76119 78.09331 42.77286 81.26638 100 #> Hornet 4 Drive    63.12684  75 54.66102 32.83582 62.47465 59.27360 84.89083 100 #> Hornet Sportabout 55.16224 100 76.27119 52.23881 63.89452 63.42183 74.32314   0 #> Valiant           53.39233  75 47.66949 31.34328 55.98377 63.79056 88.29694 100 #>                    am gear carb #> Mazda RX4         100   80 50.0 #> Mazda RX4 Wag     100   80 50.0 #> Datsun 710        100   80 12.5 #> Hornet 4 Drive      0   60 12.5 #> Hornet Sportabout   0   60 25.0 #> Valiant             0   60 12.5 fmax(mtcars, g) #>        mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> 4.0.1 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> 4.1.0 24.4   4 146.7  97 3.92 3.190 22.90  1  0    4    2 #> 4.1.1 33.9   4 121.0 113 4.93 2.780 19.90  1  1    5    2 #> 6.0.1 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6 #> 6.1.0 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4 #> 8.0.0 19.2   8 472.0 245 3.73 5.424 18.00  0  0    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fmax(mtcars, g, use.g.names = FALSE) # No row-names generated #>    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> 1 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> 2 24.4   4 146.7  97 3.92 3.190 22.90  1  0    4    2 #> 3 33.9   4 121.0 113 4.93 2.780 19.90  1  1    5    2 #> 4 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6 #> 5 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4 #> 6 19.2   8 472.0 245 3.73 5.424 18.00  0  0    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## matrix method m <- qM(mtcars) fmax(m) #>     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear  #>  33.900   8.000 472.000 335.000   4.930   5.424  22.900   1.000   1.000   5.000  #>    carb  #>   8.000  head(fmax(m, TRA = \"%\")) #>                        mpg cyl     disp       hp     drat       wt     qsec  vs #> Mazda RX4         61.94690  75 33.89831 32.83582 79.10751 48.30383 71.87773   0 #> Mazda RX4 Wag     61.94690  75 33.89831 32.83582 79.10751 53.00516 74.32314   0 #> Datsun 710        67.25664  50 22.88136 27.76119 78.09331 42.77286 81.26638 100 #> Hornet 4 Drive    63.12684  75 54.66102 32.83582 62.47465 59.27360 84.89083 100 #> Hornet Sportabout 55.16224 100 76.27119 52.23881 63.89452 63.42183 74.32314   0 #> Valiant           53.39233  75 47.66949 31.34328 55.98377 63.79056 88.29694 100 #>                    am gear carb #> Mazda RX4         100   80 50.0 #> Mazda RX4 Wag     100   80 50.0 #> Datsun 710        100   80 12.5 #> Hornet 4 Drive      0   60 12.5 #> Hornet Sportabout   0   60 25.0 #> Valiant             0   60 12.5 fmax(m, g) # etc.. #>        mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> 4.0.1 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> 4.1.0 24.4   4 146.7  97 3.92 3.190 22.90  1  0    4    2 #> 4.1.1 33.9   4 121.0 113 4.93 2.780 19.90  1  1    5    2 #> 6.0.1 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6 #> 6.1.0 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4 #> 8.0.0 19.2   8 472.0 245 3.73 5.424 18.00  0  0    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fmax() #>   cyl vs am  mpg  disp  hp drat    wt  qsec gear carb #> 1   4  0  1 26.0 120.3  91 4.43 2.140 16.70    5    2 #> 2   4  1  0 24.4 146.7  97 3.92 3.190 22.90    4    2 #> 3   4  1  1 33.9 121.0 113 4.93 2.780 19.90    5    2 #> 4   6  0  1 21.0 160.0 175 3.90 2.875 17.02    5    6 #> 5   6  1  0 21.4 258.0 123 3.92 3.460 20.22    4    4 #> 6   8  0  0 19.2 472.0 245 3.73 5.424 18.00    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fmax(\"%\") #>                   cyl vs am       mpg      disp       hp      drat        wt #> Mazda RX4           6  0  1 100.00000 100.00000 62.85714 100.00000  91.13043 #> Mazda RX4 Wag       6  0  1 100.00000 100.00000 62.85714 100.00000 100.00000 #> Datsun 710          4  1  1  67.25664  89.25620 82.30088  78.09331  83.45324 #> Hornet 4 Drive      6  1  0 100.00000 100.00000 89.43089  78.57143  92.91908 #> Hornet Sportabout   8  0  0  97.39583  76.27119 71.42857  84.45040  63.42183 #> Valiant             6  1  0  84.57944  87.20930 85.36585  70.40816 100.00000 #>                        qsec gear     carb #> Mazda RX4          96.70975   80 66.66667 #> Mazda RX4 Wag     100.00000   80 66.66667 #> Datsun 710         93.51759   80 50.00000 #> Hornet 4 Drive     96.14243   75 25.00000 #> Hornet Sportabout  94.55556  100 50.00000 #> Valiant           100.00000   75 25.00000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl,vs,am) |> fselect(mpg) |> fmax() #>   cyl vs am  mpg #> 1   4  0  1 26.0 #> 2   4  1  0 24.4 #> 3   4  1  1 33.9 #> 4   6  0  1 21.0 #> 5   6  1  0 21.4 #> 6   8  0  0 19.2 #> 7   8  0  1 15.8"},{"path":"https://sebkrantz.github.io/collapse/reference/fmode.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","title":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","text":"fmode generic function returns (column-wise) statistical mode .e. frequent value x, (optionally) grouped g /weighted w. TRA argument can used transform x using (grouped, weighted) mode. Ties multiple possible modes can resolved taking minimum, maximum, (default) first last occurring mode.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","text":"","code":"fmode(x, ...)  # Default S3 method fmode(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, ties = \"first\", nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'matrix' fmode(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, ties = \"first\", nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'data.frame' fmode(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, ties = \"first\", nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'grouped_df' fmode(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = FALSE, keep.group_vars = TRUE, keep.w = TRUE, stub = .op[[\"stub\"]],       ties = \"first\", nthreads = .op[[\"nthreads\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fmode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","text":"x vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. w numeric vector (non-negative) weights, may contain missing values. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE, NA treated value. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. ties integer character string specifying method resolve ties multiple possible modes .e. multiple values maximum frequency sum weights: Note: \"min\"/\"max\" work character data.          See also Details. nthreads integer. number threads utilize. Parallelism across groups grouped computations column-level otherwise. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. keep.w grouped_df method: Logical. Retain sum weighting variable computation (contained grouped_df). stub character. keep.w = TRUE stub = TRUE (default), summed weights column prefixed \"sum.\". Users can specify different prefix argument, set FALSE avoid prefixing. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmode.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","text":"fmode implements pretty fast C-level hashing algorithm inspired kit package find statistical mode. na.rm = FALSE, NA removed treated value (.e. frequency counted). values NA, NA always returned. weighted mode computed summing weights distinct values choosing value largest sum. na.rm = TRUE, missing values removed x w .e. utilizing x[complete.cases(x,w)] w[complete.cases(x,w)]. possible multiple values mode (maximum frequency sum weights). Typical cases simply values either distinct. cases, default option ties = \"first\" returns first occurring value data reaching maximum frequency count sum weights. example sample x = c(1, 3, 2, 2, 4, 4, 1, 7), first mode 2 fmode goes data left right. ties = \"last\" hand gives 1. also possible take minimum maximum mode, .e. fmode(x, ties = \"min\") returns 1, fmode(x, ties = \"max\") returns 4. noted options ties = \"min\" ties = \"max\" give unintuitive results character data (strict alphabetic sorting, similar using < > compare character values R). options also best avoided missing values counted (na.rm = FALSE) since proper logical comparison missing values possible: numeric data depends, since C++ comparison NA_real_ evaluates FALSE, NA_real_ chosen min max mode also first mode, never otherwise. integer data, NA_integer_ stored smallest integer C++, always chosen min mode never max mode. character data, NA_character_ stored string \"NA\" C++ thus behavior depends character content. fmode preserves attributes objects applied (apart names row-names adjusted necessary grouped operations). data frame passed fmode drop = TRUE (default), unlist called result, might sensible depending data hand.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fmode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","text":"(w weighted) statistical mode x, grouped g, (TRA used) x transformed (grouped, weighed) mode.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fmode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Statistical Mode for Matrix-Like Objects — fmode","text":"","code":"x <- c(1, 3, 2, 2, 4, 4, 1, 7, NA, NA, NA) fmode(x)                            # Default is ties = \"first\" #> [1] 2 fmode(x, ties = \"last\") #> [1] 1 fmode(x, ties = \"min\") #> [1] 1 fmode(x, ties = \"max\") #> [1] 4 fmode(x, na.rm = FALSE)             # Here NA is the mode, regardless of ties option #> [1] NA fmode(x[-length(x)], na.rm = FALSE) # Not anymore.. #> [1] 2  ## World Development Data attach(wlddev) ## default vector method fmode(PCGDP)                      # Numeric mode #> [1] 330.3036 #> attr(,\"label\") #> [1] \"GDP per capita (constant 2010 US$)\" head(fmode(PCGDP, iso3c))         # Grouped numeric mode #>         ABW         AFG         AGO         ALB         AND         ARE  #>  15669.6160    330.3036   3193.4039   1992.2919  41701.5444 103604.9068  head(fmode(PCGDP, iso3c, LIFEEX)) # Grouped and weighted numeric mode #>        ABW        AFG        AGO        ALB        AND        ARE  #> 26630.2053   573.2876  3111.1577  5210.6883         NA 41420.4830  fmode(region)                     # Factor mode #> [1] Europe & Central Asia #> attr(,\"label\") #> [1] Region #> 7 Levels: East Asia & Pacific ... Sub-Saharan Africa fmode(date)                       # Date mode (defaults to first value since panel is balanced) #> [1] \"1961-01-01\" fmode(country)                    # Character mode (also defaults to first value) #> [1] \"Afghanistan\" #> attr(,\"label\") #> [1] \"Country Name\" fmode(OECD)                       # Logical mode #> [1] FALSE #> attr(,\"label\") #> [1] \"Is OECD Member Country?\"                                   # ..all the above can also be performed grouped and weighted ## matrix method m <- qM(airquality) fmode(m) #>   Ozone Solar.R    Wind    Temp   Month     Day  #>    23.0   259.0    11.5    81.0     5.0     1.0  fmode(m, na.rm = FALSE)         # NA frequency is also counted #>   Ozone Solar.R    Wind    Temp   Month     Day  #>      NA      NA    11.5    81.0     5.0     1.0  fmode(m, airquality$Month)      # Groupwise #>   Ozone Solar.R Wind Temp Month Day #> 5    11     190  9.7   66     5   1 #> 6    29     250 11.5   76     6   1 #> 7    97     175  7.4   81     7   1 #> 8    44     255 11.5   86     8   1 #> 9    13     238 10.3   71     9   1 fmode(m, w = airquality$Day)    # Weighted: Later days in the month are given more weight #>   Ozone Solar.R    Wind    Temp   Month     Day  #>    23.0   223.0    11.5    76.0     5.0    30.0  fmode(m>50, airquality$Month)   # Groupwise logical mode #>   Ozone Solar.R  Wind Temp Month   Day #> 5 FALSE    TRUE FALSE TRUE FALSE FALSE #> 6 FALSE    TRUE FALSE TRUE FALSE FALSE #> 7  TRUE    TRUE FALSE TRUE FALSE FALSE #> 8 FALSE    TRUE FALSE TRUE FALSE FALSE #> 9 FALSE    TRUE FALSE TRUE FALSE FALSE                                 # etc.. ## data.frame method fmode(wlddev)                      # Calling unlist -> coerce to character vector #>            country              iso3c               date               year  #>      \"Afghanistan\"                \"2\"            \"-3287\"             \"1960\"  #>             decade             region             income               OECD  #>             \"1960\"                \"2\"                \"1\"            \"FALSE\"  #>              PCGDP             LIFEEX               GINI                ODA  #> \"330.303552908057\"           \"62.869\"             \"26.8\" \"70000.0002980232\"  #>                POP  #>            \"61786\"  fmode(wlddev, drop = FALSE)        # Gives one row #>       country iso3c       date year decade                region      income #> 1 Afghanistan   AFG 1961-01-01 1960   1960 Europe & Central Asia High income #>    OECD    PCGDP LIFEEX GINI   ODA   POP #> 1 FALSE 330.3036 62.869 26.8 70000 61786 head(fmode(wlddev, iso3c))         # Grouped mode #>         country iso3c       date year decade                    region #> ABW       Aruba   ABW 1961-01-01 1960   1960 Latin America & Caribbean #> AFG Afghanistan   AFG 1961-01-01 1960   1960                South Asia #> AGO      Angola   AGO 1961-01-01 1960   1960        Sub-Saharan Africa #> ALB     Albania   ALB 1961-01-01 1960   1960     Europe & Central Asia #> AND     Andorra   AND 1961-01-01 1960   1960     Europe & Central Asia #>                  income  OECD      PCGDP LIFEEX GINI       ODA     POP #> ABW         High income FALSE 15669.6160 65.662   NA  36860001   54211 #> AFG          Low income FALSE   330.3036 32.446   NA 116769997 8996973 #> AGO Lower middle income FALSE  3193.4039 45.201   52   -390000 5454933 #> ALB Upper middle income FALSE  1992.2919 71.860   27   9310000 1608800 #> AND         High income FALSE 41701.5444     NA   NA        NA   13411 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(fmode(wlddev, iso3c, LIFEEX)) # Grouped and weighted mode #>         country iso3c       date year decade                    region #> ABW       Aruba   ABW 2020-01-01 2019   2010 Latin America & Caribbean #> AFG Afghanistan   AFG 2020-01-01 2019   2010                South Asia #> AGO      Angola   AGO 2020-01-01 2019   2010        Sub-Saharan Africa #> ALB     Albania   ALB 2020-01-01 2019   2010     Europe & Central Asia #> AND     Andorra   AND 2021-01-01 2020   2020     Europe & Central Asia #>                  income  OECD      PCGDP LIFEEX GINI        ODA      POP #> ABW         High income FALSE 26630.2053 76.293   NA  -12840000   106314 #> AFG          Low income FALSE   573.2876 64.833   NA 4339979980 38041754 #> AGO Lower middle income FALSE  3111.1577 45.201 51.3   49230000 31825295 #> ALB Upper middle income FALSE  5210.6883 71.860 33.2   31410000  2854191 #> AND         High income FALSE         NA     NA   NA         NA       NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  detach(wlddev)"},{"path":"https://sebkrantz.github.io/collapse/reference/fndistinct.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","title":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","text":"fndistinct generic function (column-wise) computes number distinct values x, (optionally) grouped g. significantly faster length(unique(x)). TRA argument can used transform x using (grouped) distinct value count.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fndistinct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","text":"","code":"fndistinct(x, ...)  # Default S3 method fndistinct(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],            use.g.names = TRUE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'matrix' fndistinct(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],            use.g.names = TRUE, drop = TRUE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'data.frame' fndistinct(x, g = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],            use.g.names = TRUE, drop = TRUE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'grouped_df' fndistinct(x, TRA = NULL, na.rm = .op[[\"na.rm\"]],            use.g.names = FALSE, keep.group_vars = TRUE, nthreads = .op[[\"nthreads\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fndistinct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","text":"x vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. TRUE: Skip missing values x (faster computation). FALSE: Also consider 'NA' one distinct value. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. nthreads integer. number threads utilize. Parallelism across groups grouped computations column-level otherwise. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fndistinct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","text":"fndistinct implements pretty fast C-level hashing algorithm inspired kit package find number distinct values. na.rm = TRUE (default), missing values skipped yielding substantial performance gains data many missing values. na.rm = FALSE, missing values simply treated value read hash-map. Thus former, numeric vector c(1.25,NaN,3.56,NA) distinct value count 2, whereas latter return distinct value count 4. fndistinct preserves attributes non-classed vectors / columns, 'label' attribute (available) classed vectors / columns (.e. dates factors). applied data frames matrices, row-names adjusted necessary.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fndistinct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","text":"Integer. number distinct values x, grouped g, (TRA used) x transformed distinct value count, grouped g.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fndistinct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped) Distinct Value Count for Matrix-Like Objects — fndistinct","text":"","code":"## default vector method fndistinct(airquality$Solar.R)                   # Simple distinct value count #> [1] 117 fndistinct(airquality$Solar.R, airquality$Month) # Grouped distinct value count #>  5  6  7  8  9  #> 27 28 29 27 27   ## data.frame method fndistinct(airquality) #>   Ozone Solar.R    Wind    Temp   Month     Day  #>      67     117      31      40       5      31  fndistinct(airquality, airquality$Month) #>   Ozone Solar.R Wind Temp Month Day #> 5    21      27   18   18     1  31 #> 6     9      28   16   19     1  30 #> 7    24      29   17   14     1  31 #> 8    24      27   18   19     1  31 #> 9    21      27   19   20     1  30 fndistinct(wlddev)                               # Works with data of all types! #> country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX  #>     216     216      61      61       7       7       4       2    9470   10548  #>    GINI     ODA     POP  #>     368    7832   12877  head(fndistinct(wlddev, wlddev$iso3c)) #>     country iso3c date year decade region income OECD PCGDP LIFEEX GINI ODA POP #> ABW       1     1   61   61      7      1      1    1    32     60    0  20  60 #> AFG       1     1   61   61      7      1      1    1    18     60    0  60  60 #> AGO       1     1   61   61      7      1      1    1    40     59    3  58  60 #> ALB       1     1   61   61      7      1      1    1    40     59    9  32  60 #> AND       1     1   61   61      7      1      1    1    50      0    0   0  60 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## matrix method aqm <- qM(airquality) fndistinct(aqm)                                  # Also works for character or logical matrices #>   Ozone Solar.R    Wind    Temp   Month     Day  #>      67     117      31      40       5      31  fndistinct(aqm, airquality$Month) #>   Ozone Solar.R Wind Temp Month Day #> 5    21      27   18   18     1  31 #> 6     9      28   16   19     1  30 #> 7    24      29   17   14     1  31 #> 8    24      27   18   19     1  31 #> 9    21      27   19   20     1  30  ## method for grouped data frames - created with dplyr::group_by or fgroup_by airquality |> fgroup_by(Month) |> fndistinct() #>   Month Ozone Solar.R Wind Temp Day #> 1     5    21      27   18   18  31 #> 2     6     9      28   16   19  30 #> 3     7    24      29   17   14  31 #> 4     8    24      27   18   19  31 #> 5     9    21      27   19   20  30 wlddev |> fgroup_by(country) |>              fselect(PCGDP,LIFEEX,GINI,ODA) |> fndistinct() #>                country PCGDP LIFEEX GINI ODA #> 1          Afghanistan    18     60    0  60 #> 2              Albania    40     59    9  32 #> 3              Algeria    60     60    3  60 #> 4       American Samoa    17      0    0   0 #> 5              Andorra    50      0    0   0 #> 6               Angola    40     59    3  58 #> 7  Antigua and Barbuda    43     60    0  47 #> 8            Argentina    60     60   29  60 #> 9              Armenia    30     59   20  29 #> 10               Aruba    32     60    0  20 #> 11           Australia    60     59    9   0 #> 12             Austria    60     60   16   0 #> 13          Azerbaijan    30     60    5  29 #> 14        Bahamas, The    60     59    0  41 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 202 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/fnobs.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","title":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","text":"fnobs generic function (column-wise) computes number non-missing values x, (optionally) grouped g. much faster sum(!.na(x)). TRA argument can used transform x using (grouped) observation count.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fnobs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","text":"","code":"fnobs(x, ...)  # Default S3 method fnobs(x, g = NULL, TRA = NULL, use.g.names = TRUE, ...)  # S3 method for class 'matrix' fnobs(x, g = NULL, TRA = NULL, use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'data.frame' fnobs(x, g = NULL, TRA = NULL, use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'grouped_df' fnobs(x, TRA = NULL, use.g.names = FALSE, keep.group_vars = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fnobs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","text":"x vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fnobs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","text":"fnobs preserves attributes non-classed vectors / columns, 'label' attribute (available) classed vectors / columns (.e. dates factors). applied data frames matrices, row-names adjusted necessary.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fnobs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","text":"Integer. number non-missing observations x, grouped g, (TRA used) x transformed number non-missing observations, grouped g.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fnobs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped) Observation Count for Matrix-Like Objects — fnobs","text":"","code":"## default vector method fnobs(airquality$Solar.R)                   # Simple Nobs #> [1] 146 fnobs(airquality$Solar.R, airquality$Month) # Grouped Nobs #>  5  6  7  8  9  #> 27 30 31 28 30   ## data.frame method fnobs(airquality) #>   Ozone Solar.R    Wind    Temp   Month     Day  #>     116     146     153     153     153     153  fnobs(airquality, airquality$Month) #>   Ozone Solar.R Wind Temp Month Day #> 5    26      27   31   31    31  31 #> 6     9      30   30   30    30  30 #> 7    26      31   31   31    31  31 #> 8    26      28   31   31    31  31 #> 9    29      30   30   30    30  30 fnobs(wlddev)                               # Works with data of all types! #> country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX  #>   13176   13176   13176   13176   13176   13176   13176   13176    9470   11670  #>    GINI     ODA     POP  #>    1744    8608   12919  head(fnobs(wlddev, wlddev$iso3c)) #>     country iso3c date year decade region income OECD PCGDP LIFEEX GINI ODA POP #> ABW      61    61   61   61     61     61     61   61    32     60    0  20  60 #> AFG      61    61   61   61     61     61     61   61    18     60    0  60  60 #> AGO      61    61   61   61     61     61     61   61    40     60    3  58  60 #> ALB      61    61   61   61     61     61     61   61    40     60    9  32  60 #> AND      61    61   61   61     61     61     61   61    50      0    0   0  60 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## matrix method aqm <- qM(airquality) fnobs(aqm)                                  # Also works for character or logical matrices #>   Ozone Solar.R    Wind    Temp   Month     Day  #>     116     146     153     153     153     153  fnobs(aqm, airquality$Month) #>   Ozone Solar.R Wind Temp Month Day #> 5    26      27   31   31    31  31 #> 6     9      30   30   30    30  30 #> 7    26      31   31   31    31  31 #> 8    26      28   31   31    31  31 #> 9    29      30   30   30    30  30  ## method for grouped data frames - created with dplyr::group_by or fgroup_by airquality |> fgroup_by(Month) |> fnobs() #>   Month Ozone Solar.R Wind Temp Day #> 1     5    26      27   31   31  31 #> 2     6     9      30   30   30  30 #> 3     7    26      31   31   31  31 #> 4     8    26      28   31   31  31 #> 5     9    29      30   30   30  30 wlddev |> fgroup_by(country) |>            fselect(PCGDP,LIFEEX,GINI,ODA) |> fnobs() #>                country PCGDP LIFEEX GINI ODA #> 1          Afghanistan    18     60    0  60 #> 2              Albania    40     60    9  32 #> 3              Algeria    60     60    3  60 #> 4       American Samoa    17      0    0   0 #> 5              Andorra    50      0    0   0 #> 6               Angola    40     60    3  58 #> 7  Antigua and Barbuda    43     60    0  47 #> 8            Argentina    60     60   31  60 #> 9              Armenia    30     60   20  29 #> 10               Aruba    32     60    0  20 #> 11           Australia    60     60   10   0 #> 12             Austria    60     60   21   0 #> 13          Azerbaijan    30     60    6  29 #> 14        Bahamas, The    60     60    0  43 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 202 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/fnth_fmedian.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","title":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","text":"fnth (column-wise) returns n'th smallest element set unsorted elements x corresponding integer index (n), probability 0 1. n passed probability, ties can resolved using lower, upper, average possible elements, (default) continuous quantile estimation. n > 1, lower element always returned (sort(x, partial = n)[n]). See Details. fmedian simple wrapper around fnth, fixes n = 0.5 (default) ties = \"mean\", .e., averages eligible elements. See Details.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fnth_fmedian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","text":"","code":"fnth(x, n = 0.5, ...) fmedian(x, ...)  # Default S3 method fnth(x, n = 0.5, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, ties = \"q7\", nthreads = .op[[\"nthreads\"]],      o = NULL, check.o = is.null(attr(o, \"sorted\")), ...) # Default S3 method fmedian(x, ..., ties = \"mean\")  # S3 method for class 'matrix' fnth(x, n = 0.5, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, ties = \"q7\", nthreads = .op[[\"nthreads\"]], ...) # S3 method for class 'matrix' fmedian(x, ..., ties = \"mean\")  # S3 method for class 'data.frame' fnth(x, n = 0.5, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, ties = \"q7\", nthreads = .op[[\"nthreads\"]], ...) # S3 method for class 'data.frame' fmedian(x, ..., ties = \"mean\")  # S3 method for class 'grouped_df' fnth(x, n = 0.5, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = FALSE, keep.group_vars = TRUE, keep.w = TRUE, stub = .op[[\"stub\"]],      ties = \"q7\", nthreads = .op[[\"nthreads\"]], ...) # S3 method for class 'grouped_df' fmedian(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],         use.g.names = FALSE, keep.group_vars = TRUE, keep.w = TRUE, stub = .op[[\"stub\"]],         ties = \"mean\", nthreads = .op[[\"nthreads\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fnth_fmedian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","text":"x numeric vector, matrix, data frame grouped data frame (class 'grouped_df'). n element return using single integer index 1 < n < NROW(x), probability 0 < n < 1. See Details. g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. w numeric vector (non-negative) weights, may contain missing values x also missing. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE NA returned encountered. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. ties integer character string specifying method resolve ties adjacent qualifying elements: nthreads integer. number threads utilize. Parallelism across groups grouped computations vectors data frames, column-level otherwise. See Details. o integer. valid ordering x, e.g. radixorder(x). groups, grouping needs accounted e.g. radixorder(g, x). check.o logical. TRUE checks element o within [1, length(x)]. default uses fact orderings radixorder \"sorted\" attribute fnth infer ordering valid. length data type o always checked, regardless check.o. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. keep.w grouped_df method: Logical. Retain sum weighting variable computation (contained grouped_df). stub character. keep.w = TRUE stub = TRUE (default), summed weights column prefixed \"sum.\". Users can specify different prefix argument, set FALSE avoid prefixing. ... fmedian: arguments passed fnth (apart n). TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fnth_fmedian.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","text":"fnth uses combination quickselect, quicksort, radixsort algorithms, combined several (weighted) quantile estimation methods , possible, OpenMP multithreading: without weights, quickselect used determine (lower) order statistic. ties %!% c(\"min\", \"max\") second order statistic found taking max upper part partitioned array, two statistics averaged using simple mean (ties = \"mean\"), weighted average according quantile method (ties = \"q4\"-\"q9\"). n = 0.5, supported quantile methods give sample median. matrices, multithreading always across columns, vectors data frames across groups unless .null(g) data frames. weights groups (.null(g)), radixorder called internally (column x). ordering used sum weights order x determine weighted order statistics quantiles. See details . Multithreading disabled radixorder called concurrently memory stack. weights groups (!.null(g)), R's quicksort algorithm used sort data group return index can used sum weights order proceed . multithreaded across columns matrices, across groups otherwise. fnth.default, ordering x can supplied 'o' e.g. fnth(x, 0.75, o = radixorder(x)). dramatically speeds estimation without weights, useful fnth invoked repeatedly data. groups, o needs also account grouping e.g. fnth(x, 0.75, g, o = radixorder(g, x)). Multithreading possible across groups. See Examples. n > 1, result equivalent (column-wise) sort(x, partial = n)[n]. Internally, n converted probability using p = (n-1)/(NROW(x)-1), probability applied set non-missing elements find .integer(p*(fnobs(x)-1))+1L'th element (corresponds option ties = \"min\").  using grouped computations n > 1, n transformed probability p = (n-1)/(NROW(x)/ng-1) (ng contains number unique groups g). weights used ties = \"q4\"-\"q9\", weighted continuous quantile estimation done described fquantile. ties %% c(\"mean\", \"min\", \"max\"), target partial sum weights p*sum(w) calculated, weighted n'th element element k elements smaller k sum weights <= p*sum(w), elements larger k sum weights <= (1 - p)*sum(w). partial-sum weights (p*sum(w)) reached exactly element k, (summing lower end) k k+1 qualify weighted n'th element. weight element k+1 zero, k, k+1 k+2 qualify... . n > 1, k chosen (consistent unweighted behavior).  0 < n < 1, ties option regulates resolve conflicts, yielding lower (ties = \"min\": k), upper (ties = \"max\": k+2) average weighted (ties = \"mean\": mean(k, k+1, k+2)) n'th elements. Thus, presence zero weights, weighted median (default ties = \"mean\") can arithmetic average >2 qualifying elements. data frames, column-attributes overall attributes preserved g used drop = FALSE.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fnth_fmedian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","text":"(w weighted) n'th element/quantile x, grouped g, (TRA used) x transformed (grouped, weighted) n'th element/quantile.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fnth_fmedian.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) N'th Element/Quantile for Matrix-Like Objects — fnth-fmedian","text":"","code":"## default vector method mpg <- mtcars$mpg fnth(mpg)                         # Simple nth element: Median (same as fmedian(mpg)) #> [1] 19.2 fnth(mpg, 5)                      # 5th smallest element #> [1] 14.7 sort(mpg, partial = 5)[5]         # Same using base R, fnth is 2x faster. #> [1] 14.7 fnth(mpg, 0.75)                   # Third quartile #> [1] 22.8 fnth(mpg, 0.75, w = mtcars$hp)    # Weighted third quartile: Weighted by hp #> [1] 20.79909 fnth(mpg, 0.75, TRA = \"-\")        # Simple transformation: Subtract third quartile #>  [1]  -1.8  -1.8   0.0  -1.4  -4.1  -4.7  -8.5   1.6   0.0  -3.6  -5.0  -6.4 #> [13]  -5.5  -7.6 -12.4 -12.4  -8.1   9.6   7.6  11.1  -1.3  -7.3  -7.6  -9.5 #> [25]  -3.6   4.5   3.2   7.6  -7.0  -3.1  -7.8  -1.4 fnth(mpg, 0.75, mtcars$cyl)             # Grouped third quartile #>     4     6     8  #> 30.40 21.00 16.25  fnth(mpg, 0.75, mtcars[c(2,8:9)])       # More groups.. #>  4.0.1  4.1.0  4.1.1  6.0.1  6.1.0  8.0.0  8.0.1  #> 26.000 23.600 31.400 21.000 19.750 16.625 15.600  g <- GRP(mtcars, ~ cyl + vs + am)       # Precomputing groups gives more speed ! fnth(mpg, 0.75, g) #>  4.0.1  4.1.0  4.1.1  6.0.1  6.1.0  8.0.0  8.0.1  #> 26.000 23.600 31.400 21.000 19.750 16.625 15.600  fnth(mpg, 0.75, g, mtcars$hp)           # Grouped weighted third quartile #>    4.0.1    4.1.0    4.1.1    6.0.1    6.1.0    8.0.0    8.0.1  #> 26.00000 23.17474 30.51538 21.00000 19.65610 16.36250 15.54621  fnth(mpg, 0.75, g, TRA = \"-\")           # Groupwise subtract third quartile #>  [1]   0.000   0.000  -8.600   1.650   2.075  -1.650  -2.325   0.800  -0.800 #> [10]  -0.550  -1.950  -0.225   0.675  -1.425  -6.225  -6.225  -1.925   1.000 #> [19]  -1.000   2.500  -2.100  -1.125  -1.425  -3.325   2.575  -4.100   0.000 #> [28]  -1.000   0.200  -1.300  -0.600 -10.000 fnth(mpg, 0.75, g, mtcars$hp, \"-\")      # Groupwise subtract weighted third quartile #>  [1]  0.0000000  0.0000000 -7.7153846  1.7439024  2.3375000 -1.5560976 #>  [7] -2.0625000  1.2252632 -0.3747368 -0.4560976 -1.8560976  0.0375000 #> [13]  0.9375000 -1.1625000 -5.9625000 -5.9625000 -1.6625000  1.8846154 #> [19] -0.1153846  3.3846154 -1.6747368 -0.8625000 -1.1625000 -3.0625000 #> [25]  2.8375000 -3.2153846  0.0000000 -0.1153846  0.2537879 -1.3000000 #> [31] -0.5462121 -9.1153846  ## data.frame method fnth(mtcars, 0.75) #>    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb  #>  22.80   8.00 326.00 180.00   3.92   3.61  18.90   1.00   1.00   4.00   4.00  head(fnth(mtcars, 0.75, TRA = \"-\")) #>                    mpg cyl disp  hp  drat     wt  qsec vs am gear carb #> Mazda RX4         -1.8  -2 -166 -70 -0.02 -0.990 -2.44 -1  0    0    0 #> Mazda RX4 Wag     -1.8  -2 -166 -70 -0.02 -0.735 -1.88 -1  0    0    0 #> Datsun 710         0.0  -4 -218 -87 -0.07 -1.290 -0.29  0  0    0   -3 #> Hornet 4 Drive    -1.4  -2  -68 -70 -0.84 -0.395  0.54  0 -1   -1   -3 #> Hornet Sportabout -4.1   0   34  -5 -0.77 -0.170 -1.88 -1 -1   -1   -2 #> Valiant           -4.7  -2 -101 -75 -1.16 -0.150  1.32  0 -1   -1   -3 fnth(mtcars, 0.75, g) #>          mpg cyl   disp     hp  drat     wt   qsec vs am gear carb #> 4.0.1 26.000   4 120.30  91.00 4.430 2.1400 16.700  0  1  5.0    2 #> 4.1.0 23.600   4 143.75  96.00 3.810 3.1700 21.455  1  0  4.0    2 #> 4.1.1 31.400   4 101.55 101.00 4.165 2.2600 19.185  1  1  4.0    2 #> 6.0.1 21.000   6 160.00 142.50 3.900 2.8225 16.740  0  1  4.5    5 #> 6.1.0 19.750   6 233.25 123.00 3.920 3.4450 19.635  1  0  4.0    4 #> 8.0.0 16.625   8 410.00 218.75 3.165 4.3650 17.655  0  0  3.0    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fnth(fgroup_by(mtcars, cyl, vs, am), 0.75)   # Another way of doing it.. #>   cyl vs am    mpg   disp     hp  drat     wt   qsec gear carb #> 1   4  0  1 26.000 120.30  91.00 4.430 2.1400 16.700  5.0    2 #> 2   4  1  0 23.600 143.75  96.00 3.810 3.1700 21.455  4.0    2 #> 3   4  1  1 31.400 101.55 101.00 4.165 2.2600 19.185  4.0    2 #> 4   6  0  1 21.000 160.00 142.50 3.900 2.8225 16.740  4.5    5 #> 5   6  1  0 19.750 233.25 123.00 3.920 3.4450 19.635  4.0    4 #> 6   8  0  0 16.625 410.00 218.75 3.165 4.3650 17.655  3.0    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fnth(mtcars, 0.75, g, use.g.names = FALSE)   # No row-names generated #>      mpg cyl   disp     hp  drat     wt   qsec vs am gear carb #> 1 26.000   4 120.30  91.00 4.430 2.1400 16.700  0  1  5.0    2 #> 2 23.600   4 143.75  96.00 3.810 3.1700 21.455  1  0  4.0    2 #> 3 31.400   4 101.55 101.00 4.165 2.2600 19.185  1  1  4.0    2 #> 4 21.000   6 160.00 142.50 3.900 2.8225 16.740  0  1  4.5    5 #> 5 19.750   6 233.25 123.00 3.920 3.4450 19.635  1  0  4.0    4 #> 6 16.625   8 410.00 218.75 3.165 4.3650 17.655  0  0  3.0    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## matrix method m <- qM(mtcars) fnth(m, 0.75) #>    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb  #>  22.80   8.00 326.00 180.00   3.92   3.61  18.90   1.00   1.00   4.00   4.00  head(fnth(m, 0.75, TRA = \"-\")) #>                    mpg cyl disp  hp  drat     wt  qsec vs am gear carb #> Mazda RX4         -1.8  -2 -166 -70 -0.02 -0.990 -2.44 -1  0    0    0 #> Mazda RX4 Wag     -1.8  -2 -166 -70 -0.02 -0.735 -1.88 -1  0    0    0 #> Datsun 710         0.0  -4 -218 -87 -0.07 -1.290 -0.29  0  0    0   -3 #> Hornet 4 Drive    -1.4  -2  -68 -70 -0.84 -0.395  0.54  0 -1   -1   -3 #> Hornet Sportabout -4.1   0   34  -5 -0.77 -0.170 -1.88 -1 -1   -1   -2 #> Valiant           -4.7  -2 -101 -75 -1.16 -0.150  1.32  0 -1   -1   -3 fnth(m, 0.75, g) # etc.. #>          mpg cyl   disp     hp  drat     wt   qsec vs am gear carb #> 4.0.1 26.000   4 120.30  91.00 4.430 2.1400 16.700  0  1  5.0    2 #> 4.1.0 23.600   4 143.75  96.00 3.810 3.1700 21.455  1  0  4.0    2 #> 4.1.1 31.400   4 101.55 101.00 4.165 2.2600 19.185  1  1  4.0    2 #> 6.0.1 21.000   6 160.00 142.50 3.900 2.8225 16.740  0  1  4.5    5 #> 6.1.0 19.750   6 233.25 123.00 3.920 3.4450 19.635  1  0  4.0    4 #> 8.0.0 16.625   8 410.00 218.75 3.165 4.3650 17.655  0  0  3.0    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fnth(0.75) #>   cyl vs am    mpg   disp     hp  drat     wt   qsec gear carb #> 1   4  0  1 26.000 120.30  91.00 4.430 2.1400 16.700  5.0    2 #> 2   4  1  0 23.600 143.75  96.00 3.810 3.1700 21.455  4.0    2 #> 3   4  1  1 31.400 101.55 101.00 4.165 2.2600 19.185  4.0    2 #> 4   6  0  1 21.000 160.00 142.50 3.900 2.8225 16.740  4.5    5 #> 5   6  1  0 19.750 233.25 123.00 3.920 3.4450 19.635  4.0    4 #> 6   8  0  0 16.625 410.00 218.75 3.165 4.3650 17.655  3.0    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fnth(0.75, hp)         # Weighted #>   cyl vs am sum.hp      mpg     disp     drat       wt     qsec     gear #> 1   4  0  1     91 26.00000 120.3000 4.430000 2.140000 16.70000 5.000000 #> 2   4  1  0    254 23.17474 142.1818 3.827947 3.159368 21.69076 4.000000 #> 3   4  1  1    564 30.51538 106.7863 4.113280 2.308710 18.95614 4.000000 #> 4   6  0  1    395 21.00000 160.0000 3.900000 2.806989 16.65727 4.685714 #> 5   6  1  0    461 19.65610 231.6000 3.920000 3.443333 19.56232 4.000000 #> 6   8  0  0   2330 16.36250 421.7391 3.198673 4.753537 17.67291 3.000000 #>       carb #> 1 2.000000 #> 2 2.000000 #> 3 2.000000 #> 4 5.371429 #> 5 4.000000 #> 6 4.000000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fnth(0.75, TRA = \"/\")  # Divide by third quartile #>                   cyl vs am       mpg      disp        hp      drat        wt #> Mazda RX4           6  0  1 1.0000000 1.0000000 0.7719298 1.0000000 0.9282551 #> Mazda RX4 Wag       6  0  1 1.0000000 1.0000000 0.7719298 1.0000000 1.0186005 #> Datsun 710          4  1  1 0.7261146 1.0635155 0.9207921 0.9243697 1.0265487 #> Hornet 4 Drive      6  1  0 1.0835443 1.1061093 0.8943089 0.7857143 0.9332366 #> Hornet Sportabout   8  0  0 1.1248120 0.8780488 0.8000000 0.9952607 0.7880871 #> Valiant             6  1  0 0.9164557 0.9646302 0.8536585 0.7040816 1.0043541 #>                        qsec      gear carb #> Mazda RX4         0.9832736 0.8888889 0.80 #> Mazda RX4 Wag     1.0167264 0.8888889 0.80 #> Datsun 710        0.9700287 1.0000000 0.50 #> Hornet 4 Drive    0.9900688 0.7500000 0.25 #> Hornet Sportabout 0.9640329 1.0000000 0.50 #> Valiant           1.0297937 0.7500000 0.25 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl,vs,am) |> fselect(mpg, hp) |>    # Faster selecting       fnth(0.75, hp, \"/\")  # Divide mpg by its third weighted group-quartile, using hp as weights #>                      hp       mpg #> Mazda RX4           110 1.0000000 #> Mazda RX4 Wag       110 1.0000000 #> Datsun 710           93 0.7471641 #> Hornet 4 Drive      110 1.0887207 #> Hornet Sportabout   175 1.1428571 #> Valiant             105 0.9208339 #> Duster 360          245 0.8739496 #> Merc 240D            62 1.0528706 #> Merc 230             95 0.9838299 #> Merc 280            123 0.9767961 #> Merc 280C           123 0.9055714 #> Merc 450SE          180 1.0022918 #> Merc 450SL          180 1.0572956 #> Merc 450SLC         180 0.9289534 #> Cadillac Fleetwood  205 0.6355997 #> Lincoln Continental 215 0.6355997 #> Chrysler Imperial   230 0.8983957 #> Fiat 128             66 1.0617595 #> Honda Civic          52 0.9962188 #> Toyota Corolla       65 1.1109150 #> Toyota Corona        97 0.9277344 #> Dodge Challenger    150 0.9472880 #> AMC Javelin         150 0.9289534 #> Camaro Z28          245 0.8128342 #> Pontiac Firebird    175 1.1734148 #> Fiat X1-9            66 0.8946307 #> Porsche 914-2        91 1.0000000 #> Lotus Europa        113 0.9962188 #> Ford Pantera L      264 1.0163247 #> Ferrari Dino        175 0.9380952 #> Maserati Bora       335 0.9648653 #> Volvo 142E          109 0.7012856 #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]   # Efficient grouped estimation of multiple quantiles mtcars |> fgroup_by(cyl,vs,am) |>     fmutate(o = radixorder(GRPid(), mpg)) |>     fsummarise(mpg_Q1 = fnth(mpg, 0.25, o = o),                mpg_median = fmedian(mpg, o = o),                mpg_Q3 = fnth(mpg, 0.75, o = o)) #>   cyl vs am mpg_Q1 mpg_median mpg_Q3 #> 1   4  0  1 26.000      26.00 26.000 #> 2   4  1  0 22.150      22.80 23.600 #> 3   4  1  1 25.050      30.40 31.400 #> 4   6  0  1 20.350      21.00 21.000 #> 5   6  1  0 18.025      18.65 19.750 #> 6   8  0  0 14.050      15.20 16.625 #> 7   8  0  1 15.200      15.40 15.600  ## fmedian() fmedian(mpg)                         # Simple median value #> [1] 19.2 fmedian(mpg, w = mtcars$hp)          # Weighted median: Weighted by hp #> [1] 16.4 fmedian(mpg, TRA = \"-\")              # Simple transformation: Subtract median value #>  [1]  1.8  1.8  3.6  2.2 -0.5 -1.1 -4.9  5.2  3.6  0.0 -1.4 -2.8 -1.9 -4.0 -8.8 #> [16] -8.8 -4.5 13.2 11.2 14.7  2.3 -3.7 -4.0 -5.9  0.0  8.1  6.8 11.2 -3.4  0.5 #> [31] -4.2  2.2 fmedian(mpg, mtcars$cyl)             # Grouped median value #>    4    6    8  #> 26.0 19.7 15.2  fmedian(mpg, mtcars[c(2,8:9)])       # More groups.. #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #> 26.00 22.80 30.40 21.00 18.65 15.20 15.40  fmedian(mpg, g) #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #> 26.00 22.80 30.40 21.00 18.65 15.20 15.40  fmedian(mpg, g, mtcars$hp)           # Grouped weighted median #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>  26.0  22.8  30.4  21.0  19.2  15.2  15.0  fmedian(mpg, g, TRA = \"-\")           # Groupwise subtract median value #>  [1]  0.00  0.00 -7.60  2.75  3.50 -0.55 -0.90  1.60  0.00  0.55 -0.85  1.20 #> [13]  2.10  0.00 -4.80 -4.80 -0.50  2.00  0.00  3.50 -1.30  0.30  0.00 -1.90 #> [25]  4.00 -3.10  0.00  0.00  0.40 -1.30 -0.40 -9.00 fmedian(mpg, g, mtcars$hp, \"-\")      # Groupwise subtract weighted median value #>  [1]  0.0  0.0 -7.6  2.2  3.5 -1.1 -0.9  1.6  0.0  0.0 -1.4  1.2  2.1  0.0 -4.8 #> [16] -4.8 -0.5  2.0  0.0  3.5 -1.3  0.3  0.0 -1.9  4.0 -3.1  0.0  0.0  0.8 -1.3 #> [31]  0.0 -9.0  ## data.frame method fmedian(mtcars) #>     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear  #>  19.200   6.000 196.300 123.000   3.695   3.325  17.710   0.000   0.000   4.000  #>    carb  #>   2.000  head(fmedian(mtcars, TRA = \"-\")) #>                    mpg cyl  disp  hp   drat     wt  qsec vs am gear carb #> Mazda RX4          1.8   0 -36.3 -13  0.205 -0.705 -1.25  0  1    0    2 #> Mazda RX4 Wag      1.8   0 -36.3 -13  0.205 -0.450 -0.69  0  1    0    2 #> Datsun 710         3.6  -2 -88.3 -30  0.155 -1.005  0.90  1  1    0   -1 #> Hornet 4 Drive     2.2   0  61.7 -13 -0.615 -0.110  1.73  1  0   -1   -1 #> Hornet Sportabout -0.5   2 163.7  52 -0.545  0.115 -0.69  0  0   -1    0 #> Valiant           -1.1   0  28.7 -18 -0.935  0.135  2.51  1  0   -1   -1 fmedian(mtcars, g) #>         mpg cyl  disp    hp  drat    wt  qsec vs am gear carb #> 4.0.1 26.00   4 120.3  91.0 4.430 2.140 16.70  0  1  5.0  2.0 #> 4.1.0 22.80   4 140.8  95.0 3.700 3.150 20.01  1  0  4.0  2.0 #> 4.1.1 30.40   4  79.0  66.0 4.080 1.935 18.61  1  1  4.0  1.0 #> 6.0.1 21.00   6 160.0 110.0 3.900 2.770 16.46  0  1  4.0  4.0 #> 6.1.0 18.65   6 196.3 116.5 3.500 3.440 19.17  1  0  3.5  2.5 #> 8.0.0 15.20   8 355.0 180.0 3.075 3.810 17.35  0  0  3.0  3.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fmedian(fgroup_by(mtcars, cyl, vs, am))   # Another way of doing it.. #>   cyl vs am   mpg  disp    hp  drat    wt  qsec gear carb #> 1   4  0  1 26.00 120.3  91.0 4.430 2.140 16.70  5.0  2.0 #> 2   4  1  0 22.80 140.8  95.0 3.700 3.150 20.01  4.0  2.0 #> 3   4  1  1 30.40  79.0  66.0 4.080 1.935 18.61  4.0  1.0 #> 4   6  0  1 21.00 160.0 110.0 3.900 2.770 16.46  4.0  4.0 #> 5   6  1  0 18.65 196.3 116.5 3.500 3.440 19.17  3.5  2.5 #> 6   8  0  0 15.20 355.0 180.0 3.075 3.810 17.35  3.0  3.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fmedian(mtcars, g, use.g.names = FALSE)   # No row-names generated #>     mpg cyl  disp    hp  drat    wt  qsec vs am gear carb #> 1 26.00   4 120.3  91.0 4.430 2.140 16.70  0  1  5.0  2.0 #> 2 22.80   4 140.8  95.0 3.700 3.150 20.01  1  0  4.0  2.0 #> 3 30.40   4  79.0  66.0 4.080 1.935 18.61  1  1  4.0  1.0 #> 4 21.00   6 160.0 110.0 3.900 2.770 16.46  0  1  4.0  4.0 #> 5 18.65   6 196.3 116.5 3.500 3.440 19.17  1  0  3.5  2.5 #> 6 15.20   8 355.0 180.0 3.075 3.810 17.35  0  0  3.0  3.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## matrix method fmedian(m) #>     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear  #>  19.200   6.000 196.300 123.000   3.695   3.325  17.710   0.000   0.000   4.000  #>    carb  #>   2.000  head(fmedian(m, TRA = \"-\")) #>                    mpg cyl  disp  hp   drat     wt  qsec vs am gear carb #> Mazda RX4          1.8   0 -36.3 -13  0.205 -0.705 -1.25  0  1    0    2 #> Mazda RX4 Wag      1.8   0 -36.3 -13  0.205 -0.450 -0.69  0  1    0    2 #> Datsun 710         3.6  -2 -88.3 -30  0.155 -1.005  0.90  1  1    0   -1 #> Hornet 4 Drive     2.2   0  61.7 -13 -0.615 -0.110  1.73  1  0   -1   -1 #> Hornet Sportabout -0.5   2 163.7  52 -0.545  0.115 -0.69  0  0   -1    0 #> Valiant           -1.1   0  28.7 -18 -0.935  0.135  2.51  1  0   -1   -1 fmedian(m, g) # etc.. #>         mpg cyl  disp    hp  drat    wt  qsec vs am gear carb #> 4.0.1 26.00   4 120.3  91.0 4.430 2.140 16.70  0  1  5.0  2.0 #> 4.1.0 22.80   4 140.8  95.0 3.700 3.150 20.01  1  0  4.0  2.0 #> 4.1.1 30.40   4  79.0  66.0 4.080 1.935 18.61  1  1  4.0  1.0 #> 6.0.1 21.00   6 160.0 110.0 3.900 2.770 16.46  0  1  4.0  4.0 #> 6.1.0 18.65   6 196.3 116.5 3.500 3.440 19.17  1  0  3.5  2.5 #> 8.0.0 15.20   8 355.0 180.0 3.075 3.810 17.35  0  0  3.0  3.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fmedian() #>   cyl vs am   mpg  disp    hp  drat    wt  qsec gear carb #> 1   4  0  1 26.00 120.3  91.0 4.430 2.140 16.70  5.0  2.0 #> 2   4  1  0 22.80 140.8  95.0 3.700 3.150 20.01  4.0  2.0 #> 3   4  1  1 30.40  79.0  66.0 4.080 1.935 18.61  4.0  1.0 #> 4   6  0  1 21.00 160.0 110.0 3.900 2.770 16.46  4.0  4.0 #> 5   6  1  0 18.65 196.3 116.5 3.500 3.440 19.17  3.5  2.5 #> 6   8  0  0 15.20 355.0 180.0 3.075 3.810 17.35  3.0  3.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fmedian(hp)           # Weighted #>   cyl vs am sum.hp  mpg  disp drat    wt  qsec gear carb #> 1   4  0  1     91 26.0 120.3 4.43 2.140 16.70    5    2 #> 2   4  1  0    254 22.8 140.8 3.70 3.150 20.01    4    2 #> 3   4  1  1    564 30.4  95.1 4.08 1.935 18.61    4    1 #> 4   6  0  1    395 21.0 160.0 3.90 2.770 16.46    4    4 #> 5   6  1  0    461 19.2 167.6 3.92 3.440 18.90    4    4 #> 6   8  0  0   2330 15.2 360.0 3.08 3.840 17.40    3    3 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fmedian(TRA = \"-\")    # De-median #>                   cyl vs am   mpg disp    hp   drat     wt  qsec gear carb #> Mazda RX4           6  0  1  0.00  0.0   0.0  0.000 -0.150  0.00  0.0  0.0 #> Mazda RX4 Wag       6  0  1  0.00  0.0   0.0  0.000  0.105  0.56  0.0  0.0 #> Datsun 710          4  1  1 -7.60 29.0  27.0 -0.230  0.385  0.00  0.0  0.0 #> Hornet 4 Drive      6  1  0  2.75 61.7  -6.5 -0.420 -0.225  0.27 -0.5 -1.5 #> Hornet Sportabout   8  0  0  3.50  5.0  -5.0  0.075 -0.370 -0.33  0.0 -1.0 #> Valiant             6  1  0 -0.55 28.7 -11.5 -0.740  0.020  1.05 -0.5 -1.5 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl,vs,am) |> fselect(mpg, hp) |>   # Faster selecting       fmedian(hp, \"-\")  # Weighted de-median mpg, using hp as weights #>                      hp  mpg #> Mazda RX4           110  0.0 #> Mazda RX4 Wag       110  0.0 #> Datsun 710           93 -7.6 #> Hornet 4 Drive      110  2.2 #> Hornet Sportabout   175  3.5 #> Valiant             105 -1.1 #> Duster 360          245 -0.9 #> Merc 240D            62  1.6 #> Merc 230             95  0.0 #> Merc 280            123  0.0 #> Merc 280C           123 -1.4 #> Merc 450SE          180  1.2 #> Merc 450SL          180  2.1 #> Merc 450SLC         180  0.0 #> Cadillac Fleetwood  205 -4.8 #> Lincoln Continental 215 -4.8 #> Chrysler Imperial   230 -0.5 #> Fiat 128             66  2.0 #> Honda Civic          52  0.0 #> Toyota Corolla       65  3.5 #> Toyota Corona        97 -1.3 #> Dodge Challenger    150  0.3 #> AMC Javelin         150  0.0 #> Camaro Z28          245 -1.9 #> Pontiac Firebird    175  4.0 #> Fiat X1-9            66 -3.1 #> Porsche 914-2        91  0.0 #> Lotus Europa        113  0.0 #> Ford Pantera L      264  0.8 #> Ferrari Dino        175 -1.3 #> Maserati Bora       335  0.0 #> Volvo 142E          109 -9.0 #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]"},{"path":"https://sebkrantz.github.io/collapse/reference/fprod.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","title":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","text":"fprod generic function computes (column-wise) product values x, (optionally) grouped g /weighted w. TRA argument can used transform x using (grouped, weighted) product.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fprod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","text":"","code":"fprod(x, ...)  # Default S3 method fprod(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, ...)  # S3 method for class 'matrix' fprod(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'data.frame' fprod(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'grouped_df' fprod(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],       use.g.names = FALSE, keep.group_vars = TRUE,       keep.w = TRUE, stub = .op[[\"stub\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fprod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","text":"x numeric vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. w numeric vector (non-negative) weights, may contain missing values. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE NA returned encountered. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. keep.w grouped_df method: Logical. Retain product weighting variable computation (contained grouped_df). stub character. keep.w = TRUE stub = TRUE (default), weights column prefixed \"prod.\". Users can specify different prefix argument, set FALSE avoid prefixing. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fprod.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","text":"Non-grouped product computations internally utilize long-doubles C, additional numeric precision. weighted product computed prod(x * w), using single pass C. na.rm = TRUE, missing values removed x w .e. utilizing x[complete.cases(x,w)] w[complete.cases(x,w)]. computational details see fsum, works equivalently.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fprod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","text":"(w weighted) product x, grouped g, (TRA used) x transformed (grouped, weighted) product.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fprod.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Product for Matrix-Like Objects — fprod","text":"","code":"## default vector method mpg <- mtcars$mpg fprod(mpg)                         # Simple product #> [1] 1.264241e+41 fprod(mpg, w = mtcars$hp)          # Weighted product #> [1] 8.870404e+108 fprod(mpg, TRA = \"/\")              # Simple transformation: Divide by product #>  [1] 1.661076e-40 1.661076e-40 1.803454e-40 1.692716e-40 1.479149e-40 #>  [6] 1.431690e-40 1.131114e-40 1.930012e-40 1.803454e-40 1.518698e-40 #> [11] 1.407960e-40 1.297221e-40 1.368410e-40 1.202303e-40 8.226283e-41 #> [16] 8.226283e-41 1.162753e-40 2.562803e-40 2.404606e-40 2.681452e-40 #> [21] 1.700626e-40 1.226032e-40 1.202303e-40 1.052015e-40 1.518698e-40 #> [26] 2.159399e-40 2.056571e-40 2.404606e-40 1.249762e-40 1.558248e-40 #> [31] 1.186483e-40 1.692716e-40 fprod(mpg, mtcars$cyl)             # Grouped product #>            4            6            8  #> 4.204745e+15 1.150054e+09 2.614398e+16  fprod(mpg, mtcars$cyl, mtcars$hp)  # Weighted grouped product #>            4            6            8  #> 3.686893e+36 4.255338e+23 5.653910e+48  fprod(mpg, mtcars[c(2,8:9)])       # More groups.. #>        4.0.1        4.1.0        4.1.1        6.0.1        6.1.0        8.0.0  #> 2.600000e+01 1.196088e+04 1.352082e+10 8.687700e+03 1.323773e+05 1.103122e+14  #>        8.0.1  #> 2.370000e+02  g <- GRP(mtcars, ~ cyl + vs + am)  # Precomputing groups gives more speed ! fprod(mpg, g) #>        4.0.1        4.1.0        4.1.1        6.0.1        6.1.0        8.0.0  #> 2.600000e+01 1.196088e+04 1.352082e+10 8.687700e+03 1.323773e+05 1.103122e+14  #>        8.0.1  #> 2.370000e+02  fprod(mpg, g, TRA = \"/\")           # Groupwise divide by product #>  [1] 2.417211e-03 2.417211e-03 1.686288e-09 1.616591e-04 1.695190e-13 #>  [6] 1.367304e-04 1.296321e-13 2.039984e-03 1.906214e-03 1.450400e-04 #> [11] 1.344641e-04 1.486690e-13 1.568277e-13 1.377908e-13 9.427792e-14 #> [16] 9.427792e-14 1.332582e-13 2.396304e-09 2.248384e-09 2.507244e-09 #> [21] 1.797527e-03 1.405104e-13 1.377908e-13 1.205670e-13 1.740515e-13 #> [26] 2.019108e-09 1.000000e+00 2.248384e-09 6.666667e-02 2.267574e-03 #> [31] 6.329114e-02 1.582744e-09  ## data.frame method fprod(mtcars) #>          mpg          cyl         disp           hp         drat           wt  #> 1.264241e+41 5.163908e+24 2.789968e+73 7.016390e+67 4.366447e+17 3.884021e+15  #>         qsec           vs           am         gear         carb  #> 9.651882e+39 0.000000e+00 0.000000e+00 7.522960e+17 1.391569e+12  head(fprod(mtcars, TRA = \"/\")) #>                            mpg          cyl         disp           hp #> Mazda RX4         1.661076e-40 1.161911e-24 5.734833e-72 1.567758e-66 #> Mazda RX4 Wag     1.661076e-40 1.161911e-24 5.734833e-72 1.567758e-66 #> Datsun 710        1.803454e-40 7.746072e-25 3.871012e-72 1.325468e-66 #> Hornet 4 Drive    1.692716e-40 1.161911e-24 9.247418e-72 1.567758e-66 #> Hornet Sportabout 1.479149e-40 1.549214e-24 1.290337e-71 2.494160e-66 #> Valiant           1.431690e-40 1.161911e-24 8.064609e-72 1.496496e-66 #>                           drat           wt         qsec  vs  am         gear #> Mazda RX4         8.931747e-18 6.745586e-16 1.705367e-39 NaN Inf 5.317056e-18 #> Mazda RX4 Wag     8.931747e-18 7.402122e-16 1.763387e-39 NaN Inf 5.317056e-18 #> Datsun 710        8.817237e-18 5.973191e-16 1.928121e-39 Inf Inf 5.317056e-18 #> Hornet 4 Drive    7.053790e-18 8.277504e-16 2.014115e-39 Inf NaN 3.987792e-18 #> Hornet Sportabout 7.214103e-18 8.856800e-16 1.763387e-39 NaN NaN 3.987792e-18 #> Valiant           6.320928e-18 8.908293e-16 2.094928e-39 Inf NaN 3.987792e-18 #>                           carb #> Mazda RX4         2.874452e-12 #> Mazda RX4 Wag     2.874452e-12 #> Datsun 710        7.186131e-13 #> Hornet 4 Drive    7.186131e-13 #> Hornet Sportabout 1.437226e-12 #> Valiant           7.186131e-13 fprod(mtcars, g) #>                mpg         cyl         disp           hp         drat #> 4.0.1 2.600000e+01           4 1.203000e+02 9.100000e+01      4.43000 #> 4.1.0 1.196088e+04          64 2.480709e+06 5.713300e+05     53.51976 #> 4.1.1 1.352082e+10       16384 4.158694e+13 1.686524e+13  20659.68036 #> 6.0.1 8.687700e+03         216 3.712000e+06 2.117500e+06     55.06020 #> 6.1.0 1.323773e+05        1296 1.630611e+09 1.747400e+08    130.62669 #> 8.0.0 1.103122e+14 68719476736 3.515354e+30 2.445279e+27 829662.18170 #>                 wt         qsec vs am   gear   carb #> 4.0.1 2.140000e+00 1.670000e+01  0  1      5      2 #> 4.1.0 2.476955e+01 9.164580e+03  1  0     48      4 #> 4.1.1 1.231073e+02 7.933555e+08  1  1  20480      8 #> 6.0.1 2.086503e+01 4.342313e+03  0  1     80     96 #> 6.1.0 1.316358e+02 1.359535e+05  1  0    144     16 #> 8.0.0 1.914897e+07 6.360378e+14  0  0 531441 442368 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fprod(mtcars, g, use.g.names = FALSE) # No row-names generated #>            mpg         cyl         disp           hp         drat           wt #> 1 2.600000e+01           4 1.203000e+02 9.100000e+01      4.43000 2.140000e+00 #> 2 1.196088e+04          64 2.480709e+06 5.713300e+05     53.51976 2.476955e+01 #> 3 1.352082e+10       16384 4.158694e+13 1.686524e+13  20659.68036 1.231073e+02 #> 4 8.687700e+03         216 3.712000e+06 2.117500e+06     55.06020 2.086503e+01 #> 5 1.323773e+05        1296 1.630611e+09 1.747400e+08    130.62669 1.316358e+02 #> 6 1.103122e+14 68719476736 3.515354e+30 2.445279e+27 829662.18170 1.914897e+07 #>           qsec vs am   gear   carb #> 1 1.670000e+01  0  1      5      2 #> 2 9.164580e+03  1  0     48      4 #> 3 7.933555e+08  1  1  20480      8 #> 4 4.342313e+03  0  1     80     96 #> 5 1.359535e+05  1  0    144     16 #> 6 6.360378e+14  0  0 531441 442368 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## matrix method m <- qM(mtcars) fprod(m) #>          mpg          cyl         disp           hp         drat           wt  #> 1.264241e+41 5.163908e+24 2.789968e+73 7.016390e+67 4.366447e+17 3.884021e+15  #>         qsec           vs           am         gear         carb  #> 9.651882e+39 0.000000e+00 0.000000e+00 7.522960e+17 1.391569e+12  head(fprod(m, TRA = \"/\")) #>                            mpg          cyl         disp           hp #> Mazda RX4         1.661076e-40 1.161911e-24 5.734833e-72 1.567758e-66 #> Mazda RX4 Wag     1.661076e-40 1.161911e-24 5.734833e-72 1.567758e-66 #> Datsun 710        1.803454e-40 7.746072e-25 3.871012e-72 1.325468e-66 #> Hornet 4 Drive    1.692716e-40 1.161911e-24 9.247418e-72 1.567758e-66 #> Hornet Sportabout 1.479149e-40 1.549214e-24 1.290337e-71 2.494160e-66 #> Valiant           1.431690e-40 1.161911e-24 8.064609e-72 1.496496e-66 #>                           drat           wt         qsec  vs  am         gear #> Mazda RX4         8.931747e-18 6.745586e-16 1.705367e-39 NaN Inf 5.317056e-18 #> Mazda RX4 Wag     8.931747e-18 7.402122e-16 1.763387e-39 NaN Inf 5.317056e-18 #> Datsun 710        8.817237e-18 5.973191e-16 1.928121e-39 Inf Inf 5.317056e-18 #> Hornet 4 Drive    7.053790e-18 8.277504e-16 2.014115e-39 Inf NaN 3.987792e-18 #> Hornet Sportabout 7.214103e-18 8.856800e-16 1.763387e-39 NaN NaN 3.987792e-18 #> Valiant           6.320928e-18 8.908293e-16 2.094928e-39 Inf NaN 3.987792e-18 #>                           carb #> Mazda RX4         2.874452e-12 #> Mazda RX4 Wag     2.874452e-12 #> Datsun 710        7.186131e-13 #> Hornet 4 Drive    7.186131e-13 #> Hornet Sportabout 1.437226e-12 #> Valiant           7.186131e-13 fprod(m, g) # etc.. #>                mpg         cyl         disp           hp         drat #> 4.0.1 2.600000e+01           4 1.203000e+02 9.100000e+01      4.43000 #> 4.1.0 1.196088e+04          64 2.480709e+06 5.713300e+05     53.51976 #> 4.1.1 1.352082e+10       16384 4.158694e+13 1.686524e+13  20659.68036 #> 6.0.1 8.687700e+03         216 3.712000e+06 2.117500e+06     55.06020 #> 6.1.0 1.323773e+05        1296 1.630611e+09 1.747400e+08    130.62669 #> 8.0.0 1.103122e+14 68719476736 3.515354e+30 2.445279e+27 829662.18170 #>                 wt         qsec vs am   gear   carb #> 4.0.1 2.140000e+00 1.670000e+01  0  1      5      2 #> 4.1.0 2.476955e+01 9.164580e+03  1  0     48      4 #> 4.1.1 1.231073e+02 7.933555e+08  1  1  20480      8 #> 6.0.1 2.086503e+01 4.342313e+03  0  1     80     96 #> 6.1.0 1.316358e+02 1.359535e+05  1  0    144     16 #> 8.0.0 1.914897e+07 6.360378e+14  0  0 531441 442368 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fprod() #>   cyl vs am          mpg         disp           hp         drat           wt #> 1   4  0  1 2.600000e+01 1.203000e+02 9.100000e+01      4.43000 2.140000e+00 #> 2   4  1  0 1.196088e+04 2.480709e+06 5.713300e+05     53.51976 2.476955e+01 #> 3   4  1  1 1.352082e+10 4.158694e+13 1.686524e+13  20659.68036 1.231073e+02 #> 4   6  0  1 8.687700e+03 3.712000e+06 2.117500e+06     55.06020 2.086503e+01 #> 5   6  1  0 1.323773e+05 1.630611e+09 1.747400e+08    130.62669 1.316358e+02 #> 6   8  0  0 1.103122e+14 3.515354e+30 2.445279e+27 829662.18170 1.914897e+07 #>           qsec   gear   carb #> 1 1.670000e+01      5      2 #> 2 9.164580e+03     48      4 #> 3 7.933555e+08  20480      8 #> 4 4.342313e+03     80     96 #> 5 1.359535e+05    144     16 #> 6 6.360378e+14 531441 442368 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fprod(TRA = \"/\") #>                   cyl vs am          mpg         disp           hp         drat #> Mazda RX4           6  0  1 2.417211e-03 4.310345e-05 5.194805e-05 7.083156e-02 #> Mazda RX4 Wag       6  0  1 2.417211e-03 4.310345e-05 5.194805e-05 7.083156e-02 #> Datsun 710          4  1  1 1.686288e-09 2.596969e-12 5.514301e-12 1.863533e-04 #> Hornet 4 Drive      6  1  0 1.616591e-04 1.582229e-07 6.295069e-07 2.357864e-02 #> Hornet Sportabout   8  0  0 1.695190e-13 1.024079e-28 7.156647e-26 3.796726e-06 #> Valiant             6  1  0 1.367304e-04 1.379851e-07 6.008929e-07 2.112891e-02 #>                             wt         qsec         gear         carb #> Mazda RX4         1.255690e-01 3.790607e-03 5.000000e-02 4.166667e-02 #> Mazda RX4 Wag     1.377904e-01 3.919570e-03 5.000000e-02 4.166667e-02 #> Datsun 710        1.884534e-02 2.345733e-08 1.953125e-04 1.250000e-01 #> Hornet 4 Drive    2.442345e-02 1.429901e-04 2.083333e-02 6.250000e-02 #> Hornet Sportabout 1.796441e-07 2.675942e-14 5.645029e-06 4.521123e-06 #> Valiant           2.628465e-02 1.487274e-04 2.083333e-02 6.250000e-02 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl,vs,am) |> fselect(mpg) |> fprod() #>   cyl vs am          mpg #> 1   4  0  1 2.600000e+01 #> 2   4  1  0 1.196088e+04 #> 3   4  1  1 1.352082e+10 #> 4   6  0  1 8.687700e+03 #> 5   6  1  0 1.323773e+05 #> 6   8  0  0 1.103122e+14 #> 7   8  0  1 2.370000e+02"},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Weighted) Sample Quantiles and Range — fquantile","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"faster alternative quantile (written fully C), supports sampling weights, can also quickly compute quantiles ordering vector (e.g. order(x)). frange provides fast alternative range.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"","code":"fquantile(x, probs = c(0, 0.25, 0.5, 0.75, 1), w = NULL,           o = if(length(x) > 1e5L && length(probs) > log(length(x)))               radixorder(x) else NULL,           na.rm = .op[[\"na.rm\"]], type = 7L, names = TRUE,           check.o = is.null(attr(o, \"sorted\")))  # Programmers version: no names, intelligent defaults, or checks .quantile(x, probs = c(0, 0.25, 0.5, 0.75, 1), w = NULL, o = NULL,           na.rm = TRUE, type = 7L, names = FALSE, check.o = FALSE)  # Fast range (min and max) frange(x, na.rm = .op[[\"na.rm\"]], finite = FALSE) .range(x, na.rm = TRUE, finite = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"x numeric integer vector. probs numeric vector probabilities values [0,1]. w numeric vector strictly positive sampling weights. Missing weights supported x also missing. o integer. vector giving ordering elements x, identical(x[o], sort(x)). available considerably speeds estimation. na.rm logical. Remove missing values, default TRUE. finite logical. Omit non-finite values. type integer. Quantile types 4-9. See quantile. details provided Hyndman Fan (1996) recommended type 8. default method type 7. names logical. Generates names form paste0(round(probs * 100, 1), \"%\") (C). Set FALSE speedup. check.o logical. o supplied, TRUE runs o checks valid, .e. element [1, length(x)]. Set FALSE significant speedup o known valid.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"fquantile implemented using quickselect algorithm C, inspired data.table's gmedian. algorithm applied incrementally different sections array find individual quantiles. many quantile probabilities requested, sorting whole array fast radixorder algorithm efficient. default threshold (length(x) > 1e5L && length(probs) > log(length(x))) conservative, given quickselect generally efficient longitudinal data similar values repeated groups. random data, investigations yield threshold length(probs) > log10(length(x)) appropriate. frange considerably efficient range, requiring one pass data instead two. probabilities 0 1, fquantile internally calls frange. Following Hyndman Fan (1996), quantile type-\\(\\) quantile function sample \\(X\\) can written weighted average two order statistics: $$\\hat{Q}_{X,}(p) = (1 - \\gamma) X_{(j)} + \\gamma X_{(j + 1)}$$ \\(j = \\lfloor pn + m \\rfloor,\\ m \\\\mathbb{R}\\) \\(\\gamma = pn + m - j,\\ 0 \\le \\gamma \\le 1\\), \\(m\\) differing quantile type (\\(\\)). example, default type 7 quantile estimator uses \\(m = 1 - p\\), see quantile. weighted data normalized weights \\(w = \\{w_1, ..., w_n\\}\\), \\(w_k > 0\\) \\(\\sum_k w_k = 1\\), let \\(\\{w_{(1)}, ..., w_{(n)}\\}\\) weights order statistic \\(W_{(k)} = \\operatorname{Weight}[X_j \\le X_{(k)}] = \\sum_{j=1}^k w_{(j)}\\) cumulative weight order statistic. can first find largest value \\(l\\) cumulative normalized weight \\(W_{(l)} \\leq p\\), replace \\(pn\\) \\(l + (p - W_{(l)})/w_{(l+1)}\\), \\(w_{(l+1)}\\) weight next observation. gives: $$j = \\lfloor l + \\frac{p - W_{(l)}}{w_{(l+1)}} + m \\rfloor$$ $$\\gamma = l + \\frac{p - W_{(l)}}{w_{(l+1)}} + m - j$$ detailed exposition see excellent notes Matthew Kay. See also R implementation weighted quantiles type 7 Examples .","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"new weighted quantile algorithm v2.1.0 skip zero weights anymore technically difficult (clear \\(j\\) hits zero weight element whether one move forward backward find alternative). Thus, non-missing elements considered weights strictly positive.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"vector quantiles. names = TRUE, fquantile generates names paste0(round(probs * 100, 1), \"%\") (C).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"Sebastian Krantz based notes Matthew Kay.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"Hyndman, R. J. Fan, Y. (1996) Sample quantiles statistical packages, American Statistician 50, 361–365. doi:10.2307/2684934. Wicklin, R. (2017) Sample quantiles: comparison 9 definitions; SAS Blog. https://blogs.sas.com/content/iml/2017/05/24/definitions-sample-quantiles.html Wikipedia: https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample Weighted Quantiles Matthew Kay: https://htmlpreview.github.io/?https://github.com/mjskay/uncertainty-examples/blob/master/weighted-quantiles.html","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fquantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Weighted) Sample Quantiles and Range — fquantile","text":"","code":"## Basic range and quantiles frange(mtcars$mpg) #> [1] 10.4 33.9 fquantile(mtcars$mpg) #>     0%    25%    50%    75%   100%  #> 10.400 15.425 19.200 22.800 33.900   ## Checking computational equivalence to stats::quantile() w = alloc(abs(rnorm(1)), 32) o = radixorder(mtcars$mpg) for (i in 5:9) print(all_obj_equal(fquantile(mtcars$mpg, type = i),                                    fquantile(mtcars$mpg, type = i, w = w),                                    fquantile(mtcars$mpg, type = i, o = o),                                    fquantile(mtcars$mpg, type = i, w = w, o = o),                                     quantile(mtcars$mpg, type = i))) #> [1] TRUE #> [1] TRUE #> [1] TRUE #> [1] TRUE #> [1] TRUE  ## Demonstaration: weighted quantiles type 7 in R wquantile7R <- function(x, w, probs = c(0.25, 0.5, 0.75), na.rm = TRUE, names = TRUE) {   if(na.rm && anyNA(x)) {             # Removing missing values (only in x)     cc = whichNA(x, invert = TRUE)    # The C code first calls radixorder(x), which places     x = x[cc]; w = w[cc]              # missing values last, so removing = early termination   }   o = radixorder(x)                   # Ordering   wo = proportions(w[o])   Wo = cumsum(wo)                     # Cumulative sum   res = sapply(probs, function(p) {     l = which.max(Wo > p) - 1L        # Lower order statistic     s = l + (p - Wo[l])/wo[l+1L] + 1 - p     j = floor(s)     gamma = s - j     (1 - gamma) * x[o[j]] + gamma * x[o[j+1L]]  # Weighted quantile   })   if(names) names(res) = paste0(as.integer(probs * 100), \"%\")   res } # Note: doesn't work for min and max.  wquantile7R(mtcars$mpg, mtcars$wt) #>      25%      50%      75%  #> 15.07936 17.89174 21.40000   all.equal(wquantile7R(mtcars$mpg, mtcars$wt),           fquantile(mtcars$mpg, c(0.25, 0.5, 0.75), mtcars$wt)) #> [1] TRUE  ## Efficient grouped quantile estimation: use .quantile for less call overhead BY(mtcars$mpg, mtcars$cyl, .quantile, names = TRUE, expand.wide = TRUE) #>     0%   25%  50%   75% 100% #> 4 21.4 22.80 26.0 30.40 33.9 #> 6 17.8 18.65 19.7 21.00 21.4 #> 8 10.4 14.40 15.2 16.25 19.2 BY(mtcars, mtcars$cyl, .quantile, names = TRUE) #>         mpg cyl   disp    hp  drat     wt  qsec vs  am gear carb #> 4.0%   21.4   4  71.10  52.0 3.690 1.5130 16.70  0 0.0    3    1 #> 4.25%  22.8   4  78.85  65.5 3.810 1.8850 18.56  1 0.5    4    1 #> 4.50%  26.0   4 108.00  91.0 4.080 2.2000 18.90  1 1.0    4    2 #> 4.75%  30.4   4 120.65  96.0 4.165 2.6225 19.95  1 1.0    4    2 #> 4.100% 33.9   4 146.70 113.0 4.930 3.1900 22.90  1 1.0    5    2 #> 6.0%   17.8   6 145.00 105.0 2.760 2.6200 15.50  0 0.0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ] mtcars |> fgroup_by(cyl) |> BY(.quantile) #>   cyl  mpg   disp    hp  drat     wt  qsec vs  am gear carb #> 1   4 21.4  71.10  52.0 3.690 1.5130 16.70  0 0.0    3    1 #> 2   4 22.8  78.85  65.5 3.810 1.8850 18.56  1 0.5    4    1 #> 3   4 26.0 108.00  91.0 4.080 2.2000 18.90  1 1.0    4    2 #> 4   4 30.4 120.65  96.0 4.165 2.6225 19.95  1 1.0    4    2 #> 5   4 33.9 146.70 113.0 4.930 3.1900 22.90  1 1.0    5    2 #> 6   6 17.8 145.00 105.0 2.760 2.6200 15.50  0 0.0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ]  ## With weights BY(mtcars$mpg, mtcars$cyl, .quantile, w = mtcars$wt, names = TRUE, expand.wide = TRUE) #>     0%      25%      50%      75% 100% #> 4 21.4 22.80000 24.53116 29.75889 33.9 #> 6 17.8 18.46561 19.55289 21.00000 21.4 #> 8 10.4 13.91543 15.15267 16.11036 19.2 BY(mtcars, mtcars$cyl, .quantile, w = mtcars$wt, names = TRUE) #>             mpg cyl     disp        hp     drat       wt     qsec vs am gear #> 4.0%   21.40000   4  71.1000  52.00000 3.690000 1.513000 16.70000  0  0    3 #> 4.25%  22.80000   4  80.2647  65.55695 3.783351 2.023886 18.60116  1  0    4 #> 4.50%  24.53116   4 119.7122  91.67897 3.996622 2.330844 19.25457  1  1    4 #> 4.75%  29.75889   4 126.2910  95.88316 4.109992 2.878872 20.00040  1  1    4 #> 4.100% 33.90000   4 146.7000 113.00000 4.930000 3.190000 22.90000  1  1    5 #> 6.0%   17.80000   6 145.0000 105.00000 2.760000 2.620000 15.50000  0  0    3 #>        carb #> 4.0%      1 #> 4.25%     1 #> 4.50%     2 #> 4.75%     2 #> 4.100%    2 #> 6.0%      1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ] mtcars |> fgroup_by(cyl) |> fselect(-wt) |> BY(.quantile, w = mtcars$wt) #>   cyl      mpg     disp        hp     drat     qsec vs am     gear     carb #> 1   4 21.40000  71.1000  52.00000 3.690000 16.70000  0  0 3.000000 1.000000 #> 2   4 22.80000  80.2647  65.55695 3.783351 18.60116  1  0 4.000000 1.000000 #> 3   4 24.53116 119.7122  91.67897 3.996622 19.25457  1  1 4.000000 2.000000 #> 4   4 29.75889 126.2910  95.88316 4.109992 20.00040  1  1 4.000000 2.000000 #> 5   4 33.90000 146.7000 113.00000 4.930000 22.90000  1  1 5.000000 2.000000 #> 6   6 17.80000 145.0000 105.00000 2.760000 15.50000  0  0 3.000000 1.000000 #> 7   6 18.46561 160.0000 110.00000 3.280086 16.89266  0  0 3.397399 2.192197 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ] mtcars |> fgroup_by(cyl) |> fsummarise(across(-wt, .quantile, w = wt)) #>   cyl      mpg     disp        hp     drat     qsec vs am     gear     carb #> 1   4 21.40000  71.1000  52.00000 3.690000 16.70000  0  0 3.000000 1.000000 #> 2   4 22.80000  80.2647  65.55695 3.783351 18.60116  1  0 4.000000 1.000000 #> 3   4 24.53116 119.7122  91.67897 3.996622 19.25457  1  1 4.000000 2.000000 #> 4   4 29.75889 126.2910  95.88316 4.109992 20.00040  1  1 4.000000 2.000000 #> 5   4 33.90000 146.7000 113.00000 4.930000 22.90000  1  1 5.000000 2.000000 #> 6   6 17.80000 145.0000 105.00000 2.760000 15.50000  0  0 3.000000 1.000000 #> 7   6 18.46561 160.0000 110.00000 3.280086 16.89266  0  0 3.397399 2.192197 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/frename.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Renaming and Relabelling Objects — frename","title":"Fast Renaming and Relabelling Objects — frename","text":"frename returns renamed shallow-copy, setrename renames objects reference. functions also work objects data frames 'names' attribute. relabel setrelabel labels attached data frame columns.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/frename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Renaming and Relabelling Objects — frename","text":"","code":"frename(.x, ..., cols = NULL, .nse = TRUE) rnm(.x, ..., cols = NULL, .nse = TRUE)     # Shorthand for frename()  setrename(.x, ..., cols = NULL, .nse = TRUE)  relabel(.x, ..., cols = NULL, attrn = \"label\")  setrelabel(.x, ..., cols = NULL, attrn = \"label\")"},{"path":"https://sebkrantz.github.io/collapse/reference/frename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Renaming and Relabelling Objects — frename","text":".x (f/set)rename: R object \"names\" attribute. (set)relabel: named list. ... either tagged vector expressions form name = newname / name = newlabel (frename also supports newname = name), (named) vector names/labels, single function (+ optional arguments function) applied names/labels (columns/elements selected cols). cols ... function, select subset columns/elements rename/relabel using names, indices, logical vector function applied columns .x list (e.g. .numeric). .nse logical. TRUE allows non-standard evaluation tagged vector expressions, allowing supply new names without quotes. Set FALSE programming passing vectors names. attrn character. Name attribute store labels retrieve labels .","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/frename.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Renaming and Relabelling Objects — frename","text":".x renamed / relabelled. setrename setrelabel return .x invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/frename.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Renaming and Relabelling Objects — frename","text":"Note relabel setrelabel modify .x reference. labels attached columns , making impossible avoid permanent modification taking shallow copy encompassing list / data.frame. hand frename makes shallow copy whereas setrename also modifies reference.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/frename.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Renaming and Relabelling Objects — frename","text":"","code":"## Using tagged expressions head(frename(iris, Sepal.Length = SL, Sepal.Width = SW,                    Petal.Length = PL, Petal.Width = PW)) #>    SL  SW  PL  PW Species #> 1 5.1 3.5 1.4 0.2  setosa #> 2 4.9 3.0 1.4 0.2  setosa #> 3 4.7 3.2 1.3 0.2  setosa #> 4 4.6 3.1 1.5 0.2  setosa #> 5 5.0 3.6 1.4 0.2  setosa #> 6 5.4 3.9 1.7 0.4  setosa head(frename(iris, Sepal.Length = \"S L\", Sepal.Width = \"S W\",                    Petal.Length = \"P L\", Petal.Width = \"P W\")) #>   S L S W P L P W Species #> 1 5.1 3.5 1.4 0.2  setosa #> 2 4.9 3.0 1.4 0.2  setosa #> 3 4.7 3.2 1.3 0.2  setosa #> 4 4.6 3.1 1.5 0.2  setosa #> 5 5.0 3.6 1.4 0.2  setosa #> 6 5.4 3.9 1.7 0.4  setosa  ## Since v2.0.0 this is also supported head(frename(iris, SL = Sepal.Length, SW = Sepal.Width,                    PL = Petal.Length, PW = Petal.Width)) #>    SL  SW  PL  PW Species #> 1 5.1 3.5 1.4 0.2  setosa #> 2 4.9 3.0 1.4 0.2  setosa #> 3 4.7 3.2 1.3 0.2  setosa #> 4 4.6 3.1 1.5 0.2  setosa #> 5 5.0 3.6 1.4 0.2  setosa #> 6 5.4 3.9 1.7 0.4  setosa  ## Using a function head(frename(iris, tolower)) #>   sepal.length sepal.width petal.length petal.width species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa head(frename(iris, tolower, cols = 1:2)) #>   sepal.length sepal.width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa head(frename(iris, tolower, cols = is.numeric)) #>   sepal.length sepal.width petal.length petal.width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa head(frename(iris, paste, \"new\", sep = \"_\", cols = 1:2)) #>   Sepal.Length_new Sepal.Width_new Petal.Length Petal.Width Species #> 1              5.1             3.5          1.4         0.2  setosa #> 2              4.9             3.0          1.4         0.2  setosa #> 3              4.7             3.2          1.3         0.2  setosa #> 4              4.6             3.1          1.5         0.2  setosa #> 5              5.0             3.6          1.4         0.2  setosa #> 6              5.4             3.9          1.7         0.4  setosa  ## Using vectors of names and programming newname = \"sepal_length\" head(frename(iris, Sepal.Length = newname, .nse = FALSE)) #>   sepal_length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa newnames = c(\"sepal_length\", \"sepal_width\") head(frename(iris, newnames, cols = 1:2)) #>   sepal_length sepal_width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa newnames = c(Sepal.Length = \"sepal_length\", Sepal.Width = \"sepal_width\") head(frename(iris, newnames, .nse = FALSE)) #>   sepal_length sepal_width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa # Since v2.0.0, this works as well newnames = c(sepal_length = \"Sepal.Length\", sepal_width = \"Sepal.Width\") head(frename(iris, newnames, .nse = FALSE)) #>   sepal_length sepal_width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa  ## Renaming by reference # setrename(iris, tolower) # head(iris) # rm(iris) # etc...  ## Relabelling (by reference) # namlab(relabel(wlddev, PCGDP = \"GDP per Capita\", LIFEEX = \"Life Expectancy\")) # namlab(relabel(wlddev, toupper))"},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"fscale generic function efficiently standardize (scale center) data. STD wrapper around fscale representing 'standardization operator', options fscale applied matrices data frames.  Standardization can simple groupwise, ordinary weighted. Arbitrary target means standard deviations can set, special options grouped scaling centering. also possible scale data without centering .e. perform mean-preserving scaling.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"","code":"fscale(x, ...)    STD(x, ...)  # Default S3 method fscale(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...) # Default S3 method STD(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...)  # S3 method for class 'matrix' fscale(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...) # S3 method for class 'matrix' STD(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1,     stub = .op[[\"stub\"]], ...)  # S3 method for class 'data.frame' fscale(x, g = NULL, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...) # S3 method for class 'data.frame' STD(x, by = NULL, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],     mean = 0, sd = 1, stub = .op[[\"stub\"]], keep.by = TRUE, keep.w = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fscale(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...) # S3 method for class 'pseries' STD(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...)  # S3 method for class 'pdata.frame' fscale(x, effect = 1L, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1, ...) # S3 method for class 'pdata.frame' STD(x, effect = 1L, w = NULL, cols = is.numeric, na.rm = .op[[\"na.rm\"]],     mean = 0, sd = 1, stub = .op[[\"stub\"]], keep.ids = TRUE, keep.w = TRUE, ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' fscale(x, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1,        keep.group_vars = TRUE, keep.w = TRUE, ...) # S3 method for class 'grouped_df' STD(x, w = NULL, na.rm = .op[[\"na.rm\"]], mean = 0, sd = 1,     stub = .op[[\"stub\"]], keep.group_vars = TRUE, keep.w = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"x numeric vector, matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. STD data.frame method: g, also allows one- two-sided formulas .e. ~ group1 var1 + var2 ~ group1 + group2. See Examples. cols STD (p)data.frame method: Select columns scale using function, column names, indices logical vector. Default: numeric columns. Note: cols ignored two-sided formula passed . w numeric vector (non-negative) weights. STD data frame pdata.frame methods also allow one-sided formula .e. ~ weightcol. grouped_df (dplyr) method supports lazy-evaluation. See Examples. na.rm logical. Skip missing values x w computing means sd's. effect plm methods: Select panel identifier used group-id. 1L takes first variable index, 2L second etc.. Index variables can also called name using character string. one variable can supplied. stub character. prefix/stub add names transformed columns. TRUE (default) uses \"STD.\", FALSE rename columns. mean mean center (default 0). mean = FALSE, centering performed. case scaling mean-preserving. numeric value different 0 (.e. mean = 5) added data subtracting mean(s), data mean 5. special option performing grouped scaling centering mean = \"overall.mean\". case overall mean data added subtracting group means. sd standard deviation scale data (default 1). numeric value different 0 (.e. sd = 3) scale data standard deviation  3. special option performing grouped scaling sd = \"within.sd\". case within standard deviation (= standard deviation group-centered series) calculated applied group. results variance data within group harmonized without forcing certain variance (1). keep., keep.ids, keep.group_vars data.frame, pdata.frame grouped_df methods: Logical. Retain grouping / panel-identifier columns output. STD.data.frame works grouping variables passed formula. keep.w data.frame, pdata.frame grouped_df methods: Logical. Retain column containing weights output. works w passed formula / lazy-expression. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"g = NULL, fscale default (column-wise) subtracts mean weighted mean (w supplied) data points x, divides difference standard deviation frequency-weighted standard deviation. result columns x (weighted) mean 0 (weighted) standard deviation 1. Alternatively, data can scaled mean mean standard deviation sd. mean = FALSE data scaled (centered) mean data preserved. Means standard deviations computed using Welford's numerically stable online algorithm. groups supplied g, standardizing becomes groupwise, group (column) data points mean mean standard deviation sd. Naturally mean = FALSE group just scaled mean preserved. centering without scaling see fwithin. na.rm = FALSE NA NaN encountered, mean sd group NA, data points belonging group also NA output. na.rm = TRUE, means sd's computed (column-wise) available data points, also weight vector can missing values. case, weighted mean sd computed (column-wise) complete.cases(x, w), x scaled using statistics. Note fscale insert missing value x weight value missing, rather, value scaled using weighted mean standard-deviated computed without ! (intention (randomly) missing weights break computation na.rm = TRUE, meant weight vectors many missing values. like behavior, prepare data using x[.na(w), ] <- NA, impute weight vector non-missing x). Special options grouped scaling mean = \"overall.mean\" sd = \"within.sd\". former group-centers vectors overall mean data (see fwithin details) latter scales data group within-group standard deviation (= standard deviation group-centered data). Thus scaling grouped vector options mean = \"overall.mean\" sd = \"within.sd\" amounts removing differences mean standard deviations groups. weighted computations, mean = \"overall.mean\" subtract weighted group-means data add overall weighted mean data, whereas sd = \"within.sd\" compute weighted within- standard deviation apply group.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"x standardized (mean = mean, standard deviation = sd), grouped g/, weighted w. See Details.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"centering without scaling see fwithin/W. simple mean-preserving scaling use fsd(..., TRA = \"/\"). sweep pre-computed means scale-factors data see TRA.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fscale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Scaling and Centering of Matrix-like Objects — fscale","text":"","code":"## Simple Scaling & Centering / Standardizing head(fscale(mtcars))               # Doesn't rename columns #>                          mpg        cyl        disp         hp       drat #> Mazda RX4          0.1508848 -0.1049878 -0.57061982 -0.5350928  0.5675137 #> Mazda RX4 Wag      0.1508848 -0.1049878 -0.57061982 -0.5350928  0.5675137 #> Datsun 710         0.4495434 -1.2248578 -0.99018209 -0.7830405  0.4739996 #> Hornet 4 Drive     0.2172534 -0.1049878  0.22009369 -0.5350928 -0.9661175 #> Hornet Sportabout -0.2307345  1.0148821  1.04308123  0.4129422 -0.8351978 #> Valiant           -0.3302874 -0.1049878 -0.04616698 -0.6080186 -1.5646078 #>                             wt       qsec         vs         am       gear #> Mazda RX4         -0.610399567 -0.7771651 -0.8680278  1.1899014  0.4235542 #> Mazda RX4 Wag     -0.349785269 -0.4637808 -0.8680278  1.1899014  0.4235542 #> Datsun 710        -0.917004624  0.4260068  1.1160357  1.1899014  0.4235542 #> Hornet 4 Drive    -0.002299538  0.8904872  1.1160357 -0.8141431 -0.9318192 #> Hornet Sportabout  0.227654255 -0.4637808 -0.8680278 -0.8141431 -0.9318192 #> Valiant            0.248094592  1.3269868  1.1160357 -0.8141431 -0.9318192 #>                         carb #> Mazda RX4          0.7352031 #> Mazda RX4 Wag      0.7352031 #> Datsun 710        -1.1221521 #> Hornet 4 Drive    -1.1221521 #> Hornet Sportabout -0.5030337 #> Valiant           -1.1221521 head(STD(mtcars))                  # By default adds a prefix #>                      STD.mpg    STD.cyl    STD.disp     STD.hp   STD.drat #> Mazda RX4          0.1508848 -0.1049878 -0.57061982 -0.5350928  0.5675137 #> Mazda RX4 Wag      0.1508848 -0.1049878 -0.57061982 -0.5350928  0.5675137 #> Datsun 710         0.4495434 -1.2248578 -0.99018209 -0.7830405  0.4739996 #> Hornet 4 Drive     0.2172534 -0.1049878  0.22009369 -0.5350928 -0.9661175 #> Hornet Sportabout -0.2307345  1.0148821  1.04308123  0.4129422 -0.8351978 #> Valiant           -0.3302874 -0.1049878 -0.04616698 -0.6080186 -1.5646078 #>                         STD.wt   STD.qsec     STD.vs     STD.am   STD.gear #> Mazda RX4         -0.610399567 -0.7771651 -0.8680278  1.1899014  0.4235542 #> Mazda RX4 Wag     -0.349785269 -0.4637808 -0.8680278  1.1899014  0.4235542 #> Datsun 710        -0.917004624  0.4260068  1.1160357  1.1899014  0.4235542 #> Hornet 4 Drive    -0.002299538  0.8904872  1.1160357 -0.8141431 -0.9318192 #> Hornet Sportabout  0.227654255 -0.4637808 -0.8680278 -0.8141431 -0.9318192 #> Valiant            0.248094592  1.3269868  1.1160357 -0.8141431 -0.9318192 #>                     STD.carb #> Mazda RX4          0.7352031 #> Mazda RX4 Wag      0.7352031 #> Datsun 710        -1.1221521 #> Hornet 4 Drive    -1.1221521 #> Hornet Sportabout -0.5030337 #> Valiant           -1.1221521 qsu(STD(mtcars))                   # See that is works #>            N  Mean  SD      Min     Max #> STD.mpg   32     0   1  -1.6079  2.2913 #> STD.cyl   32     0   1  -1.2249  1.0149 #> STD.disp  32    -0   1  -1.2879  1.9468 #> STD.hp    32     0   1   -1.381  2.7466 #> STD.drat  32    -0   1  -1.5646  2.4939 #> STD.wt    32    -0   1  -1.7418  2.2553 #> STD.qsec  32    -0   1   -1.874  2.8268 #> STD.vs    32     0   1   -0.868   1.116 #> STD.am    32    -0   1  -0.8141  1.1899 #> STD.gear  32    -0   1  -0.9318  1.7789 #> STD.carb  32    -0   1  -1.1222  3.2117 qsu(STD(mtcars, mean = 5, sd = 3)) # Assigning a mean of 5 and a standard deviation of 3 #>            N  Mean  SD      Min      Max #> STD.mpg   32     5   3   0.1764  11.8738 #> STD.cyl   32     5   3   1.3254   8.0446 #> STD.disp  32     5   3   1.1363  10.8403 #> STD.hp    32     5   3   0.8569  13.2397 #> STD.drat  32     5   3   0.3062  12.4817 #> STD.wt    32     5   3  -0.2253   11.766 #> STD.qsec  32     5   3   -0.622  13.4803 #> STD.vs    32     5   3   2.3959   8.3481 #> STD.am    32     5   3   2.5576   8.5697 #> STD.gear  32     5   3   2.2045  10.3368 #> STD.carb  32     5   3   1.6335   14.635 qsu(STD(mtcars, mean = FALSE))     # No centering: Scaling is mean-preserving #>            N      Mean  SD       Min       Max #> STD.mpg   32   20.0906   1   18.4827   22.3819 #> STD.cyl   32    6.1875   1    4.9626    7.2024 #> STD.disp  32  230.7219   1   229.434  232.6686 #> STD.hp    32  146.6875   1  145.3065  149.4341 #> STD.drat  32    3.5966   1     2.032    6.0905 #> STD.wt    32    3.2172   1    1.4755    5.4726 #> STD.qsec  32   17.8487   1   15.9747   20.6755 #> STD.vs    32    0.4375   1   -0.4305    1.5535 #> STD.am    32    0.4062   1   -0.4079    1.5962 #> STD.gear  32    3.6875   1    2.7557    5.4664 #> STD.carb  32    2.8125   1    1.6903    6.0242  ## Panel Data head(fscale(get_vars(wlddev,9:12), wlddev$iso3c))   # Standardizing 4 series within each country #>   PCGDP    LIFEEX GINI        ODA #> 1    NA -1.653181   NA -0.6498451 #> 2    NA -1.602256   NA -0.5951801 #> 3    NA -1.552023   NA -0.6517082 #> 4    NA -1.502678   NA -0.5925063 #> 5    NA -1.454122   NA -0.5649154 #> 6    NA -1.406257   NA -0.5431461 head(STD(wlddev, ~iso3c, cols = 9:12))              # Same thing using STD, id's added #>   iso3c STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA #> 1   AFG        NA  -1.653181       NA -0.6498451 #> 2   AFG        NA  -1.602256       NA -0.5951801 #> 3   AFG        NA  -1.552023       NA -0.6517082 #> 4   AFG        NA  -1.502678       NA -0.5925063 #> 5   AFG        NA  -1.454122       NA -0.5649154 #> 6   AFG        NA  -1.406257       NA -0.5431461 pwcor(fscale(get_vars(wlddev,9:12), wlddev$iso3c))  # Correlaing panel series after standardizing #>        PCGDP LIFEEX  GINI   ODA #> PCGDP     1     .62  -.20   .09 #> LIFEEX   .62     1   -.12   .32 #> GINI    -.20   -.12    1   -.09 #> ODA      .09    .32  -.09    1   fmean(get_vars(wlddev, 9:12))                       # This calculates the overall means #>        PCGDP       LIFEEX         GINI          ODA  #> 1.204878e+04 6.429630e+01 3.853412e+01 4.547201e+08  fsd(fwithin(get_vars(wlddev, 9:12), wlddev$iso3c))  # This calculates the within standard deviations #>        PCGDP       LIFEEX         GINI          ODA  #> 6.723681e+03 6.084205e+00 2.927700e+00 6.507096e+08  head(qsu(fscale(get_vars(wlddev, 9:12),             # This group-centers on the overall mean and     wlddev$iso3c,                                   # group-scales to the within standard deviation     mean = \"overall.mean\", sd = \"within.sd\"),       # -> data harmonized in the first 2 moments     by = wlddev$iso3c)) #> , , PCGDP #>  #>       N       Mean         SD          Min         Max #> ABW  32  12048.778  6723.6808  -13032.4333  19779.7812 #> AFG  18  12048.778  6723.6808    2023.3142   18822.252 #> AGO  40  12048.778  6723.6808   -1027.1433  22877.0816 #> ALB  40  12048.778  6723.6808    3212.4503  25460.2799 #> AND  50  12048.778  6723.6808    -111.8716  24171.0081 #> ARE  45  12048.778  6723.6808    3206.3558  26904.4271 #>  #> , , LIFEEX #>  #>       N     Mean      SD      Min      Max #> ABW  60  64.2963  6.0842  49.8607  72.6147 #> AFG  60  64.2963  6.0842   54.238  73.6849 #> AGO  60  64.2963  6.0842  55.6648  77.7463 #> ALB  60  64.2963  6.0842  50.8542  74.1559 #> AND   0        0       -        0        0 #> ARE  60  64.2963  6.0842  49.6668  71.3434 #>  #> , , GINI #>  #>      N     Mean      SD      Min      Max #> ABW  0        0       -        0        0 #> AFG  0        0       -        0        0 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 slice ]   ## Indexed data wldi <- findex_by(wlddev, iso3c, year) head(STD(wldi))                                  # Standardizing all numeric variables by country #>   iso3c year STD.decade STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA    STD.POP #> 1   AFG 1960  -1.448413        NA  -1.653181       NA -0.6498451 -1.0754727 #> 2   AFG 1961  -1.448413        NA  -1.602256       NA -0.5951801 -1.0556707 #> 3   AFG 1962  -1.448413        NA  -1.552023       NA -0.6517082 -1.0347669 #> 4   AFG 1963  -1.448413        NA  -1.502678       NA -0.5925063 -1.0127455 #> 5   AFG 1964  -1.448413        NA  -1.454122       NA -0.5649154 -0.9895973 #> 6   AFG 1965  -1.448413        NA  -1.406257       NA -0.5431461 -0.9653050 #>  #> Indexed by:  iso3c [1] | year [6 (61)]  head(STD(wldi, effect = 2L))                     # Standardizing all numeric variables by year #>   iso3c year STD.decade STD.PCGDP STD.LIFEEX STD.GINI      STD.ODA     STD.POP #> 1   AFG 1960        NaN        NA  -1.755632       NA -0.185893459 -0.08682756 #> 2   AFG 1961        NaN        NA  -1.759208       NA -0.059419593 -0.08697252 #> 3   AFG 1962        NaN        NA  -1.769154       NA -0.282990408 -0.08693992 #> 4   AFG 1963        NaN        NA  -1.772255       NA -0.071937146 -0.08682708 #> 5   AFG 1964        NaN        NA  -1.774865       NA -0.010951326 -0.08661029 #> 6   AFG 1965        NaN        NA  -1.790502       NA  0.006003786 -0.08627312 #>  #> Indexed by:  iso3c [1] | year [6 (61)]   ## Weighted Standardizing weights = abs(rnorm(nrow(wlddev))) head(fscale(get_vars(wlddev,9:12), wlddev$iso3c, weights)) #>   PCGDP    LIFEEX GINI        ODA #> 1    NA -1.467564   NA -0.4994722 #> 2    NA -1.414317   NA -0.4338014 #> 3    NA -1.361793   NA -0.5017104 #> 4    NA -1.310197   NA -0.4305893 #> 5    NA -1.259427   NA -0.3974435 #> 6    NA -1.209379   NA -0.3712913 head(STD(wlddev, ~iso3c, weights, 9:12)) #>   iso3c STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA #> 1   AFG        NA  -1.467564       NA -0.4994722 #> 2   AFG        NA  -1.414317       NA -0.4338014 #> 3   AFG        NA  -1.361793       NA -0.5017104 #> 4   AFG        NA  -1.310197       NA -0.4305893 #> 5   AFG        NA  -1.259427       NA -0.3974435 #> 6   AFG        NA  -1.209379       NA -0.3712913  # Grouped data wlddev |> fgroup_by(iso3c) |> fselect(PCGDP,LIFEEX) |> STD() #>    STD.PCGDP  STD.LIFEEX #> 1         NA -1.65318076 #> 2         NA -1.60225647 #> 3         NA -1.55202301 #> 4         NA -1.50267777 #> 5         NA -1.45412205 #> 6         NA -1.40625717 #> 7         NA -1.35868835 #> 8         NA -1.31092216 #> 9         NA -1.26266251 #> 10        NA -1.21361334 #> 11        NA -1.16337988 #> 12        NA -1.11196214 #> 13        NA -1.05955749 #> 14        NA -1.00606725 #> 15        NA -0.95129403 #> 16        NA -0.89504046 #> 17        NA -0.83710914 #> 18        NA -0.77740140 #> 19        NA -0.71581853 #> 20        NA -0.65255793 #> 21        NA -0.58752090 #> 22        NA -0.52051007 #> 23        NA -0.45201887 #> 24        NA -0.38224470 #> 25        NA -0.31158231 #> 26        NA -0.24042647 #> 27        NA -0.16887587 #> 28        NA -0.09732527 #> 29        NA -0.02636681 #> 30        NA  0.04370344 #> 31        NA  0.11189856 #> 32        NA  0.17782381 #> 33        NA  0.24118310 #> 34        NA  0.30187774 #> 35        NA  0.35971037 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13141 rows ] #>  #> Grouped by:  iso3c  [216 | 61 (0)]  wlddev |> fgroup_by(iso3c) |> fselect(PCGDP,LIFEEX) |> STD(weights) # weighted standardizing #>    STD.PCGDP   STD.LIFEEX #> 1         NA -1.467563659 #> 2         NA -1.414317039 #> 3         NA -1.361792758 #> 4         NA -1.310197196 #> 5         NA -1.259427164 #> 6         NA -1.209379469 #> 7         NA -1.159641348 #> 8         NA -1.109696844 #> 9         NA -1.059236385 #> 10        NA -1.007950397 #> 11        NA -0.955426116 #> 12        NA -0.901663540 #> 13        NA -0.846869054 #> 14        NA -0.790939466 #> 15        NA -0.733668392 #> 16        NA -0.674849452 #> 17        NA -0.614276263 #> 18        NA -0.551845634 #> 19        NA -0.487454373 #> 20        NA -0.421308863 #> 21        NA -0.353305913 #> 22        NA -0.283239140 #> 23        NA -0.211624501 #> 24        NA -0.138668377 #> 25        NA -0.064783533 #> 26        NA  0.009617267 #> 27        NA  0.084430831 #> 28        NA  0.159244395 #> 29        NA  0.233438812 #> 30        NA  0.306704510 #> 31        NA  0.378009576 #> 32        NA  0.446941246 #> 33        NA  0.513189947 #> 34        NA  0.576652487 #> 35        NA  0.637122485 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13141 rows ] #>  #> Grouped by:  iso3c  [216 | 61 (0)]  wlddev |> fgroup_by(iso3c) |> fselect(PCGDP,LIFEEX,POP) |> STD(POP) # weighting by POP -> #>         POP STD.PCGDP STD.LIFEEX #> 1   8996973        NA -2.1727695 #> 2   9169410        NA -2.1194893 #> 3   9351441        NA -2.0669319 #> 4   9543205        NA -2.0153038 #> 5   9744781        NA -1.9645018 #> 6   9956320        NA -1.9144225 #> 7  10174836        NA -1.8646530 #> 8  10399926        NA -1.8146770 #> 9  10637063        NA -1.7641848 #> 10 10893776        NA -1.7128664 #> 11 11173642        NA -1.6603090 #> 12 11475445        NA -1.6065126 #> 13 11791215        NA -1.5516835 #> 14 12108963        NA -1.4957187 #> 15 12412950        NA -1.4384115 #> 16 12689160        NA -1.3795555 #> 17 12943093        NA -1.3189441 #> 18 13171306        NA -1.2564741 #> 19 13341198        NA -1.1920422 #> 20 13411056        NA -1.1258550 #> 21 13356511        NA -1.0578092 #> 22 13171673        NA -0.9876983 #> 23 12882528        NA -0.9160385 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13153 rows ] #>  #> Grouped by:  iso3c  [216 | 61 (0)]  # ..keeps the weight column unless keep.w = FALSE"},{"path":"https://sebkrantz.github.io/collapse/reference/fslice.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Slicing of Matrix-Like Objects — fslice","title":"Fast Slicing of Matrix-Like Objects — fslice","text":"fast function extract rows matrix data frame-like object (groups).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fslice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Slicing of Matrix-Like Objects — fslice","text":"","code":"fslice(x, ..., n = 1, how = \"first\", order.by = NULL,        na.rm = .op[[\"na.rm\"]], sort = FALSE, with.ties = FALSE)  fslicev(x, cols = NULL, n = 1, how = \"first\", order.by = NULL,         na.rm = .op[[\"na.rm\"]], sort = FALSE, with.ties = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fslice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Slicing of Matrix-Like Objects — fslice","text":"x matrix, data frame list-like object, including 'grouped_df'. ... fslice: names sequences columns group - passed fselect. x matrix: atomic vectors group x. Can empty operate (un)grouped data. fslicev: arguments passed GRP (decreasing, na.last, method). cols select columns group , using column names, indices, logical vector selector function (e.g. is_categorical). can also list vectors, , x matrix, single vector. n integer proportion (< 1). Number rows select group. proportion provided, converted equivalent number rows. character. Method select rows. One : \"first\": select first n rows \"last\": select last n rows \"min\": select n rows minimum values order.\"max\": select n rows maximum values order. order.vector column name order \"min\" \"max\". Must length rows x. fslice must quoted. na.rm logical. TRUE, missing values order.removed selecting rows. sort logical. TRUE, sort selected rows grouping columns. FALSE uses first-appearance order (including grouping columns \"first\" \"last\") - fastest. .ties logical. TRUE \"min\" \"max\", returns rows extreme value. Currently supported n = 1 sort = FALSE.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fslice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Slicing of Matrix-Like Objects — fslice","text":"subset x containing selected rows.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fslice.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Slicing of Matrix-Like Objects — fslice","text":"","code":"# Basic usage fslice(mtcars, n = 3)                    # First 3 rows #>                mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 fslice(mtcars, n = 3, how = \"last\")      # Last 3 rows #>                mpg cyl disp  hp drat   wt qsec vs am gear carb #> Ferrari Dino  19.7   6  145 175 3.62 2.77 15.5  0  1    5    6 #> Maserati Bora 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8 #> Volvo 142E    21.4   4  121 109 4.11 2.78 18.6  1  1    4    2 fslice(mtcars, n = 0.1)                  # First 10% of rows #>                mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  # Using order.by fslice(mtcars, n = 3, how = \"min\", order.by = mpg)  # 3 cars with lowest mpg #>                      mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4 #> Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4 fslice(mtcars, n = 3, how = \"max\", order.by = mpg)  # 3 cars with highest mpg #>                 mpg cyl disp hp drat    wt  qsec vs am gear carb #> Toyota Corolla 33.9   4 71.1 65 4.22 1.835 19.90  1  1    4    1 #> Fiat 128       32.4   4 78.7 66 4.08 2.200 19.47  1  1    4    1 #> Honda Civic    30.4   4 75.7 52 4.93 1.615 18.52  1  1    4    2  # With grouping mtcars |> fslice(cyl, n = 2)                        # First 2 cars per cylinder #>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Duster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 mtcars |> fslice(cyl, n = 2, sort = TRUE)           # with sorting (slightly less efficient) #>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Datsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 #> Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Duster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 mtcars |> fslice(cyl, n = 2, how = \"min\", order.by = mpg)  # 2 lowest mpg cars per cylinder #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2 #> Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1 #> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  # Using with.ties mtcars |> fslice(cyl, n = 1, how = \"min\", order.by = mpg, with.ties = TRUE) #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  # With grouped data mtcars |>   fgroup_by(cyl) |>   fslice(n = 2, how = \"max\", order.by = mpg)        # 2 highest mpg cars per cylinder #>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Toyota Corolla    33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 #> Fiat 128          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Pontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 #> Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2"},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Subsetting Matrix-Like Objects — fsubset","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":"fsubset returns subsets vectors, matrices data frames meet conditions. programmed efficiently uses C source code data.table package.   methods also provide enhanced functionality compared subset. function ss provides (internal generic) programmers alternative [ drop dimensions significantly faster [ data frames.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":"","code":"fsubset(.x, ...) sbt(.x, ...)     # Shorthand for fsubset  # Default S3 method fsubset(.x, subset, ...)  # S3 method for class 'matrix' fsubset(.x, subset, ..., drop = FALSE)  # S3 method for class 'data.frame' fsubset(.x, subset, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' fsubset(.x, subset, ..., drop.index.levels = \"id\")  # S3 method for class 'pdata.frame' fsubset(.x, subset, ..., drop.index.levels = \"id\")   # Fast subsetting (replaces `[` with drop = FALSE, programmers choice) ss(x, i, j, check = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":".x object subsetted according different methods. x data frame / list, matrix vector/array (). subset logical expression indicating elements rows keep:     missing values taken FALSE. default, matrix pseries methods support logical vectors row-indices (character vector rownames matrix rownames). ... matrix data frame method: multiple comma-separated expressions indicating columns select. Otherwise: arguments passed methods. drop passed [ indexing operator. available matrix method. positive negative row-indices logical vector subset rows x. j vector column names, positive negative indices suitable logical vector subset columns x. Note: Negative indices converted positive ones using j <- seq_along(x)[j]. check logical. FALSE skips checks j, e.g. whether indices negative. offers speedup programmers, can terminate R zero negative indices passed. drop.index.levels character. Either \"id\", \"time\", \"\" \"none\". See indexing.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":"fsubset generic function, methods supplied vectors, matrices, data   frames (including lists). represents improvement subset terms speed functionality. function ss improvement [ subset (vectors) matrices data frames without dropping dimensions. significantly faster [.data.frame. ordinary vectors, subset can integer logical, subsetting done C efficient [ large vectors. matrices implementation base-R slightly efficient versatile subset.matrix. Thus possible subset matrix rows using logical integer vectors, character vectors matching rownames. drop argument passed [ method matrices. matrices data frames, ... argument can used subset columns, evaluated non-standard way. Thus can support vectors column names, indices logical vectors, also multiple comma separated column names passed without quotes, may also replaced sequence columns .e. col1:coln, new column names may assigned e.g. fsubset(data, col1 > 20, newname =  col2, col3:col6) (see examples). data frames, subset argument also evaluated non-standard way. Thus next vector row-indices logical vectors, supports logical expressions form col2 > 5 & col2 < col3 etc. (see examples). data frame method implemented C, hence significantly faster subset.data.frame. fast data frame subsetting required non-standard evaluation, function ss slightly simpler faster. Factors may empty levels subsetting; unused levels automatically removed.  See fdroplevels drop unused levels data frame.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":"object similar .x/x containing just selected elements (  vector), rows columns (matrix data frame).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":"ss offers support indexed data. Use fsubset indices instead. replacement method fsubset<- ss<- offered collapse. efficient subset replacement (without copying) use data.table::set, can also used data frames tibbles. search replace certain elements without copying, efficiently copy elements / rows equally sized vector / data frame, see setv. subsetting columns alone, please also see selecting replacing columns. Note use %==% can yield significant performance gains large data.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fsubset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Subsetting Matrix-Like Objects — fsubset","text":"","code":"fsubset(airquality, Temp > 90, Ozone, Temp) #>    Ozone Temp #> 1     NA   93 #> 2     NA   92 #> 3     97   92 #> 4     97   92 #> 5     NA   91 #> 6     NA   92 #> 7     76   97 #> 8    118   94 #> 9     84   96 #> 10    85   94 #> 11    96   91 #> 12    78   92 #> 13    73   93 #> 14    91   93 fsubset(airquality, Temp > 90, OZ = Ozone, Temp) # With renaming #>     OZ Temp #> 1   NA   93 #> 2   NA   92 #> 3   97   92 #> 4   97   92 #> 5   NA   91 #> 6   NA   92 #> 7   76   97 #> 8  118   94 #> 9   84   96 #> 10  85   94 #> 11  96   91 #> 12  78   92 #> 13  73   93 #> 14  91   93 fsubset(airquality, Day == 1, -Temp) #>   Ozone Solar.R Wind Month Day #> 1    41     190  7.4     5   1 #> 2    NA     286  8.6     6   1 #> 3   135     269  4.1     7   1 #> 4    39      83  6.9     8   1 #> 5    96     167  6.9     9   1 fsubset(airquality, Day == 1, -(Day:Temp)) #>   Ozone Solar.R Wind #> 1    41     190  7.4 #> 2    NA     286  8.6 #> 3   135     269  4.1 #> 4    39      83  6.9 #> 5    96     167  6.9 fsubset(airquality, Day == 1, Ozone:Wind) #>   Ozone Solar.R Wind #> 1    41     190  7.4 #> 2    NA     286  8.6 #> 3   135     269  4.1 #> 4    39      83  6.9 #> 5    96     167  6.9 fsubset(airquality, Day == 1 & !is.na(Ozone), Ozone:Wind, Month) #>   Ozone Solar.R Wind Month #> 1    41     190  7.4     5 #> 2   135     269  4.1     7 #> 3    39      83  6.9     8 #> 4    96     167  6.9     9 fsubset(airquality, Day %==% 1, -Temp)  # Faster for big data, as %==% directly returns indices #>   Ozone Solar.R Wind Month Day #> 1    41     190  7.4     5   1 #> 2    NA     286  8.6     6   1 #> 3   135     269  4.1     7   1 #> 4    39      83  6.9     8   1 #> 5    96     167  6.9     9   1  ss(airquality, 1:10, 2:3)         # Significantly faster than airquality[1:10, 2:3] #>    Solar.R Wind #> 1      190  7.4 #> 2      118  8.0 #> 3      149 12.6 #> 4      313 11.5 #> 5       NA 14.3 #> 6       NA 14.9 #> 7      299  8.6 #> 8       99 13.8 #> 9       19 20.1 #> 10     194  8.6 fsubset(airquality, 1:10, 2:3)    # This is possible but not advised #>    Solar.R Wind #> 1      190  7.4 #> 2      118  8.0 #> 3      149 12.6 #> 4      313 11.5 #> 5       NA 14.3 #> 6       NA 14.9 #> 7      299  8.6 #> 8       99 13.8 #> 9       19 20.1 #> 10     194  8.6"},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"fsum generic function computes (column-wise) sum values x, (optionally) grouped g /weighted w (e.g. calculate survey totals). TRA argument can used transform x using (grouped, weighted) sum.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"","code":"fsum(x, ...)  # Default S3 method fsum(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, fill = FALSE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'matrix' fsum(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, fill = FALSE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'data.frame' fsum(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, fill = FALSE, nthreads = .op[[\"nthreads\"]], ...)  # S3 method for class 'grouped_df' fsum(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = FALSE, keep.group_vars = TRUE, keep.w = TRUE, stub = .op[[\"stub\"]],      fill = FALSE, nthreads = .op[[\"nthreads\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"x numeric vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. w numeric vector (non-negative) weights, may contain missing values. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE NA returned encountered. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. fill logical. Initialize result 0 instead NA na.rm = TRUE e.g. fsum(NA, fill = TRUE) returns 0 instead NA. nthreads integer. number threads utilize. See Details. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. keep.w grouped_df method: Logical. Retain summed weighting variable computation (contained grouped_df). stub character. keep.w = TRUE stub = TRUE (default), summed weights column prefixed \"sum.\". Users can specify different prefix argument, set FALSE avoid prefixing. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"weighted sum (e.g. survey total) computed sum(x * w), one pass twice efficient. na.rm = TRUE, missing values removed x w .e. utilizing x[complete.cases(x,w)] w[complete.cases(x,w)]. seamlessly generalizes grouped computations, performed single pass (without splitting data) therefore extremely fast. See Benchmark Examples . applied data frames groups drop = FALSE, fsum preserves column attributes. attributes data frame also preserved. Since v1.6.0 fsum explicitly supports integers. Integers summed using long long type C bounded +-9,223,372,036,854,775,807 (~4.3 billion times greater minimum/maximum R integer bounded +-2,147,483,647). value sum outside +-2,147,483,647, double containing result returned, otherwise integer returned. groups, integer results vector initialized, integer overflow error provided sum group outside +-2,147,483,647. Data needs coerced double beforehand cases. Multithreading, added v1.8.0, applies column-level unless g = NULL nthreads > NCOL(x). Parallelism groups available sums computed simultaneously within group. nthreads = 1L uses serial version code, parallel code running one thread. serial code always used less 100,000 obs (length(x) < 100000 vectors matrices), parallel execution overhead.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"(w weighted) sum x, grouped g, (TRA used) x transformed (grouped, weighted) sum.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":"see-also","dir":"Reference","previous_headings":"","what":"See Also","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"fprod, fmean, Fast Statistical Functions, Collapse Overview","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Sum for Matrix-Like Objects — fsum","text":"","code":"## default vector method mpg <- mtcars$mpg fsum(mpg)                         # Simple sum #> [1] 642.9 fsum(mpg, w = mtcars$hp)          # Weighted sum (total): Weighted by hp #> [1] 84362.7 fsum(mpg, TRA = \"%\")              # Simple transformation: obtain percentages of mpg #>  [1] 3.266449 3.266449 3.546430 3.328667 2.908695 2.815368 2.224296 3.795303 #>  [9] 3.546430 2.986468 2.768704 2.550941 2.690932 2.364287 1.617670 1.617670 #> [17] 2.286514 5.039664 4.728574 5.272982 3.344221 2.410950 2.364287 2.068751 #> [25] 2.986468 4.246384 4.044175 4.728574 2.457614 3.064240 2.333178 3.328667 fsum(mpg, mtcars$cyl)             # Grouped sum #>     4     6     8  #> 293.3 138.2 211.4  fsum(mpg, mtcars$cyl, mtcars$hp)  # Weighted grouped sum (total) #>       4       6       8  #> 23743.0 16873.0 43746.7  fsum(mpg, mtcars[c(2,8:9)])       # More groups.. #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>  26.0  68.7 198.6  61.7  76.5 180.6  30.8  g <- GRP(mtcars, ~ cyl + vs + am) # Precomputing groups gives more speed ! fsum(mpg, g) #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>  26.0  68.7 198.6  61.7  76.5 180.6  30.8  fmean(mpg, g) == fsum(mpg, g) / fnobs(mpg, g) #> 4.0.1 4.1.0 4.1.1 6.0.1 6.1.0 8.0.0 8.0.1  #>  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  fsum(mpg, g, TRA = \"%\")           # Percentages by group #>  [1]  34.035656  34.035656  11.480363  27.973856  10.354374  23.660131 #>  [7]   7.918051  35.516739  33.187773  25.098039  23.267974   9.080842 #> [13]   9.579181   8.416390   5.758583   5.758583   8.139535  16.314199 #> [19]  15.307150  17.069486  31.295488   8.582503   8.416390   7.364341 #> [25]  10.631229  13.746224 100.000000  15.307150  51.298701  31.928687 #> [31]  48.701299  10.775428  ## data.frame method fsum(mtcars) #>      mpg      cyl     disp       hp     drat       wt     qsec       vs  #>  642.900  198.000 7383.100 4694.000  115.090  102.952  571.160   14.000  #>       am     gear     carb  #>   13.000  118.000   90.000  fsum(mtcars, TRA = \"%\") #>                        mpg      cyl     disp       hp     drat       wt #> Mazda RX4         3.266449 3.030303 2.167111 2.343417 3.388652 2.544875 #> Mazda RX4 Wag     3.266449 3.030303 2.167111 2.343417 3.388652 2.792564 #> Datsun 710        3.546430 2.020202 1.462800 1.981253 3.345208 2.253477 #> Hornet 4 Drive    3.328667 3.030303 3.494467 2.343417 2.676166 3.122815 #> Hornet Sportabout 2.908695 4.040404 4.876001 3.728164 2.736988 3.341363 #> Valiant           2.815368 3.030303 3.047500 2.236898 2.398123 3.360789 #>                       qsec       vs       am     gear     carb #> Mazda RX4         2.881854 0.000000 7.692308 3.389831 4.444444 #> Mazda RX4 Wag     2.979901 0.000000 7.692308 3.389831 4.444444 #> Datsun 710        3.258281 7.142857 7.692308 3.389831 1.111111 #> Hornet 4 Drive    3.403600 7.142857 0.000000 2.542373 1.111111 #> Hornet Sportabout 2.979901 0.000000 0.000000 2.542373 2.222222 #> Valiant           3.540164 7.142857 0.000000 2.542373 1.111111 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] fsum(mtcars, g) #>         mpg cyl   disp   hp  drat     wt   qsec vs am gear carb #> 4.0.1  26.0   4  120.3   91  4.43  2.140  16.70  0  1    5    2 #> 4.1.0  68.7  12  407.6  254 11.31  8.805  62.91  3  0   11    5 #> 4.1.1 198.6  28  628.6  564 29.04 14.198 130.90  7  7   29   10 #> 6.0.1  61.7  18  465.0  395 11.42  8.265  48.98  0  3   13   14 #> 6.1.0  76.5  24  818.2  461 13.68 13.555  76.86  4  0   14   10 #> 8.0.0 180.6  96 4291.4 2330 37.45 49.249 205.71  0  0   36   37 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fsum(mtcars, g, TRA = \"%\") #>                        mpg       cyl      disp       hp      drat        wt #> Mazda RX4         34.03566 33.333333 34.408602 27.84810 34.150613 31.699940 #> Mazda RX4 Wag     34.03566 33.333333 34.408602 27.84810 34.150613 34.785239 #> Datsun 710        11.48036 14.285714 17.181037 16.48936 13.257576 16.340330 #> Hornet 4 Drive    27.97386 25.000000 31.532633 23.86117 22.514620 23.718185 #> Hornet Sportabout 10.35437  8.333333  8.388871  7.51073  8.411215  6.984913 #> Valiant           23.66013 25.000000 27.499389 22.77657 20.175439 25.525636 #>                        qsec       vs       am      gear      carb #> Mazda RX4         33.605553      NaN 33.33333 30.769231 28.571429 #> Mazda RX4 Wag     34.748877      NaN 33.33333 30.769231 28.571429 #> Datsun 710        14.216960 14.28571 14.28571 13.793103 10.000000 #> Hornet 4 Drive    25.292740 25.00000      NaN 21.428571 10.000000 #> Hornet Sportabout  8.273783      NaN      NaN  8.333333  5.405405 #> Valiant           26.307572 25.00000      NaN 21.428571 10.000000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  ## matrix method m <- qM(mtcars) fsum(m) #>      mpg      cyl     disp       hp     drat       wt     qsec       vs  #>  642.900  198.000 7383.100 4694.000  115.090  102.952  571.160   14.000  #>       am     gear     carb  #>   13.000  118.000   90.000  fsum(m, TRA = \"%\") #>                          mpg      cyl      disp       hp     drat       wt #> Mazda RX4           3.266449 3.030303 2.1671114 2.343417 3.388652 2.544875 #> Mazda RX4 Wag       3.266449 3.030303 2.1671114 2.343417 3.388652 2.792564 #> Datsun 710          3.546430 2.020202 1.4628002 1.981253 3.345208 2.253477 #> Hornet 4 Drive      3.328667 3.030303 3.4944671 2.343417 2.676166 3.122815 #> Hornet Sportabout   2.908695 4.040404 4.8760006 3.728164 2.736988 3.341363 #> Valiant             2.815368 3.030303 3.0475004 2.236898 2.398123 3.360789 #>                         qsec       vs       am     gear     carb #> Mazda RX4           2.881854 0.000000 7.692308 3.389831 4.444444 #> Mazda RX4 Wag       2.979901 0.000000 7.692308 3.389831 4.444444 #> Datsun 710          3.258281 7.142857 7.692308 3.389831 1.111111 #> Hornet 4 Drive      3.403600 7.142857 0.000000 2.542373 1.111111 #> Hornet Sportabout   2.979901 0.000000 0.000000 2.542373 2.222222 #> Valiant             3.540164 7.142857 0.000000 2.542373 1.111111 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] fsum(m, g) #>         mpg cyl   disp   hp  drat     wt   qsec vs am gear carb #> 4.0.1  26.0   4  120.3   91  4.43  2.140  16.70  0  1    5    2 #> 4.1.0  68.7  12  407.6  254 11.31  8.805  62.91  3  0   11    5 #> 4.1.1 198.6  28  628.6  564 29.04 14.198 130.90  7  7   29   10 #> 6.0.1  61.7  18  465.0  395 11.42  8.265  48.98  0  3   13   14 #> 6.1.0  76.5  24  818.2  461 13.68 13.555  76.86  4  0   14   10 #> 8.0.0 180.6  96 4291.4 2330 37.45 49.249 205.71  0  0   36   37 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] fsum(m, g, TRA = \"%\") #>                            mpg        cyl       disp         hp       drat #> Mazda RX4            34.035656  33.333333  34.408602  27.848101  34.150613 #> Mazda RX4 Wag        34.035656  33.333333  34.408602  27.848101  34.150613 #> Datsun 710           11.480363  14.285714  17.181037  16.489362  13.257576 #> Hornet 4 Drive       27.973856  25.000000  31.532633  23.861171  22.514620 #> Hornet Sportabout    10.354374   8.333333   8.388871   7.510730   8.411215 #> Valiant              23.660131  25.000000  27.499389  22.776573  20.175439 #>                             wt       qsec       vs        am       gear #> Mazda RX4            31.699940  33.605553      NaN  33.33333  30.769231 #> Mazda RX4 Wag        34.785239  34.748877      NaN  33.33333  30.769231 #> Datsun 710           16.340330  14.216960 14.28571  14.28571  13.793103 #> Hornet 4 Drive       23.718185  25.292740 25.00000       NaN  21.428571 #> Hornet Sportabout     6.984913   8.273783      NaN       NaN   8.333333 #> Valiant              25.525636  26.307572 25.00000       NaN  21.428571 #>                           carb #> Mazda RX4            28.571429 #> Mazda RX4 Wag        28.571429 #> Datsun 710           10.000000 #> Hornet 4 Drive       10.000000 #> Hornet Sportabout     5.405405 #> Valiant              10.000000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fsum(hp)  # Weighted grouped sum (total) #>   cyl vs am sum.hp     mpg     disp    drat       wt     qsec gear carb #> 1   4  0  1     91  2366.0  10947.3  403.13  194.740  1519.70  455  182 #> 2   4  1  0    254  5764.3  34121.1  960.08  736.135  5356.47  919  411 #> 3   4  1  1    564 15612.7  52945.4 2301.27 1165.914 10456.79 2369  838 #> 4   6  0  1    395  8067.5  60575.0 1491.50 1089.200  6395.30 1755 1930 #> 5   6  1  0    461  8805.5  93234.6 1592.92 1563.190  8837.10 1629 1199 #> 6   8  0  0   2330 34550.5 846042.0 7323.40 9689.735 39807.80 6990 7480 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fsum(TRA = \"%\") #>                   cyl vs am      mpg      disp       hp      drat        wt #> Mazda RX4           6  0  1 34.03566 34.408602 27.84810 34.150613 31.699940 #> Mazda RX4 Wag       6  0  1 34.03566 34.408602 27.84810 34.150613 34.785239 #> Datsun 710          4  1  1 11.48036 17.181037 16.48936 13.257576 16.340330 #> Hornet 4 Drive      6  1  0 27.97386 31.532633 23.86117 22.514620 23.718185 #> Hornet Sportabout   8  0  0 10.35437  8.388871  7.51073  8.411215  6.984913 #> Valiant             6  1  0 23.66013 27.499389 22.77657 20.175439 25.525636 #>                        qsec      gear      carb #> Mazda RX4         33.605553 30.769231 28.571429 #> Mazda RX4 Wag     34.748877 30.769231 28.571429 #> Datsun 710        14.216960 13.793103 10.000000 #> Hornet 4 Drive    25.292740 21.428571 10.000000 #> Hornet Sportabout  8.273783  8.333333  5.405405 #> Valiant           26.307572 21.428571 10.000000 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]  mtcars |> fgroup_by(cyl,vs,am) |> fselect(mpg) |> fsum() #>   cyl vs am   mpg #> 1   4  0  1  26.0 #> 2   4  1  0  68.7 #> 3   4  1  1 198.6 #> 4   6  0  1  61.7 #> 5   6  1  0  76.5 #> 6   8  0  0 180.6 #> 7   8  0  1  30.8    ## This compares fsum with data.table and base::rowsum # Starting with small data library(data.table) #>  #> Attaching package: ‘data.table’ #> The following objects are masked from ‘package:dplyr’: #>  #>     between, first, last #> The following object is masked from ‘package:collapse’: #>  #>     fdroplevels opts <- set_collapse(nthreads = getDTthreads()) mtcDT <- qDT(mtcars) f <- qF(mtcars$cyl)  library(microbenchmark) microbenchmark(mtcDT[, lapply(.SD, sum), by = f],                rowsum(mtcDT, f, reorder = FALSE),                fsum(mtcDT, f, na.rm = FALSE), unit = \"relative\") #> Unit: relative #>                               expr       min        lq     mean   median #>  mtcDT[, lapply(.SD, sum), by = f] 88.831579 78.634703 70.62758 73.28992 #>  rowsum(mtcDT, f, reorder = FALSE)  3.673684  3.424658  3.33947  3.42437 #>      fsum(mtcDT, f, na.rm = FALSE)  1.000000  1.000000  1.00000  1.00000 #>         uq        max neval #>  62.802817 136.422175   100 #>   3.165493   5.507463   100 #>   1.000000   1.000000   100  # Now larger data tdata <- qDT(replicate(100, rnorm(1e5), simplify = FALSE)) # 100 columns with 100.000 obs f <- qF(sample.int(1e4, 1e5, TRUE))                        # A factor with 10.000 groups  microbenchmark(tdata[, lapply(.SD, sum), by = f],                rowsum(tdata, f, reorder = FALSE),                fsum(tdata, f, na.rm = FALSE), unit = \"relative\") #> Unit: relative #>                               expr      min       lq     mean   median       uq #>  tdata[, lapply(.SD, sum), by = f] 3.333374 3.196280 3.049802 3.229762 3.371307 #>  rowsum(tdata, f, reorder = FALSE) 2.424533 2.313368 2.263252 2.331422 2.425420 #>      fsum(tdata, f, na.rm = FALSE) 1.000000 1.000000 1.000000 1.000000 1.000000 #>       max neval #>  4.441966   100 #>  4.096463   100 #>  1.000000   100 # Reset options set_collapse(opts)"},{"path":"https://sebkrantz.github.io/collapse/reference/fsummarise.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Summarise — fsummarise","title":"Fast Summarise — fsummarise","text":"fsummarise much faster version dplyr::summarise, used together Fast Statistical Functions. fsummarize fsummarise synonyms.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsummarise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Summarise — fsummarise","text":"","code":"fsummarise(.data, ..., keep.group_vars = TRUE, .cols = NULL) fsummarize(.data, ..., keep.group_vars = TRUE, .cols = NULL) smr(.data, ..., keep.group_vars = TRUE, .cols = NULL)        # Shorthand"},{"path":"https://sebkrantz.github.io/collapse/reference/fsummarise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Summarise — fsummarise","text":".data (grouped) data frame named list columns. Grouped data can created fgroup_by dplyr::group_by. ... name-value pairs summary functions, across statements, arbitrary expressions resulting list. See Examples. fast performance use Fast Statistical Functions. keep.group_vars logical. FALSE removes grouping variables computation. .cols expressions involving .data, .cols can used subset columns, e.g. mtcars |> gby(cyl) |> smr(mctl(cor(.data), TRUE), .cols = 5:7). Can pass column names, indices, logical vector selector function (e.g. .numericr).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsummarise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Summarise — fsummarise","text":".data grouped fgroup_by dplyr::group_by, result data frame class attributes rows reduced number groups. .data grouped, result data frame class attributes 1 row.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fsummarise.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Summarise — fsummarise","text":"Since v1.7, fsummarise fully featured, allowing expressions using functions columns data well external scalar values (just like dplyr::summarise). NOTE however Fast Statistical Function used, execution vectorized instead split-apply-combine computing groups. Please see first Example.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fsummarise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Summarise — fsummarise","text":"","code":"## Since v1.7, fsummarise supports arbitrary expressions, and expressions ## containing fast statistical functions receive vectorized execution:  # (a) This is an expression using base R functions which is executed by groups mtcars |> fgroup_by(cyl) |> fsummarise(res = mean(mpg) + min(qsec)) #>   cyl      res #> 1   4 43.36364 #> 2   6 35.24286 #> 3   8 29.60000  # (b) Here, the use of fmean causes the whole expression to be executed # in a vectorized way i.e. the expression is translated to something like # fmean(mpg, g = cyl) + min(mpg) and executed, thus the result is different # from (a), because the minimum is calculated over the entire sample mtcars |> fgroup_by(cyl) |> fsummarise(mpg = fmean(mpg) + min(qsec)) #>   cyl      mpg #> 1   4 41.16364 #> 2   6 34.24286 #> 3   8 29.60000  # (c) For fully vectorized execution, use fmin. This yields the same as (a) mtcars |> fgroup_by(cyl) |> fsummarise(mpg = fmean(mpg) + fmin(qsec)) #>   cyl      mpg #> 1   4 43.36364 #> 2   6 35.24286 #> 3   8 29.60000  # More advanced use: vectorized grouped regression slopes: mpg ~ carb mtcars |>   fgroup_by(cyl) |>   fmutate(dm_carb = fwithin(carb)) |>   fsummarise(beta = fsum(mpg, dm_carb) %/=% fsum(dm_carb^2)) #>   cyl         beta #> 1   4 -1.680000000 #> 2   6 -0.006521739 #> 3   8 -0.647619048   # In across() statements it is fine to mix different functions, each will # be executed on its own terms (i.e. vectorized for fmean and standard for sum) mtcars |> fgroup_by(cyl) |> fsummarise(across(mpg:hp, list(fmean, sum))) #>   cyl mpg_fmean mpg_sum cyl_fmean cyl_sum disp_fmean disp_sum  hp_fmean hp_sum #> 1   4  26.66364   293.3         4      44   105.1364   1156.5  82.63636    909 #> 2   6  19.74286   138.2         6      42   183.3143   1283.2 122.28571    856 #> 3   8  15.10000   211.4         8     112   353.1000   4943.4 209.21429   2929  # Note that this still detects fmean as a fast function, the names of the list # are irrelevant, but the function name must be typed or passed as a character vector, # Otherwise functions will be executed by groups e.g. function(x) fmean(x) won't vectorize mtcars |> fgroup_by(cyl) |> fsummarise(across(mpg:hp, list(mu = fmean, sum = sum))) #>   cyl   mpg_mu mpg_sum cyl_mu cyl_sum  disp_mu disp_sum     hp_mu hp_sum #> 1   4 26.66364   293.3      4      44 105.1364   1156.5  82.63636    909 #> 2   6 19.74286   138.2      6      42 183.3143   1283.2 122.28571    856 #> 3   8 15.10000   211.4      8     112 353.1000   4943.4 209.21429   2929  # We can force none-vectorized execution by setting .apply = TRUE mtcars |> fgroup_by(cyl) |> fsummarise(across(mpg:hp, list(mu = fmean, sum = sum), .apply = TRUE)) #>   cyl   mpg_mu mpg_sum cyl_mu cyl_sum  disp_mu disp_sum     hp_mu hp_sum #> 1   4 26.66364   293.3      4      44 105.1364   1156.5  82.63636    909 #> 2   6 19.74286   138.2      6      42 183.3143   1283.2 122.28571    856 #> 3   8 15.10000   211.4      8     112 353.1000   4943.4 209.21429   2929  # Another argument of across(): Order the result first by function, then by column mtcars |> fgroup_by(cyl) |>      fsummarise(across(mpg:hp, list(mu = fmean, sum = sum), .transpose = FALSE)) #>   cyl   mpg_mu cyl_mu  disp_mu     hp_mu mpg_sum cyl_sum disp_sum hp_sum #> 1   4 26.66364      4 105.1364  82.63636   293.3      44   1156.5    909 #> 2   6 19.74286      6 183.3143 122.28571   138.2      42   1283.2    856 #> 3   8 15.10000      8 353.1000 209.21429   211.4     112   4943.4   2929   # Since v1.9.0, can also evaluate arbitrary expressions mtcars |> fgroup_by(cyl, vs, am) |>    fsummarise(mctl(cor(cbind(mpg, wt, carb)), names = TRUE)) #>    cyl vs am        mpg         wt       carb #> 1    4  0  1         NA         NA         NA #> 2    4  0  1         NA         NA         NA #> 3    4  0  1         NA         NA         NA #> 4    4  1  0  1.0000000  0.8606981  0.8346751 #> 5    4  1  0  0.8606981  1.0000000  0.9987950 #> 6    4  1  0  0.8346751  0.9987950  1.0000000 #> 7    4  1  1  1.0000000 -0.7185019 -0.1909932 #> 8    4  1  1 -0.7185019  1.0000000 -0.1253054 #> 9    4  1  1 -0.1909932 -0.1253054  1.0000000 #> 10   6  0  1  1.0000000 -0.1013606 -1.0000000 #> 11   6  0  1 -0.1013606  1.0000000  0.1013606 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 rows ]  # This can also be achieved using across(): corfun <- function(x) mctl(cor(x), names = TRUE) mtcars |> fgroup_by(cyl, vs, am) |>    fsummarise(across(c(mpg, wt, carb), corfun, .apply = FALSE)) #>    cyl vs am        mpg         wt       carb #> 1    4  0  1         NA         NA         NA #> 2    4  0  1         NA         NA         NA #> 3    4  0  1         NA         NA         NA #> 4    4  1  0  1.0000000  0.8606981  0.8346751 #> 5    4  1  0  0.8606981  1.0000000  0.9987950 #> 6    4  1  0  0.8346751  0.9987950  1.0000000 #> 7    4  1  1  1.0000000 -0.7185019 -0.1909932 #> 8    4  1  1 -0.7185019  1.0000000 -0.1253054 #> 9    4  1  1 -0.1909932 -0.1253054  1.0000000 #> 10   6  0  1  1.0000000 -0.1013606 -1.0000000 #> 11   6  0  1 -0.1013606  1.0000000  0.1013606 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 rows ]  #---------------------------------------------------------------------------- # Examples that also work for pre 1.7 versions  # Simple use fsummarise(mtcars, mean_mpg = fmean(mpg),                    sd_mpg = fsd(mpg)) #>   mean_mpg   sd_mpg #> 1 20.09062 6.026948  # Using base functions (not a big difference without groups) fsummarise(mtcars, mean_mpg = mean(mpg),                    sd_mpg = sd(mpg)) #>   mean_mpg   sd_mpg #> 1 20.09062 6.026948  # Grouped use mtcars |> fgroup_by(cyl) |>   fsummarise(mean_mpg = fmean(mpg),              sd_mpg = fsd(mpg)) #>   cyl mean_mpg   sd_mpg #> 1   4 26.66364 4.509828 #> 2   6 19.74286 1.453567 #> 3   8 15.10000 2.560048  # This is still efficient but quite a bit slower on large data (many groups) mtcars |> fgroup_by(cyl) |>   fsummarise(mean_mpg = mean(mpg),              sd_mpg = sd(mpg)) #>   cyl mean_mpg   sd_mpg #> 1   4 26.66364 4.509828 #> 2   6 19.74286 1.453567 #> 3   8 15.10000 2.560048  # Weighted aggregation mtcars |> fgroup_by(cyl) |>   fsummarise(w_mean_mpg = fmean(mpg, wt),              w_sd_mpg = fsd(mpg, wt)) #>   cyl w_mean_mpg w_sd_mpg #> 1   4   25.93504 4.275234 #> 2   6   19.64578 1.397297 #> 3   8   14.80643 2.638850    ## Can also group with dplyr::group_by, but at a conversion cost, see ?GRP library(dplyr) mtcars |> group_by(cyl) |>   fsummarise(mean_mpg = fmean(mpg),              sd_mpg = fsd(mpg)) #> # A tibble: 3 × 3 #>     cyl mean_mpg sd_mpg #>   <dbl>    <dbl>  <dbl> #> 1     4     26.7   4.51 #> 2     6     19.7   1.45 #> 3     8     15.1   2.56  # Again less efficient... mtcars |> group_by(cyl) |>   fsummarise(mean_mpg = mean(mpg),              sd_mpg = sd(mpg)) #> # A tibble: 3 × 3 #>     cyl mean_mpg sd_mpg #>   <dbl>    <dbl>  <dbl> #> 1     4     26.7   4.51 #> 2     6     19.7   1.45 #> 3     8     15.1   2.56"},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Transform and Compute Columns on a Data Frame — ftransform","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":"ftransform much faster version transform data frames. returns data frame new columns computed /existing columns modified deleted. settransform reference. fcompute computes returns new columns. functions evaluate arguments simultaneously, allow list-input (nested pipelines) disregard grouped data. Catering tidyverse user, v1.7.0 introduced fmutate, providing familiar functionality .e. arguments evaluated sequentially, computation grouped data done groups, functions can applied multiple columns using across. See also Details.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":"","code":"# dplyr-style mutate (sequential evaluation + across() feature) fmutate(.data, ..., .keep = \"all\", .cols = NULL) mtt(.data, ..., .keep = \"all\", .cols = NULL) # Shorthand for fmutate  # Modify and return data frame ftransform(.data, ...) ftransformv(.data, vars, FUN, ..., apply = TRUE) tfm(.data, ...)               # Shorthand for ftransform tfmv(.data, vars, FUN, ..., apply = TRUE)  # Modify data frame by reference settransform(.data, ...) settransformv(.data, ...)     # Same arguments as ftransformv settfm(.data, ...)            # Shorthand for settransform settfmv(.data, ...)  # Replace/add modified columns in/to a data frame ftransform(.data) <- valueftransform(.data) <- value tfm(.data) <- value           # Shorthand for ftransform<-  # Compute columns, returned as a new data frame fcompute(.data, ..., keep = NULL) fcomputev(.data, vars, FUN, ..., apply = TRUE, keep = NULL)"},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":".data data frame named list columns. ... arguments form column = value. value can combination columns, scalar value, NULL, deletes column. Alternatively also possible place single list , treated like list column = value arguments. ftransformv fcomputev, ... can used pass arguments FUN. ellipsis (...) always evaluated within data frame (.data) environment. See Examples. fmutate additionally supports across statements, evaluates tagged vector expressions sequentially. grouped execution, dots can also contain arbitrary expressions result list data-length columns. See Examples. vars variables transformed applying FUN : select using names, indices, logical vector selector function (e.g. .numeric). Since v1.7 vars evaluated within .data environment, permitting expressions columns e.g. c(col1, col3:coln). FUN single function yielding result length NROW(.data) 1. See also apply. apply logical. TRUE (default) apply FUN column selected vars; FALSE apply FUN subsetted data frame .e. FUN(get_vars(.data, vars), ...). latter useful collapse functions data frame grouped / panel data frame methods, yielding performance gains enabling grouped transformations. See Examples. value named list replacements, treated like evaluated list column = value arguments. keep select columns preserve using column names, indices function (e.g. .numeric). default computed columns added preserved ones, unless assigned name case preserved columns replaced order. .keep either one \"\", \"used\", \"unused\" \"none\" (see mutate), columns names/indices/function keep. Note work well across() expressions supported since v1.9.0. sensible option supply character vector columns final dataset want keep. .cols expressions involving .data, .cols can used subset columns, e.g. mtcars |> gby(cyl) |> mtt(broom::augment(lm(mpg ~., .data)), .cols = 1:7). Can pass column names, indices, logical vector selector function (e.g. .numericr).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":"... arguments ftransform tagged   vector expressions, evaluated data frame   .data.  tags matched names(.data),   match, values replace corresponding variable   .data, whereas others appended .data. also possible delete columns assigning NULL , .e. ftransform(data, colk = NULL) removes colk data. Note names(.data) names ... arguments checked uniqueness beforehand, yielding error case. Since collapse v1.3.0, also possible pass single named list ..., .e. ftransform(data, newdata). list treated like list tagged vector expressions. Note different behavior: ftransform(data, list(newcol = col1)) ftransform(data, newcol = col1), whereas ftransform(data, newcol = .list(col1)) creates list column. Something like ftransform(data, .list(col1)) gives error list named. See Examples.   function ftransformv added v1.3.2 provides fast replacement functions dplyr::mutate_at dplyr::mutate_if (without grouping feature) facilitating mutations groups columns (dplyr::mutate_all already accounted dapply). See Examples. function settransform reference, uses base-R's copy-modify semantics, equivalent replacing data <- (thus still memory efficient data different memory address afterwards). function fcompute(v) works just like ftransform(v), returns changed / computed columns without modifying appending data .data. See Examples. function fmutate added v1.7.0, provides functionality familiar dplyr 1.0.0 higher. evaluates tagged vector expressions sequentially operations groups grouped frame (thus slower ftransform many tagged expressions grouped data frame). Note however collapse depend rlang, things like lambda expressions available. Note also fmutate operates differently grouped data whether use .FAST_FUN base R functions / functions packages. .FAST_FUN (including .OPERATOR_FUN, excluding fhdbetween / fhdwithin / HDW / HDB), fmutate performs efficient vectorized execution, .e. grouping object grouped data frame passed g argument functions, .FAST_STAT_FUN also TRA = \"replace_fill\" set (overwritten user), yielding internal grouped computation functions without need splitting data groups. base R functions, fmutate performs classical split-apply combine computing .e. relevant columns data selected split groups, expression evaluated group, result recombined suitably expanded match original data frame. Note possible mix vectorized standard execution expression!! Vectorized execution performed .FAST_FUN .OPERATOR_FUN part expression, thus code like mtcars |> gby(cyl) |> fmutate(new = fmin(mpg) / min(mpg)) expanded something like mtcars |> gby(cyl) |> ftransform(new = fmin(mpg, g = GRP(.), TRA = \"replace_fill\") / min(mpg)) executed, .e. fmin(mpg) executed vectorized way, min(mpg) executed groups .","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":"ftransform ignores grouped data. purpose allows non-grouped transformation inside pipeline grouped data, affords greater flexibility performance programming .FAST_FUN. particular, can run nested pipeline inside ftransform, decide expressions grouped, can use ad-hoc grouping functionality .FAST_FUN, allowing operations different groupings applied simultaneously expression. See Examples answer provided . fmutate hand supports grouped operations just like dplyr::mutate, works two different ways depending whether use .FAST_FUN expression functions. See Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":"modified data frame .data, , fcompute, new data frame columns computed .data. attributes .data preserved.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/ftransform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Transform and Compute Columns on a Data Frame — ftransform","text":"","code":"## fmutate() examples ---------------------------------------------------------------  # Please note that expressions are vectorized whenever they contain 'ANY' fast function mtcars |>   fgroup_by(cyl, vs, am) |>   fmutate(mean_mpg = fmean(mpg),                     # Vectorized           mean_mpg_base = mean(mpg),                 # Non-vectorized           mpg_cumpr = fcumsum(mpg) / fsum(mpg),      # Vectorized           mpg_cumpr_base = cumsum(mpg) / sum(mpg),   # Non-vectorized           mpg_cumpr_mixed = fcumsum(mpg) / sum(mpg)) # Vectorized: division by overall sum #>                 mpg cyl disp  hp drat    wt  qsec vs am gear carb mean_mpg #> Mazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 20.56667 #> Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 20.56667 #> Datsun 710     22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 28.37143 #> Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 19.12500 #>                mean_mpg_base mpg_cumpr mpg_cumpr_base mpg_cumpr_mixed #> Mazda RX4           20.56667 0.3403566      0.3403566      0.03266449 #> Mazda RX4 Wag       20.56667 0.6807131      0.6807131      0.06532898 #> Datsun 710          28.37143 0.1148036      0.1148036      0.03546430 #> Hornet 4 Drive      19.12500 0.2797386      0.2797386      0.03328667 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 28 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]   # Using across: here fmean() gets vectorized across both groups and columns (requiring a single # call to fmean.data.frame which goes to C), whereas weighted.mean needs to be called many times. mtcars |> fgroup_by(cyl, vs, am) |>   fmutate(across(disp:qsec, list(mu = fmean, mu2 = weighted.mean), w = wt, .names = \"flip\")) #>                mpg cyl disp  hp drat    wt  qsec vs am gear carb  mu_disp #> Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 154.9728 #> Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 154.9728 #> Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  92.2352 #>               mu2_disp     mu_hp    mu2_hp  mu_drat mu2_drat    mu_wt   mu2_wt #> Mazda RX4     154.9728 131.78463 131.78463 3.806158 3.806158 2.758975 2.758975 #> Mazda RX4 Wag 154.9728 131.78463 131.78463 3.806158 3.806158 2.758975 2.758975 #> Datsun 710     92.2352  82.11819  82.11819 4.130037 4.130037 2.110131 2.110131 #>                mu_qsec mu2_qsec #> Mazda RX4     16.33306 16.33306 #> Mazda RX4 Wag 16.33306 16.33306 #> Datsun 710    18.75509 18.75509 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 29 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]   # Can do more complex things... mtcars |> fgroup_by(cyl) |>   fmutate(res = resid(lm(mpg ~ carb + hp, weights = wt))) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb       res #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  1.125008 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  1.125008 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 -2.716748 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  1.870410 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2  2.229621 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ] #>  #> Grouped by:  cyl  [3 | 11 (3.5) 7-14]   # Since v1.9.0: supports arbitrary expressions returning suitable lists if (FALSE)   mtcars |> fgroup_by(cyl) |>   fmutate(broom::augment(lm(mpg ~ carb + hp, weights = wt)))  # Same thing using across() (supported before 1.9.0) modelfun <- function(data) broom::augment(lm(mpg ~ carb + hp, data, weights = wt)) mtcars |> fgroup_by(cyl) |>   fmutate(across(c(mpg, carb, hp, wt), modelfun, .apply = FALSE)) #> Error in loadNamespace(x): there is no package called ‘broom’  # \\dontrun{}   ## ftransform() / fcompute() examples: ----------------------------------------------  ## ftransform modifies and returns a data.frame head(ftransform(airquality, Ozone = -Ozone)) #>   Ozone Solar.R Wind Temp Month Day #> 1   -41     190  7.4   67     5   1 #> 2   -36     118  8.0   72     5   2 #> 3   -12     149 12.6   74     5   3 #> 4   -18     313 11.5   62     5   4 #> 5    NA      NA 14.3   56     5   5 #> 6   -28      NA 14.9   66     5   6 head(ftransform(airquality, new = -Ozone, Temp = (Temp-32)/1.8)) #>   Ozone Solar.R Wind     Temp Month Day new #> 1    41     190  7.4 19.44444     5   1 -41 #> 2    36     118  8.0 22.22222     5   2 -36 #> 3    12     149 12.6 23.33333     5   3 -12 #> 4    18     313 11.5 16.66667     5   4 -18 #> 5    NA      NA 14.3 13.33333     5   5  NA #> 6    28      NA 14.9 18.88889     5   6 -28 head(ftransform(airquality, new = -Ozone, new2 = 1, Temp = NULL))  # Deleting Temp #>   Ozone Solar.R Wind Month Day new new2 #> 1    41     190  7.4     5   1 -41    1 #> 2    36     118  8.0     5   2 -36    1 #> 3    12     149 12.6     5   3 -12    1 #> 4    18     313 11.5     5   4 -18    1 #> 5    NA      NA 14.3     5   5  NA    1 #> 6    28      NA 14.9     5   6 -28    1 head(ftransform(airquality, Ozone = NULL, Temp = NULL))            # Deleting columns #>   Solar.R Wind Month Day #> 1     190  7.4     5   1 #> 2     118  8.0     5   2 #> 3     149 12.6     5   3 #> 4     313 11.5     5   4 #> 5      NA 14.3     5   5 #> 6      NA 14.9     5   6  # With collapse's grouped and weighted functions, complex operations are done on the fly head(ftransform(airquality, # Grouped operations by month:                 Ozone_Month_median = fmedian(Ozone, Month, TRA = \"fill\"),                 Ozone_Month_sd = fsd(Ozone, Month, TRA = \"replace\"),                 Ozone_Month_centered = fwithin(Ozone, Month))) #>   Ozone Solar.R Wind Temp Month Day Ozone_Month_median Ozone_Month_sd #> 1    41     190  7.4   67     5   1                 18       22.22445 #> 2    36     118  8.0   72     5   2                 18       22.22445 #> 3    12     149 12.6   74     5   3                 18       22.22445 #> 4    18     313 11.5   62     5   4                 18       22.22445 #> 5    NA      NA 14.3   56     5   5                 18             NA #> 6    28      NA 14.9   66     5   6                 18       22.22445 #>   Ozone_Month_centered #> 1            17.384615 #> 2            12.384615 #> 3           -11.615385 #> 4            -5.615385 #> 5                   NA #> 6             4.384615  # Grouping by month and above/below average temperature in each month head(ftransform(airquality, Ozone_Month_high_median =                   fmedian(Ozone, list(Month, Temp > fbetween(Temp, Month)), TRA = \"fill\"))) #>   Ozone Solar.R Wind Temp Month Day Ozone_Month_high_median #> 1    41     190  7.4   67     5   1                      28 #> 2    36     118  8.0   72     5   2                      28 #> 3    12     149 12.6   74     5   3                      28 #> 4    18     313 11.5   62     5   4                      14 #> 5    NA      NA 14.3   56     5   5                      14 #> 6    28      NA 14.9   66     5   6                      28  ## ftransformv can be used to modify multiple columns using a function head(ftransformv(airquality, 1:3, log)) #>      Ozone  Solar.R     Wind Temp Month Day #> 1 3.713572 5.247024 2.001480   67     5   1 #> 2 3.583519 4.770685 2.079442   72     5   2 #> 3 2.484907 5.003946 2.533697   74     5   3 #> 4 2.890372 5.746203 2.442347   62     5   4 #> 5       NA       NA 2.660260   56     5   5 #> 6 3.332205       NA 2.701361   66     5   6 head(`[<-`(airquality, 1:3, value = lapply(airquality[1:3], log))) # Same thing in base R #>      Ozone  Solar.R     Wind Temp Month Day #> 1 3.713572 5.247024 2.001480   67     5   1 #> 2 3.583519 4.770685 2.079442   72     5   2 #> 3 2.484907 5.003946 2.533697   74     5   3 #> 4 2.890372 5.746203 2.442347   62     5   4 #> 5       NA       NA 2.660260   56     5   5 #> 6 3.332205       NA 2.701361   66     5   6  head(ftransformv(airquality, 1:3, log, apply = FALSE)) #>      Ozone  Solar.R     Wind Temp Month Day #> 1 3.713572 5.247024 2.001480   67     5   1 #> 2 3.583519 4.770685 2.079442   72     5   2 #> 3 2.484907 5.003946 2.533697   74     5   3 #> 4 2.890372 5.746203 2.442347   62     5   4 #> 5       NA       NA 2.660260   56     5   5 #> 6 3.332205       NA 2.701361   66     5   6 head(`[<-`(airquality, 1:3, value = log(airquality[1:3])))         # Same thing in base R #>      Ozone  Solar.R     Wind Temp Month Day #> 1 3.713572 5.247024 2.001480   67     5   1 #> 2 3.583519 4.770685 2.079442   72     5   2 #> 3 2.484907 5.003946 2.533697   74     5   3 #> 4 2.890372 5.746203 2.442347   62     5   4 #> 5       NA       NA 2.660260   56     5   5 #> 6 3.332205       NA 2.701361   66     5   6  # Using apply = FALSE yields meaningful performance gains with collapse functions # This calls fwithin.default, and repeates the grouping by month 3 times: head(ftransformv(airquality, 1:3, fwithin, Month)) #>        Ozone    Solar.R       Wind Temp Month Day #> 1  17.384615   8.703704 -4.2225806   67     5   1 #> 2  12.384615 -63.296296 -3.6225806   72     5   2 #> 3 -11.615385 -32.296296  0.9774194   74     5   3 #> 4  -5.615385 131.703704 -0.1225806   62     5   4 #> 5         NA         NA  2.6774194   56     5   5 #> 6   4.384615         NA  3.2774194   66     5   6  # This calls fwithin.data.frame, and only groups one time -> 5x faster! head(ftransformv(airquality, 1:3, fwithin, Month, apply = FALSE)) #>        Ozone    Solar.R       Wind Temp Month Day #> 1  17.384615   8.703704 -4.2225806   67     5   1 #> 2  12.384615 -63.296296 -3.6225806   72     5   2 #> 3 -11.615385 -32.296296  0.9774194   74     5   3 #> 4  -5.615385 131.703704 -0.1225806   62     5   4 #> 5         NA         NA  2.6774194   56     5   5 #> 6   4.384615         NA  3.2774194   66     5   6  # This also works for grouped and panel data frames (calling fwithin.grouped_df) airquality |> fgroup_by(Month) |>   ftransformv(1:3, fwithin, apply = FALSE) |> head() #>        Ozone    Solar.R       Wind Temp Month Day #> 1  17.384615   8.703704 -4.2225806   67     5   1 #> 2  12.384615 -63.296296 -3.6225806   72     5   2 #> 3 -11.615385 -32.296296  0.9774194   74     5   3 #> 4  -5.615385 131.703704 -0.1225806   62     5   4 #> 5         NA         NA  2.6774194   56     5   5 #> 6   4.384615         NA  3.2774194   66     5   6  # But this gives the WRONG result (calling fwithin.default). Need option apply = FALSE!! airquality |> fgroup_by(Month) |>   ftransformv(1:3, fwithin) |> head() #>       Ozone    Solar.R      Wind Temp Month Day #> 1  -1.12931   4.068493 -2.557516   67     5   1 #> 2  -6.12931 -67.931507 -1.957516   72     5   2 #> 3 -30.12931 -36.931507  2.642484   74     5   3 #> 4 -24.12931 127.068493  1.542484   62     5   4 #> 5        NA         NA  4.342484   56     5   5 #> 6 -14.12931         NA  4.942484   66     5   6  # For grouped modification of single columns in a grouped dataset, we can use GRP(): library(magrittr) airquality |> fgroup_by(Month) %>%   ftransform(W_Ozone = fwithin(Ozone, GRP(.)),                 # Grouped centering              sd_Ozone_m = fsd(Ozone, GRP(.), TRA = \"replace\"), # In-Month standard deviation              sd_Ozone = fsd(Ozone, TRA = \"replace\"),           # Overall standard deviation              sd_Ozone2 = fsd(Ozone, TRA = \"fill\"),             # Same, overwriting NA's              sd_Ozone3 = fsd(Ozone)) |> head()                 # Same thing (calling alloc()) #>   Ozone Solar.R Wind Temp Month Day    W_Ozone sd_Ozone_m sd_Ozone sd_Ozone2 #> 1    41     190  7.4   67     5   1  17.384615   22.22445 32.98788  32.98788 #> 2    36     118  8.0   72     5   2  12.384615   22.22445 32.98788  32.98788 #> 3    12     149 12.6   74     5   3 -11.615385   22.22445 32.98788  32.98788 #> 4    18     313 11.5   62     5   4  -5.615385   22.22445 32.98788  32.98788 #> 5    NA      NA 14.3   56     5   5         NA         NA       NA  32.98788 #> 6    28      NA 14.9   66     5   6   4.384615   22.22445 32.98788  32.98788 #>   sd_Ozone3 #> 1  32.98788 #> 2  32.98788 #> 3  32.98788 #> 4  32.98788 #> 5  32.98788 #> 6  32.98788  ## For more complex mutations we can use ftransform with compound pipes airquality |> fgroup_by(Month) %>%   ftransform(get_vars(., 1:3) |> fwithin() |> flag(0:2)) |> head() #>        Ozone    Solar.R       Wind Temp Month Day   L1.Ozone  L2.Ozone #> 1  17.384615   8.703704 -4.2225806   67     5   1         NA        NA #> 2  12.384615 -63.296296 -3.6225806   72     5   2  17.384615        NA #> 3 -11.615385 -32.296296  0.9774194   74     5   3  12.384615  17.38462 #> 4  -5.615385 131.703704 -0.1225806   62     5   4 -11.615385  12.38462 #> 5         NA         NA  2.6774194   56     5   5  -5.615385 -11.61538 #>   L1.Solar.R L2.Solar.R    L1.Wind    L2.Wind #> 1         NA         NA         NA         NA #> 2   8.703704         NA -4.2225806         NA #> 3 -63.296296   8.703704 -3.6225806 -4.2225806 #> 4 -32.296296 -63.296296  0.9774194 -3.6225806 #> 5 131.703704 -32.296296 -0.1225806  0.9774194 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  airquality %>% ftransform(STD(., cols = 1:3) |> replace_na(0)) |> head() #>   Ozone Solar.R Wind Temp Month Day   STD.Ozone STD.Solar.R   STD.Wind #> 1    41     190  7.4   67     5   1 -0.03423409  0.04517615 -0.7259482 #> 2    36     118  8.0   72     5   2 -0.18580489 -0.75430487 -0.5556388 #> 3    12     149 12.6   74     5   3 -0.91334473 -0.41008388  0.7500660 #> 4    18     313 11.5   62     5   4 -0.73145977  1.41095624  0.4378323 #> 5    NA      NA 14.3   56     5   5  0.00000000  0.00000000  1.2326091 #> 6    28      NA 14.9   66     5   6 -0.42831817  0.00000000  1.4029185  # The list argument feature also allows flexible operations creating multiple new columns airquality |> # The variance of Wind and Ozone, by month, weighted by temperature:   ftransform(fvar(list(Wind_var = Wind, Ozone_var = Ozone), Month, Temp, \"replace\")) |> head() #>   Ozone Solar.R Wind Temp Month Day Wind_var Ozone_var #> 1    41     190  7.4   67     5   1 12.08975  533.2819 #> 2    36     118  8.0   72     5   2 12.08975  533.2819 #> 3    12     149 12.6   74     5   3 12.08975  533.2819 #> 4    18     313 11.5   62     5   4 12.08975  533.2819 #> 5    NA      NA 14.3   56     5   5 12.08975        NA #> 6    28      NA 14.9   66     5   6 12.08975  533.2819  # Same as above using a grouped data frame (a bit more complex) airquality |> fgroup_by(Month) %>%   ftransform(fselect(., Wind, Ozone) |> fvar(Temp, \"replace\") |> add_stub(\"_var\", FALSE)) |>   fungroup() |> head() #>   Ozone Solar.R Wind Temp Month Day Wind_var Ozone_var #> 1    41     190  7.4   67     5   1 12.08975  533.2819 #> 2    36     118  8.0   72     5   2 12.08975  533.2819 #> 3    12     149 12.6   74     5   3 12.08975  533.2819 #> 4    18     313 11.5   62     5   4 12.08975  533.2819 #> 5    NA      NA 14.3   56     5   5 12.08975        NA #> 6    28      NA 14.9   66     5   6 12.08975  533.2819  # This performs 2 different multi-column grouped operations (need c() to make it one list) ftransform(airquality, c(fmedian(list(Wind_Day_median = Wind,                                       Ozone_Day_median = Ozone), Day, TRA = \"replace\"),                          fsd(list(Wind_Month_sd = Wind,                                   Ozone_Month_sd = Ozone), Month, TRA = \"replace\"))) |> head() #>   Ozone Solar.R Wind Temp Month Day Wind_Day_median Ozone_Day_median #> 1    41     190  7.4   67     5   1             6.9             68.5 #> 2    36     118  8.0   72     5   2             9.2             42.5 #> 3    12     149 12.6   74     5   3             9.2             24.0 #> 4    18     313 11.5   62     5   4             9.2             78.0 #> 5    NA      NA 14.3   56     5   5             7.4               NA #> 6    28      NA 14.9   66     5   6            14.3             36.0 #>   Wind_Month_sd Ozone_Month_sd #> 1       3.53145       22.22445 #> 2       3.53145       22.22445 #> 3       3.53145       22.22445 #> 4       3.53145       22.22445 #> 5       3.53145             NA #> 6       3.53145       22.22445  ## settransform(v) works like ftransform(v) but modifies a data frame in the global environment.. settransform(airquality, Ratio = Ozone / Temp, Ozone = NULL, Temp = NULL) head(airquality) #>   Solar.R Wind Month Day     Ratio #> 1     190  7.4     5   1 0.6119403 #> 2     118  8.0     5   2 0.5000000 #> 3     149 12.6     5   3 0.1621622 #> 4     313 11.5     5   4 0.2903226 #> 5      NA 14.3     5   5        NA #> 6      NA 14.9     5   6 0.4242424 rm(airquality)  # Grouped and weighted centering settransformv(airquality, 1:3, fwithin, Month, Temp, apply = FALSE) head(airquality) #>       Ozone    Solar.R        Wind Temp Month Day #> 1  16.22536   3.571669 -4.08917323   67     5   1 #> 2  11.22536 -68.428331 -3.48917323   72     5   2 #> 3 -12.77464 -37.428331  1.11082677   74     5   3 #> 4  -6.77464 126.571669  0.01082677   62     5   4 #> 5        NA         NA  2.81082677   56     5   5 #> 6   3.22536         NA  3.41082677   66     5   6 rm(airquality)  # Suitably lagged first-differences settransform(airquality, get_vars(airquality, 1:3) |> fdiff() |> flag(0:2)) head(airquality) #>   Ozone Solar.R Wind Temp Month Day L1.Ozone L2.Ozone L1.Solar.R L2.Solar.R #> 1    NA      NA   NA   67     5   1       NA       NA         NA         NA #> 2    -5     -72  0.6   72     5   2       NA       NA         NA         NA #> 3   -24      31  4.6   74     5   3       -5       NA        -72         NA #> 4     6     164 -1.1   62     5   4      -24       -5         31        -72 #> 5    NA      NA  2.8   56     5   5        6      -24        164         31 #>   L1.Wind L2.Wind #> 1      NA      NA #> 2      NA      NA #> 3     0.6      NA #> 4     4.6     0.6 #> 5    -1.1     4.6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] rm(airquality)  # Same as above using magrittr::`%<>%` airquality %<>% ftransform(get_vars(., 1:3) |> fdiff() |> flag(0:2)) head(airquality) #>   Ozone Solar.R Wind Temp Month Day L1.Ozone L2.Ozone L1.Solar.R L2.Solar.R #> 1    NA      NA   NA   67     5   1       NA       NA         NA         NA #> 2    -5     -72  0.6   72     5   2       NA       NA         NA         NA #> 3   -24      31  4.6   74     5   3       -5       NA        -72         NA #> 4     6     164 -1.1   62     5   4      -24       -5         31        -72 #> 5    NA      NA  2.8   56     5   5        6      -24        164         31 #>   L1.Wind L2.Wind #> 1      NA      NA #> 2      NA      NA #> 3     0.6      NA #> 4     4.6     0.6 #> 5    -1.1     4.6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] rm(airquality)  # It is also possible to achieve the same thing via a replacement method (if needed) ftransform(airquality) <- get_vars(airquality, 1:3) |> fdiff() |> flag(0:2) head(airquality) #>   Ozone Solar.R Wind Temp Month Day L1.Ozone L2.Ozone L1.Solar.R L2.Solar.R #> 1    NA      NA   NA   67     5   1       NA       NA         NA         NA #> 2    -5     -72  0.6   72     5   2       NA       NA         NA         NA #> 3   -24      31  4.6   74     5   3       -5       NA        -72         NA #> 4     6     164 -1.1   62     5   4      -24       -5         31        -72 #> 5    NA      NA  2.8   56     5   5        6      -24        164         31 #>   L1.Wind L2.Wind #> 1      NA      NA #> 2      NA      NA #> 3     0.6      NA #> 4     4.6     0.6 #> 5    -1.1     4.6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] rm(airquality)  ## fcompute only returns the modified / computed columns head(fcompute(airquality, Ozone = -Ozone)) #>   Ozone #> 1   -41 #> 2   -36 #> 3   -12 #> 4   -18 #> 5    NA #> 6   -28 head(fcompute(airquality, new = -Ozone, Temp = (Temp-32)/1.8)) #>   new     Temp #> 1 -41 19.44444 #> 2 -36 22.22222 #> 3 -12 23.33333 #> 4 -18 16.66667 #> 5  NA 13.33333 #> 6 -28 18.88889 head(fcompute(airquality, new = -Ozone, new2 = 1)) #>   new new2 #> 1 -41    1 #> 2 -36    1 #> 3 -12    1 #> 4 -18    1 #> 5  NA    1 #> 6 -28    1  # Can preserve existing columns, computed ones are added to the right if names are different head(fcompute(airquality, new = -Ozone, new2 = 1, keep = 1:3)) #>   Ozone Solar.R Wind new new2 #> 1    41     190  7.4 -41    1 #> 2    36     118  8.0 -36    1 #> 3    12     149 12.6 -12    1 #> 4    18     313 11.5 -18    1 #> 5    NA      NA 14.3  NA    1 #> 6    28      NA 14.9 -28    1  # If given same name as preserved columns, preserved columns are replaced in order... head(fcompute(airquality, Ozone = -Ozone, new = 1, keep = 1:3)) #>   Ozone Solar.R Wind new #> 1   -41     190  7.4   1 #> 2   -36     118  8.0   1 #> 3   -12     149 12.6   1 #> 4   -18     313 11.5   1 #> 5    NA      NA 14.3   1 #> 6   -28      NA 14.9   1  # Same holds for fcomputev head(fcomputev(iris, is.numeric, log)) # Same as: #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     1.629241    1.252763    0.3364722  -1.6094379 #> 2     1.589235    1.098612    0.3364722  -1.6094379 #> 3     1.547563    1.163151    0.2623643  -1.6094379 #> 4     1.526056    1.131402    0.4054651  -1.6094379 #> 5     1.609438    1.280934    0.3364722  -1.6094379 #> 6     1.686399    1.360977    0.5306283  -0.9162907 iris |> get_vars(is.numeric) |> dapply(log) |> head() #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     1.629241    1.252763    0.3364722  -1.6094379 #> 2     1.589235    1.098612    0.3364722  -1.6094379 #> 3     1.547563    1.163151    0.2623643  -1.6094379 #> 4     1.526056    1.131402    0.4054651  -1.6094379 #> 5     1.609438    1.280934    0.3364722  -1.6094379 #> 6     1.686399    1.360977    0.5306283  -0.9162907  head(fcomputev(iris, is.numeric, log, keep = \"Species\"))   # Adds in front #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa     1.629241    1.252763    0.3364722  -1.6094379 #> 2  setosa     1.589235    1.098612    0.3364722  -1.6094379 #> 3  setosa     1.547563    1.163151    0.2623643  -1.6094379 #> 4  setosa     1.526056    1.131402    0.4054651  -1.6094379 #> 5  setosa     1.609438    1.280934    0.3364722  -1.6094379 #> 6  setosa     1.686399    1.360977    0.5306283  -0.9162907 head(fcomputev(iris, is.numeric, log, keep = names(iris))) # Preserve order #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1     1.629241    1.252763    0.3364722  -1.6094379  setosa #> 2     1.589235    1.098612    0.3364722  -1.6094379  setosa #> 3     1.547563    1.163151    0.2623643  -1.6094379  setosa #> 4     1.526056    1.131402    0.4054651  -1.6094379  setosa #> 5     1.609438    1.280934    0.3364722  -1.6094379  setosa #> 6     1.686399    1.360977    0.5306283  -0.9162907  setosa  # Keep a subset of the data, add standardized columns head(fcomputev(iris, 3:4, STD, apply = FALSE, keep = names(iris)[3:5])) #>   Petal.Length Petal.Width Species STD.Petal.Length STD.Petal.Width #> 1          1.4         0.2  setosa        -1.335752       -1.311052 #> 2          1.4         0.2  setosa        -1.335752       -1.311052 #> 3          1.3         0.2  setosa        -1.392399       -1.311052 #> 4          1.5         0.2  setosa        -1.279104       -1.311052 #> 5          1.4         0.2  setosa        -1.335752       -1.311052 #> 6          1.7         0.4  setosa        -1.165809       -1.048667"},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Unique Elements / Rows — funique","title":"Fast Unique Elements / Rows — funique","text":"funique efficient alternative unique (unique.data.table, kit::funique, dplyr::distinct). fnunique alternative NROW(unique(x)) (data.table::uniqueN, kit::uniqLen, dplyr::n_distinct). fduplicated alternative duplicated (duplicated.data.table, kit::fduplicated). collapse versions versatile highly competitive. any_duplicated(x) faster (fduplicated(x)). Note atomic vectors, anyDuplicated currently efficient duplicates beginning vector.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Unique Elements / Rows — funique","text":"","code":"funique(x, ...)  # Default S3 method funique(x, sort = FALSE, method = \"auto\", ...)  # S3 method for class 'data.frame' funique(x, cols = NULL, sort = FALSE, method = \"auto\", ...)  # S3 method for class 'sf' funique(x, cols = NULL, sort = FALSE, method = \"auto\", ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' funique(x, sort = FALSE, method = \"auto\", drop.index.levels = \"id\", ...)  # S3 method for class 'pdata.frame' funique(x, cols = NULL, sort = FALSE, method = \"auto\", drop.index.levels = \"id\", ...)   fnunique(x)                  # Fast NROW(unique(x)), for vectors and lists fduplicated(x, all = FALSE)  # Fast duplicated(x), for vectors and lists any_duplicated(x)            # Simple logical TRUE|FALSE duplicates check"},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Unique Elements / Rows — funique","text":"x atomic vector data frame / list equal-length columns. sort logical. TRUE orders unique elements / rows. FALSE returns unique values order first occurrence. method integer character string specifying method computation:  cols compute unique rows according subset columns. Columns can selected using column names, indices, logical vector selector function (e.g. .character). Note: columns returned. ... arguments passed radixorder, e.g. decreasing na.last. applicable method = \"radix\". drop.index.levels character. Either \"id\", \"time\", \"\" \"none\". See indexing. logical. TRUE returns duplicated values, including first occurrence.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Unique Elements / Rows — funique","text":"values/rows already unique, x returned. Otherwise copy x duplicate rows removed returned.  See group additional computational details. sf method simply ignores geometry column determining unique values. Methods indexed data also subset index accordingly. any_duplicated currently simply implemented fnunique(x) < NROW(x), means facilities terminate early, users advised use anyDuplicated atomic vectors chances high duplicates beginning vector. duplicate values data frames, any_duplicated considerably faster anyDuplicated.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Unique Elements / Rows — funique","text":"functions treat lists like data frames, unlike unique list method determine uniqueness (non-atomic/heterogeneous) elements list. matrix method provided. Please use alternatives provided package kit matrices.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Unique Elements / Rows — funique","text":"funique returns x duplicate elements/rows removed, fnunique returns integer giving number unique values/rows, fduplicated gives logical vector TRUE indicating duplicated elements/rows.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/funique.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Unique Elements / Rows — funique","text":"","code":"funique(mtcars$cyl) #> [1] 6 4 8 funique(gv(mtcars, c(2,8,9))) #>                   cyl vs am #> Mazda RX4           6  0  1 #> Datsun 710          4  1  1 #> Hornet 4 Drive      6  1  0 #> Hornet Sportabout   8  0  0 #> Merc 240D           4  1  0 #> Porsche 914-2       4  0  1 #> Ford Pantera L      8  0  1 funique(mtcars, cols = c(2,8,9)) #>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Datsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 #> Porsche 914-2     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] fnunique(gv(mtcars, c(2,8,9))) #> [1] 7 fduplicated(gv(mtcars, c(2,8,9))) #>  [1] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE #> [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE #> [25]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE fduplicated(gv(mtcars, c(2,8,9)), all = TRUE) #>  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE #> [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE #> [25]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE any_duplicated(gv(mtcars, c(2,8,9))) #> [1] TRUE any_duplicated(mtcars) #> [1] FALSE"},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"fvar fsd generic functions compute (column-wise) variance standard deviation x, (optionally) grouped g /frequency-weighted w. TRA argument can used transform x using (grouped, weighted) variance/sd.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"","code":"fvar(x, ...) fsd(x, ...)  # Default S3 method fvar(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, stable.algo = .op[[\"stable.algo\"]], ...) # Default S3 method fsd(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],     use.g.names = TRUE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'matrix' fvar(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, stable.algo = .op[[\"stable.algo\"]], ...) # S3 method for class 'matrix' fsd(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],     use.g.names = TRUE, drop = TRUE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'data.frame' fvar(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = TRUE, drop = TRUE, stable.algo = .op[[\"stable.algo\"]], ...) # S3 method for class 'data.frame' fsd(x, g = NULL, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],     use.g.names = TRUE, drop = TRUE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'grouped_df' fvar(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],      use.g.names = FALSE, keep.group_vars = TRUE, keep.w = TRUE,      stub = .op[[\"stub\"]], stable.algo = .op[[\"stable.algo\"]], ...) # S3 method for class 'grouped_df' fsd(x, w = NULL, TRA = NULL, na.rm = .op[[\"na.rm\"]],     use.g.names = FALSE, keep.group_vars = TRUE, keep.w = TRUE,     stub = .op[[\"stub\"]], stable.algo = .op[[\"stable.algo\"]], ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"x numeric vector, matrix, data frame grouped data frame (class 'grouped_df'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. w numeric vector (non-negative) weights, may contain missing values. TRA integer quoted operator indicating transformation perform: 0 - \"na\"     |     1 - \"fill\"     |     2 - \"replace\"     |     3 - \"-\"     |     4 - \"-+\"     |     5 - \"/\"     |     6 - \"%\"     |     7 - \"+\"     |     8 - \"*\"     |     9 - \"%%\"     |     10 - \"-%%\". See TRA. na.rm logical. Skip missing values x. Defaults TRUE implemented little computational cost. na.rm = FALSE NA returned encountered. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. drop matrix data.frame method: Logical. TRUE drops dimensions returns atomic vector g = NULL TRA = NULL. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. keep.w grouped_df method: Logical. Retain summed weighting variable computation (contained grouped_df). stub character. keep.w = TRUE stub = TRUE (default), summed weights column prefixed \"sum.\". Users can specify different prefix argument, set FALSE avoid prefixing. stable.algo logical. TRUE (default) use Welford's numerically stable online algorithm. FALSE implements faster numerically unstable one-pass method. See Details. ... arguments passed methods. TRA used, passing set = TRUE transform data reference return result invisibly.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"Welford's online algorithm used default compute variance well described (section Weighted incremental algorithm also shows weighted variance obtained algorithm). stable.algo = FALSE, variance computed one-pass (sum(x^2)-n*mean(x)^2)/(n-1), sum(x^2) sum squares expected sum squares n*mean(x)^2 subtracted, normalized n-1 (Bessel's correction). numerically unstable sum(x^2) n*mean(x)^2 large numbers close together, case large n, large x-values small variances (catastrophic cancellation occurs, leading loss numeric precision). Numeric precision however still maximized internal use long doubles C++, fast algorithm can 4-times faster compared Welford's method. weighted variance computed frequency weights (sum(x^2*w)-sum(w)*weighted.mean(x,w)^2)/(sum(w)-1). na.rm = TRUE, missing values removed x w .e. utilizing x[complete.cases(x,w)] w[complete.cases(x,w)]. computational detail see fsum.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"fvar returns (w weighted) variance x, grouped g, (TRA used) x transformed (grouped, weighted) variance. fsd computes standard deviation x like manor.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"Welford, B. P. (1962). Note method calculating corrected sums squares products. Technometrics. 4 (3): 419-420. doi:10.2307/1266577.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/fvar_fsd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Variance and Standard Deviation for Matrix-Like Objects — fvar-fsd","text":"","code":"## default vector method fvar(mtcars$mpg)                            # Simple variance (all examples also hold for fvar!) #> [1] 36.3241 fsd(mtcars$mpg)                             # Simple standard deviation #> [1] 6.026948 fsd(mtcars$mpg, w = mtcars$hp)              # Weighted sd: Weighted by hp #> [1] 5.150858 fsd(mtcars$mpg, TRA = \"/\")                  # Simple transformation: scaling (See also ?fscale) #>  [1] 3.484351 3.484351 3.783009 3.550719 3.102731 3.003178 2.372677 4.048484 #>  [9] 3.783009 3.185692 2.953402 2.721112 2.870441 2.522006 1.725583 1.725583 #> [17] 2.439045 5.375855 5.044012 5.624737 3.567311 2.571783 2.522006 2.206755 #> [25] 3.185692 4.529656 4.313958 5.044012 2.621559 3.268653 2.488822 3.550719 fsd(mtcars$mpg, mtcars$cyl)                 # Grouped sd #>        4        6        8  #> 4.509828 1.453567 2.560048  fsd(mtcars$mpg, mtcars$cyl, mtcars$hp)      # Grouped weighted sd #>        4        6        8  #> 4.250863 1.294689 2.390448  fsd(mtcars$mpg, mtcars$cyl, TRA = \"/\")      # Scaling by group #>  [1] 14.447218 14.447218  5.055626 14.722403  7.304550 12.452126  5.585833 #>  [8]  5.410406  5.055626 13.208885 12.245737  6.406130  6.757686  5.937388 #> [15]  4.062424  4.062424  5.742080  7.184310  6.740834  7.516917  4.767366 #> [22]  6.054574  5.937388  5.195215  7.499859  6.053446  5.765187  6.740834 #> [29]  6.171759 13.552866  5.859265  4.745192 fsd(mtcars$mpg, mtcars$cyl, mtcars$hp, \"/\") # Group-scaling using weighted group sds #>  [1] 16.220111 16.220111  5.363617 16.529066  7.822800 13.980191  5.982141 #>  [8]  5.740011  5.363617 14.829816 13.748475  6.860638  7.237136  6.358640 #> [15]  4.350648  4.350648  6.149474  7.621982  7.151489  7.974852  5.057797 #> [22]  6.484139  6.358640  5.563810  8.031966  6.422226  6.116405  7.151489 #> [29]  6.609639 15.216009  6.274973  5.034272  ## data.frame method fsd(iris)                           # This works, although 'Species' is a factor variable #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species  #>    0.8280661    0.4358663    1.7652982    0.7622377    0.8192319  fsd(mtcars, drop = FALSE)           # This works, all columns are numeric variables #>        mpg      cyl     disp       hp      drat        wt     qsec        vs #> 1 6.026948 1.785922 123.9387 68.56287 0.5346787 0.9784574 1.786943 0.5040161 #>          am      gear   carb #> 1 0.4989909 0.7378041 1.6152 fsd(iris[-5], iris[5])              # By Species: iris[5] is still a list, and thus passed to GRP() #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa        0.3524897   0.3790644    0.1736640   0.1053856 #> versicolor    0.5161711   0.3137983    0.4699110   0.1977527 #> virginica     0.6358796   0.3224966    0.5518947   0.2746501 fsd(iris[-5], iris[[5]])            # Same thing much faster: fsd recognizes 'Species' is a factor #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa        0.3524897   0.3790644    0.1736640   0.1053856 #> versicolor    0.5161711   0.3137983    0.4699110   0.1977527 #> virginica     0.6358796   0.3224966    0.5518947   0.2746501 head(fsd(iris[-5], iris[[5]], TRA = \"/\")) # Data scaled by species (see also fscale) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     14.46851    9.233260     8.061544    1.897793 #> 2     13.90112    7.914223     8.061544    1.897793 #> 3     13.33372    8.441838     7.485720    1.897793 #> 4     13.05003    8.178031     8.637369    1.897793 #> 5     14.18481    9.497068     8.061544    1.897793 #> 6     15.31960   10.288490     9.789018    3.795585  ## matrix method m <- qM(mtcars) fsd(m) #>         mpg         cyl        disp          hp        drat          wt  #>   6.0269481   1.7859216 123.9386938  68.5628685   0.5346787   0.9784574  #>        qsec          vs          am        gear        carb  #>   1.7869432   0.5040161   0.4989909   0.7378041   1.6152000  fsd(m, mtcars$cyl) # etc.. #>        mpg cyl     disp       hp      drat        wt     qsec        vs #> 4 4.509828   0 26.87159 20.93453 0.3654711 0.5695637 1.682445 0.3015113 #> 6 1.453567   0 41.56246 24.26049 0.4760552 0.3563455 1.706866 0.5345225 #> 8 2.560048   0 67.77132 50.97689 0.3723618 0.7594047 1.196014 0.0000000 #>          am      gear     carb #> 4 0.4670994 0.5393599 0.522233 #> 6 0.5345225 0.6900656 1.812654 #> 8 0.3631365 0.7262730 1.556624  ## method for grouped data frames - created with dplyr::group_by or fgroup_by mtcars |> fgroup_by(cyl,vs,am) |> fsd() #>   cyl vs am       mpg      disp       hp      drat        wt      qsec #> 1   4  0  1        NA        NA       NA        NA        NA        NA #> 2   4  1  0 1.4525839 13.969371 19.65536 0.1300000 0.4075230 1.6714365 #> 3   4  1  1 4.7577005 18.802128 24.14441 0.3783926 0.4400840 0.9454628 #> 4   6  0  1 0.7505553  8.660254 37.52777 0.1616581 0.1281601 0.7687219 #> 5   6  1  0 1.6317169 44.742634  9.17878 0.5919459 0.1162164 0.8159044 #> 6   8  0  0 2.7743959 71.823494 33.35984 0.2302749 0.7683069 0.8016475 #>        gear      carb #> 1        NA        NA #> 2 0.5773503 0.5773503 #> 3 0.3779645 0.5345225 #> 4 0.5773503 1.1547005 #> 5 0.5773503 1.7320508 #> 6 0.0000000 0.9003366 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fsd(keep.group_vars = FALSE) # Remove grouping columns #>         mpg      disp       hp      drat        wt       qsec      gear #> 1        NA        NA       NA        NA        NA         NA        NA #> 2 1.4525839 13.969371 19.65536 0.1300000 0.4075230 1.67143651 0.5773503 #> 3 4.7577005 18.802128 24.14441 0.3783926 0.4400840 0.94546285 0.3779645 #> 4 0.7505553  8.660254 37.52777 0.1616581 0.1281601 0.76872188 0.5773503 #> 5 1.6317169 44.742634  9.17878 0.5919459 0.1162164 0.81590441 0.5773503 #> 6 2.7743959 71.823494 33.35984 0.2302749 0.7683069 0.80164745 0.0000000 #> 7 0.5656854 35.355339 50.20458 0.4808326 0.2828427 0.07071068 0.0000000 #>        carb #> 1        NA #> 2 0.5773503 #> 3 0.5345225 #> 4 1.1547005 #> 5 1.7320508 #> 6 0.9003366 #> 7 2.8284271 mtcars |> fgroup_by(cyl,vs,am) |> fsd(hp)      # Weighted by hp #>   cyl vs am sum.hp       mpg      disp      drat         wt      qsec      gear #> 1   4  0  1     91 0.0000000  0.000000 0.0000000 0.00000000 0.0000000 0.0000000 #> 2   4  1  0    254 1.1242936 11.439070 0.1086204 0.34150141 1.4030344 0.4868090 #> 3   4  1  1    564 4.5643045 17.861603 0.3117619 0.44698787 0.9335417 0.4006210 #> 4   6  0  1    395 0.6465871  7.460621 0.1392649 0.09592878 0.6512685 0.4973747 #> 5   6  1  0    461 1.3956451 38.774265 0.5094278 0.09888402 0.7006934 0.4994102 #> 6   8  0  0   2330 2.6621685 68.845963 0.2331010 0.75554498 0.8299946 0.0000000 #>        carb #> 1 0.0000000 #> 2 0.4868090 #> 3 0.5002424 #> 4 0.9947494 #> 5 1.4982306 #> 6 0.8510728 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] mtcars |> fgroup_by(cyl,vs,am) |> fsd(hp, \"/\") # Weighted scaling transformation #>                   cyl vs am  hp       mpg      disp      drat        wt #> Mazda RX4           6  0  1 110 32.478221 21.445937 28.004181 27.311929 #> Mazda RX4 Wag       6  0  1 110 32.478221 21.445937 28.004181 29.970151 #> Datsun 710          4  1  1  93  4.995285  6.046490 12.349167  5.190297 #> Hornet 4 Drive      6  1  0 110 15.333411  6.653898  6.045999 32.512836 #> Hornet Sportabout   8  0  0 175  7.024349  5.229065 13.513454  4.553005 #> Valiant             6  1  0 105 12.968913  5.802818  5.417844 34.990486 #>                       qsec     gear     carb #> Mazda RX4         25.27376 8.042226 4.021113 #> Mazda RX4 Wag     26.13362 8.042226 4.021113 #> Datsun 710        19.93483 9.984498 1.999031 #> Hornet 4 Drive    27.74395 6.007086 0.667454 #> Hornet Sportabout 20.50616      Inf 2.349975 #> Valiant           28.85713 6.007086 0.667454 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] #>  #> Grouped by:  cyl, vs, am  [7 | 5 (3.8) 1-12]"},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Hash-Based Grouping — group","title":"Fast Hash-Based Grouping — group","text":"group() scans rows data frame (atomic vector / list atomic vectors), assigning unique row integer id - starting 1 proceeding first-appearance order rows. function written C optimized R's data structures. workhorse behind functions like GRP / fgroup_by, collap, qF, qG, finteraction funique, called argument sort = FALSE.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Hash-Based Grouping — group","text":"","code":"group(..., starts = FALSE, group.sizes = FALSE)  groupv(x, starts = FALSE, group.sizes = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Hash-Based Grouping — group","text":"... comma separated atomic vectors group. Also supports single list vectors backward compatibility. x atomic vector data frame / list equal-length atomic vectors. starts logical. TRUE, additional attribute \"starts\" attached giving vector group starts (= index first-occurrence unique rows). group.sizes logical. TRUE, additional attribute \"group.sizes\" attached giving size group.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Hash-Based Grouping — group","text":"data frame grouped column--column basis, starting leftmost column. new column grouping vector obtained previous column also fed back hash function unique values determined running basis. algorithm terminates soon number unique rows reaches size data frame. Missing values also grouped just like values. Invoking arguments starts /group.sizes requires additional pass final grouping vector.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Hash-Based Grouping — group","text":"object class 'qG' see qG.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast Hash-Based Grouping — group","text":"Hash Function inspiration taken excellent kit package Morgan Jacob, algorithm developed Sebastian Krantz.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Hash-Based Grouping — group","text":"","code":"# Let's replicate what funique does g <- groupv(wlddev, starts = TRUE) if(attr(g, \"N.groups\") == fnrow(wlddev)) wlddev else    ss(wlddev, attr(g, \"starts\")) #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13171 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/groupid.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Run-Length Type Group-Id — groupid","title":"Generate Run-Length Type Group-Id — groupid","text":"groupid enhanced version data.table::rleid atomic vectors. generates run-length type group-id consecutive identical values assigned integer. generalization can applied unordered vectors, generate group id's starting arbitrary value, skip missing values.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/groupid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Run-Length Type Group-Id — groupid","text":"","code":"groupid(x, o = NULL, start = 1L, na.skip = FALSE, check.o = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/reference/groupid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Run-Length Type Group-Id — groupid","text":"x atomic vector type. Attributes considered. o (optional) integer ordering vector specifying order pass x. start integer. starting value resulting group-id. Default starting 1.   na.skip logical. Skip missing values .e. TRUE something like groupid(c(\"\", NA, \"\")) gives c(1, NA, 1) whereas FALSE gives c(1, 2, 3). check.o logical. Programmers option: FALSE prevents checking element o range [1, length(x)], checks length o. gives extra speed, terminate R element o large small.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/groupid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Run-Length Type Group-Id — groupid","text":"integer vector class 'qG'. See qG.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/groupid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Run-Length Type Group-Id — groupid","text":"","code":"groupid(airquality$Month) #>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 #> [39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 83 entries ] #> attr(,\"N.groups\") #> [1] 5 #> attr(,\"class\") #> [1] \"qG\"          \"na.included\" groupid(airquality$Month, start = 0) #>  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 #> [39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 83 entries ] #> attr(,\"N.groups\") #> [1] 5 groupid(wlddev$country)[1:100] #>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #> [39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 30 entries ]  ## Same thing since country is alphabetically ordered: (groupid is faster..) all.equal(groupid(wlddev$country), qG(wlddev$country, na.exclude = FALSE)) #> [1] TRUE  ## When data is unordered, group-id can be generated through an ordering.. uo <- order(rnorm(fnrow(airquality))) monthuo <- airquality$Month[uo] o <- order(monthuo) groupid(monthuo, o) #>  [1] 1 3 4 5 2 1 1 4 5 5 3 2 2 2 4 5 1 2 4 5 2 1 3 3 3 2 4 1 5 1 1 2 2 4 1 2 1 1 #> [39] 1 4 5 1 2 5 1 4 2 5 1 2 4 4 3 3 5 3 4 5 4 4 2 3 5 3 5 3 5 5 3 2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 83 entries ] #> attr(,\"N.groups\") #> [1] 5 #> attr(,\"class\") #> [1] \"qG\"          \"na.included\" identical(groupid(monthuo, o)[o], unattrib(groupid(airquality$Month))) #> [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/reference/indexing.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Indexed Time Series and Panels — indexing","title":"Fast Indexed Time Series and Panels — indexing","text":"fast flexible indexed time series panel data class inherits plm's 'pseries' 'pdata.frame', rigorous, natively handles irregularity, can superimposed data.frame/list, matrix vector, supports ad-hoc computations inside data masking functions model formulas.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/indexing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Indexed Time Series and Panels — indexing","text":"","code":"## Create an 'indexed_frame' containing 'indexed_series' findex_by(.X, ..., single = \"auto\", interact.ids = TRUE) iby(.X, ..., single = \"auto\", interact.ids = TRUE)  # Shorthand  ## Retrieve the index ('index_df') from an 'indexed_frame' or 'indexed_series' findex(x) ix(x)     # Shorthand  ## Remove index from 'indexed_frame' or 'indexed_series' (i.e. get .X back) unindex(x)  ## Reindex 'indexed_frame' or 'indexed_series' (or index vectors / matrices) reindex(x, index = findex(x), single = \"auto\")  ## Check if 'indexed_frame', 'indexed_series', index or time vector is irregular is_irregular(x, any_id = TRUE)  ## Convert 'indexed_frame'/'indexed_series' to normal 'pdata.frame'/'pseries' to_plm(x, row.names = FALSE)  # Subsetting & replacement methods: [(<-) methods call NextMethod(). # Also methods for fsubset, funique and roworder(v), na_omit (internal).  # S3 method for class 'indexed_series' x[i, ..., drop.index.levels = \"id\"]`[`(x, i, ..., drop.index.levels = \"id\")  # S3 method for class 'indexed_frame' x[i, ..., drop.index.levels = \"id\"]`[`(x, i, ..., drop.index.levels = \"id\")  # S3 method for class 'indexed_frame' x[i, j] <- value`[`(x, i, j) <- value  # S3 method for class 'indexed_frame' x$name`$`(x, name)  # S3 method for class 'indexed_frame' x$name <- value`$`(x, name) <- value  # S3 method for class 'indexed_frame' x[[i, ...]]`[[`(x, i, ...)  # S3 method for class 'indexed_frame' x[[i]] <- value`[[`(x, i) <- value  # Index subsetting and printing: optimized using ss()  # S3 method for class 'index_df' x[i, j, drop = FALSE, drop.index.levels = \"id\"]`[`(x, i, j, drop = FALSE, drop.index.levels = \"id\")  # S3 method for class 'index_df' print(x, topn = 5, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/indexing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Indexed Time Series and Panels — indexing","text":".X data frame list-like object equal-length columns. x 'indexed_frame' 'indexed_series'. findex also works 'pseries' 'pdata.frame's created plm. is_irregular x can also index (inherits 'pindex') vector representing time. ... findex_by: variables identifying individual (id) /time dimensions data. Passed either unquoted comma-separated column names (tagged) expressions involving columns, vector column names, indices, logical vector selector function. time variable must enter last. See Examples. Otherwise: arguments passed NextMethod(). single character. one indexing variable supplied, can declared \"id\" \"time\" variable. \"auto\" chooses \"id\" variable anyDuplicated values. interact.ids logical. n > 2 indexing variables passed, TRUE calls finteraction first n-1 (n'th variable must time). FALSE keeps variables index. latter slows computations lags / differences etc. ad-hoc interactions need computed, gives flexibility scaling / centering / summarising different data dimensions. index index (inherits 'pindex'), atomic vector list factors matching data dimensions. Atomic vectors lists 1 factor must declared, see single. Atomic vectors additionally grouped / turned time-factors. See Details. drop.index.levels character. Subset methods also subset index (= data.frame factors), argument regulates factor levels dropped: either \"\", \"id\", \"time\" \"none\". default \"id\" drops levels id's. \"\" \"time\" used caution time-factors may contain levels missing time periods (gaps irregular sequences, periods within sequence removed subsetting), dropping levels create variable ordinal longer represents time. benefit dropping levels can speed-subsequent computations reducing size intermediate vectors created C++. any_id logical. panel series: FALSE returns irregularity check performed id, TRUE calls checks. row.names logical. TRUE creates descriptive row-names (names pseries) plm. can expensive usually required plm models work. topn integer. number first last rows print. , j, name, drop, value Arguments passed NextMethod, data.frame methods. Note index subsetting work, needs integer logical (expression evaluation integer logical x data.table).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/indexing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Indexed Time Series and Panels — indexing","text":"first thing note new 'indexed_frame', 'indexed_series' 'index_df' classes inherit plm's 'pdata.frame', 'pseries' 'pindex' classes, respectively. add, improve, , cases, remove functionality offered plm, aim striking optimal balance flexibility performance. inheritance means 'pseries' 'pdata.frame' methods collapse, also methods plm, apply . compatibility performance considerations allow , collapse continue create methods plm's classes instead new classes. use classes require much knowledge plm, basic background: 'pdata.frame' data.frame index attribute: data.frame 2 factors identifying individual time-dimension data. pulling variable pdata.frame using method like $.pdata.frame [[.pdata.frame (defined plm), 'pseries' created transferring index attribute vector. Methods defined functions like lag / flag etc. use index correct computations panel data, also inside plm's estimation commands. Main Features Enhancements 'indexed_frame' 'indexed_series' classes extend enhance 'pdata.frame' 'pseries' number critical dimensions. notably : Support time series panel data, allowing indexation data one, two variables. class-agnostic: data.frame/list (data.table, tibble, tsibble, sf etc.) can become 'indexed_frame' continue function usual use cases. Similarly, vector matrix (ts, mts, xts) can become 'indexed_series'. also allows transient workflows e.g. some_df |> findex_by(...) |> 'something using collapse functions' |> unindex() |> 'continue working some_df'. comprehensive efficient set methods subsetting manipulation, including methods fsubset, funique, roworder(v) (internal) na_omit (internal, na.omit also works slower). also possible group indexed data fgroup_by transformations e.g. using fmutate, aggregation requires unindex()ing. Natively handle irregularity: time objects ('Date', 'POSIXct' etc.) passed timeid, efficiently determines temporal structure finding greatest common divisor (GCD), creates time-factor levels corresponding complete time-sequence. latter also done plain numeric vectors, assumed represent unit time steps (GDC = 1) coerced integer (can also passed timeid non-unitary). Character time variables converted factor, might also capture irregular gaps panel series. Using time-factor index, collapse's functions efficiently perform correct computations irregular sequences panels without need 'expand' data / fill gaps. is_irregular can used check irregularity entire sequence / panel separately individual panel data. Support computations inside data-masking functions formulas, virtue \"deep indexation\": variable inside 'indexed_frame' 'indexed_series' contains 'index_df' attribute external pointer 'index_df' attribute frame. Functions operating 'indexed_series' stored inside frame ((data, flag(column))) can fetch index pointer. allows worry-free application inside arbitrary data masking environments (, %$%, attach, etc..) estimation commands (glm, feols, lmrob etc..) without duplication index memory. limitation external pointers valid present R session, thus saving 'indexed_frame' loading , need call data = reindex(data) computing . Indexed series also simple Math Ops methods, apply operation unindexed series shallow copy attributes original object result, unless result logical vector (operations like !, == etc.). Ops methods, LHS object 'indexed_series' attributes taken, otherwise attributes RHS object taken. Limits plm Compatibility contrast 'pseries' 'pdata.frame's, 'indexed_series' 'indexed_frames' descriptive \"names\" \"row.names\" attributes attached , mainly efficiency reasons. Furthermore, index stored attribute named 'index_df' (class name), 'index' plm, mainly make classes work data.table, tsibble xts, also utilize 'index' attributes. part poses problem plm compatibility plm source code fetches index using attr(x, \"index\"), attr default performs partial matching. much greater obstacle working plm internal plm code hinged [.pseries method, existence [.indexed_series limits use classes plm estimation commands. Therefore to_plm function provided efficiently coerce classes ordinary plm objects estimation. See Examples. Overall classes really benefit plm, especially given collapse's plm methods also support native plm objects. However, work well inside models software, including stats models, fixest / lfe, whole bunch time series ML models. See Examples. Performance Considerations indexing long time-series panels single variable, setting single = \"id\" \"time\" avoids potentially expensive call anyDuplicated. Note also panel-data regular sorted, omitting time variable index can bring >= 2x performance improvements operations like lagging differencing (alternatively use shift = \"row\" argument flag, fdiff etc.) . dealing long Date POSIXct time sequences, may also internal processing timeid slow simply calling strftime sequences create factor levels slow. case may choose generate index factor integer levels passing timeid(t) findex_by reindex (default generates 'qG' object internally converted factor using as_factor_qG. lazy evaluation expressions like .character(seq_len(nlev)) modern R makes extremely efficient). multiple id variables e.g. findex_by(data, id1, id2, id3, time), default call finteraction() can expensive pasting levels together. case, users may gain performance manually invoking finteraction() (shorthand itn()) argument factor = FALSE e.g. findex_by(data, ids = itn(id1, id2, id3, factor = FALSE), time). generate factor integer levels instead. Print Method print methods 'indexed_frame' 'indexed_series' first call print(unindex(x), ...), followed index variables number categories (index factor levels) square brackets. time factor contains unused levels (= irregularity sequence), square brackets indicate number used levels (periods), followed total number levels (periods sequence) parentheses.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/indexing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Indexed Time Series and Panels — indexing","text":"","code":"oldopts <- options(max.print = 70) # Indexing panel data ----------------------------------------------------------  wldi <- findex_by(wlddev, iso3c, year) wldi #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13171 rows ] #>  #> Indexed by:  iso3c [216] | year [61]  wldi[1:100,1]                 # Works like a data frame #>  [1] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #>  [6] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [11] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [16] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [21] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [26] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [31] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [36] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [41] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [46] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [51] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [56] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" #> [61] \"Afghanistan\" \"Albania\"     \"Albania\"     \"Albania\"     \"Albania\"     #> [66] \"Albania\"     \"Albania\"     \"Albania\"     \"Albania\"     \"Albania\"     #>  [ reached 'max' / getOption(\"max.print\") -- omitted 30 entries ] #>  #> Indexed by:  iso3c [2] | year [61]  POP <- wldi$POP               # indexed_series qsu(POP)                      # Summary statistics #>              N/T         Mean           SD          Min             Max #> Overall    12919  24'245971.6   102'120674         2833  1.39771500e+09 #> Between      216    24'178573  98'616506.7    8343.3333  1.08786967e+09 #> Within   59.8102  24'245971.6  26'803077.4  -405'793067      510'077008 G(POP)                        # Population growth #>  [1]         NA  1.9166113  1.9851986  2.0506358  2.1122464  2.1707928 #>  [7]  2.1947467  2.2122224  2.2801797  2.4133823  2.5690449  2.7010262 #> [13]  2.7517016  2.6947859  2.5104297  2.2251761  2.0011805  1.7632030 #> [19]  1.2898645  0.5236261 -0.4067167 -1.3838794 -2.1952033 -2.6764778 #> [25] -2.6594766 -2.1802494 -1.6922892 -1.1217024  0.1160839  2.1593380 #> [31]  4.5786219  7.1437882  8.9219301  9.1888632  7.9607739  6.0608254 #> [37]  4.1013421  2.6716031  1.9664025  2.1941643  3.0197497  3.9799657 #> [43]  4.5993546  4.7790451  4.4162776  3.7513845  3.0356420  2.5251987 #> [49]  2.2941982  2.4259805  2.7846424  3.1930437  3.4663103  3.5563673 #> [55]  3.4125163  3.1249152  2.8172726  2.5810946  2.4134239  2.3387468 #> [61]         NA         NA  3.1700646  3.1039282  2.9978046  2.9225795 #> [67]  2.7922950  2.6695753  2.6650851  2.8832956 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13106 entries ] #> attr(,\"label\") #> [1] \"Population, total\" #>  #> Indexed by:  iso3c [216] | year [61]  STD(G(POP, c(1, 10)))         # Within-standardized 1 and 10-year growth rates #>                     G1         L10G1 #>     [1,]            NA            NA #>     [2,] -2.404379e-01            NA #>     [3,] -2.122758e-01            NA #>     [4,] -1.854071e-01            NA #>     [5,] -1.601097e-01            NA #>     [6,] -1.360704e-01            NA #>     [7,] -1.262349e-01            NA #>     [8,] -1.190593e-01            NA #>     [9,] -9.115593e-02            NA #>    [10,] -3.646264e-02            NA #>    [11,]  2.745275e-02 -2.502986e-01 #>    [12,]  8.164455e-02 -2.072055e-01 #>    [13,]  1.024520e-01 -1.648010e-01 #>    [14,]  7.908227e-02 -1.289204e-01 #>    [15,]  3.385202e-03 -1.066142e-01 #>    [16,] -1.137405e-01 -1.035575e-01 #>    [17,] -2.057136e-01 -1.144404e-01 #>    [18,] -3.034277e-01 -1.396334e-01 #>    [19,] -4.977815e-01 -1.949160e-01 #>    [20,] -8.124006e-01 -2.992510e-01 #>    [21,] -1.194401e+00 -4.602688e-01 #>    [22,] -1.595626e+00 -6.746138e-01 #>    [23,] -1.928758e+00 -9.237424e-01 #>    [24,] -2.126370e+00 -1.181362e+00 #>    [25,] -2.119389e+00 -1.416777e+00 #>    [26,] -1.922618e+00 -1.607795e+00 #>    [27,] -1.722260e+00 -1.761378e+00 #>    [28,] -1.487976e+00 -1.877266e+00 #>    [29,] -9.797383e-01 -1.923294e+00 #>    [30,] -1.407738e-01 -1.859412e+00 #>    [31,]  8.525893e-01 -1.659693e+00 #>    [32,]  1.905852e+00 -1.297409e+00 #>    [33,]  2.635961e+00 -7.800175e-01 #>    [34,]  2.745564e+00 -1.619956e-01 #>    [35,]  2.241308e+00  4.585063e-01 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13141 rows ] #> attr(,\"label\") #> [1] \"Population, total\" #> attr(,\"class\") #> [1] \"numeric\" \"matrix\"  #>  #> Indexed by:  iso3c [216] | year [61]  psmat(POP)                    # Panel-Series Matrix #>         1960     1961     1962     1963     1964     1965     1966     1967 #> ABW 5.42e+04 5.54e+04 5.62e+04 5.67e+04 5.70e+04 5.74e+04 5.77e+04 5.81e+04 #>         1968     1969     1970     1971     1972     1973     1974     1975 #> ABW 5.84e+04 5.87e+04 5.91e+04 5.94e+04 5.98e+04 6.02e+04 6.05e+04 6.07e+04 #>         1976     1977     1978     1979     1980     1981     1982     1983 #> ABW 6.06e+04 6.04e+04 6.01e+04 6.00e+04 6.01e+04 6.06e+04 6.13e+04 6.22e+04 #>         1984     1985     1986     1987     1988     1989     1990     1991 #> ABW 6.28e+04 6.30e+04 6.26e+04 6.18e+04 6.11e+04 6.10e+04 6.21e+04 6.46e+04 #>         1992     1993     1994     1995     1996     1997     1998     1999 #> ABW 6.82e+04 7.25e+04 7.67e+04 8.03e+04 8.32e+04 8.55e+04 8.73e+04 8.90e+04 #>         2000     2001     2002     2003     2004     2005     2006     2007 #> ABW 9.09e+04 9.29e+04 9.50e+04 9.70e+04 9.87e+04 1.00e+05 1.01e+05 1.01e+05 #>         2008     2009     2010     2011     2012     2013     2014     2015 #> ABW 1.01e+05 1.01e+05 1.02e+05 1.02e+05 1.03e+05 1.03e+05 1.04e+05 1.04e+05 #>         2016     2017     2018     2019 2020 #> ABW 1.05e+05 1.05e+05 1.06e+05 1.06e+05   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 215 rows ] plot(psmat(log10(POP)))   POP[30:5000]                  # Subsetting indexed_series #>  [1] 11868877 12412308 13299017 14485546 15816603 17075727 18110657 18853437 #>  [9] 19357126 19737765 20170844 20779953 21606988 22600770 23680871 24726684 #> [17] 25654277 26433049 27100536 27722276 28394813 29185507 30117413 31161376 #> [25] 32269589 33370794 34413603 35383128 36296400 37172386 38041754       NA #> [33]  1608800  1659800  1711319  1762621  1814135  1864791  1914573  1965598 #> [41]  2022272  2081695  2135479  2187853  2243126  2296752  2350124  2404831 #> [49]  2458526  2513546  2566266  2617832  2671997  2726056  2784278  2843960 #> [57]  2904429  2964762  3022635  3083605  3142336  3227943  3286542  3266790 #> [65]  3247039  3227287  3207536  3187784  3168033  3148281 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4901 entries ] #>  #> Indexed by:  iso3c [82] | year [61]  Dlog(POP[30:5000])            # Log-difference of subset #>  [1]           NA  0.044768965  0.069001562  0.085461202  0.087908886 #>  [6]  0.076597770  0.058842568  0.040194682  0.026365389  0.019473185 #> [11]  0.021704390  0.029750528  0.039028057  0.044967196  0.046683615 #> [16]  0.043215393  0.036827318  0.029904781  0.024938423  0.022682771 #> [21]  0.023970210  0.027465763  0.031431259  0.034075870  0.034945890 #> [26]  0.033555817  0.030770836  0.027783174  0.025483466  0.023847611 #> [31]  0.023118172           NA           NA  0.031208554  0.030567305 #> [36]  0.029537488  0.028806864  0.027540212  0.026345639  0.026301903 #> [41]  0.028425107  0.028960834  0.025508512  0.024229720  0.024949731 #> [46]  0.023625522  0.022972142  0.023011538  0.022082353  0.022132522 #> [51]  0.020757419  0.019894570  0.020479639  0.020029743  0.021132718 #> [56]  0.021208853  0.021039366  0.020559946  0.019332208  0.019970400 #> [61]  0.018867105  0.026878620  0.017990856 -0.006028097 -0.006064347 #> [66] -0.006101658 -0.006138805 -0.006177037 -0.006215114 -0.006254301 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4901 entries ] #>  #> Indexed by:  iso3c [82] | year [61]  psacf(identity(POP[30:5000])) # ACF of subset  L(Dlog(POP[30:5000], c(1, 10)), -1:1) # Multiple computations on subset #>              F1.Dlog1         Dlog1      L1.Dlog1   F1.L10Dlog1      L10Dlog1 #>    [1,]  4.476897e-02            NA            NA            NA            NA #>    [2,]  6.900156e-02  4.476897e-02            NA            NA            NA #>    [3,]  8.546120e-02  6.900156e-02  4.476897e-02            NA            NA #>    [4,]  8.790889e-02  8.546120e-02  6.900156e-02            NA            NA #>    [5,]  7.659777e-02  8.790889e-02  8.546120e-02            NA            NA #>    [6,]  5.884257e-02  7.659777e-02  8.790889e-02            NA            NA #>    [7,]  4.019468e-02  5.884257e-02  7.659777e-02            NA            NA #>    [8,]  2.636539e-02  4.019468e-02  5.884257e-02            NA            NA #>    [9,]  1.947319e-02  2.636539e-02  4.019468e-02            NA            NA #>   [10,]  2.170439e-02  1.947319e-02  2.636539e-02  0.5303185995            NA #>   [11,]  2.975053e-02  2.170439e-02  1.947319e-02  0.5153001629  0.5303185995 #>           L1.L10Dlog1 #>    [1,]            NA #>    [2,]            NA #>    [3,]            NA #>    [4,]            NA #>    [5,]            NA #>    [6,]            NA #>    [7,]            NA #>    [8,]            NA #>    [9,]            NA #>   [10,]            NA #>   [11,]            NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4960 rows ] #> attr(,\"class\") #> [1] \"numeric\" \"matrix\"  #>  #> Indexed by:  iso3c [82] | year [61]   # Fast Statistical Functions don't have dedicated methods # Thus for aggregation we need to unindex beforehand ... fmean(unindex(POP)) #> [1] 24245972 #> attr(,\"label\") #> [1] \"Population, total\" wldi |> unindex() |>   fgroup_by(iso3c) |> num_vars() |> fmean() #>   iso3c year   decade      PCGDP   LIFEEX     GINI        ODA         POP #> 1   ABW 1990 1985.574 25413.8370 72.40653       NA   33245000    76268.63 #> 2   AFG 1990 1985.574   483.8351 49.19717       NA 1487548499 18362258.22 #> 3   AGO 1990 1985.574  2887.6879 46.75805 48.66667  267452068 13823228.03 #> 4   ALB 1990 1985.574  2819.2400 71.68027 31.41111  312928126  2708297.17 #> 5   AND 1990 1985.574 40083.0911       NA       NA         NA    51547.35 #> 6   ARE 1990 1985.574 64616.4864 69.37793 29.25000   13384222  3089064.62 #> 7   ARG 1990 1985.574  7907.8326 71.12565 45.92258  106930833 32301197.52 #> 8   ARM 1990 1985.574  2520.1808 70.67953 32.24500  282426894  2912376.95 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 208 rows ]  library(magrittr) # ... or unindex after taking group identifiers from the index fmean(unindex(fgrowth(POP)), ix(POP)$iso3c) #>         ABW         AFG         AGO         ALB         AND         ARE  #>  1.15986116  2.50218519  3.04019111  0.98728941  3.03828499  8.33912222  #>         ARG         ARM         ASM         ATG         AUS         AUT  #>  1.34100846  0.78894579  1.74127860  0.99964449  1.54424580  0.39301937  #>         AZE         BDI         BEL         BEN         BFA         BGD  #>  1.61742943  2.43172184  0.38832202  2.71473569  2.46698310  2.09594336  #>         BGR         BHR         BHS         BIH         BLR         BLZ  #> -0.20095889  4.01156764  2.17972600  0.04870005  0.23697183  2.48033866  #>         BMU         BOL         BRA         BRB         BRN         BTN  #>  0.62656641  1.96310764  1.83756339  0.36894714  2.87598784  2.10961311  #>         BWA         CAF         CAN         CHE         CHI         CHL  #>  2.61689824  1.97141241  1.26547829  0.81130910  0.77252949  1.44461721  #>         CHN         CIV         CMR         COD         COG         COL  #>  1.26475057  3.43990856  2.76520875  2.99210471  2.86194380  1.95730439  #>         COM         CPV         CRI         CUB         CUW         CYM  #>  2.56471792  1.71779560  2.28825488  0.78844375  0.40380104  3.65940831  #>         CYP         CZE         DEU         DJI         DMA         DNK  #>  1.26066375  0.17964024  0.22511110  4.28964392  0.30770228  0.40573747  #>         DOM         DZA         ECU         EGY         ERI         ESP  #>  2.02552207  2.33239690  2.30053844  2.27489144  2.30900292  0.74430651  #>         EST         ETH         FIN         FJI         FRA         FRO  #>  0.15848536  2.78733512  0.37433864  1.39760426  0.61832292  0.58122004  #>         FSM         GAB         GBR         GEO  #>  1.61438473  2.51981406  0.41364183  0.04208078  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 146 entries ] #> attr(,\"label\") #> [1] \"Population, total\" wldi |> num_vars() %>%   fgroup_by(iso3c = ix(.)$iso3c) |>   unindex() |> fmean() #>   iso3c year   decade      PCGDP   LIFEEX     GINI        ODA         POP #> 1   ABW 1990 1985.574 25413.8370 72.40653       NA   33245000    76268.63 #> 2   AFG 1990 1985.574   483.8351 49.19717       NA 1487548499 18362258.22 #> 3   AGO 1990 1985.574  2887.6879 46.75805 48.66667  267452068 13823228.03 #> 4   ALB 1990 1985.574  2819.2400 71.68027 31.41111  312928126  2708297.17 #> 5   AND 1990 1985.574 40083.0911       NA       NA         NA    51547.35 #> 6   ARE 1990 1985.574 64616.4864 69.37793 29.25000   13384222  3089064.62 #> 7   ARG 1990 1985.574  7907.8326 71.12565 45.92258  106930833 32301197.52 #> 8   ARM 1990 1985.574  2520.1808 70.67953 32.24500  282426894  2912376.95 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 208 rows ]  # With matrix methods it is easier as most attributes are dropped upon aggregation. G(POP, c(1, 10)) %>% fmean(ix(.)$iso3c) #>              G1       L10G1 #> ABW  1.15986116  13.5405797 #> AFG  2.50218519  29.7453631 #> AGO  3.04019111  37.2423846 #> ALB  0.98728941  10.4611010 #> AND  3.03828499  36.8630696 #> ARE  8.33912222 145.2957118 #> ARG  1.34100846  14.3289740 #> ARM  0.78894579   7.1746628 #> ASM  1.74127860  20.2992819 #> ATG  0.99964449  10.0195522 #> AUS  1.54424580  16.0434792 #> AUT  0.39301937   3.5211125 #> AZE  1.61742943  16.4526447 #> BDI  2.43172184  26.7915415 #> BEL  0.38832202   3.6354631 #> BEN  2.71473569  31.9252634 #> BFA  2.46698310  28.3385127 #> BGD  2.09594336  23.3556445 #> BGR -0.20095889  -2.3160917 #> BHR  4.01156764  50.7327853 #> BHS  2.17972600  22.6999691 #> BIH  0.04870005   0.5070281 #> BLR  0.23697183   2.1725510 #> BLZ  2.48033866  27.8347280 #> BMU  0.62656641   5.7036637 #> BOL  1.96310764  21.9315509 #> BRA  1.83756339  20.1289837 #> BRB  0.36894714   3.9565564 #> BRN  2.87598784  33.4451942 #> BTN  2.10961311  23.9024113 #> BWA  2.61689824  31.1856395 #> CAF  1.97141241  22.7889510 #> CAN  1.26547829  12.8834454 #> CHE  0.81130910   7.3670756 #> CHI  0.77252949   7.6461689 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 181 rows ]  # Example of index with multiple ids GGDC10S |> findex_by(Variable, Country, Year) |> head() # default is interact.ids = TRUE #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #> 4     BWA        SSA Sub-saharan Africa       VA 1963  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA #> 4  NA   NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] #>  #> Indexed by:  Variable.Country [1] | Year [6 (67)]  GGDCi <- GGDC10S |> findex_by(Variable, Country, Year, interact.ids = FALSE) head(GGDCi) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #> 4     BWA        SSA Sub-saharan Africa       VA 1963  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA #> 4  NA   NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] #>  #> Indexed by:  Variable [1] Country [1] | Year [6 (67)]  findex(GGDCi) #>   Variable Country Year #> 1       VA     BWA 1960 #> 2       VA     BWA 1961 #> 3       VA     BWA 1962 #> 4       VA     BWA 1963 #> 5       VA     BWA 1964 #> ---                  #> 5023 EMP EGY 2008 #> 5024 EMP EGY 2009 #> 5025 EMP EGY 2010 #> 5026 EMP EGY 2011 #> 5027 EMP EGY 2012 #>  #> Variable [2] Country [43] | Year [67] # The benefit is increased flexibility for summary statistics and data transformation qsu(GGDCi, effect = \"Country\") #> , , Country #>  #>              N/T  Mean  SD  Min  Max #> Overall     5027     -   -    -    - #> Between       43     -   -    -    - #> Within   116.907     -   -    -    - #>  #> , , Regioncode #>  #>              N/T  Mean  SD  Min  Max #> Overall     5027     -   -    -    - #> Between       43     -   -    -    - #> Within   116.907     -   -    -    - #>  #> , , Region #>  #>              N/T  Mean  SD  Min  Max #> Overall     5027     -   -    -    - #> Between       43     -   -    -    - #> Within   116.907     -   -    -    - #>  #> , , Variable #>  #>              N/T  Mean  SD  Min  Max #> Overall     5027     -   -    -    - #> Between       43     -   -    -    - #> Within   116.907     -   -    -    - #>  #> , , Year #>  #>              N/T       Mean       SD        Min        Max #> Overall     5027  1981.5801  17.5704       1947       2013 #> Between       43  1982.4236   5.0799  1978.7519     2011.5 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 11 slices ]  STD(GGDCi$SUM, effect = \"Variable\")            # Standardizing by variable #>  [1]         NA         NA         NA         NA -0.1226776 -0.1226776 #>  [7] -0.1226776 -0.1226776 -0.1226776 -0.1226776 -0.1226775 -0.1226775 #> [13] -0.1226774 -0.1226773 -0.1226772 -0.1226771 -0.1226769 -0.1226767 #> [19] -0.1226766 -0.1226762 -0.1226756 -0.1226753 -0.1226752 -0.1226744 #> [25] -0.1226738 -0.1226724 -0.1226704 -0.1226692 -0.1226661 -0.1226627 #> [31] -0.1226607 -0.1226582 -0.1226567 -0.1226550 -0.1226499 -0.1226448 #> [37] -0.1226376 -0.1226320 -0.1226275 -0.1226148 -0.1226044 -0.1225979 #> [43] -0.1225908 -0.1225861 -0.1225770 -0.1225543 -0.1225338 -0.1225158 #> [49] -0.1224963 -0.1225084 -0.1224488         NA         NA         NA #> [55]         NA         NA -0.3807465 -0.3806958 -0.3806859 -0.3806942 #> [61] -0.3806630 -0.3805993 -0.3805239 -0.3804590 -0.3803244 -0.3802321 #> [67] -0.3801543 -0.3800409 -0.3799774 -0.3799017 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4957 entries ] #> attr(,\"label\") #> [1] \"Summation of sector GDP\" #> attr(,\"format.stata\") #> [1] \"%10.0g\" #>  #> Indexed by:  Variable [2] Country [43] | Year [67]  STD(GGDCi$SUM, effect = c(\"Variable\", \"Year\")) # ... by variable and year #>  [1]         NA         NA         NA         NA -0.2041456 -0.2039970 #>  [7] -0.2023122 -0.2041736 -0.2058182 -0.2068329 -0.1799561 -0.1784717 #> [13] -0.1794400 -0.1809847 -0.1852166 -0.1881118 -0.1915147 -0.1952319 #> [19] -0.2000250 -0.2086539 -0.2176069 -0.2246298 -0.2285386 -0.2384116 #> [25] -0.2442721 -0.2480965 -0.2533789 -0.2618757 -0.2684392 -0.2739245 #> [31] -0.2788689 -0.2831778 -0.2934982 -0.3006316 -0.3052873 -0.3081273 #> [37] -0.3074253 -0.3039395 -0.2818521 -0.2751017 -0.2694049 -0.2606845 #> [43] -0.2576762 -0.2567207 -0.2531264 -0.2434613 -0.2351500 -0.2288238 #> [49] -0.2182615 -0.2127535 -0.2090734         NA         NA         NA #> [55]         NA         NA -0.4596772 -0.4497862 -0.4399738 -0.4363115 #> [61] -0.4331453 -0.4247106 -0.4027655 -0.4096142 -0.4114065 -0.4141980 #> [67] -0.4082425 -0.4024895 -0.4026850 -0.4047382 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4957 entries ] #> attr(,\"label\") #> [1] \"Summation of sector GDP\" #> attr(,\"format.stata\") #> [1] \"%10.0g\" #>  #> Indexed by:  Variable [2] Country [43] | Year [67]  # But time-based operations are a bit more expensive because of the necessary interactions D(GGDCi$SUM) #>  [1]            NA            NA            NA            NA            NA #>  [6]     1.8648058     3.7996670    -1.7515810    -0.2525958    10.0790095 #> [11]    15.2179271    12.1300779    31.7872318    37.4213552    35.1940244 #> [16]    36.8116252    85.0015379    44.7177833    52.5841475   158.9853717 #> [21]   198.2115288   103.9129599    23.7520357   288.7796000   214.2149426 #> [26]   515.3786450   726.9937149   439.7886989  1085.0704934  1220.4747092 #> [31]   737.8868699   879.8964485   529.0696305   617.1251820  1841.1712884 #> [36]  1803.6482296  2600.8383913  1986.0421431  1631.5058344  4552.4978100 #> [41]  3712.3024701  2342.9411903  2519.9965561  1715.6370519  3237.7063677 #> [46]  8127.4111125  7374.8681409  6430.3096108  6983.2264598 -4322.3684709 #> [51] 21358.0356275            NA            NA            NA            NA #> [56]            NA            NA     4.8807941     0.9545929    -0.8001645 #> [61]     3.0111420     6.1276582     7.2716726     6.2475417    12.9682512 #> [66]     8.8843341     7.4951964    10.9173822     6.1179949     7.2972389 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4957 entries ] #> attr(,\"label\") #> [1] \"Summation of sector GDP\" #> attr(,\"format.stata\") #> [1] \"%10.0g\" #>  #> Indexed by:  Variable [2] Country [43] | Year [67]   # Panel-Data modelling ---------------------------------------------------------  # Linear model of 5-year annualized growth rates of GDP on Life Expactancy + 5y lag lm(G(PCGDP, 5, p = 1/5) ~ L(G(LIFEEX, 5, p = 1/5), c(0, 5)), wldi) # p abbreviates \"power\" #>  #> Call: #> lm(formula = G(PCGDP, 5, p = 1/5) ~ L(G(LIFEEX, 5, p = 1/5),  #>     c(0, 5)), data = wldi) #>  #> Coefficients: #>                         (Intercept)  L(G(LIFEEX, 5, p = 1/5), c(0, 5))--   #>                              1.6021                               0.4739   #> L(G(LIFEEX, 5, p = 1/5), c(0, 5))L5   #>                              0.1716   #>   # Same, adding time fixed effects via plm package: need to utilize to_plm function plm::plm(G(PCGDP, 5, p = 1/5) ~ L(G(LIFEEX, 5, p = 1/5), c(0, 5)), to_plm(wldi), effect = \"time\") #>  #> Model Formula: G(PCGDP, 5, p = 1/5) ~ L(G(LIFEEX, 5, p = 1/5), c(0, 5)) #> <environment: 0x116768320> #>  #> Coefficients: #> L(G(LIFEEX, 5, p = 1/5), c(0, 5))-- L(G(LIFEEX, 5, p = 1/5), c(0, 5))L5  #>                             0.26902                             0.34879  #>   # With country and time fixed effects via fixest fixest::feols(G(PCGDP, 5, p=1/5) ~ L(G(LIFEEX, 5, p=1/5), c(0, 5)), wldi, fixef = .c(iso3c, year)) #> NOTE: 5,596 observations removed because of NA values (LHS: 4,720, RHS: 3,522). #> OLS estimation, Dep. Var.: G(PCGDP, 5, p = 1/5) #> Observations: 7,580 #> Fixed-effects: iso3c: 192,  year: 50 #> Standard-errors: Clustered (iso3c)  #>                                     Estimate Std. Error t value   Pr(>|t|)     #> L(G(LIFEEX, 5, p = 1/5), c(0, 5))-- 0.392178   0.104398 3.75657 2.2867e-04 *** #> L(G(LIFEEX, 5, p = 1/5), c(0, 5))L5 0.476969   0.112195 4.25125 3.3228e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> RMSE: 3.05102     Adj. R2: 0.279618 #>                 Within R2: 0.019289 if (FALSE)   # Running a robust MM regression without fixed effects robustbase::lmrob(G(PCGDP, 5, p = 1/5) ~ L(G(LIFEEX, 5, p = 1/5), c(0, 5)), wldi)  # Running a robust MM regression with country and time fixed effects wldi |> fselect(PCGDP, LIFEEX) |>   fgrowth(5, power = 1/5) |> ftransform(LIFEEX_L5 = L(LIFEEX, 5)) |>   # drop abbreviates drop.index.levels (not strictly needed here but more consistent)   na_omit(drop = \"all\") |> fhdwithin(na.rm = FALSE) |> # For TFE use fwithin(effect = \"year\")   unindex() |> robustbase::lmrob(formula = PCGDP ~.)    # using lm() gives same result as fixest #> Error in loadNamespace(x): there is no package called ‘robustbase’  # Using a random forest model without fixed effects # ranger does not support these kinds of formulas, thus we need some preprocessing... wldi |> fselect(PCGDP, LIFEEX) |>   fgrowth(5, power = 1/5) |> ftransform(LIFEEX_L5 = L(LIFEEX, 5)) |>   unindex() |> na_omit() |> ranger::ranger(formula = PCGDP ~.) #> Error in loadNamespace(x): there is no package called ‘ranger’  # \\dontrun{}  # Indexing other data frame based classes --------------------------------------  library(tibble) wlditbl <- qTBL(wlddev) |> findex_by(iso3c, year) wlditbl[,2] # Works like a tibble... #> # A tibble: 13,176 × 1 #>    iso3c #>    <fct> #>  1 AFG   #>  2 AFG   #>  3 AFG   #>  4 AFG   #>  5 AFG   #>  6 AFG   #>  7 AFG   #>  8 AFG   #>  9 AFG   #> 10 AFG   #> # ℹ 13,166 more rows #>  #> Indexed by:  iso3c [216] | year [61]  wlditbl[[2]] #>  [1] AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG #> [20] AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG #> [39] AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG AFG #> [58] AFG AFG AFG AFG ALB ALB ALB ALB ALB ALB ALB ALB ALB #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13106 entries ] #> attr(,\"label\") #> [1] Country Code #> 216 Levels: ABW AFG AGO ALB AND ARE ARG ARM ASM ATG AUS AUT AZE BDI BEL ... ZWE #>  #> Indexed by:  iso3c [216] | year [61]  wlditbl[1:1000, 10] #> # A tibble: 1,000 × 1 #>    LIFEEX #>     <dbl> #>  1   32.4 #>  2   33.0 #>  3   33.5 #>  4   34.0 #>  5   34.5 #>  6   34.9 #>  7   35.4 #>  8   35.9 #>  9   36.4 #> 10   36.9 #> # ℹ 990 more rows #>  #> Indexed by:  iso3c [17] | year [61]  head(wlditbl) #> # A tibble: 6 × 13 #>   country   iso3c date        year decade region income OECD  PCGDP LIFEEX  GINI #>   <chr>     <fct> <date>     <int>  <int> <fct>  <fct>  <lgl> <dbl>  <dbl> <dbl> #> 1 Afghanis… AFG   1961-01-01  1960   1960 South… Low i… FALSE    NA   32.4    NA #> 2 Afghanis… AFG   1962-01-01  1961   1960 South… Low i… FALSE    NA   33.0    NA #> 3 Afghanis… AFG   1963-01-01  1962   1960 South… Low i… FALSE    NA   33.5    NA #> 4 Afghanis… AFG   1964-01-01  1963   1960 South… Low i… FALSE    NA   34.0    NA #> 5 Afghanis… AFG   1965-01-01  1964   1960 South… Low i… FALSE    NA   34.5    NA #> 6 Afghanis… AFG   1966-01-01  1965   1960 South… Low i… FALSE    NA   34.9    NA #> # ℹ 2 more variables: ODA <dbl>, POP <dbl> #>  #> Indexed by:  iso3c [1] | year [6 (61)]   library(data.table) wldidt <- qDT(wlddev) |> findex_by(iso3c, year) wldidt[1:1000]      # Works like a data.table... #>           country  iso3c       date  year decade                    region #>            <char> <fctr>     <Date> <int>  <int>                    <fctr> #>    1: Afghanistan    AFG 1961-01-01  1960   1960                South Asia #>    2: Afghanistan    AFG 1962-01-01  1961   1960                South Asia #>    3: Afghanistan    AFG 1963-01-01  1962   1960                South Asia #>    4: Afghanistan    AFG 1964-01-01  1963   1960                South Asia #>            income   OECD    PCGDP LIFEEX  GINI       ODA     POP #>            <fctr> <lgcl>    <num>  <num> <num>     <num>   <num> #>    1:  Low income  FALSE       NA 32.446    NA 116769997 8996973 #>    2:  Low income  FALSE       NA 32.962    NA 232080002 9169410 #>    3:  Low income  FALSE       NA 33.471    NA 112839996 9351441 #>    4:  Low income  FALSE       NA 33.971    NA 237720001 9543205 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] #>  #> Indexed by:  iso3c [17] | year [61]  wldidt[year > 2000] #>           country  iso3c       date  year decade             region #>            <char> <fctr>     <Date> <int>  <int>             <fctr> #>    1: Afghanistan    AFG 2002-01-01  2001   2000         South Asia #>    2: Afghanistan    AFG 2003-01-01  2002   2000         South Asia #>    3: Afghanistan    AFG 2004-01-01  2003   2000         South Asia #>    4: Afghanistan    AFG 2005-01-01  2004   2000         South Asia #>                    income   OECD     PCGDP LIFEEX  GINI        ODA      POP #>                    <fctr> <lgcl>     <num>  <num> <num>      <num>    <num> #>    1:          Low income  FALSE        NA 56.308    NA  682969971 21606988 #>    2:          Low income  FALSE  330.3036 56.784    NA 1790479980 22600770 #>    3:          Low income  FALSE  343.0809 57.271    NA 1972890015 23680871 #>    4:          Low income  FALSE  333.2167 57.772    NA 2681449951 24726684 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] #>  #> Indexed by:  iso3c [216] | year [20 (61)]  wldidt[, .(sum_PCGDP = sum(PCGDP, na.rm = TRUE)), by = country] # Aggregation unindexes the result #>                    country   sum_PCGDP #>                     <char>       <num> #>   1:           Afghanistan    8709.031 #>   2:               Albania  112769.600 #>   3:               Algeria  211936.285 #>   4:        American Samoa  171208.120 #>   5:               Andorra 2004154.553 #>  ---                                   #> 212: Virgin Islands (U.S.)  570075.738 #> 213:    West Bank and Gaza   62099.304 #> 214:           Yemen, Rep.   32089.789 #> 215:                Zambia   79131.760 #> 216:              Zimbabwe   73166.158 wldidt[, lapply(.SD, sum, na.rm = TRUE), by = country, .SDcols = .c(PCGDP, LIFEEX)] #>                    country       PCGDP   LIFEEX #>                     <char>       <num>    <num> #>   1:           Afghanistan    8709.031 2951.830 #>   2:               Albania  112769.600 4300.816 #>   3:               Algeria  211936.285 3813.774 #>   4:        American Samoa  171208.120    0.000 #>   5:               Andorra 2004154.553    0.000 #>  ---                                            #> 212: Virgin Islands (U.S.)  570075.738 4422.775 #> 213:    West Bank and Gaza   62099.304 2148.234 #> 214:           Yemen, Rep.   32089.789 3152.224 #> 215:                Zambia   79131.760 3065.558 #> 216:              Zimbabwe   73166.158 3272.016 # This also works but is a bit inefficient since the index is subset and then dropped # -> better unindex beforehand wldidt[year > 2000, .(sum_PCGDP = sum(PCGDP, na.rm = TRUE)), by = country] #>                    country  sum_PCGDP #>                     <char>      <num> #>   1:           Afghanistan   8709.031 #>   2:               Albania  73832.383 #>   3:               Algeria  84017.769 #>   4:        American Samoa 171208.120 #>   5:               Andorra 827219.533 #>  ---                                  #> 212: Virgin Islands (U.S.) 570075.738 #> 213:    West Bank and Gaza  47661.621 #> 214:           Yemen, Rep.  20313.628 #> 215:                Zambia  26490.196 #> 216:              Zimbabwe  21161.694 wldidt[, PCGDP_gr_5Y := G(PCGDP, 5, power = 1/5)]  # Can add Variables by reference # Note that .SD is a data.table of indexed_series, not an indexed_frame, so this is WRONG! wldidt[, .c(PCGDP_gr_5Y, LIFEEX_gr_5Y) := G(slt(.SD, PCGDP, LIFEEX), 5, power = 1/5)] #> Warning: Found '.SD' in the call but no 'apply' function. Please note that .SD is not an indexed_frame but a plain data.table containing indexed_series. Thus indexed_frame / pdata.frame methods don't work on .SD! Consider using (m/l)apply(.SD, FUN) or reindex(.SD, ix(data)). If you are not performing indexed operations on .SD please ignore or suppress this warning. # This gives the correct outcome wldidt[, .c(PCGDP_gr_5Y, LIFEEX_gr_5Y) := lapply(slt(.SD, PCGDP, LIFEEX), G, 5, power = 1/5)] if (FALSE) { # \\dontrun{ library(sf) nc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE) nci <- findex_by(nc, SID74) nci[1:10, \"AREA\"] st_centroid(nci) # The geometry column is never indexed, thus sf computations work normally st_coordinates(nci) fmean(st_area(nci))  library(tsibble) pedi <- findex_by(pedestrian, Sensor, Date_Time) pedi[1:5, ] findex(pedi) # Time factor with 17k levels from POSIXct # Now here is a case where integer levels in the index can really speed things up ix(iby(pedestrian, Sensor, timeid(Date_Time))) library(microbenchmark) microbenchmark(descriptive_levels = findex_by(pedestrian, Sensor, Date_Time),                integer_levels = findex_by(pedestrian, Sensor, timeid(Date_Time))) # Data has irregularity is_irregular(pedi) is_irregular(pedi, any_id = FALSE) # irregularity in all sequences # Manipulation such as lagging with tsibble/dplyr requires expanding rows and grouping # Collapse can just compute correct lag on indexed series or frames library(dplyr) microbenchmark(   dplyr = fill_gaps(pedestrian) |> group_by_key() |> mutate(Lag_Count = lag(Count)),   collapse = fmutate(pedi, Lag_Count = flag(Count)), times = 10) } # } # Indexing Atomic objects ---------------------------------------------------------  ## ts print(AirPassengers) #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949 112 118 132 129 121 135 148 148 136 119 104 118 #> 1950 115 126 141 135 125 149 170 170 158 133 114 140 #> 1951 145 150 178 163 172 178 199 199 184 162 146 166 #> 1952 171 180 193 181 183 218 230 242 209 191 172 194 #> 1953 196 196 236 235 229 243 264 272 237 211 180 201 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] AirPassengers[-(20:30)]        # Ts class does not support irregularity, subsetting drops class #>  [1] 112 118 132 129 121 135 148 148 136 119 104 118 115 126 141 135 125 149 170 #> [20] 199 199 184 162 146 166 171 180 193 181 183 218 230 242 209 191 172 194 196 #> [39] 196 236 235 229 243 264 272 237 211 180 201 204 188 235 227 234 264 302 293 #> [58] 259 229 203 229 242 233 267 269 270 315 364 347 312 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 63 entries ] G(AirPassengers[-(20:30)], 12) # Annual Growth Rate: Wrong! #>  [1]         NA         NA         NA         NA         NA         NA #>  [7]         NA         NA         NA         NA         NA         NA #> [13]  2.6785714  6.7796610  6.8181818  4.6511628  3.3057851 10.3703704 #> [19] 14.8648649 34.4594595 46.3235294 54.6218487 55.7692308 23.7288136 #> [25] 44.3478261 35.7142857 27.6595745 42.9629630 44.8000000 22.8187919 #> [31] 28.2352941 15.5778894 21.6080402 13.5869565 17.9012346 17.8082192 #> [37] 16.8674699 14.6198830  8.8888889 22.2797927 29.8342541 25.1366120 #> [43] 11.4678899 14.7826087 12.3966942 13.3971292 10.4712042  4.6511628 #> [49]  3.6082474  4.0816327 -4.0816327 -0.4237288 -3.4042553  2.1834061 #> [55]  8.6419753 14.3939394  7.7205882  9.2827004  8.5308057 12.7777778 #> [61] 13.9303483 18.6274510 23.9361702 13.6170213 18.5022026 15.3846154 #> [67] 19.3181818 20.5298013 18.4300341 20.4633205 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 63 entries ] # Now indexing AirPassengers (identity() is a trick so that the index is named time(AirPassengers)) iAP <- reindex(AirPassengers, identity(time(AirPassengers))) iAP #>      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #> 1949 112 118 132 129 121 135 148 148 136 119 104 118 #> 1950 115 126 141 135 125 149 170 170 158 133 114 140 #> 1951 145 150 178 163 172 178 199 199 184 162 146 166 #> 1952 171 180 193 181 183 218 230 242 209 191 172 194 #> 1953 196 196 236 235 229 243 264 272 237 211 180 201 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 rows ] #>  #> Indexed by:  time(AirPassengers) [144]  findex(iAP)    # See the index #>   time(AirPassengers) #> 1                1949 #> 2         1949.083333 #> 3         1949.166666 #> 4         1949.249999 #> 5         1949.333332 #> ---                #> 140 1960.583287 #> 141  1960.66662 #> 142 1960.749953 #> 143 1960.833286 #> 144 1960.916619 #>  #> time(AirPassengers) [144] iAP[-(20:30)]  # Subsetting #>  [1] 112 118 132 129 121 135 148 148 136 119 104 118 115 126 141 135 125 149 170 #> [20] 199 199 184 162 146 166 171 180 193 181 183 218 230 242 209 191 172 194 196 #> [39] 196 236 235 229 243 264 272 237 211 180 201 204 188 235 227 234 264 302 293 #> [58] 259 229 203 229 242 233 267 269 270 315 364 347 312 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 63 entries ] #>  #> Indexed by:  time(AirPassengers) [133 (144)]  G(iAP[-(20:30)], 12)                # Annual Growth Rate: Correct! #>  [1]         NA         NA         NA         NA         NA         NA #>  [7]         NA         NA         NA         NA         NA         NA #> [13]  2.6785714  6.7796610  6.8181818  4.6511628  3.3057851 10.3703704 #> [19] 14.8648649 17.0588235         NA         NA         NA         NA #> [25]         NA         NA         NA         NA         NA         NA #> [31]         NA 15.5778894 21.6080402 13.5869565 17.9012346 17.8082192 #> [37] 16.8674699 14.6198830  8.8888889 22.2797927 29.8342541 25.1366120 #> [43] 11.4678899 14.7826087 12.3966942 13.3971292 10.4712042  4.6511628 #> [49]  3.6082474  4.0816327 -4.0816327 -0.4237288 -3.4042553  2.1834061 #> [55]  8.6419753 14.3939394  7.7205882  9.2827004  8.5308057 12.7777778 #> [61] 13.9303483 18.6274510 23.9361702 13.6170213 18.5022026 15.3846154 #> [67] 19.3181818 20.5298013 18.4300341 20.4633205 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 63 entries ] #>  #> Indexed by:  time(AirPassengers) [133 (144)]  L(G(iAP[-(20:30)], c(0,1,12)), 0:1) # Lagged level, period and annual growth rates... #>         -- L1.--          G1       L1.G1      L12G1   L1.L12G1 #>   [1,] 112    NA          NA          NA         NA         NA #>   [2,] 118   112   5.3571429          NA         NA         NA #>   [3,] 132   118  11.8644068   5.3571429         NA         NA #>   [4,] 129   132  -2.2727273  11.8644068         NA         NA #>   [5,] 121   129  -6.2015504  -2.2727273         NA         NA #>   [6,] 135   121  11.5702479  -6.2015504         NA         NA #>   [7,] 148   135   9.6296296  11.5702479         NA         NA #>   [8,] 148   148   0.0000000   9.6296296         NA         NA #>   [9,] 136   148  -8.1081081   0.0000000         NA         NA #>  [10,] 119   136 -12.5000000  -8.1081081         NA         NA #>  [11,] 104   119 -12.6050420 -12.5000000         NA         NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 122 rows ] #> attr(,\"class\") #> [1] \"numeric\" \"matrix\"  #>  #> Indexed by:  time(AirPassengers) [133 (144)]    ## xts library(xts) #> Loading required package: zoo #>  #> Attaching package: ‘zoo’ #> The following objects are masked from ‘package:data.table’: #>  #>     yearmon, yearqtr #> The following objects are masked from ‘package:base’: #>  #>     as.Date, as.Date.numeric #>  #> ######################### Warning from 'xts' package ########################## #> #                                                                             # #> # The dplyr lag() function breaks how base R's lag() function is supposed to  # #> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       # #> # source() into this session won't work correctly.                            # #> #                                                                             # #> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add # #> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           # #> # dplyr from breaking base R's lag() function.                                # #> #                                                                             # #> # Code in packages is not affected. It's protected by R's namespace mechanism # #> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  # #> #                                                                             # #> ############################################################################### #>  #> Attaching package: ‘xts’ #> The following objects are masked from ‘package:data.table’: #>  #>     first, last #> The following objects are masked from ‘package:dplyr’: #>  #>     first, last library(zoo) # Needed for as.yearmon() and index() functions X <- wlddev |> fsubset(iso3c == \"DEU\", date, PCGDP:POP) %>% {   xts(num_vars(.), order.by = as.yearmon(.$date))   } |> ss(-(30:40)) %>% reindex(identity(index(.))) # Introducing a gap # plot(G(unindex(X))) diff(unindex(X))    # diff.xts gixes wrong result #>                PCGDP       LIFEEX GINI ODA      POP #> Jan 1961          NA           NA   NA  NA       NA #> Jan 1962          NA  0.197975610   NA  NA   562732 #> Jan 1963          NA  0.183536585   NA  NA   648152 #> Jan 1964          NA  0.168073171   NA  NA   688569 #> Jan 1965          NA  0.154097561   NA  NA   603984 #> Jan 1966          NA  0.138121951   NA  NA   645358 #> Jan 1967          NA  0.119585366   NA  NA   636616 #> Jan 1968          NA  0.102585366   NA  NA   351025 #> Jan 1969          NA  0.091097561   NA  NA   342978 #> Jan 1970          NA  0.085585366   NA  NA   615368 #> Jan 1971          NA  0.089097561   NA  NA   259607 #> Jan 1972   579.34931  0.103097561   NA  NA   143553 #> Jan 1973   770.40722  0.124121951   NA  NA   375610 #> Jan 1974   935.46605  0.149682927   NA  NA   248214 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 36 rows ] fdiff(X)            # fdiff gives right result #>                PCGDP       LIFEEX GINI ODA      POP #> Jan 1961          NA           NA   NA  NA       NA #> Jan 1962          NA  0.197975610   NA  NA   562732 #> Jan 1963          NA  0.183536585   NA  NA   648152 #> Jan 1964          NA  0.168073171   NA  NA   688569 #> Jan 1965          NA  0.154097561   NA  NA   603984 #> Jan 1966          NA  0.138121951   NA  NA   645358 #> Jan 1967          NA  0.119585366   NA  NA   636616 #> Jan 1968          NA  0.102585366   NA  NA   351025 #> Jan 1969          NA  0.091097561   NA  NA   342978 #> Jan 1970          NA  0.085585366   NA  NA   615368 #> Jan 1971          NA  0.089097561   NA  NA   259607 #> Jan 1972   579.34931  0.103097561   NA  NA   143553 #> Jan 1973   770.40722  0.124121951   NA  NA   375610 #> Jan 1974   935.46605  0.149682927   NA  NA   248214 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 36 rows ] #>  #> Indexed by:  index(.) [50 (61)]   # But xts range-based subsets do not work... if (FALSE) { # \\dontrun{ X[\"1980/\"] } # } # Thus a better way is not to index and perform ad-hoc omputations on the xts index X <- unindex(X) X[\"1980/\"] %>% fdiff(t = index(.)) # xts index is internally processed by timeid() #>                PCGDP       LIFEEX GINI ODA      POP #> Jan 1980          NA           NA   NA  NA       NA #> Jan 1981   309.62150  0.269365854   NA  NA   162226 #> Jan 1982    98.34802  0.272487805   NA  NA   119331 #> Jan 1983   -78.72879  0.276609756   NA  NA   -74541 #> Jan 1984   481.07564  0.278195122   NA  NA  -205084 #> Jan 1985   846.88924  0.277731707   NA  NA  -269597 #> Jan 1986   702.81344  0.271707317   NA  NA  -173812 #> Jan 1987   631.60512  0.259097561   NA  NA    35563 #> Jan 1988   359.26028  0.245951220   NA  NA   119484 #> Jan 1989   963.80172  0.232804878   NA  NA   304699 #> Jan 2001          NA           NA   NA  NA       NA #> Jan 2002   573.02200  0.402439024  1.5  NA   138417 #> Jan 2003  -140.79423 -0.100000000 -0.4  NA   138570 #> Jan 2004  -289.69804  0.151219512  0.1  NA    45681 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 17 rows ]  ## Of course you can also index plain vectors / matrices... options(oldopts)"},{"path":"https://sebkrantz.github.io/collapse/reference/is_unlistable.html","id":null,"dir":"Reference","previous_headings":"","what":"Unlistable Lists — is_unlistable","title":"Unlistable Lists — is_unlistable","text":"(nested) list atomic objects final nodes list-tree unlistable - checked is_unlistable.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/is_unlistable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unlistable Lists — is_unlistable","text":"","code":"is_unlistable(l, DF.as.list = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/is_unlistable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unlistable Lists — is_unlistable","text":"l list. DF..list logical. TRUE treats data frames like (sub-)lists; FALSE like atomic elements.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/is_unlistable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Unlistable Lists — is_unlistable","text":"is_unlistable DF..list = TRUE defined (rapply(l, .atomic)), whereas DF..list = FALSE yields checking using (unlist(rapply2d(l, function(x) .atomic(x) || .list(x)), use.names = FALSE)), assuming data frames lists composed atomic elements.  l contains data frames, latter can lot faster applying .atomic every data frame column.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/is_unlistable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unlistable Lists — is_unlistable","text":"logical(1) - TRUE FALSE.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/is_unlistable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unlistable Lists — is_unlistable","text":"","code":"l <- list(1, 2, list(3, 4, \"b\", FALSE)) is_unlistable(l) #> [1] TRUE l <- list(1, 2, list(3, 4, \"b\", FALSE, e ~ b)) is_unlistable(l) #> [1] FALSE"},{"path":"https://sebkrantz.github.io/collapse/reference/join.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast and Verbose Table Joins — join","title":"Fast and Verbose Table Joins — join","text":"Join two data frame like objects x y columns. Inspired polars default uses vectorized hash join algorithm (workhorse function fmatch), several verbose options.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/join.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast and Verbose Table Joins — join","text":"","code":"join(x, y,      on = NULL,      how = \"left\",      suffix = NULL,      validate = \"m:m\",      multiple = FALSE,      sort = FALSE,      keep.col.order = TRUE,      drop.dup.cols = FALSE,      verbose = .op[[\"verbose\"]],      require = NULL,      column = NULL,      attr = NULL,      ... )"},{"path":"https://sebkrantz.github.io/collapse/reference/join.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast and Verbose Table Joins — join","text":"x data frame-like object. result inherit attributes object. y data frame-like object join x. character. vector columns join . NULL uses intersect(names(x), names(y)). Use named vector match columns named differently x y, e.g. c(\"x_id\" = \"y_id\"). character. Join type: \"left\", \"right\", \"inner\", \"full\", \"semi\" \"anti\". first letter suffices. suffix character(1 2). Suffix add duplicate column names. NULL renames duplicate y columns paste(col, y_name, sep = \"_\"), y_name = .character(substitute(y)) .e. name data frame passed function. general, passing suffix length 1 rename y, whereas length 2 suffix rename x y, respectively. verbose > 0 message printed. validate character. (Optional) check join specified type. One \"1:1\", \"1:m\", \"m:1\" \"m:m\". default \"m:m\" perform checks. Checks done actual join step failure results error. Note argument affect result, triggers check. multiple logical. Handling rows x multiple matches y. default FALSE takes first match y. TRUE returns every match y (full cartesian product), increasing size joined table. sort logical. TRUE implements sort-merge-join: completely separate join algorithm sorts datasets join columns using radixorder matches rows without hashing. Note case result sorted join columns, whereas sort = FALSE preserves order rows x. keep.col.order logical. Keep order columns x? FALSE places columns front. drop.dup.cols instead renaming duplicate columns x y using suffix, option simply drops : TRUE \"y\" drops y, \"x\" x. verbose integer. Prints information join. One 0 (), 1 (default, see Details) 2 (additionally prints classes columns). Note: verbose > 0 validate != \"m:m\" invoke count argument fmatch, verbose = 0 slightly efficient. require (optional) named list form list(x = 1, y = 0.5, fail = \"warning\") (fail.want expressive) giving proportions records need matched action requirement fails (\"message\", \"warning\", \"error\"). elements list can omitted, default action \"error\". column (optional) name extra column generate output indicating dataset record came . TRUE calls column \".join\" (inspired STATA's '_merge' column). default column generated last column, , keep.col.order = FALSE, placed '' columns. column factor variable levels corresponding dataset names (inferred input) \"matched\" matched records. Alternatively, possible specify list 2, first element column name, second length 3 (!) vector levels e.g. column = list(\"joined\", c(\"x\", \"y\", \"x_y\")), \"x_y\" replaces \"matched\". column additional attribute \".cols\" giving join columns corresponding factor levels. See Examples. attr (optional) name attribute providing information join performed (including output fmatch) result. TRUE calls attribute \"join.match\". Note: also invokes count argument fmatch. ... arguments fmatch (sort = FALSE). Notably, overid can bet set 0 2 (default 1) control matching process join condition identifies records.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/join.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast and Verbose Table Joins — join","text":"verbose > 0, join prints compact summary join operation using cat. names x y can extracted (.character(substitute(x)) yields single string) displayed (otherwise 'x' 'y' used) followed respective join keys brackets. followed summary records used table. multiple = FALSE, first matches y used counted (first matches x = \"right\"). Note = \"full\" matches simply appended results table, thus may make sense use multiple = TRUE full join suspecting multiple matches. multiple = TRUE, join performs full cartesian product matching every key x every matching key y. can considerably increase size resulting table. memory checks performed (system simply run memory; usually terminate R). cases, join also determine average order join number records used table divided number unique matches display two tables 2 digits. example \"<4:1.5>\" means average 4 records x match 1.5 records y, implying average 4*1.5 = 6 records generated per unique match. multiple = FALSE \"1st\" displayed using table (y unless = \"right\"), indicating multiple matches first retained. Note order '1' either table must imply key unique value generated round(v, 2). sure keys uniqueness employ validate argument.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/join.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast and Verbose Table Joins — join","text":"data frame-like object type attributes x. \"row.names\" x preserved left-join operations.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/join.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast and Verbose Table Joins — join","text":"","code":"df1 <- data.frame(   id1 = c(1, 1, 2, 3),   id2 = c(\"a\", \"b\", \"b\", \"c\"),   name = c(\"John\", \"Jane\", \"Bob\", \"Carl\"),   age = c(35, 28, 42, 50) ) df2 <- data.frame(   id1 = c(1, 2, 3, 3),   id2 = c(\"a\", \"b\", \"c\", \"e\"),   salary = c(60000, 55000, 70000, 80000),   dept = c(\"IT\", \"Marketing\", \"Sales\", \"IT\") )  # Different types of joins for(i in c(\"l\",\"i\",\"r\",\"f\",\"s\",\"a\"))     join(df1, df2, how = i) |> print() #> left join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age salary      dept #> 1   1   a John  35  60000        IT #> 2   1   b Jane  28     NA      <NA> #> 3   2   b  Bob  42  55000 Marketing #> 4   3   c Carl  50  70000     Sales #> inner join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age salary      dept #> 1   1   a John  35  60000        IT #> 2   2   b  Bob  42  55000 Marketing #> 3   3   c Carl  50  70000     Sales #> right join: df1[id1, id2] 3/4 (75%) <1st:1> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age salary      dept #> 1   1   a John  35  60000        IT #> 2   2   b  Bob  42  55000 Marketing #> 3   3   c Carl  50  70000     Sales #> 4   3   e <NA>  NA  80000        IT #> full join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age salary      dept #> 1   1   a John  35  60000        IT #> 2   1   b Jane  28     NA      <NA> #> 3   2   b  Bob  42  55000 Marketing #> 4   3   c Carl  50  70000     Sales #> 5   3   e <NA>  NA  80000        IT #> semi join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age #> 1   1   a John  35 #> 2   2   b  Bob  42 #> 3   3   c Carl  50 #> anti join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age #> 1   1   b Jane  28  # With multiple matches for(i in c(\"l\",\"i\",\"r\",\"f\",\"s\",\"a\"))     join(df1, df2, on = \"id2\", how = i, multiple = TRUE) |> print() #> left join: df1[id2] 4/4 (100%) <1.33:1> df2[id2] 3/4 (75%) #> duplicate columns: id1 => renamed using suffix '_df2' for y #>   id1 id2 name age id1_df2 salary      dept #> 1   1   a John  35       1  60000        IT #> 2   1   b Jane  28       2  55000 Marketing #> 3   2   b  Bob  42       2  55000 Marketing #> 4   3   c Carl  50       3  70000     Sales #> inner join: df1[id2] 4/4 (100%) <1.33:1> df2[id2] 3/4 (75%) #> duplicate columns: id1 => renamed using suffix '_df2' for y #>   id1 id2 name age id1_df2 salary      dept #> 1   1   a John  35       1  60000        IT #> 2   1   b Jane  28       2  55000 Marketing #> 3   2   b  Bob  42       2  55000 Marketing #> 4   3   c Carl  50       3  70000     Sales #> right join: df1[id2] 4/4 (100%) <1.33:1> df2[id2] 3/4 (75%) #> duplicate columns: id1 => renamed using suffix '_df2' for y #>   id1 id2 name age id1_df2 salary      dept #> 1   1   a John  35       1  60000        IT #> 2   1   b Jane  28       2  55000 Marketing #> 3   2   b  Bob  42       2  55000 Marketing #> 4   3   c Carl  50       3  70000     Sales #> 5  NA   e <NA>  NA       3  80000        IT #> full join: df1[id2] 4/4 (100%) <1.33:1> df2[id2] 3/4 (75%) #> duplicate columns: id1 => renamed using suffix '_df2' for y #>   id1 id2 name age id1_df2 salary      dept #> 1   1   a John  35       1  60000        IT #> 2   1   b Jane  28       2  55000 Marketing #> 3   2   b  Bob  42       2  55000 Marketing #> 4   3   c Carl  50       3  70000     Sales #> 5  NA   e <NA>  NA       3  80000        IT #> semi join: df1[id2] 4/4 (100%) <1.33:1> df2[id2] 3/4 (75%) #>   id1 id2 name age #> 1   1   a John  35 #> 2   1   b Jane  28 #> 3   2   b  Bob  42 #> 4   3   c Carl  50 #> anti join: df1[id2] 4/4 (100%) <1.33:1> df2[id2] 3/4 (75%) #> [1] id1  id2  name age  #> <0 rows> (or 0-length row.names)  # Adding join column: useful esp. for full join join(df1, df2, how = \"f\", column = TRUE) #> full join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age salary      dept   .join #> 1   1   a John  35  60000        IT matched #> 2   1   b Jane  28     NA      <NA>     df1 #> 3   2   b  Bob  42  55000 Marketing matched #> 4   3   c Carl  50  70000     Sales matched #> 5   3   e <NA>  NA  80000        IT     df2 # Custom column + rearranging join(df1, df2, how = \"f\", column = list(\"join\", c(\"x\", \"y\", \"x_y\")), keep = FALSE) #> full join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 join name age salary      dept #> 1   1   a  x_y John  35  60000        IT #> 2   1   b    x Jane  28     NA      <NA> #> 3   2   b  x_y  Bob  42  55000 Marketing #> 4   3   c  x_y Carl  50  70000     Sales #> 5   3   e    y <NA>  NA  80000        IT  # Attaching match attribute str(join(df1, df2, attr = TRUE)) #> left join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #> 'data.frame':\t4 obs. of  6 variables: #>  $ id1   : num  1 1 2 3 #>  $ id2   : chr  \"a\" \"b\" \"b\" \"c\" #>  $ name  : chr  \"John\" \"Jane\" \"Bob\" \"Carl\" #>  $ age   : num  35 28 42 50 #>  $ salary: num  60000 NA 55000 70000 #>  $ dept  : chr  \"IT\" NA \"Marketing\" \"Sales\" #>  - attr(*, \"join.match\")=List of 3 #>   ..$ call   : language join(x = df1, y = df2, attr = TRUE) #>   ..$ on.cols:List of 2 #>   .. ..$ x: chr [1:2] \"id1\" \"id2\" #>   .. ..$ y: chr [1:2] \"id1\" \"id2\" #>   ..$ match  : 'qG' int [1:4] 1 NA 2 3 #>   .. ..- attr(*, \"N.nomatch\")= int 1 #>   .. ..- attr(*, \"N.groups\")= int 4 #>   .. ..- attr(*, \"N.distinct\")= int 3"},{"path":"https://sebkrantz.github.io/collapse/reference/ldepth.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the Depth / Level of Nesting of a List — ldepth","title":"Determine the Depth / Level of Nesting of a List — ldepth","text":"ldepth provides depth list list-like structure.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ldepth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the Depth / Level of Nesting of a List — ldepth","text":"","code":"ldepth(l, DF.as.list = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/ldepth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the Depth / Level of Nesting of a List — ldepth","text":"l list. DF..list logical. TRUE treats data frames like (sub-)lists; FALSE like atomic elements.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ldepth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Determine the Depth / Level of Nesting of a List — ldepth","text":"depth level nesting list list-like structure (e.g. model object) found recursing bottom list adding integer count 1 level passed. example depth data frame 1. data frame list-columns, depth 2. However reasons efficiency, l data frame DF..list = FALSE, data frames found inside l checked list column's assumed depth 1.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/ldepth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the Depth / Level of Nesting of a List — ldepth","text":"single integer indicating depth list.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/ldepth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the Depth / Level of Nesting of a List — ldepth","text":"","code":"l <- list(1, 2) ldepth(l) #> [1] 1 l <- list(1, 2, mtcars) ldepth(l) #> [1] 1 ldepth(l, DF.as.list = FALSE) #> [1] 1 l <- list(1, 2, list(4, 5, list(6, mtcars))) ldepth(l) #> [1] 3 ldepth(l, DF.as.list = FALSE) #> [1] 3"},{"path":"https://sebkrantz.github.io/collapse/reference/list-processing.html","id":null,"dir":"Reference","previous_headings":"","what":"List Processing — list-processing","title":"List Processing — list-processing","text":"collapse provides following set functions efficiently work lists R objects: Search Identification is_unlistable checks whether (nested) list composed atomic objects final nodes, thus unlistable atomic vector using unlist. ldepth determines level nesting list (.e. maximum number nodes list-tree). has_elem searches elements list using element names, regular expressions applied element names, function applied elements, returns TRUE matches found. Subsetting atomic_elem examines top-level list returns sublist atomic elements. Conversely list_elem returns sublist elements lists list-like objects. reg_elem irreg_elem recursive versions former. reg_elem extracts 'regular' part list-tree leading atomic elements final nodes, irreg_elem extracts 'irregular' part list tree leading non-atomic elements final nodes. (Tip: try calling lm object). Naturally lists l, is_unlistable(reg_elem(l)) evaluates TRUE. get_elem extracts elements list using element names, regular expressions applied element names, function applied elements, element-indices used subset lowest-level sub-lists. default result presented simplified list containing matching elements. keep.tree option however get_elem can also used subset lists .e. maintain full tree cut non-matching branches. Splitting Transposition rsplit recursively splits vector data frame subsets according combinations (multiple) vectors / factors - default returning (nested) list. flatten = TRUE, list flattened yielding result split. rsplit also faster split, particularly data frames. t_list efficiently transposes nested lists lists, obtained splitting data frame multiple variables using rsplit. Apply Functions rapply2d recursive version lapply two key differences rapply apply function nested lists data frames list-based objects. Unlisting / Row-Binding unlist2d efficiently unlists unlistable lists 2-dimensions creates data frame (data.table) representation list. done recursively flattening row-binding R objects list creating identifier columns level list-tree (optionally) saving row-names objects separate column. unlist2d can thus also understood recursive generalization .call(rbind, l), lists vectors, data frames, arrays heterogeneous objects. simpler version non-recursive row-binding lists lists / data.frames, also available rowbind.","code":""},{"path":[]},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/pad.html","id":null,"dir":"Reference","previous_headings":"","what":"Pad Matrix-Like Objects with a Value — pad","title":"Pad Matrix-Like Objects with a Value — pad","text":"pad function inserts elements / rows filled value vector matrix data frame X positions given . particularly useful expand objects returned statistical procedures remove missing values original data dimensions.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pad Matrix-Like Objects with a Value — pad","text":"","code":"pad(X, i, value = NA, method = c(\"auto\", \"xpos\", \"vpos\"))"},{"path":"https://sebkrantz.github.io/collapse/reference/pad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pad Matrix-Like Objects with a Value — pad","text":"X vector, matrix, data frame list equal-length columns. either integer (positive negative) logical vector giving positions / rows X value's inserted, , alternatively, positive integer vector length() == NROW(X), gaps indices value's can inserted, logical vector sum() == NROW(X) value's can inserted FALSE values logical vector. See also method Examples. value scalar value replicated inserted X positions / rows given . Default NA. method integer string specifying use . options :","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pad Matrix-Like Objects with a Value — pad","text":"X elements / rows filled value inserted positions given .","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/pad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pad Matrix-Like Objects with a Value — pad","text":"","code":"v <- 1:3  pad(v, 1:2)       # Automatic selection of method \"vpos\" #> [1] NA NA  1  2  3 pad(v, -(1:2))    # Same thing #> [1] NA NA  1  2  3 pad(v, c(TRUE, TRUE, FALSE, FALSE, FALSE)) # Same thing #> [1] NA NA  1  2  3  pad(v, c(1, 3:4)) # Automatic selection of method \"xpos\" #> [1]  1 NA  2  3 pad(v, c(TRUE, FALSE, TRUE, TRUE, FALSE))  # Same thing #> [1]  1 NA  2  3 NA  head(pad(wlddev, 1:3)) # Insert 3 missing rows at the beginning of the data #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 2        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 3        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 4 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1     NA   NA        NA      NA #> 2     NA   NA        NA      NA #> 3     NA   NA        NA      NA #> 4 32.446   NA 116769997 8996973 #> 5 32.962   NA 232080002 9169410 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(pad(wlddev, 2:4)) # ... at rows positions 2-4 #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 3        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 4        <NA>  <NA>       <NA>   NA     NA       <NA>       <NA>    NA    NA #> 5 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2     NA   NA        NA      NA #> 3     NA   NA        NA      NA #> 4     NA   NA        NA      NA #> 5 32.962   NA 232080002 9169410 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  # pad() is mostly useful for statistical models which only use the complete cases: mod <- lm(LIFEEX ~ PCGDP, wlddev) # Generating a residual column in the original data (automatic selection of method \"vpos\") settfm(wlddev, resid = pad(resid(mod), mod$na.action)) #> Error in mod$na.action: object of type 'builtin' is not subsettable # Another way to do it: r <- resid(mod) i <- as.integer(names(r)) resid2 <- pad(r, i)        # automatic selection of method \"xpos\" # here we need to add some elements as flast(i) < nrow(wlddev) resid2 <- c(resid2, rep(NA, nrow(wlddev)-length(resid2))) # See that these are identical: identical(unattrib(wlddev$resid), resid2) #> [1] FALSE  # Can also easily get a model matrix at the dimensions of the original data mm <- pad(model.matrix(mod), mod$na.action)"},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast and Easy Data Reshaping — pivot","title":"Fast and Easy Data Reshaping — pivot","text":"pivot() collapse's data reshaping command. combines longer-, wider-, recast-pivoting functionality single parsimonious API. Notably, can also accommodate variable labels.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast and Easy Data Reshaping — pivot","text":"","code":"pivot(data,               # Summary of Documentation:       ids = NULL,         # identifier cols to preserve       values = NULL,      # cols containing the data       names = NULL,       # name(s) of new col(s) | col(s) containing names       labels = NULL,      # name of new labels col | col(s) containing labels       how = \"longer\",     # method: \"longer\"/\"l\", \"wider\"/\"w\" or \"recast\"/\"r\"       na.rm = FALSE,      # remove rows missing 'values' in reshaped data       factor = c(\"names\", \"labels\"), # create new id col(s) as factor variable(s)?       check.dups = FALSE, # detect duplicate 'ids'+'names' combinations        # Only apply if how = \"wider\" or \"recast\"       FUN = \"last\",       # aggregation function (internal or external)       FUN.args = NULL,    # list of arguments passed to aggregation function       nthreads = .op[[\"nthreads\"]], # minor gains as grouping remains serial       fill = NULL,        # value to insert for unbalanced data (default NA/NULL)       drop = TRUE,        # drop unused levels (=columns) if 'names' is factor       sort = FALSE,       # \"ids\": sort 'ids' and/or \"names\": alphabetic casting        # Only applies if how = \"wider\" with multiple long columns ('values')       transpose = FALSE   # \"columns\": applies t_list() before flattening, and/or )                         # \"names\": sets names nami_colj. default: colj_nami"},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast and Easy Data Reshaping — pivot","text":"data data frame-like object (list equal-length columns). ids identifier columns keep. Specified using column names, indices, logical vector identifier function e.g. is_categorical. values columns containing data reshaped. Specified like ids. names names columns generate, retrieve variable names : labels names columns generate, retrieve variable labels : character. pivoting method: one \"longer\", \"wider\" \"recast\". can abbreviated first letter .e. \"l\"/\"w\"/\"r\". na.rm logical. TRUE remove missing values reshaped data row missing data columns - selected 'values'. wide/recast pivots using internal FUN's \"first\"/\"last\"/\"count\", also toggles skipping missing values. factor character. Whether generate new 'names' /'labels' columns factor variables. generally recommended factors memory efficient character vectors also faster subsequent filtering grouping. Internally, argument evaluated factor <- c(\"names\", \"labels\") %% factor, passing anything \"names\" /\"labels\" disable . check.dups logical. TRUE checks duplicate 'ids'+'names' combinations, , 'labels' specified, also duplicate 'names'+'labels' combinations. default FALSE implies algorithm just runs data, leading effectively FUN option executed (default last value). See Details. FUN function aggregate values. present, single function allowed. Fast Statistical Functions receive vectorized execution. maximum efficiency, small set internal functions provided: \"first\", \"last\", \"count\", \"sum\", \"mean\", \"min\", \"max\". options \"first\"/\"last\"/\"count\" setting na.rm = TRUE skips missing values. options \"sum\"/\"mean\"/\"min\"/\"max\" missing values always skipped (see Details ). fill argument ignored \"count\"/\"sum\"/\"mean\"/\"min\"/\"max\" (\"count\"/\"sum\" force fill = 0 else NA used). FUN.args (optional) list arguments passed FUN (using external function). Data-length arguments weight vectors supported. nthreads integer. = \"wider\"|\"recast\": number threads use OpenMP (default get_collapse(\"nthreads\"), initialized 1). distribution values columns = \"wider\"|\"recast\" multithreaded . Since grouping id columns long data frame expensive serial, gains minor. = \"long\", multithreading make much sense expensive operation allocating long results vectors. rest couple memset()'s C copy values. fill = \"wider\"|\"recast\": value insert 'ids'-'names' combinations present long format. NULL uses NA atomic vectors NULL lists. drop logical. = \"wider\"|\"recast\" 'names' single factor variable: TRUE check drop unused levels factor, avoiding generation empty columns. sort = \"wider\"|\"recast\": specifying \"ids\" applies ordered grouping id-columns, returning data sorted ids. Specifying \"names\" sorts names casting (unless 'names' factor), yielding columns cast alphabetic order. options can passed character vector, , alternatively, TRUE can used enable . transpose = \"wider\"|\"recast\" multiple columns selected 'values': specifying \"columns\" applies t_list result flattening, resulting different column order. Specifying \"names\" generates names form nami_colj, instead colj_nami. options can passed character vector, , alternatively, TRUE can used enable .","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast and Easy Data Reshaping — pivot","text":"Pivot wider essentially works follows: compute g_rows = group(ids) also g_cols = group(names) (using group sort = FALSE). g_rows gives row-numbers wider data frame g_cols column numbers. , C function generates wide data frame runs long column ('values'), assigning value corresponding row column wide frame. process FUN always applied. default, \"last\", nothing , .e., duplicates, values overwritten. \"first\" works similarly just C-loop executed way around. hard-coded options count, sum, average, compare observations fly. Missing values internally skipped statistical functions way distinguish incoming NA initial NA - apart counting occurrences using internal structure size result data frame costly thus implemented. passing R-function FUN, data grouped using g_full = group(g_rows, g_cols), aggregated groups, expanded full length using TRA entering reshaping algorithm. Thus, significantly expensive optimized internal functions. Fast Statistical Functions aggregation vectorized across groups, functions applied using - far slowest option. check.dups = TRUE, check form  fnunique(list(g_rows, g_cols)) < fnrow(data) run, informative warning issued duplicates found. Recast pivoting works similarly. long pivots FUN ignored check simply amounts fnunique(ids) < fnrow(data).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast and Easy Data Reshaping — pivot","text":"reshaped data frame class attributes (except 'names'/'row-names') input frame.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast and Easy Data Reshaping — pivot","text":"Leaving either 'ids' 'values' empty assign columns (except \"variable\" = \"wider\"|\"recast\") non-specified argument. also possible leave empty, e.g. complete melting = \"wider\" data transposition = \"recast\" (similar data.table::transpose supporting multiple names columns variable labels). See Examples. pivot currently support concurrently melting/pivoting longer multiple columns. See data.table::melt pivot_longer tidyr tidytable efficient alternative feature. also possible achieve just little bit programming. example provided .","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/pivot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast and Easy Data Reshaping — pivot","text":"","code":"# -------------------------------- PIVOT LONGER --------------------------------- # Simple Melting (Reshaping Long) pivot(mtcars) |> head() #>   variable value #> 1      mpg  21.0 #> 2      mpg  21.0 #> 3      mpg  22.8 #> 4      mpg  21.4 #> 5      mpg  18.7 #> 6      mpg  18.1 pivot(iris, \"Species\") |> head() #>   Species     variable value #> 1  setosa Sepal.Length   5.1 #> 2  setosa Sepal.Length   4.9 #> 3  setosa Sepal.Length   4.7 #> 4  setosa Sepal.Length   4.6 #> 5  setosa Sepal.Length   5.0 #> 6  setosa Sepal.Length   5.4 pivot(iris, values = 1:4) |> head() # Same thing #>   Species     variable value #> 1  setosa Sepal.Length   5.1 #> 2  setosa Sepal.Length   4.9 #> 3  setosa Sepal.Length   4.7 #> 4  setosa Sepal.Length   4.6 #> 5  setosa Sepal.Length   5.0 #> 6  setosa Sepal.Length   5.4  # Using collapse's datasets head(wlddev) #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] pivot(wlddev, 1:8, na.rm = TRUE) |> head() #>       country iso3c       date year decade     region     income  OECD variable #> 1 Afghanistan   AFG 2003-01-01 2002   2000 South Asia Low income FALSE    PCGDP #> 2 Afghanistan   AFG 2004-01-01 2003   2000 South Asia Low income FALSE    PCGDP #> 3 Afghanistan   AFG 2005-01-01 2004   2000 South Asia Low income FALSE    PCGDP #> 4 Afghanistan   AFG 2006-01-01 2005   2000 South Asia Low income FALSE    PCGDP #> 5 Afghanistan   AFG 2007-01-01 2006   2000 South Asia Low income FALSE    PCGDP #> 6 Afghanistan   AFG 2008-01-01 2007   2000 South Asia Low income FALSE    PCGDP #>      value #> 1 330.3036 #> 2 343.0809 #> 3 333.2167 #> 4 357.2347 #> 5 365.2845 #> 6 405.5490 pivot(wlddev, c(\"iso3c\", \"year\"), c(\"PCGDP\", \"LIFEEX\"), na.rm = TRUE) |> head() #>   iso3c year variable    value #> 1   AFG 2002    PCGDP 330.3036 #> 2   AFG 2003    PCGDP 343.0809 #> 3   AFG 2004    PCGDP 333.2167 #> 4   AFG 2005    PCGDP 357.2347 #> 5   AFG 2006    PCGDP 365.2845 #> 6   AFG 2007    PCGDP 405.5490 head(GGDC10S) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #> 4     BWA        SSA Sub-saharan Africa       VA 1963  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA #> 4  NA   NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] pivot(GGDC10S, 1:5, names = list(\"Sectorcode\", \"Value\"), na.rm = TRUE) |> head() #>   Country Regioncode             Region Variable Year Sectorcode    Value #> 1     BWA        SSA Sub-saharan Africa       VA 1964        AGR 16.30154 #> 2     BWA        SSA Sub-saharan Africa       VA 1965        AGR 15.72700 #> 3     BWA        SSA Sub-saharan Africa       VA 1966        AGR 17.68066 #> 4     BWA        SSA Sub-saharan Africa       VA 1967        AGR 19.14591 #> 5     BWA        SSA Sub-saharan Africa       VA 1968        AGR 21.09957 #> 6     BWA        SSA Sub-saharan Africa       VA 1969        AGR 21.86221 # Can also set by name: variable and/or value. Note that 'value' here remains lowercase pivot(GGDC10S, 1:5, names = list(variable = \"Sectorcode\"), na.rm = TRUE) |> head() #>   Country Regioncode             Region Variable Year Sectorcode    value #> 1     BWA        SSA Sub-saharan Africa       VA 1964        AGR 16.30154 #> 2     BWA        SSA Sub-saharan Africa       VA 1965        AGR 15.72700 #> 3     BWA        SSA Sub-saharan Africa       VA 1966        AGR 17.68066 #> 4     BWA        SSA Sub-saharan Africa       VA 1967        AGR 19.14591 #> 5     BWA        SSA Sub-saharan Africa       VA 1968        AGR 21.09957 #> 6     BWA        SSA Sub-saharan Africa       VA 1969        AGR 21.86221  # Melting including saving labels pivot(GGDC10S, 1:5, na.rm = TRUE, labels = TRUE) |> head() #>   Country Regioncode             Region Variable Year variable        label #> 1     BWA        SSA Sub-saharan Africa       VA 1964      AGR Agriculture  #> 2     BWA        SSA Sub-saharan Africa       VA 1965      AGR Agriculture  #> 3     BWA        SSA Sub-saharan Africa       VA 1966      AGR Agriculture  #> 4     BWA        SSA Sub-saharan Africa       VA 1967      AGR Agriculture  #> 5     BWA        SSA Sub-saharan Africa       VA 1968      AGR Agriculture  #> 6     BWA        SSA Sub-saharan Africa       VA 1969      AGR Agriculture  #>      value #> 1 16.30154 #> 2 15.72700 #> 3 17.68066 #> 4 19.14591 #> 5 21.09957 #> 6 21.86221 pivot(GGDC10S, 1:5, na.rm = TRUE, labels = \"description\") |> head() #>   Country Regioncode             Region Variable Year variable  description #> 1     BWA        SSA Sub-saharan Africa       VA 1964      AGR Agriculture  #> 2     BWA        SSA Sub-saharan Africa       VA 1965      AGR Agriculture  #> 3     BWA        SSA Sub-saharan Africa       VA 1966      AGR Agriculture  #> 4     BWA        SSA Sub-saharan Africa       VA 1967      AGR Agriculture  #> 5     BWA        SSA Sub-saharan Africa       VA 1968      AGR Agriculture  #> 6     BWA        SSA Sub-saharan Africa       VA 1969      AGR Agriculture  #>      value #> 1 16.30154 #> 2 15.72700 #> 3 17.68066 #> 4 19.14591 #> 5 21.09957 #> 6 21.86221  # Also assigning new labels pivot(GGDC10S, 1:5, na.rm = TRUE, labels = list(\"description\",             c(\"Sector Code\", \"Sector Description\", \"Value\"))) |> namlab() #>      Variable              Label #> 1     Country            Country #> 2  Regioncode        Region code #> 3      Region             Region #> 4    Variable           Variable #> 5        Year               Year #> 6    variable        Sector Code #> 7 description Sector Description #> 8       value              Value  # Can leave out value column by providing named vector of labels pivot(GGDC10S, 1:5, na.rm = TRUE, labels = list(\"description\",           c(variable = \"Sector Code\", description = \"Sector Description\"))) |> namlab() #>      Variable              Label #> 1     Country            Country #> 2  Regioncode        Region code #> 3      Region             Region #> 4    Variable           Variable #> 5        Year               Year #> 6    variable        Sector Code #> 7 description Sector Description #> 8       value               <NA>  # Now here is a nice example that is explicit and respects the dataset naming conventions pivot(GGDC10S, ids = 1:5, na.rm = TRUE,       names = list(variable = \"Sectorcode\",                    value = \"Value\"),       labels = list(name = \"Sector\",                     new = c(Sectorcode = \"GGDC10S Sector Code\",                             Sector = \"Long Sector Description\",                             Value = \"Employment or Value Added\"))) |>   namlab(N = TRUE, Nd = TRUE, class = TRUE) #>     Variable     Class     N Ndist                     Label #> 1    Country character 46942    43                   Country #> 2 Regioncode character 46942     6               Region code #> 3     Region character 46942     6                    Region #> 4   Variable character 46942     2                  Variable #> 5       Year   numeric 46942    67                      Year #> 6 Sectorcode    factor 46942    11       GGDC10S Sector Code #> 7     Sector    factor 46942    11   Long Sector Description #> 8      Value   numeric 46942 46478 Employment or Value Added  # Note that pivot() currently does not support melting to multiple columns # But you can tackle the issue with a bit of programming: wide <- pivot(GGDC10S, c(\"Country\", \"Year\"), c(\"AGR\", \"MAN\", \"SUM\"), \"Variable\",               how = \"wider\", na.rm = TRUE) head(wide) #>   Country Year   AGR_VA  AGR_EMP    MAN_VA  MAN_EMP   SUM_VA  SUM_EMP #> 1     BWA 1964 16.30154 152.1179 0.7365696 2.420000 37.48229 173.8829 #> 2     BWA 1965 15.72700 153.2971 1.0181992 2.330406 39.34710 178.7637 #> 3     BWA 1966 17.68066 153.8867 0.8038415 1.281642 43.14677 179.7183 #> 4     BWA 1967 19.14591 155.0659 0.9378151 1.041623 41.39519 178.9181 #> 5     BWA 1968 21.09957 156.2451 0.7502521 1.069332 41.14259 181.9292 #> 6     BWA 1969 21.86221 157.4243 2.1396077 2.124402 51.22160 188.0569 library(magrittr) wide %>% {av(pivot(., 1:2, grep(\"_VA\", names(.))), pivot(gvr(., \"_EMP\")))} |> head() #>   Country Year variable    value variable    value #> 1     BWA 1964   AGR_VA 16.30154  AGR_EMP 152.1179 #> 2     BWA 1965   AGR_VA 15.72700  AGR_EMP 153.2971 #> 3     BWA 1966   AGR_VA 17.68066  AGR_EMP 153.8867 #> 4     BWA 1967   AGR_VA 19.14591  AGR_EMP 155.0659 #> 5     BWA 1968   AGR_VA 21.09957  AGR_EMP 156.2451 #> 6     BWA 1969   AGR_VA 21.86221  AGR_EMP 157.4243 wide %>% {av(av(gv(., 1:2), rm_stub(gvr(., \"_VA\"), \"_VA\", pre = FALSE)) |>                    pivot(1:2, names = list(\"Sectorcode\", \"VA\"), labels = \"Sector\"),              EMP = vec(gvr(., \"_EMP\")))} |> head() #>   Country Year Sectorcode       Sector       VA      EMP #> 1     BWA 1964        AGR Agriculture  16.30154 152.1179 #> 2     BWA 1965        AGR Agriculture  15.72700 153.2971 #> 3     BWA 1966        AGR Agriculture  17.68066 153.8867 #> 4     BWA 1967        AGR Agriculture  19.14591 155.0659 #> 5     BWA 1968        AGR Agriculture  21.09957 156.2451 #> 6     BWA 1969        AGR Agriculture  21.86221 157.4243 rm(wide)  # -------------------------------- PIVOT WIDER --------------------------------- iris_long <- pivot(iris, \"Species\") # Getting a long frame head(iris_long) #>   Species     variable value #> 1  setosa Sepal.Length   5.1 #> 2  setosa Sepal.Length   4.9 #> 3  setosa Sepal.Length   4.7 #> 4  setosa Sepal.Length   4.6 #> 5  setosa Sepal.Length   5.0 #> 6  setosa Sepal.Length   5.4 # If 'names'/'values' not supplied, searches for 'variable' and 'value' columns pivot(iris_long, how = \"wider\") #>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa          5.0         3.3          1.4         0.2 #> 2 versicolor          5.7         2.8          4.1         1.3 #> 3  virginica          5.9         3.0          5.1         1.8 # But here the records are not identified by 'Species': thus aggregation with last value: pivot(iris_long, how = \"wider\", check = TRUE) # issues a warning #> Warning: duplicates detected: there are 12 unique combinations of id- and name-columns, but the data has 600 rows. This means you have on average 50 duplicates per id-name-combination. If how = 'wider', pivot() will take the last of those duplicates in first-appearance-order. Consider aggregating your data e.g. using collap() before applying pivot(). #>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa          5.0         3.3          1.4         0.2 #> 2 versicolor          5.7         2.8          4.1         1.3 #> 3  virginica          5.9         3.0          5.1         1.8 rm(iris_long)  # This works better, these two are inverse operations wlddev |> pivot(1:8) |> pivot(how = \"w\") |> head() #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] # ...but not perfect, we loose labels namlab(wlddev) #>    Variable #> 1   country #> 2     iso3c #> 3      date #> 4      year #> 5    decade #> 6    region #> 7    income #> 8      OECD #> 9     PCGDP #> 10   LIFEEX #> 11     GINI #> 12      ODA #> 13      POP #>                                                                                Label #> 1                                                                       Country Name #> 2                                                                       Country Code #> 3                                                         Date Recorded (Fictitious) #> 4                                                                               Year #> 5                                                                             Decade #> 6                                                                             Region #> 7                                                                       Income Level #> 8                                                            Is OECD Member Country? #> 9                                                 GDP per capita (constant 2010 US$) #> 10                                           Life expectancy at birth, total (years) #> 11                                                  Gini index (World Bank estimate) #> 12 Net official development assistance and official aid received (constant 2018 US$) #> 13                                                                 Population, total wlddev |> pivot(1:8) |> pivot(how = \"w\") |> namlab() #>    Variable                      Label #> 1   country               Country Name #> 2     iso3c               Country Code #> 3      date Date Recorded (Fictitious) #> 4      year                       Year #> 5    decade                     Decade #> 6    region                     Region #> 7    income               Income Level #> 8      OECD    Is OECD Member Country? #> 9     PCGDP                       <NA> #> 10   LIFEEX                       <NA> #> 11     GINI                       <NA> #> 12      ODA                       <NA> #> 13      POP                       <NA> # But pivot() supports labels: these are perfect inverse operations wlddev |> pivot(1:8, labels = \"label\") |> print(max = 50) |> # Notice the \"label\" column   pivot(how = \"w\", labels = \"label\") |> namlab() #>       country iso3c       date year decade     region     income  OECD variable #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    PCGDP #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    PCGDP #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    PCGDP #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    PCGDP #>                                label value #> 1 GDP per capita (constant 2010 US$)    NA #> 2 GDP per capita (constant 2010 US$)    NA #> 3 GDP per capita (constant 2010 US$)    NA #> 4 GDP per capita (constant 2010 US$)    NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 65876 rows ] #>    Variable #> 1   country #> 2     iso3c #> 3      date #> 4      year #> 5    decade #> 6    region #> 7    income #> 8      OECD #> 9     PCGDP #> 10   LIFEEX #> 11     GINI #> 12      ODA #> 13      POP #>                                                                                Label #> 1                                                                       Country Name #> 2                                                                       Country Code #> 3                                                         Date Recorded (Fictitious) #> 4                                                                               Year #> 5                                                                             Decade #> 6                                                                             Region #> 7                                                                       Income Level #> 8                                                            Is OECD Member Country? #> 9                                                 GDP per capita (constant 2010 US$) #> 10                                           Life expectancy at birth, total (years) #> 11                                                  Gini index (World Bank estimate) #> 12 Net official development assistance and official aid received (constant 2018 US$) #> 13                                                                 Population, total  # If the data does not have 'variable'/'value' cols: need to specify 'names'/'values' # Using a single column: pivot(GGDC10S, c(\"Country\", \"Year\"), \"SUM\", \"Variable\", how = \"w\") |> head() #>   Country Year       VA      EMP #> 1     BWA 1960       NA       NA #> 2     BWA 1961       NA       NA #> 3     BWA 1962       NA       NA #> 4     BWA 1963       NA       NA #> 5     BWA 1964 37.48229 173.8829 #> 6     BWA 1965 39.34710 178.7637 SUM_wide <- pivot(GGDC10S, c(\"Country\", \"Year\"), \"SUM\", \"Variable\", how = \"w\", na.rm = TRUE) head(SUM_wide) # na.rm = TRUE here removes all new rows completely missing data #>   Country Year       VA      EMP #> 1     BWA 1964 37.48229 173.8829 #> 2     BWA 1965 39.34710 178.7637 #> 3     BWA 1966 43.14677 179.7183 #> 4     BWA 1967 41.39519 178.9181 #> 5     BWA 1968 41.14259 181.9292 #> 6     BWA 1969 51.22160 188.0569 tail(SUM_wide) # But there may still be NA's, notice the NA in the final row #>      Country Year        VA      EMP #> 2341     EGY 2008  844222.3 21039.90 #> 2342     EGY 2009  978684.0 21863.86 #> 2343     EGY 2010 1133629.1 22019.88 #> 2344     EGY 2011 1290896.1 22219.39 #> 2345     EGY 2012 1487175.1 22532.56 #> 2346     EGY 2013 1650962.8       NA # We could use fill to set another value pivot(GGDC10S, c(\"Country\", \"Year\"), \"SUM\", \"Variable\", how = \"w\",       na.rm = TRUE, fill = -9999) |> tail() #>      Country Year        VA      EMP #> 2341     EGY 2008  844222.3 21039.90 #> 2342     EGY 2009  978684.0 21863.86 #> 2343     EGY 2010 1133629.1 22019.88 #> 2344     EGY 2011 1290896.1 22219.39 #> 2345     EGY 2012 1487175.1 22532.56 #> 2346     EGY 2013 1650962.8 -9999.00 # This will keep the label of \"SUM\", unless we supply a column with new labels namlab(SUM_wide) #>   Variable                   Label #> 1  Country                 Country #> 2     Year                    Year #> 3       VA Summation of sector GDP #> 4      EMP Summation of sector GDP # Such a column is not available here, but we could use \"Variable\" twice pivot(GGDC10S, c(\"Country\", \"Year\"), \"SUM\", \"Variable\", \"Variable\", how = \"w\",       na.rm = TRUE) |> namlab() #>   Variable   Label #> 1  Country Country #> 2     Year    Year #> 3       VA      VA #> 4      EMP     EMP # Alternatively, can of course relabel ex-post SUM_wide |> relabel(VA = \"Value Added\", EMP = \"Employment\") |> namlab() #>   Variable       Label #> 1  Country     Country #> 2     Year        Year #> 3       VA Value Added #> 4      EMP  Employment rm(SUM_wide)  # Multiple-column pivots pivot(GGDC10S, c(\"Country\", \"Year\"), c(\"AGR\", \"MAN\", \"SUM\"), \"Variable\", how = \"w\",       na.rm = TRUE) |> head() #>   Country Year   AGR_VA  AGR_EMP    MAN_VA  MAN_EMP   SUM_VA  SUM_EMP #> 1     BWA 1964 16.30154 152.1179 0.7365696 2.420000 37.48229 173.8829 #> 2     BWA 1965 15.72700 153.2971 1.0181992 2.330406 39.34710 178.7637 #> 3     BWA 1966 17.68066 153.8867 0.8038415 1.281642 43.14677 179.7183 #> 4     BWA 1967 19.14591 155.0659 0.9378151 1.041623 41.39519 178.9181 #> 5     BWA 1968 21.09957 156.2451 0.7502521 1.069332 41.14259 181.9292 #> 6     BWA 1969 21.86221 157.4243 2.1396077 2.124402 51.22160 188.0569 # Here we may prefer a transposed column order pivot(GGDC10S, c(\"Country\", \"Year\"), c(\"AGR\", \"MAN\", \"SUM\"), \"Variable\", how = \"w\",       na.rm = TRUE, transpose = \"columns\") |> head() #>   Country Year   AGR_VA    MAN_VA   SUM_VA  AGR_EMP  MAN_EMP  SUM_EMP #> 1     BWA 1964 16.30154 0.7365696 37.48229 152.1179 2.420000 173.8829 #> 2     BWA 1965 15.72700 1.0181992 39.34710 153.2971 2.330406 178.7637 #> 3     BWA 1966 17.68066 0.8038415 43.14677 153.8867 1.281642 179.7183 #> 4     BWA 1967 19.14591 0.9378151 41.39519 155.0659 1.041623 178.9181 #> 5     BWA 1968 21.09957 0.7502521 41.14259 156.2451 1.069332 181.9292 #> 6     BWA 1969 21.86221 2.1396077 51.22160 157.4243 2.124402 188.0569 # Can also flip the order of names (independently of columns) pivot(GGDC10S, c(\"Country\", \"Year\"), c(\"AGR\", \"MAN\", \"SUM\"), \"Variable\", how = \"w\",       na.rm = TRUE, transpose = \"names\") |> head() #>   Country Year   VA_AGR  EMP_AGR    VA_MAN  EMP_MAN   VA_SUM  EMP_SUM #> 1     BWA 1964 16.30154 152.1179 0.7365696 2.420000 37.48229 173.8829 #> 2     BWA 1965 15.72700 153.2971 1.0181992 2.330406 39.34710 178.7637 #> 3     BWA 1966 17.68066 153.8867 0.8038415 1.281642 43.14677 179.7183 #> 4     BWA 1967 19.14591 155.0659 0.9378151 1.041623 41.39519 178.9181 #> 5     BWA 1968 21.09957 156.2451 0.7502521 1.069332 41.14259 181.9292 #> 6     BWA 1969 21.86221 157.4243 2.1396077 2.124402 51.22160 188.0569 # Can also enable both (complete transposition) pivot(GGDC10S, c(\"Country\", \"Year\"), c(\"AGR\", \"MAN\", \"SUM\"), \"Variable\", how = \"w\",       na.rm = TRUE, transpose = TRUE) |> head() # or tranpose = c(\"columns\", \"names\") #>   Country Year   VA_AGR    VA_MAN   VA_SUM  EMP_AGR  EMP_MAN  EMP_SUM #> 1     BWA 1964 16.30154 0.7365696 37.48229 152.1179 2.420000 173.8829 #> 2     BWA 1965 15.72700 1.0181992 39.34710 153.2971 2.330406 178.7637 #> 3     BWA 1966 17.68066 0.8038415 43.14677 153.8867 1.281642 179.7183 #> 4     BWA 1967 19.14591 0.9378151 41.39519 155.0659 1.041623 178.9181 #> 5     BWA 1968 21.09957 0.7502521 41.14259 156.2451 1.069332 181.9292 #> 6     BWA 1969 21.86221 2.1396077 51.22160 157.4243 2.124402 188.0569  # Finally, here is a nice, simple way to reshape the entire dataset. pivot(GGDC10S, values = 6:16, names = \"Variable\", na.rm = TRUE, how = \"w\") |>   namlab(N = TRUE, Nd = TRUE, class = TRUE) #>      Variable     Class    N Ndist         Label #> 1     Country character 2346    43       Country #> 2  Regioncode character 2346     6   Region code #> 3      Region character 2346     6        Region #> 4        Year   numeric 2346    67          Year #> 5      AGR_VA   numeric 2139  2135  Agriculture  #> 6     AGR_EMP   numeric 2225  2219  Agriculture  #> 7      MIN_VA   numeric 2139  2072        Mining #> 8     MIN_EMP   numeric 2216  2153        Mining #> 9      MAN_VA   numeric 2139  2139 Manufacturing #> 10    MAN_EMP   numeric 2216  2214 Manufacturing #> 11      PU_VA   numeric 2139  2097     Utilities #> 12     PU_EMP   numeric 2215  2141     Utilities #> 13     CON_VA   numeric 2139  2130  Construction #> 14    CON_EMP   numeric 2216  2209  Construction #>  [ reached 'max' / getOption(\"max.print\") -- omitted 12 rows ]  # -------------------------------- PIVOT RECAST --------------------------------- # Look at the data again head(GGDC10S) #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #> 4     BWA        SSA Sub-saharan Africa       VA 1963  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA #> 4  NA   NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] # Let's stack the sectors and instead create variable columns pivot(GGDC10S, .c(Country, Regioncode, Region, Year),       names = list(\"Variable\", \"Sectorcode\"), how = \"r\") |> head() #>   Country Regioncode             Region Year Sectorcode       VA      EMP #> 1     BWA        SSA Sub-saharan Africa 1960        AGR       NA       NA #> 2     BWA        SSA Sub-saharan Africa 1961        AGR       NA       NA #> 3     BWA        SSA Sub-saharan Africa 1962        AGR       NA       NA #> 4     BWA        SSA Sub-saharan Africa 1963        AGR       NA       NA #> 5     BWA        SSA Sub-saharan Africa 1964        AGR 16.30154 152.1179 #> 6     BWA        SSA Sub-saharan Africa 1965        AGR 15.72700 153.2971 # Same thing (a bit easier) pivot(GGDC10S, values = 6:16, names = list(\"Variable\", \"Sectorcode\"), how = \"r\") |> head() #>   Country Regioncode             Region Year Sectorcode       VA      EMP #> 1     BWA        SSA Sub-saharan Africa 1960        AGR       NA       NA #> 2     BWA        SSA Sub-saharan Africa 1961        AGR       NA       NA #> 3     BWA        SSA Sub-saharan Africa 1962        AGR       NA       NA #> 4     BWA        SSA Sub-saharan Africa 1963        AGR       NA       NA #> 5     BWA        SSA Sub-saharan Africa 1964        AGR 16.30154 152.1179 #> 6     BWA        SSA Sub-saharan Africa 1965        AGR 15.72700 153.2971 # Removing missing values pivot(GGDC10S, values = 6:16, names = list(\"Variable\", \"Sectorcode\"), how = \"r\",       na.rm = TRUE) |> head() #>   Country Regioncode             Region Year Sectorcode       VA      EMP #> 1     BWA        SSA Sub-saharan Africa 1960        AGR       NA       NA #> 2     BWA        SSA Sub-saharan Africa 1961        AGR       NA       NA #> 3     BWA        SSA Sub-saharan Africa 1962        AGR       NA       NA #> 4     BWA        SSA Sub-saharan Africa 1963        AGR       NA       NA #> 5     BWA        SSA Sub-saharan Africa 1964        AGR 16.30154 152.1179 #> 6     BWA        SSA Sub-saharan Africa 1965        AGR 15.72700 153.2971 # Saving Labels pivot(GGDC10S, values = 6:16, names = list(\"Variable\", \"Sectorcode\"),       labels = list(to = \"Sector\"), how = \"r\", na.rm = TRUE) |> head() #>   Country Regioncode             Region Year Sectorcode       Sector       VA #> 1     BWA        SSA Sub-saharan Africa 1960        AGR Agriculture        NA #> 2     BWA        SSA Sub-saharan Africa 1961        AGR Agriculture        NA #> 3     BWA        SSA Sub-saharan Africa 1962        AGR Agriculture        NA #> 4     BWA        SSA Sub-saharan Africa 1963        AGR Agriculture        NA #> 5     BWA        SSA Sub-saharan Africa 1964        AGR Agriculture  16.30154 #> 6     BWA        SSA Sub-saharan Africa 1965        AGR Agriculture  15.72700 #>        EMP #> 1       NA #> 2       NA #> 3       NA #> 4       NA #> 5 152.1179 #> 6 153.2971  # Supplying new labels for generated columns: as complete as it gets pivot(GGDC10S, values = 6:16, names = list(\"Variable\", \"Sectorcode\"),       labels = list(to = \"Sector\",                     new = c(Sectorcode = \"GGDC10S Sector Code\",                             Sector = \"Long Sector Description\",                             VA = \"Value Added\",                             EMP = \"Employment\")), how = \"r\", na.rm = TRUE) |>   namlab(N = TRUE, Nd = TRUE, class = TRUE) #>     Variable     Class     N Ndist                   Label #> 1    Country character 27852    43                 Country #> 2 Regioncode character 27852     6             Region code #> 3     Region character 27852     6                  Region #> 4       Year   numeric 27852    67                    Year #> 5 Sectorcode    factor 27852    11     GGDC10S Sector Code #> 6     Sector    factor 27852    11 Long Sector Description #> 7         VA   numeric 23092 22915             Value Added #> 8        EMP   numeric 23850 23610              Employment  # Now another (slightly unconventional) use case here is data transposition # Let's get the data for Botswana BWA <- GGDC10S |> fsubset(Country == \"BWA\", Variable, Year, AGR:SUM) head(BWA) #>   Variable Year      AGR      MIN       MAN        PU       CON      WRT #> 1       VA 1960       NA       NA        NA        NA        NA       NA #> 2       VA 1961       NA       NA        NA        NA        NA       NA #> 3       VA 1962       NA       NA        NA        NA        NA       NA #> 4       VA 1963       NA       NA        NA        NA        NA       NA #> 5       VA 1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 #>        TRA     FIRE      GOV      OTH      SUM #> 1       NA       NA       NA       NA       NA #> 2       NA       NA       NA       NA       NA #> 3       NA       NA       NA       NA       NA #> 4       NA       NA       NA       NA       NA #> 5 1.658928 1.119194 4.822485 2.341328 37.48229 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] # By supplying no ids or values, we are simply requesting a transpose operation pivot(BWA, names = list(from = c(\"Variable\", \"Year\"), to = \"Sectorcode\"), how = \"r\") #>      Sectorcode VA_1960 VA_1961 VA_1962 VA_1963 VA_1964 VA_1965 VA_1966 VA_1967 #>      VA_1968 VA_1969 VA_1970 VA_1971 VA_1972 VA_1973 VA_1974 VA_1975 VA_1976 #>      VA_1977 VA_1978 VA_1979 VA_1980 VA_1981 VA_1982 VA_1983 VA_1984 VA_1985 #>      VA_1986 VA_1987 VA_1988 VA_1989 VA_1990 VA_1991 VA_1992 VA_1993 VA_1994 #>      VA_1995 VA_1996 VA_1997 VA_1998 VA_1999 VA_2000 VA_2001 VA_2002 VA_2003 #>      VA_2004 VA_2005 VA_2006 VA_2007 VA_2008 VA_2009 VA_2010 VA_2011 EMP_1960 #>      EMP_1961 EMP_1962 EMP_1963 EMP_1964 EMP_1965 EMP_1966 EMP_1967 EMP_1968 #>      EMP_1969 EMP_1970 EMP_1971 EMP_1972 EMP_1973 EMP_1974 EMP_1975 EMP_1976 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 35 columns ] #>  [ reached 'max' / getOption(\"max.print\") -- omitted 11 rows ] # Same with labels pivot(BWA, names = list(from = c(\"Variable\", \"Year\"), to = \"Sectorcode\"),       labels = list(to = \"Sector\"), how = \"r\") #>      Sectorcode Sector VA_1960 VA_1961 VA_1962 VA_1963 VA_1964 VA_1965 VA_1966 #>      VA_1967 VA_1968 VA_1969 VA_1970 VA_1971 VA_1972 VA_1973 VA_1974 VA_1975 #>      VA_1976 VA_1977 VA_1978 VA_1979 VA_1980 VA_1981 VA_1982 VA_1983 VA_1984 #>      VA_1985 VA_1986 VA_1987 VA_1988 VA_1989 VA_1990 VA_1991 VA_1992 VA_1993 #>      VA_1994 VA_1995 VA_1996 VA_1997 VA_1998 VA_1999 VA_2000 VA_2001 VA_2002 #>      VA_2003 VA_2004 VA_2005 VA_2006 VA_2007 VA_2008 VA_2009 VA_2010 VA_2011 #>      EMP_1960 EMP_1961 EMP_1962 EMP_1963 EMP_1964 EMP_1965 EMP_1966 EMP_1967 #>      EMP_1968 EMP_1969 EMP_1970 EMP_1971 EMP_1972 EMP_1973 EMP_1974 EMP_1975 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 36 columns ] #>  [ reached 'max' / getOption(\"max.print\") -- omitted 11 rows ] # For simple cases, data.table::transpose() will be more efficient, but with multiple # columns to generate names and/or variable labels to be saved/assigned, pivot() is handy rm(BWA)"},{"path":"https://sebkrantz.github.io/collapse/reference/psacf.html","id":null,"dir":"Reference","previous_headings":"","what":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","title":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","text":"psacf, pspacf psccf compute (default plot) estimates auto-, partial auto- cross- correlation covariance functions panel series. analogues acf, pacf ccf.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psacf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","text":"","code":"psacf(x, ...) pspacf(x, ...) psccf(x, y, ...)  # Default S3 method psacf(x, g, t = NULL, lag.max = NULL, type = c(\"correlation\", \"covariance\",\"partial\"),       plot = TRUE, gscale = TRUE, ...) # Default S3 method pspacf(x, g, t = NULL, lag.max = NULL, plot = TRUE, gscale = TRUE, ...) # Default S3 method psccf(x, y, g, t = NULL, lag.max = NULL, type = c(\"correlation\", \"covariance\"),       plot = TRUE, gscale = TRUE, ...)  # S3 method for class 'data.frame' psacf(x, by, t = NULL, cols = is.numeric, lag.max = NULL,       type = c(\"correlation\", \"covariance\",\"partial\"), plot = TRUE, gscale = TRUE, ...) # S3 method for class 'data.frame' pspacf(x, by, t = NULL, cols = is.numeric, lag.max = NULL,        plot = TRUE, gscale = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' psacf(x, lag.max = NULL, type = c(\"correlation\", \"covariance\",\"partial\"),       plot = TRUE, gscale = TRUE, ...) # S3 method for class 'pseries' pspacf(x, lag.max = NULL, plot = TRUE, gscale = TRUE, ...) # S3 method for class 'pseries' psccf(x, y, lag.max = NULL, type = c(\"correlation\", \"covariance\"),       plot = TRUE, gscale = TRUE, ...)   # S3 method for class 'pdata.frame' psacf(x, cols = is.numeric, lag.max = NULL,       type = c(\"correlation\", \"covariance\",\"partial\"), plot = TRUE, gscale = TRUE, ...) # S3 method for class 'pdata.frame' pspacf(x, cols = is.numeric, lag.max = NULL, plot = TRUE, gscale = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/psacf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","text":"x, y numeric vector, 'indexed_series' ('pseries'), data frame 'indexed_frame' ('pdata.frame'). g factor, GRP object, atomic vector / list vectors (internally grouped group) used group x. data.frame method: input g, also allows one- two-sided formulas using variables x, .e. ~ idvar var1 + var2 ~ idvar1 + idvar2. t time vector list vectors. See flag. cols data.frame method: Select columns using function, column names, indices logical vector. Note: cols ignored two-sided formula passed . lag.max integer. Maximum lag calculate acf. Default 2*sqrt(length(x)/ng) ng number groups panel series / supplied g. type character. String giving type acf computed. Allowed values \"correlation\" (default), \"covariance\" \"partial\". plot logical. TRUE (default) acf plotted. gscale logical. groupwise scaling / standardization x, y (using fscale groups supplied g) computing panel-autocovariances / correlations. See Details. ... arguments passed plot.acf.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psacf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","text":"gscale = TRUE data standardized within group (using fscale) group-mean 0 group-standard deviation 1. strongly recommended panels get rid individual-specific heterogeneity corrupt ACF computations. scaling, psacf, pspacf psccf compute ACF/CCF creating matrix panel-lags series using flag computing covariance matrix series (x, y) using cov pairwise-complete observations, dividing variance (x, y). Creating lag matrix may require lot memory large data, passing sequence lags flag thus calling flag cov one time generally much faster calling lag.max times. partial ACF computed ACF using Yule-Walker decomposition, way pacf.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psacf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","text":"object class 'acf', see acf. result returned invisibly plot = TRUE.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/psacf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Auto- and Cross- Covariance and Correlation Function Estimation for Panel Series — psacf","text":"","code":"## World Development Panel Data head(wlddev)                                                    # See also help(wlddev) #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] psacf(wlddev$PCGDP, wlddev$country, wlddev$year)                # ACF of GDP per Capita  psacf(wlddev, PCGDP ~ country, ~year)                           # Same using data.frame method  psacf(wlddev$PCGDP, wlddev$country)                             # The Data is sorted, can omit t  pspacf(wlddev$PCGDP, wlddev$country)                            # Partial ACF  psccf(wlddev$PCGDP, wlddev$LIFEEX, wlddev$country)              # CCF with Life-Expectancy at Birth   psacf(wlddev, PCGDP + LIFEEX + ODA ~ country, ~year)            # ACF and CCF of GDP, LIFEEX and ODA  psacf(wlddev, ~ country, ~year, c(9:10,12))                     # Same, using cols argument pspacf(wlddev, ~ country, ~year, c(9:10,12))                    # Partial ACF   ## Using indexed data: wldi <- findex_by(wlddev, iso3c, year)  # Creating a indexed frame PCGDP <- wldi$PCGDP                     # Indexed Series of GDP per Capita LIFEEX <- wldi$LIFEEX                   # Indexed Series of Life Expectancy psacf(PCGDP)                            # Same as above, more parsimonious  pspacf(PCGDP)  psccf(PCGDP, LIFEEX)  psacf(wldi[c(9:10,12)])  pspacf(wldi[c(9:10,12)])"},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix / Array from Panel Series — psmat","title":"Matrix / Array from Panel Series — psmat","text":"psmat efficiently expands panel-vector 'indexed_series' ('pseries') matrix. data frame 'indexed_frame' ('pdata.frame') passed, psmat returns 3D array list matrices.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix / Array from Panel Series — psmat","text":"","code":"psmat(x, ...)  # Default S3 method psmat(x, g, t = NULL, transpose = FALSE, fill = NULL, ...)  # S3 method for class 'data.frame' psmat(x, by, t = NULL, cols = NULL, transpose = FALSE, fill = NULL, array = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' psmat(x, transpose = FALSE, fill = NULL, drop.index.levels = \"none\", ...)  # S3 method for class 'pdata.frame' psmat(x, cols = NULL, transpose = FALSE, fill = NULL, array = TRUE,       drop.index.levels = \"none\", ...)   # S3 method for class 'psmat' plot(x, legend = FALSE, colours = legend, labs = NULL, grid = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix / Array from Panel Series — psmat","text":"x vector, indexed series 'indexed_series' ('pseries'), data frame 'indexed_frame' ('pdata.frame'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. panel balanced integer indicating number groups can also supplied. See Examples. data.frame method: input g, also allows one- two-sided formulas using variables x, .e. ~ idvar var1 + var2 ~ idvar1 + idvar2. t inputs g/, indicate time-variable(s) second identifier(s). g t together fully identify panel. t = NULL, data assumed sorted seq_col used generate rownames output matrix. cols data.frame method: Select columns using function, column names, indices logical vector. Note: cols ignored two-sided formula passed . transpose logical. TRUE generates matrix g/-> columns, t -> rows. Default g/-> rows, t -> columns. fill element fill empty slots matrix / array panel unbalanced. NULL generate NA right type. array data.frame / pdata.frame methods: logical. TRUE returns 3D array (just one column selected matrix returned). FALSE returns list matrices. drop.index.levels character. Either \"id\", \"time\", \"\" \"none\". See indexing. ... arguments passed methods, plot method additional arguments passed ts.plot. legend logical. Automatically create legend panel-groups. colours either TRUE automatically colour panel-groups using rainbow character vector colours matching number panel-groups (series). labs character. Provide character-vector variable labels / series titles plotting array. grid logical. Calls grid draw gridlines plot.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Matrix / Array from Panel Series — psmat","text":"n > 2 index variables attached indexed series frame, first n-1 variables index interacted.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix / Array from Panel Series — psmat","text":"matrix 3D array containing data x, default rows constitute groups-ids (g/) columns time variable individual ids (t). 3D arrays contain variables 3rd dimension. objects class 'psmat', also 'transpose' attribute indicating whether transpose = TRUE.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Matrix / Array from Panel Series — psmat","text":"pdata.frame method works properly subsetted objects class 'pdata.frame'. list 'pseries' work. also exist simple aperm [ (subset) methods 'psmat' objects. differ default methods keeping class 'transpose' attribute.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/psmat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix / Array from Panel Series — psmat","text":"","code":"## World Development Panel Data head(wlddev)                                                    # View data #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP #> 1 32.446   NA 116769997 8996973 #> 2 32.962   NA 232080002 9169410 #> 3 33.471   NA 112839996 9351441 #> 4 33.971   NA 237720001 9543205 #> 5 34.463   NA 295920013 9744781 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] qsu(wlddev, pid = ~ iso3c, cols = 9:12, vlabels = TRUE)         # Sumarizing data #> , , PCGDP: GDP per capita (constant 2010 US$) #>  #>              N/T        Mean          SD          Min         Max #> Overall     9470   12048.778  19077.6416     132.0776  196061.417 #> Between      206  12962.6054  20189.9007     253.1886   141200.38 #> Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254 #>  #> , , LIFEEX: Life expectancy at birth, total (years) #>  #>              N/T     Mean       SD      Min      Max #> Overall    11670  64.2963  11.4764   18.907  85.4171 #> Between      207  64.9537   9.8936  40.9663  85.4171 #> Within   56.3768  64.2963   6.0842  32.9068  84.4198 #>  #> , , GINI: Gini index (World Bank estimate) #>  #>              N/T     Mean      SD      Min      Max #> Overall     1744  38.5341  9.2006     20.7     65.8 #> Between      167  39.4233  8.1356  24.8667  61.7143 #> Within   10.4431  38.5341  2.9277  25.3917  55.3591 #>  #> , , ODA: Net official development assistance and official aid received (constant 2018 US$) #>  #>              N/T        Mean          SD              Min             Max #> Overall     8608  454'720131  868'712654      -997'679993  2.56715605e+10 #> Between      178  439'168412  569'049959       468717.916  3.62337432e+09 #> Within   48.3596  454'720131  650'709624  -2.44379420e+09  2.45610972e+10 #>  str(psmat(wlddev$PCGDP, wlddev$iso3c, wlddev$year))             # Generating matrix of GDP #>  'psmat' num [1:216, 1:61] NA NA NA NA NA ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #>   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #>  - attr(*, \"transpose\")= logi FALSE r <- psmat(wlddev, PCGDP ~ iso3c, ~ year)                       # Same thing using data.frame method plot(r, main = vlabels(wlddev)[9], xlab = \"Year\")               # Plot the matrix  str(r)                                                          # See srructure #>  'psmat' num [1:216, 1:61] NA NA NA NA NA ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #>   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #>  - attr(*, \"transpose\")= logi FALSE str(psmat(wlddev$PCGDP, wlddev$iso3c))                          # The Data is sorted, could omit t #>  'psmat' num [1:216, 1:61] NA NA NA NA NA ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #>   ..$ : chr [1:61] \"1\" \"2\" \"3\" \"4\" ... #>  - attr(*, \"transpose\")= logi FALSE str(psmat(wlddev$PCGDP, 216))                                   # This panel is also balanced, so #>  num [1:216, 1:61] NA NA 2481 NA NA ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : chr [1:216] \"GRP.1\" \"GRP.2\" \"GRP.3\" \"GRP.4\" ... #>   ..$ : chr [1:61] \"1\" \"2\" \"3\" \"4\" ... # ..indicating the number of groups would be sufficient to obtain a matrix  ar <- psmat(wlddev, ~ iso3c, ~ year, 9:12)                      # Get array of transposed matrices str(ar) #>  'psmat' num [1:216, 1:61, 1:4] NA NA NA NA NA ... #>  - attr(*, \"dimnames\")=List of 3 #>   ..$ : chr [1:216] \"ABW\" \"AFG\" \"AGO\" \"ALB\" ... #>   ..$ : chr [1:61] \"1960\" \"1961\" \"1962\" \"1963\" ... #>   ..$ : chr [1:4] \"PCGDP\" \"LIFEEX\" \"GINI\" \"ODA\" #>  - attr(*, \"transpose\")= logi FALSE plot(ar)  plot(ar, legend = TRUE)  plot(psmat(collap(wlddev, ~region+year, cols = 9:12),           # More legible and fancy plot            ~region, ~year), legend = TRUE,      labs = vlabels(wlddev)[9:12])   psml <- psmat(wlddev, ~ iso3c, ~ year, 9:12, array = FALSE)     # This gives list of ps-matrices head(unlist2d(psml, \"Variable\", \"Country\", id.factor = TRUE),2) # Using unlist2d, can generate DF #>   Variable Country 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 #> 1    PCGDP     ABW   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA #>   1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 #> 1   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA #>       1986     1987     1988     1989     1990     1991     1992     1993 #> 1 15669.62 18427.61 22134.02 24837.95 25357.79 26329.31 26401.97 26663.21 #>       1994     1995     1996    1997     1998    1999     2000     2001    2002 #> 1 27272.31 26705.18 26087.78 27190.5 27151.92 26954.4 28417.38 26966.05 25508.3 #>       2003     2004     2005     2006     2007     2008     2009    2010  2011 #> 1 25469.29 27005.53 26979.89 27046.22 27427.58 27365.93 24463.69 23512.6 24233 #>       2012     2013     2014     2015     2016     2017 2018 2019 2020 #> 1 23781.26 24635.76 24563.23 25822.25 26231.03 26630.21   NA   NA   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]  ## Indexing simplifies things wldi <- findex_by(wlddev, iso3c, year)  # Creating an indexed frame PCGDP <- wldi$PCGDP                     # An indexed_series of GDP per Capita head(psmat(PCGDP), 2)                   # Same as above, more parsimonious #>     1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 #> ABW   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA #>     1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985  1986  1987  1988 #> ABW   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA 15670 18428 22134 #>      1989  1990  1991  1992  1993  1994  1995  1996  1997  1998  1999  2000 #> ABW 24838 25358 26329 26402 26663 27272 26705 26088 27191 27152 26954 28417 #>      2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  2012 #> ABW 26966 25508 25469 27006 26980 27046 27428 27366 24464 23513 24233 23781 #>      2013  2014  2015  2016  2017 2018 2019 2020 #> ABW 24636 24563 25822 26231 26630   NA   NA   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] plot(psmat(PCGDP))  plot(psmat(wldi[9:12]))  plot(psmat(G(wldi[9:12])))              # Here plotting panel-growth rates"},{"path":"https://sebkrantz.github.io/collapse/reference/pwcor_pwcov_pwnobs.html","id":null,"dir":"Reference","previous_headings":"","what":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","title":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","text":"Computes (pairwise, weighted) Pearson's correlations, covariances observation counts. Pairwise correlations covariances can computed together observation counts p-values, output 3D array (default) list matrices. pwcor pwcov offer elaborate print method.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pwcor_pwcov_pwnobs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","text":"","code":"pwcor(X, ..., w = NULL, N = FALSE, P = FALSE, array = TRUE, use = \"pairwise.complete.obs\")  pwcov(X, ..., w = NULL, N = FALSE, P = FALSE, array = TRUE, use = \"pairwise.complete.obs\")  pwnobs(X)  # S3 method for class 'pwcor' print(x, digits = .op[[\"digits\"]], sig.level = 0.05,       show = c(\"all\",\"lower.tri\",\"upper.tri\"), spacing = 1L, return = FALSE, ...)  # S3 method for class 'pwcov' print(x, digits = .op[[\"digits\"]], sig.level = 0.05,       show = c(\"all\",\"lower.tri\",\"upper.tri\"), spacing = 1L, return = FALSE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/pwcor_pwcov_pwnobs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","text":"X matrix data.frame, pwcor pwcov columns must numeric. functions faster matrices, converting advised large data (see qM). x object class 'pwcor' / 'pwcov'. w numeric. vector (frequency) weights. N logical. TRUE also computes pairwise observation counts. P logical. TRUE also computes pairwise p-values (cor.test Hmisc::rcorr). array logical. N = TRUE P = TRUE, TRUE (default) returns output 3D array whereas FALSE returns list matrices. use argument passed cor / cov. use != \"pairwise.complete.obs\", sum(complete.cases(X)) used N, p-values computed accordingly. digits integer. number digits round print. sig.level numeric. P-value threshold '*' displayed significant coefficients P = TRUE. show character. part correlation / covariance matrix display. spacing integer. Controls spacing different reported quantities printout matrix: 0 - compressed, 1 - single space, 2 - double space. return logical. TRUE returns formatted object print method exporting. default return x invisibly. ... arguments passed cor cov. sensible P = FALSE.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pwcor_pwcov_pwnobs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","text":"numeric matrix, 3D array list matrices computed statistics. pwcor pwcov object class 'pwcor' 'pwcov', respectively.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/pwcor_pwcov_pwnobs.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","text":"weights::wtd.cors imported weighted pairwise correlations (written C speed). weighted correlations bootstrap SE's see weights::wtd.cor (bootstrap can slow). Weighted correlations complex surveys implemented jtools::svycor. equivalent faster implementation pwcor (without weights) provided Hmisc::rcorr (written Fortran).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/pwcor_pwcov_pwnobs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"(Pairwise, Weighted) Correlations, Covariances and Observation Counts — pwcor-pwcov-pwnobs","text":"","code":"mna <- na_insert(mtcars) pwcor(mna) #>        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #> mpg     1   -.86  -.81  -.71   .74  -.87   .38   .67   .53   .41  -.55 #> cyl   -.86    1    .91   .81  -.75   .79  -.58  -.86  -.50  -.54   .53 #> disp  -.81   .91    1    .85  -.79   .91  -.36  -.72  -.66  -.75   .38 #> hp    -.71   .81   .85    1   -.36   .60  -.72  -.69  -.05  -.06   .74 #> drat   .74  -.75  -.79  -.36    1   -.75   .10   .48   .70   .70  -.14 #> wt    -.87   .79   .91   .60  -.75    1   -.16  -.58  -.62  -.58   .42 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] pwcov(mna) #>            mpg       cyl      disp        hp      drat        wt      qsec #> mpg      34.37     -9.68   -588.25   -264.29      2.37     -5.04      4.22 #> cyl      -9.68      3.35    219.83     98.14     -0.79      1.53     -2.03 #> disp   -588.25    219.83  16172.58   5973.18    -58.42    127.09    -74.75 #> hp     -264.29     98.14   5973.18   4545.18    -12.04     40.86    -97.99 #> drat      2.37     -0.79    -58.42    -12.04      0.31     -0.43      0.11 #> wt       -5.04      1.53    127.09     40.86     -0.43      1.04     -0.30 #>             vs        am      gear      carb #> mpg       2.06      1.40      1.89     -5.33 #> cyl      -0.82     -0.41     -0.75      1.65 #> disp    -49.07    -38.96    -67.91     66.84 #> hp      -23.37     -1.74     -3.53     84.37 #> drat      0.13      0.16      0.29     -0.11 #> wt       -0.31     -0.28     -0.46      0.75 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] pwnobs(mna) #>      mpg cyl disp hp drat wt qsec vs am gear carb #> mpg   29  26   26 26   27 26   26 26 26   26   27 #> cyl   26  29   26 26   26 26   26 26 26   27   26 #> disp  26  26   29 26   26 26   26 26 26   26   26 #> hp    26  26   26 29   26 27   26 26 27   27   27 #> drat  27  26   26 26   29 26   26 27 27   26   26 #> wt    26  26   26 27   26 29   27 27 26   27   26 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] pwcor(mna, N = TRUE) #>            mpg       cyl      disp        hp      drat        wt      qsec #> mpg    1  (29) -.86 (26) -.81 (26) -.71 (26)  .74 (27) -.87 (26)  .38 (26) #> cyl  -.86 (26)   1  (29)  .91 (26)  .81 (26) -.75 (26)  .79 (26) -.58 (26) #> disp -.81 (26)  .91 (26)   1  (29)  .85 (26) -.79 (26)  .91 (26) -.36 (26) #> hp   -.71 (26)  .81 (26)  .85 (26)   1  (29) -.36 (26)  .60 (27) -.72 (26) #> drat  .74 (27) -.75 (26) -.79 (26) -.36 (26)   1  (29) -.75 (26)  .10 (26) #> wt   -.87 (26)  .79 (26)  .91 (26)  .60 (27) -.75 (26)   1  (29) -.16 (27) #>             vs        am      gear      carb #> mpg   .67 (26)  .53 (26)  .41 (26) -.55 (27) #> cyl  -.86 (26) -.50 (26) -.54 (27)  .53 (26) #> disp -.72 (26) -.66 (26) -.75 (26)  .38 (26) #> hp   -.69 (26) -.05 (27) -.06 (27)  .74 (27) #> drat  .48 (27)  .70 (27)  .70 (26) -.14 (26) #> wt   -.58 (27) -.62 (26) -.58 (27)  .42 (26) #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] pwcor(mna, P = TRUE) #>         mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear #> mpg     1    -.86*  -.81*  -.71*   .74*  -.87*   .38    .67*   .53*   .41* #> cyl   -.86*    1     .91*   .81*  -.75*   .79*  -.58*  -.86*  -.50*  -.54* #> disp  -.81*   .91*    1     .85*  -.79*   .91*  -.36   -.72*  -.66*  -.75* #> hp    -.71*   .81*   .85*    1    -.36    .60*  -.72*  -.69*  -.05   -.06  #> drat   .74*  -.75*  -.79*  -.36     1    -.75*   .10    .48*   .70*   .70* #> wt    -.87*   .79*   .91*   .60*  -.75*    1    -.16   -.58*  -.62*  -.58* #>        carb #> mpg   -.55* #> cyl    .53* #> disp   .38  #> hp     .74* #> drat  -.14  #> wt     .42* #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] pwcor(mna, N = TRUE, P = TRUE) #>             mpg        cyl       disp         hp       drat         wt #> mpg    1   (29) -.86* (26) -.81* (26) -.71* (26)  .74* (27) -.87* (26) #> cyl  -.86* (26)   1   (29)  .91* (26)  .81* (26) -.75* (26)  .79* (26) #> disp -.81* (26)  .91* (26)   1   (29)  .85* (26) -.79* (26)  .91* (26) #> hp   -.71* (26)  .81* (26)  .85* (26)   1   (29) -.36  (26)  .60* (27) #> drat  .74* (27) -.75* (26) -.79* (26) -.36  (26)   1   (29) -.75* (26) #> wt   -.87* (26)  .79* (26)  .91* (26)  .60* (27) -.75* (26)   1   (29) #>            qsec         vs         am       gear       carb #> mpg   .38  (26)  .67* (26)  .53* (26)  .41* (26) -.55* (27) #> cyl  -.58* (26) -.86* (26) -.50* (26) -.54* (27)  .53* (26) #> disp -.36  (26) -.72* (26) -.66* (26) -.75* (26)  .38  (26) #> hp   -.72* (26) -.69* (26) -.05  (27) -.06  (27)  .74* (27) #> drat  .10  (26)  .48* (27)  .70* (27)  .70* (26) -.14  (26) #> wt   -.16  (27) -.58* (27) -.62* (26) -.58* (27)  .42* (26) #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] aperm(pwcor(mna, N = TRUE, P = TRUE)) #> , , mpg #>  #>   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #> r   1  -.86  -.81  -.71   .74  -.87   .38   .67   .53   .41  -.55 #> N  29    26    26    26    27    26    26    26    26    26    27 #> P       .00   .00   .00   .00   .00   .06   .00   .01   .04   .00 #>  #> , , cyl #>  #>     mpg    cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #> r  -.86   1.00   .91   .81  -.75   .79  -.58  -.86  -.50  -.54   .53 #> N    26     29    26    26    26    26    26    26    26    27    26 #> P   .00          .00   .00   .00   .00   .00   .00   .01   .00   .01 #>  #> , , disp #>  #>     mpg   cyl disp    hp #> r  -.81   .91    1   .85 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 8 slices ]  print(pwcor(mna, N = TRUE, P = TRUE), digits = 3, sig.level = 0.01, show = \"lower.tri\") #>              mpg         cyl        disp          hp        drat          wt #> mpg    1    (29)                                                             #> cyl  -.859* (26)   1    (29)                                                 #> disp -.811* (26)  .913* (26)   1    (29)                                     #> hp   -.713* (26)  .812* (26)  .852* (26)   1    (29)                         #> drat  .736* (27) -.753* (26) -.795* (26) -.361  (26)   1    (29)             #> wt   -.866* (26)  .793* (26)  .910* (26)  .601* (27) -.748* (26)   1    (29) #>             qsec          vs          am        gear        carb #> mpg                                                              #> cyl                                                              #> disp                                                             #> hp                                                               #> drat                                                             #> wt                                                               #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] pwcor(mna, N = TRUE, P = TRUE, array = FALSE) #> $r #>        mpg    cyl  disp    hp  drat    wt  qsec    vs     am  gear   carb #> mpg      1   -.86  -.81  -.71   .74  -.87   .38   .67    .53   .41   -.55 #> cyl   -.86   1.00   .91   .81  -.75   .79  -.58  -.86   -.50  -.54    .53 #> disp  -.81    .91     1   .85  -.79   .91  -.36  -.72   -.66  -.75    .38 #> hp    -.71    .81   .85     1  -.36   .60  -.72  -.69   -.05  -.06    .74 #> drat   .74   -.75  -.79  -.36     1  -.75   .10   .48    .70   .70   -.14 #> wt    -.87    .79   .91   .60  -.75     1  -.16  -.58   -.62  -.58    .42 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>  #> $N #>      mpg cyl disp  hp drat  wt qsec  vs  am gear carb #> mpg   29  26   26  26   27  26   26  26  26   26   27 #> cyl   26  29   26  26   26  26   26  26  26   27   26 #> disp  26  26   29  26   26  26   26  26  26   26   26 #> hp    26  26   26  29   26  27   26  26  27   27   27 #> drat  27  26   26  26   29  26   26  27  27   26   26 #> wt    26  26   26  27   26  29   27  27  26   27   26 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>  #> $P #>       mpg  cyl disp   hp drat   wt qsec   vs   am gear carb #> mpg        .00  .00  .00  .00  .00  .06  .00  .01  .04  .00 #> cyl   .00       .00  .00  .00  .00  .00  .00  .01  .00  .01 #> disp  .00  .00       .00  .00  .00  .07  .00  .00  .00  .06 #> hp    .00  .00  .00       .07  .00  .00  .00  .79  .75  .00 #> drat  .00  .00  .00  .07       .00  .61  .01  .00  .00  .51 #> wt    .00  .00  .00  .00  .00       .42  .00  .00  .00  .03 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>  print(pwcor(mna, N = TRUE, P = TRUE, array = FALSE), show = \"lower.tri\") #> $r #>        mpg    cyl  disp    hp  drat    wt  qsec    vs     am  gear   carb #> mpg      1                                                                #> cyl   -.86   1.00                                                         #> disp  -.81    .91     1                                                   #> hp    -.71    .81   .85     1                                             #> drat   .74   -.75  -.79  -.36     1                                       #> wt    -.87    .79   .91   .60  -.75     1                                 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>  #> $N #>      mpg cyl disp  hp drat  wt qsec  vs  am gear carb #> mpg   29                                              #> cyl   26  29                                          #> disp  26  26   29                                     #> hp    26  26   26  29                                 #> drat  27  26   26  26   29                            #> wt    26  26   26  27   26  29                        #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>  #> $P #>       mpg  cyl disp   hp drat   wt qsec   vs   am gear carb #> mpg                                                         #> cyl   .00                                                   #> disp  .00  .00                                              #> hp    .00  .00  .00                                         #> drat  .00  .00  .00  .07                                    #> wt    .00  .00  .00  .00  .00                               #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>"},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"qF, shorthand 'quick-factor' implements fast factor generation atomic vectors using either radix ordering index hashing followed sorting. qG, shorthand 'quick-group', generates kind factor-light without levels attribute instead attribute providing number levels. Optionally levels / groups can attached, without converting character (can large performance implications). Objects class 'qG'. finteraction generates factor 'qG' object interacting multiple vectors factors. process missing values always replaced level unused levels/combinations always dropped. collapse internally makes optimal use factors 'qG' objects passed grouping vectors statistical functions (g/, t arguments) .e. typically grouping ordering performed objects used directly statistical C/C++ code.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"","code":"qF(x, ordered = FALSE, na.exclude = TRUE, sort = .op[[\"sort\"]], drop = FALSE,    keep.attr = TRUE, method = \"auto\")  qG(x, ordered = FALSE, na.exclude = TRUE, sort = .op[[\"sort\"]],    return.groups = FALSE, method = \"auto\")  is_qG(x)  as_factor_qG(x, ordered = FALSE, na.exclude = TRUE)  finteraction(..., factor = TRUE, ordered = FALSE, sort = factor && .op[[\"sort\"]],              method = \"auto\", sep = \".\") itn(...) # Shorthand for finteraction"},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"x atomic vector, factor quick-group. ordered logical. Adds class 'ordered'. na.exclude logical. TRUE preserves missing values (.e. level generated NA). FALSE attaches additional class \"na.included\" used skip missing value checks performed sending objects C/C++. See Details. sort logical. TRUE sorts levels ascending order (like factor); FALSE provides levels order first appearance, can significantly faster. Note factor passed input, sort = FALSE takes effect unused levels dropped (factors usually sorted levels checking sortedness can expensive). drop logical. x factor, TRUE efficiently drops unused factor levels beforehand using fdroplevels. keep.attr logical. TRUE x additional attributes apart 'levels' 'class', preserved conversion factor. method integer character string specifying method computation: Note finteraction, method = \"hash\" always unsorted method = \"rcpp_hash\" available. return.groups logical. TRUE returns unique elements / groups / levels x attribute called \"groups\". Unlike qF, converted character. factor logical. TRUE returns factor, FALSE returns 'qG' object. sep character. separator passed paste creating factor levels multiple grouping variables. ... multiple atomic vectors factors, single list equal-length vectors factors. See Details.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"Whenever vector passed Fast Statistical Function fmean(mtcars, mtcars$cyl), grouped using qF, qG use.g.names = FALSE. qF combination .factor factor. Applying vector .e. qF(x) gives result .factor(x). qF(x, ordered = TRUE) generates ordered factor (factor(x, ordered = TRUE)), qF(x, na.exclude = FALSE) generates level missing values (factor(x, exclude = NULL)). important addition qF(x, na.exclude = FALSE) also adds class 'na.included'. prevents collapse functions checking missing values factor, thus computationally efficient. Therefore factors used grouped operations preferably generated using qF(x, na.exclude = FALSE). Setting sort = FALSE gathers levels first-appearance order (unless method = \"radix\" x numeric, case levels always sorted). often gives noticeable speed improvement. 3 internal methods computation: radix ordering, hashing, Rcpp sugar hashing. Radix ordering done combining functions radixorder groupid. generally faster hashing large numeric data pre-sorted data (although exceptions). Hashing uses group, followed radixorder unique elements sort = TRUE. generally fastest character data. Rcpp hashing uses Rcpp::sugar::sort_unique Rcpp::sugar::match. often less efficient former large data, sorting properties (relying std::sort) may superior borderline cases radixorder fails deliver exact lexicographic ordering factor levels. Regarding speed: general qF around 5x faster .factor character data 30x faster numeric data. Automatic method dispatch typically good job delivering optimal performance. qG first place programmers function. generates factor-'light' class 'qG' consisting integer grouping vector attribute providing number groups. slightly faster memory efficient GRP grouping atomic vectors, also convenient can stored data frame column, main reasons existence. finteraction simply wrapper around as_factor_GRP(GRP.default(X)), X replaced arguments '...' combined list (really interaction function just multivariate grouping converted factor, see GRP computational details). general: vectors, factors, lists vectors / factors passed can interacted. Interactions always create level missing values always drop unused levels.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"qF returns (ordered) factor. qG returns object class 'qG': integer grouping vector attribute \"N.groups\" indicating number groups, , return.groups = TRUE, attribute \"groups\" containing vector unique groups / elements x corresponding integer-id. finteraction can return either.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"efficient alternative character vectors multithreading support provided kit::charToFact. qG(x, sort = FALSE, na.exclude = FALSE, method = \"hash\") internally calls group(x) can also used directly also supports multivariate groupings. Neither qF qG reorder groups / factor levels. exception added v1.7, calling qF(f, sort = FALSE) factor f, levels recast first appearance order. objects can however converted one another using qF/qG direct method as_factor_qG (called inside qF). also possible add class 'ordered' (ordered = TRUE) create extra level / integer missing values (na.exclude = FALSE) factors 'qG' objects passed qF qG.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/qF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Factor Generation, Interactions and Vector Grouping — qF-qG-finteraction","text":"","code":"cylF <- qF(mtcars$cyl)     # Factor from atomic vector cylG <- qG(mtcars$cyl)     # Quick-group from atomic vector cylG                       # See the simple structure of this object #>  [1] 2 2 1 2 3 2 3 1 1 2 2 3 3 3 3 3 3 1 1 1 1 3 3 3 3 1 1 1 3 2 3 1 #> attr(,\"N.groups\") #> [1] 3 #> attr(,\"class\") #> [1] \"qG\"  cf  <- qF(wlddev$country)  # Bigger data cf2 <- qF(wlddev$country, na.exclude = FALSE)  # With na.included class dat <- num_vars(wlddev)   # cf2 is faster in grouped operations because no missing value check is performed library(microbenchmark) microbenchmark(fmax(dat, cf), fmax(dat, cf2)) #> Unit: microseconds #>            expr    min      lq     mean  median       uq     max neval #>   fmax(dat, cf) 93.234 94.0745 97.88012 96.7600 100.3475 119.515   100 #>  fmax(dat, cf2) 87.412 88.3755 92.40908 91.7375  94.1155 130.626   100  finteraction(mtcars$cyl, mtcars$vs)  # Interacting two variables (can be factors) #>  [1] 6.0 6.0 4.1 6.1 8.0 6.1 8.0 4.1 4.1 6.1 6.1 8.0 8.0 8.0 8.0 8.0 8.0 4.1 4.1 #> [20] 4.1 4.1 8.0 8.0 8.0 8.0 4.1 4.0 4.1 8.0 6.0 8.0 4.1 #> Levels: 4.0 4.1 6.0 6.1 8.0 head(finteraction(mtcars))           # A more crude example.. #> [1] 21.6.160.110.3.9.2.62.16.46.0.1.4.4     #> [2] 21.6.160.110.3.9.2.875.17.02.0.1.4.4    #> [3] 22.8.4.108.93.3.85.2.32.18.61.1.1.4.1   #> [4] 21.4.6.258.110.3.08.3.215.19.44.1.0.3.1 #> [5] 18.7.8.360.175.3.15.3.44.17.02.0.0.3.2  #> [6] 18.1.6.225.105.2.76.3.46.20.22.1.0.3.1  #> 32 Levels: 10.4.8.460.215.3.5.424.17.82.0.0.3.4 ...  finteraction(mtcars$cyl, mtcars$vs, factor = FALSE) # Returns 'qG', by default unsorted #>  [1] 1 1 2 3 4 3 4 2 2 3 3 4 4 4 4 4 4 2 2 2 2 4 4 4 4 2 5 2 4 1 4 2 #> attr(,\"N.groups\") #> [1] 5 #> attr(,\"class\") #> [1] \"qG\"          \"na.included\" group(mtcars$cyl, mtcars$vs) # Same thing #>  [1] 1 1 2 3 4 3 4 2 2 3 3 4 4 4 4 4 4 2 2 2 2 4 4 4 4 2 5 2 4 1 4 2 #> attr(,\"N.groups\") #> [1] 5 #> attr(,\"class\") #> [1] \"qG\"          \"na.included\""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"qsu, shorthand quick-summary, extremely fast summary command inspired (xt)summarize command STATA statistical software. computes set 7 statistics (nobs, mean, sd, min, max, skewness kurtosis) using numerically stable one-pass method generalized Welford's Algorithm. Statistics can computed weighted, groups, also within-entities (panel data, see Details).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"","code":"qsu(x, ...)  # Default S3 method qsu(x, g = NULL, pid = NULL, w = NULL, higher = FALSE,     array = TRUE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'matrix' qsu(x, g = NULL, pid = NULL, w = NULL, higher = FALSE,     array = TRUE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'data.frame' qsu(x, by = NULL, pid = NULL, w = NULL, cols = NULL, higher = FALSE,     array = TRUE, labels = FALSE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'grouped_df' qsu(x, pid = NULL, w = NULL, higher = FALSE,     array = TRUE, labels = FALSE, stable.algo = .op[[\"stable.algo\"]], ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' qsu(x, g = NULL, w = NULL, effect = 1L, higher = FALSE,     array = TRUE, stable.algo = .op[[\"stable.algo\"]], ...)  # S3 method for class 'pdata.frame' qsu(x, by = NULL, w = NULL, cols = NULL, effect = 1L, higher = FALSE,     array = TRUE, labels = FALSE, stable.algo = .op[[\"stable.algo\"]], ...)  # Methods for compatibility with sf:  # S3 method for class 'sf' qsu(x, by = NULL, pid = NULL, w = NULL, cols = NULL, higher = FALSE,     array = TRUE, labels = FALSE, stable.algo = .op[[\"stable.algo\"]], ...)   # S3 method for class 'qsu' as.data.frame(x, ..., gid = \"Group\", stringsAsFactors = TRUE)  # S3 method for class 'qsu' print(x, digits = .op[[\"digits\"]] + 2L, nonsci.digits = 9, na.print = \"-\",       return = FALSE, print.gap = 2, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"x vector, matrix, data frame, 'indexed_series' ('pseries') 'indexed_frame' ('pdata.frame'). g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. (p)data.frame method: g, also allows one- two-sided formulas .e. ~ group1 + group2 var1 + var2 ~ group1 + group2. See Examples. pid input g/: Specify panel-identifier also compute statistics - within- transformed data. Data frame method also supports one- two-sided formulas, grouped_df method supports expressions evaluated data environment. Transformations taken independently grouping g/(grouped statistics computed transformed data g/also used). However, passing LHS variables pid overwrite LHS variables passed . w vector (non-negative) weights. Adding weights compute weighted mean, sd, skewness kurtosis, transform data using weighted individual means pid used. \"WeightSum\" column added giving sum weights, see also Details. Data frame method supports formula, grouped_df method supports expression. cols select columns summarize using column names, indices, logical vector function (e.g. .numeric). Two-sided formulas passed pid overwrite cols. higher logical. Add higher moments (skewness kurtosis). array logical. computations 2 dimensions (maximum 4D: variables, statistics, groups panel-decomposition) TRUE returns array, FALSE returns (nested) list matrices. stable.algo logical. FALSE uses faster less stable method calculate standard deviation (see Details fsd). available w = NULL higher = FALSE. labels logical TRUE function: display variable labels summary. See Details. effect plm methods: Select panel identifier used within transformations data. 1L takes first variable index, 2L second etc.. Index variables can also called name using character string. one variable can supplied. ... arguments passed methods. gid character. Name assigned group-id column, summarising variables groups. stringsAsFactors logical. Make factors dimension names 'qsu' array. option .data.frame.table. digits number digits print comma/dot. nonsci.digits number digits print resorting scientific notation (default print numbers 9 digits print larger numbers scientifically). na.print character string substitute missing values. return logical. print instead return formatted object. print.gap integer. Spacing printed columns. Passed print.default.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"algorithm used compute statistics well described [see sections Welford's online algorithm, Weighted incremental algorithm Higher-order statistics. Skewness kurtosis calculated described Higher-order statistics mathematically identical implemented moments package. Just note qsu computes kurtosis (like momens::kurtosis), excess-kurtosis (=  kurtosis - 3) defined Higher-order statistics. Weighted incremental algorithm described can easily generalized higher-order statistics]. Grouped computations specified g/carried extremely efficiently fsum (single pass, without splitting data). pid used, qsu performs panel-decomposition variable computes 3 sets statistics: Statistics computed 'Overall' (raw) data, statistics computed '' - transformed (pid - averaged) data, statistics computed 'Within' - transformed (pid - demeaned) data. formally, let x (bold) panel vector data N individuals indexed , recorded T periods, indexed t. xit denotes single data-point belonging individual time-period t (t/T must represent time). xi. denotes average values individual (averaged t), extension xN. vector (length N) averages individuals. groups supplied g/, '' statistics computed xN., vector individual averages. (means non-balanced panel presence missing values, 'Overall' mean computed x can slightly different '' mean computed xN., variance decomposition exact). groups supplied g/, xN. expanded vector xi. (length N x T) replacing value xit x xi., preserving missing values x. Grouped -statistics computed xi., difference number observations ('-N') reported group number distinct non-missing values xi. group (total number non-missing values xi. group, already reported 'Overall-N'). See Examples. 'Within' statistics always computed vector x - xi. + x.., x.. simply 'Overall' mean computed x, added back preserve level data. 'Within' mean computed data always identical 'Overall' mean. summary output, qsu reports 'N', identical 'Overall-N', 'T', average number time-periods data available individual obtained 'T' = 'Overall-N / '-N'. using weights (w) panel data (pid), '' sum weights also simply number groups, 'Within' sum weights 'Overall' sum weights divided number groups. See Examples. Apart 'N/T' extrema, standard-deviations ('SD') computed - within- transformed data extremely valuable indicate much variation panel-variable -individuals much variation within-individuals (time). extremes, variables common values across individuals (time-variable(s) 't' balanced panel), can readily identified individual-invariant '-SD' variable 0 'Within-SD' equal 'Overall-SD'. Analogous, time-invariant individual characteristics (individual-id '') 0 'Within-SD' '-SD' equal 'Overall-SD'. See Examples. data frame methods, labels = TRUE, qsu uses function(x) paste(names(x), setv(vlabels(x), NA, \"\"), sep = \": \") combine variable names labels display. Alternatively, user can pass custom function applied data frame, e.g. using labels = vlabels just displays labels. See also vlabels. qsu comes print method default writes 9 digits 4 decimal places. Larger numbers printed scientific format. numbers 7 9 digits, apostrophe (') placed 6th digit designate millions. Missing values printed using '-'. sf method simply ignores geometry column.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"vector, matrix, array list matrices summary statistics. matrices arrays class 'qsu' class 'table' attached.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"weighted summaries, observations missing zero weights skipped, thus affect calculated statistics, including observation count. also implies logical vector passed w can used efficiently summarize subset data.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"Welford, B. P. (1962). Note method calculating corrected sums squares products. Technometrics. 4 (3): 419-420. doi:10.2307/1266577.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"note-1","dir":"Reference","previous_headings":"","what":"Note","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"weights w used together pid, transformed data computed using weighted individual means .e. weighted xi. weighted x... Weighted statistics subsequently computed weighted-transformed data.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/qsu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Grouped, Weighted) Summary Statistics for Cross-Sectional and Panel Data — qsu","text":"","code":"## World Development Panel Data # Simple Summaries ------------------------- qsu(wlddev)                                 # Simple summary #>              N         Mean          SD          Min             Max #> country  13176            -           -            -               - #> iso3c    13176            -           -            -               - #> date     13176            -           -            -               - #> year     13176         1990     17.6075         1960            2020 #> decade   13176    1985.5738     17.5117         1960            2020 #> region   13176            -           -            -               - #> income   13176            -           -            -               - #> OECD     13176            -           -            -               - #> PCGDP     9470    12048.778  19077.6416     132.0776      196061.417 #> LIFEEX   11670      64.2963     11.4764       18.907         85.4171 #> GINI      1744      38.5341      9.2006         20.7            65.8 #> ODA       8608   454'720131  868'712654  -997'679993  2.56715605e+10 #> POP      12919  24'245971.6  102'120674         2833  1.39771500e+09 qsu(wlddev, labels = TRUE)                  # Display variable labels #>                                                                                             N #> country: Country Name                                                                   13176 #> iso3c: Country Code                                                                     13176 #> date: Date Recorded (Fictitious)                                                        13176 #> year: Year                                                                              13176 #> decade: Decade                                                                          13176 #> region: Region                                                                          13176 #> income: Income Level                                                                    13176 #> OECD: Is OECD Member Country?                                                           13176 #> PCGDP: GDP per capita (constant 2010 US$)                                                9470 #> LIFEEX: Life expectancy at birth, total (years)                                         11670 #> GINI: Gini index (World Bank estimate)                                                   1744 #> ODA: Net official development assistance and official aid received (constant 2018 US$)   8608 #> POP: Population, total                                                                  12919 #>                                                                                                Mean #> country: Country Name                                                                             - #> iso3c: Country Code                                                                               - #> date: Date Recorded (Fictitious)                                                                  - #> year: Year                                                                                     1990 #> decade: Decade                                                                            1985.5738 #> region: Region                                                                                    - #> income: Income Level                                                                              - #> OECD: Is OECD Member Country?                                                                     - #> PCGDP: GDP per capita (constant 2010 US$)                                                 12048.778 #> LIFEEX: Life expectancy at birth, total (years)                                             64.2963 #> GINI: Gini index (World Bank estimate)                                                      38.5341 #> ODA: Net official development assistance and official aid received (constant 2018 US$)   454'720131 #> POP: Population, total                                                                  24'245971.6 #>                                                                                                 SD #> country: Country Name                                                                            - #> iso3c: Country Code                                                                              - #> date: Date Recorded (Fictitious)                                                                 - #> year: Year                                                                                 17.6075 #> decade: Decade                                                                             17.5117 #> region: Region                                                                                   - #> income: Income Level                                                                             - #> OECD: Is OECD Member Country?                                                                    - #> PCGDP: GDP per capita (constant 2010 US$)                                               19077.6416 #> LIFEEX: Life expectancy at birth, total (years)                                            11.4764 #> GINI: Gini index (World Bank estimate)                                                      9.2006 #> ODA: Net official development assistance and official aid received (constant 2018 US$)  868'712654 #> POP: Population, total                                                                  102'120674 #>                                                                                                 Min #> country: Country Name                                                                             - #> iso3c: Country Code                                                                               - #> date: Date Recorded (Fictitious)                                                                  - #> year: Year                                                                                     1960 #> decade: Decade                                                                                 1960 #> region: Region                                                                                    - #> income: Income Level                                                                              - #> OECD: Is OECD Member Country?                                                                     - #> PCGDP: GDP per capita (constant 2010 US$)                                                  132.0776 #> LIFEEX: Life expectancy at birth, total (years)                                              18.907 #> GINI: Gini index (World Bank estimate)                                                         20.7 #> ODA: Net official development assistance and official aid received (constant 2018 US$)  -997'679993 #> POP: Population, total                                                                         2833 #>                                                                                                    Max #> country: Country Name                                                                                - #> iso3c: Country Code                                                                                  - #> date: Date Recorded (Fictitious)                                                                     - #> year: Year                                                                                        2020 #> decade: Decade                                                                                    2020 #> region: Region                                                                                       - #> income: Income Level                                                                                 - #> OECD: Is OECD Member Country?                                                                        - #> PCGDP: GDP per capita (constant 2010 US$)                                                   196061.417 #> LIFEEX: Life expectancy at birth, total (years)                                                85.4171 #> GINI: Gini index (World Bank estimate)                                                            65.8 #> ODA: Net official development assistance and official aid received (constant 2018 US$)  2.56715605e+10 #> POP: Population, total                                                                  1.39771500e+09 qsu(wlddev, higher = TRUE)                  # Add skewness and kurtosis #>              N         Mean          SD          Min             Max     Skew #> country  13176            -           -            -               -        - #> iso3c    13176            -           -            -               -        - #> date     13176            -           -            -               -        - #> year     13176         1990     17.6075         1960            2020       -0 #> decade   13176    1985.5738     17.5117         1960            2020   0.0326 #> region   13176            -           -            -               -        - #> income   13176            -           -            -               -        - #> OECD     13176            -           -            -               -        - #> PCGDP     9470    12048.778  19077.6416     132.0776      196061.417   3.1276 #> LIFEEX   11670      64.2963     11.4764       18.907         85.4171  -0.6748 #>              Kurt #> country         - #> iso3c           - #> date            - #> year       1.7994 #> decade     1.7917 #> region          - #> income          - #> OECD            - #> PCGDP     17.1154 #> LIFEEX     2.6718 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]  # Grouped Summaries ------------------------ qsu(wlddev, ~ region, labels = TRUE)        # Statistics by World Bank Region #> , , country: Country Name #>  #>                                N  Mean  SD  Min  Max #> East Asia & Pacific         2196     -   -    -    - #> Europe & Central Asia       3538     -   -    -    - #> Latin America & Caribbean   2562     -   -    -    - #> Middle East & North Africa  1281     -   -    -    - #> North America                183     -   -    -    - #> South Asia                   488     -   -    -    - #> Sub-Saharan Africa          2928     -   -    -    - #>  #> , , iso3c: Country Code #>  #>                                N  Mean  SD  Min  Max #> East Asia & Pacific         2196     -   -    -    - #> Europe & Central Asia       3538     -   -    -    - #> Latin America & Caribbean   2562     -   -    -    - #> Middle East & North Africa  1281     -   -    -    - #> North America                183     -   -    -    - #> South Asia                   488     -   -    -    - #> Sub-Saharan Africa          2928     -   -    -    - #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 slices ]  qsu(wlddev, PCGDP + LIFEEX ~ income)        # Summarize GDP per Capita and Life Expectancy by #> , , PCGDP #>  #>                         N        Mean          SD       Min         Max #> High income          3179  30280.7283  23847.0483  932.0417  196061.417 #> Low income           1311    597.4053    288.4392  164.3366   1864.7925 #> Lower middle income  2246   1574.2535    858.7183  144.9863   4818.1922 #> Upper middle income  2734   4945.3258   2979.5609  132.0776  20532.9523 #>  #> , , LIFEEX #>  #>                         N     Mean      SD     Min      Max #> High income          3831  73.6246  5.6693  42.672  85.4171 #> Low income           1800  49.7301  9.0944  26.172    74.43 #> Lower middle income  2790  58.1481  9.3115  18.907   76.699 #> Upper middle income  3249  66.6466   7.537  36.535   80.279 #>  stats <- qsu(wlddev, ~ region + income,     # World Bank Income Level              cols = 9:10, higher = TRUE)    # Same variables, by both region and income aperm(stats)                                # A different perspective on the same stats #> , , East Asia & Pacific.High income #>  #>           N        Mean          SD       Min         Max     Skew    Kurt #> PCGDP   487  26766.9163  14823.5175  932.0417  71992.1517   0.2811  2.5566 #> LIFEEX  664     73.3724       6.659     54.81      85.078  -0.4945  2.6876 #>  #> , , East Asia & Pacific.Lower middle income #>  #>           N       Mean        SD       Min        Max     Skew    Kurt #> PCGDP   562  1599.4748  895.5278  144.9863  4503.1454   0.4987  3.0945 #> LIFEEX  780    59.1597    9.6542    18.907       75.4  -1.0076  4.1679 #>  #> , , East Asia & Pacific.Upper middle income #>  #>           N       Mean         SD       Min         Max     Skew    Kurt #> PCGDP   418  3561.0911  2468.1843  132.0776  12486.6788   1.4194    4.89 #> LIFEEX  363    66.9366     5.6713    43.725       77.15  -0.9194  4.8567 #>  #> , , Europe & Central Asia.High income #>  #>            N        Mean          SD        Min         Max    Skew     Kurt #> PCGDP   1551  35718.5696  26466.4643  4500.7362  196061.417  2.2834  10.2728 #> LIFEEX  1845     74.8234      4.3845    63.0749     85.4171   0.042   2.1824 #>  #> , , Europe & Central Asia.Low income #>  #>          N      Mean        SD       Min        Max    Skew    Kurt #> PCGDP   35  809.4753  336.5425  366.9354  1458.9932   0.426  1.9987 #> LIFEEX  60   60.1129     6.147    50.613     71.097  0.3991  1.9875 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 18 slices ]   # Grouped summary wlddev |> fgroup_by(region) |> fselect(PCGDP, LIFEEX) |> qsu() #> , , PCGDP #>  #>                                N        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , LIFEEX #>  #>                                N     Mean       SD      Min      Max #> East Asia & Pacific         1807  65.9445  10.1633   18.907   85.078 #> Europe & Central Asia       3046  72.1625   5.7602   45.369  85.4171 #> Latin America & Caribbean   2107  68.3486   7.3768   41.762  82.1902 #> Middle East & North Africa  1226  66.2508   9.8306   29.919  82.8049 #> North America                144  76.2867   3.5734  68.8978  82.0488 #> South Asia                   480  57.5585  11.3004   32.446   78.921 #> Sub-Saharan Africa          2860   51.581   8.6876   26.172  74.5146 #>   # Panel Data Summaries --------------------- qsu(wlddev, pid = ~ iso3c, labels = TRUE)   # Adding between and within countries statistics #> , , country: Country Name #>  #>            N/T  Mean  SD  Min  Max #> Overall  13176     -   -    -    - #> Between    216     -   -    -    - #> Within      61     -   -    -    - #>  #> , , date: Date Recorded (Fictitious) #>  #>            N/T  Mean  SD  Min  Max #> Overall  13176     -   -    -    - #> Between    216     -   -    -    - #> Within      61     -   -    -    - #>  #> , , year: Year #>  #>            N/T  Mean       SD   Min   Max #> Overall  13176  1990  17.6075  1960  2020 #> Between    216  1990        0  1990  1990 #> Within      61  1990  17.6075  1960  2020 #>  #> , , decade: Decade #>  #>            N/T       Mean       SD        Min        Max #> Overall  13176  1985.5738  17.5117       1960       2020 #> Between    216  1985.5738        0  1985.5738  1985.5738 #> Within      61  1985.5738  17.5117       1960       2020 #>  #> , , region: Region #>  #>            N/T  Mean  SD  Min  Max #> Overall  13176     -   -    -    - #> Between    216     -   -    -    - #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 7 slices ]  # -> They show amongst other things that year and decade are individual-invariant, # that we have GINI-data on only 161 countries, with only 8.42 observations per country on average, # and that GDP, LIFEEX and GINI vary more between-countries, but ODA received varies more within # countries over time.  # Let's do this manually for PCGDP: x <- wlddev$PCGDP g <- wlddev$iso3c  # This is the exact variance decomposion all.equal(fvar(x), fvar(B(x, g)) + fvar(W(x, g))) #> [1] TRUE  # What qsu does is calculate r <- rbind(Overall = qsu(x),            Between = qsu(fmean(x, g)), # Aggregation instead of between-transform            Within = qsu(fwithin(x, g, mean = \"overall.mean\"))) # Same as qsu(W(x, g) + fmean(x)) r[3, 1] <- r[1, 1] / r[2, 1] print.qsu(r) #>                N        Mean          SD          Min         Max #> Overall     9470   12048.778  19077.6416     132.0776  196061.417 #> Between      206  12962.6054  20189.9007     253.1886   141200.38 #> Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254 # Proof: qsu(x, pid = g) #>              N/T        Mean          SD          Min         Max #> Overall     9470   12048.778  19077.6416     132.0776  196061.417 #> Between      206  12962.6054  20189.9007     253.1886   141200.38 #> Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254  # Using indexed data: wldi <- findex_by(wlddev, iso3c, year)   # Creating a Indexed Data Frame frame from this data qsu(wldi)                                # Summary for pdata.frame -> qsu(wlddev, pid = ~ iso3c) #> , , country #>  #>            N/T  Mean  SD  Min  Max #> Overall  13176     -   -    -    - #> Between    216     -   -    -    - #> Within      61     -   -    -    - #>  #> , , iso3c #>  #>            N/T  Mean  SD  Min  Max #> Overall  13176     -   -    -    - #> Between    216     -   -    -    - #> Within      61     -   -    -    - #>  #> , , date #>  #>            N/T  Mean  SD  Min  Max #> Overall  13176     -   -    -    - #> Between    216     -   -    -    - #> Within      61     -   -    -    - #>  #> , , year #>  #>            N/T  Mean       SD   Min   Max #> Overall  13176  1990  17.6075  1960  2020 #> Between    216  1990        0  1990  1990 #> Within      61  1990  17.6075  1960  2020 #>  #> , , decade #>  #>            N/T       Mean       SD        Min        Max #> Overall  13176  1985.5738  17.5117       1960       2020 #> Between    216  1985.5738        0  1985.5738  1985.5738 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 8 slices ]  qsu(wldi$PCGDP)                          # Default summary for Panel Series #>              N/T        Mean          SD          Min         Max #> Overall     9470   12048.778  19077.6416     132.0776  196061.417 #> Between      206  12962.6054  20189.9007     253.1886   141200.38 #> Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254 qsu(G(wldi$PCGDP))                       # Summarizing GDP growth, see also ?G #>              N/T    Mean      SD       Min       Max #> Overall     9264  2.0762  6.0081  -64.9924  140.3708 #> Between      202  2.0752  1.8684   -7.6806   10.3106 #> Within   45.8614  2.0762   5.785   -67.359  133.0971  # Grouped Panel Data Summaries ------------- qsu(wlddev, ~ region, ~ iso3c, cols = 9:12) # Panel-Statistics by region #> , , Overall, PCGDP #>  #>                              N/T        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , Between, PCGDP #>  #>                             N/T        Mean          SD         Min         Max #> East Asia & Pacific          34  10513.2441   12771.742    444.2899  39722.0077 #> Europe & Central Asia        56  25992.9618   24051.035    809.4753   141200.38 #> Latin America & Caribbean    38   7628.4477   8470.9708   1357.3326  77403.7443 #> Middle East & North Africa   20  13878.4213  17251.6962   1069.6596  64878.4021 #> North America                 3    48699.76  18604.4369  35260.4708  74934.5874 #> South Asia                    8   1235.9256   1488.3669      413.68   6621.5002 #> Sub-Saharan Africa           47   1840.0259   2234.3254    253.1886   9922.0052 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 slices ]  psr <- qsu(wldi, ~ region, cols = 9:12)     # Same on indexed data psr                                         # -> Gives a 4D array #> , , Overall, PCGDP #>  #>                              N/T        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , Between, PCGDP #>  #>                             N/T        Mean          SD         Min         Max #> East Asia & Pacific          34  10513.2441   12771.742    444.2899  39722.0077 #> Europe & Central Asia        56  25992.9618   24051.035    809.4753   141200.38 #> Latin America & Caribbean    38   7628.4477   8470.9708   1357.3326  77403.7443 #> Middle East & North Africa   20  13878.4213  17251.6962   1069.6596  64878.4021 #> North America                 3    48699.76  18604.4369  35260.4708  74934.5874 #> South Asia                    8   1235.9256   1488.3669      413.68   6621.5002 #> Sub-Saharan Africa           47   1840.0259   2234.3254    253.1886   9922.0052 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 10 slices ]  psr[,\"N/T\",,]                               # Checking out the number of observations: #> , , PCGDP #>  #>                             Overall  Between   Within #> East Asia & Pacific            1467       34  43.1471 #> Europe & Central Asia          2243       56  40.0536 #> Latin America & Caribbean      1976       38       52 #> Middle East & North Africa      842       20     42.1 #> North America                   180        3       60 #> South Asia                      382        8    47.75 #> Sub-Saharan Africa             2380       47  50.6383 #>  #> , , LIFEEX #>  #>                             Overall  Between   Within #> East Asia & Pacific            1807       32  56.4688 #> Europe & Central Asia          3046       55  55.3818 #> Latin America & Caribbean      2107       40   52.675 #> Middle East & North Africa     1226       21   58.381 #> North America                   144        3       48 #> South Asia                      480        8       60 #> Sub-Saharan Africa             2860       48  59.5833 #>  #> , , GINI #>  #>                             Overall  Between   Within #> East Asia & Pacific             154       23   6.6957 #> Europe & Central Asia           798       49  16.2857 #> Latin America & Caribbean       413       25    16.52 #> Middle East & North Africa       91       15   6.0667 #> North America                    49        2     24.5 #> South Asia                       46        7   6.5714 #> Sub-Saharan Africa              193       46   4.1957 #>  #> , , ODA #>  #>                             Overall  Between   Within #> East Asia & Pacific            1537       31  49.5806 #> Europe & Central Asia           787       32  24.5938 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]  # In North america we only have 3 countries, for the GINI we only have 3.91 observations on average # for 45 Sub-Saharan-African countries, etc.. psr[,\"SD\",,]                                # Considering only standard deviations #> , , PCGDP #>  #>                                Overall     Between      Within #> East Asia & Pacific         14383.5507   12771.742   6615.8248 #> Europe & Central Asia       26435.1316   24051.035  10971.0483 #> Latin America & Caribbean    8818.5055   8470.9708   2451.2636 #> Middle East & North Africa  18419.7912  17251.6962   6455.0512 #> North America               24196.2855  18604.4369  15470.4609 #> South Asia                   1611.2232   1488.3669    617.0934 #> Sub-Saharan Africa           2596.0104   2234.3254    1321.764 #>  #> , , LIFEEX #>  #>                             Overall  Between  Within #> East Asia & Pacific         10.1633   7.6833  6.6528 #> Europe & Central Asia        5.7602   4.4378  3.6723 #> Latin America & Caribbean    7.3768   4.9199  5.4965 #> Middle East & North Africa   9.8306    5.922  7.8467 #> North America                3.5734   1.3589  3.3049 #> South Asia                  11.3004   5.6158  9.8062 #> Sub-Saharan Africa           8.6876    5.657  6.5933 #>  #> , , GINI #>  #>                             Overall  Between  Within #> East Asia & Pacific          5.0318   4.3005  2.6125 #> Europe & Central Asia        4.5809   4.0611  2.1195 #> Latin America & Caribbean    5.4821   4.0492  3.6955 #> Middle East & North Africa   5.2073   4.7002  2.2415 #> North America                3.6972   3.3563  1.5507 #> South Asia                   3.9898   3.0052  2.6244 #> Sub-Saharan Africa           8.2003   6.8844  4.4553 #>  #> , , ODA #>  #>                                    Overall         Between          Within #> East Asia & Pacific             622'847624      457'183279      422'992450 #> Europe & Central Asia           568'237036      438'074771      361'916875 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]  # -> In all regions variations in inequality (GINI) between countries are greater than variations # in inequality within countries. The opposite is true for Life-Expectancy in all regions apart # from Europe, etc..  # Again let's do this manually for PDGCP: d <- cbind(Overall = x,            Between = fbetween(x, g),            Within = fwithin(x, g, mean = \"overall.mean\"))  r <- qsu(d, g = wlddev$region) r[,\"N\",\"Between\"] <- fndistinct(g[!is.na(x)], wlddev$region[!is.na(x)]) r[,\"N\",\"Within\"] <- r[,\"N\",\"Overall\"] / r[,\"N\",\"Between\"] r #> , , Overall #>  #>                                N        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , Between #>  #>                              N        Mean          SD         Min         Max #> East Asia & Pacific         34  10513.2441   12771.742    444.2899  39722.0077 #> Europe & Central Asia       56  25992.9618   24051.035    809.4753   141200.38 #> Latin America & Caribbean   38   7628.4477   8470.9708   1357.3326  77403.7443 #> Middle East & North Africa  20  13878.4213  17251.6962   1069.6596  64878.4021 #> North America                3    48699.76  18604.4369  35260.4708  74934.5874 #> South Asia                   8   1235.9256   1488.3669      413.68   6621.5002 #> Sub-Saharan Africa          47   1840.0259   2234.3254    253.1886   9922.0052 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 slice ]   # Proof: qsu(wlddev, PCGDP ~ region, ~ iso3c) #> , , Overall #>  #>                              N/T        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , Between #>  #>                             N/T        Mean          SD         Min         Max #> East Asia & Pacific          34  10513.2441   12771.742    444.2899  39722.0077 #> Europe & Central Asia        56  25992.9618   24051.035    809.4753   141200.38 #> Latin America & Caribbean    38   7628.4477   8470.9708   1357.3326  77403.7443 #> Middle East & North Africa   20  13878.4213  17251.6962   1069.6596  64878.4021 #> North America                 3    48699.76  18604.4369  35260.4708  74934.5874 #> South Asia                    8   1235.9256   1488.3669      413.68   6621.5002 #> Sub-Saharan Africa           47   1840.0259   2234.3254    253.1886   9922.0052 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 slice ]   # Weighted Summaries ----------------------- n <- nrow(wlddev) weights <- abs(rnorm(n))                    # Generate random weights qsu(wlddev, w = weights, higher = TRUE)     # Computed weighted mean, SD, skewness and kurtosis #>              N   WeightSum         Mean           SD          Min #> country  13176  10446.1645            -            -            - #> iso3c    13176  10446.1645            -            -            - #> date     13176  10446.1645            -            -            - #> year     13176  10446.1645    1990.0653       17.583         1960 #> decade   13176  10446.1645     1985.626      17.4879         1960 #> region   13176  10446.1645            -            -            - #> income   13176  10446.1645            -            -            - #> OECD     13176  10446.1645            -            -            - #>                     Max     Skew      Kurt #> country               -        -         - #> iso3c                 -        -         - #> date                  -        -         - #> year               2020   -0.007    1.8062 #> decade             2020   0.0297    1.8019 #> region                -        -         - #> income                -        -         - #> OECD                  -        -         - #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] weightsNA <- weights                        # Weights may contain missing values.. inserting 1000 weightsNA[sample.int(n, 1000)] <- NA qsu(wlddev, w = weightsNA, higher = TRUE)   # But now these values are removed from all variables #>              N  WeightSum         Mean          SD          Min             Max #> country  12176  9626.6245            -           -            -               - #> iso3c    12176  9626.6245            -           -            -               - #> date     12176  9626.6245            -           -            -               - #> year     12176  9626.6245    1990.1234      17.579         1960            2020 #> decade   12176  9626.6245    1985.6685     17.4903         1960            2020 #> region   12176  9626.6245            -           -            -               - #> income   12176  9626.6245            -           -            -               - #> OECD     12176  9626.6245            -           -            -               - #>             Skew     Kurt #> country        -        - #> iso3c          -        - #> date           -        - #> year     -0.0103    1.801 #> decade    0.0266   1.7994 #> region         -        - #> income         -        - #> OECD           -        - #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]  # Grouped and panel-summaries can also be weighted in the same manner  # Alternative Output Formats --------------- # Simple case as.data.frame(qsu(mtcars)) #>    Variable  N       Mean          SD    Min     Max #> 1       mpg 32  20.090625   6.0269481 10.400  33.900 #> 2       cyl 32   6.187500   1.7859216  4.000   8.000 #> 3      disp 32 230.721875 123.9386938 71.100 472.000 #> 4        hp 32 146.687500  68.5628685 52.000 335.000 #> 5      drat 32   3.596563   0.5346787  2.760   4.930 #> 6        wt 32   3.217250   0.9784574  1.513   5.424 #> 7      qsec 32  17.848750   1.7869432 14.500  22.900 #> 8        vs 32   0.437500   0.5040161  0.000   1.000 #> 9        am 32   0.406250   0.4989909  0.000   1.000 #> 10     gear 32   3.687500   0.7378041  3.000   5.000 #> 11     carb 32   2.812500   1.6152000  1.000   8.000 # For matrices can also use qDF/qDT/qTBL to assign custom name and get a character-id qDF(qsu(mtcars), \"car\") #>     car  N       Mean          SD    Min     Max #> 1   mpg 32  20.090625   6.0269481 10.400  33.900 #> 2   cyl 32   6.187500   1.7859216  4.000   8.000 #> 3  disp 32 230.721875 123.9386938 71.100 472.000 #> 4    hp 32 146.687500  68.5628685 52.000 335.000 #> 5  drat 32   3.596563   0.5346787  2.760   4.930 #> 6    wt 32   3.217250   0.9784574  1.513   5.424 #> 7  qsec 32  17.848750   1.7869432 14.500  22.900 #> 8    vs 32   0.437500   0.5040161  0.000   1.000 #> 9    am 32   0.406250   0.4989909  0.000   1.000 #> 10 gear 32   3.687500   0.7378041  3.000   5.000 #> 11 carb 32   2.812500   1.6152000  1.000   8.000 # DF from 3D array: do not combine with aperm(), might introduce wrong column labels as.data.frame(stats, gid = \"Region_Income\") #>   Variable                             Region_Income    N       Mean         SD #> 1    PCGDP           East Asia & Pacific.High income  487 26766.9163 14823.5175 #> 2    PCGDP   East Asia & Pacific.Lower middle income  562  1599.4748   895.5278 #> 3    PCGDP   East Asia & Pacific.Upper middle income  418  3561.0911  2468.1843 #> 4    PCGDP         Europe & Central Asia.High income 1551 35718.5696 26466.4643 #> 5    PCGDP          Europe & Central Asia.Low income   35   809.4753   336.5425 #> 6    PCGDP Europe & Central Asia.Lower middle income  125  1804.0338   964.3793 #> 7    PCGDP Europe & Central Asia.Upper middle income  532  4979.0901  2720.4775 #>         Min        Max      Skew      Kurt #> 1  932.0417  71992.152 0.2811494  2.556594 #> 2  144.9863   4503.145 0.4987346  3.094502 #> 3  132.0776  12486.679 1.4193597  4.889957 #> 4 4500.7362 196061.417 2.2834403 10.272790 #> 5  366.9354   1458.993 0.4260281  1.998718 #> 6  534.9587   4243.774 0.6385915  2.298402 #> 7  700.7008  15190.099 1.0954612  4.170587 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 39 rows ] # DF from 4D array: also no aperm() as.data.frame(qsu(wlddev, ~ income, ~ iso3c, cols = 9:10), gid = \"Region\") #>   Variable              Region   Trans        N/T       Mean         SD #> 1    PCGDP         High income Overall 3179.00000 30280.7283 23847.0483 #> 2    PCGDP         High income Between   71.00000 30280.7283 20908.5323 #> 3    PCGDP         High income  Within   44.77465 12048.7780 11467.9987 #> 4    PCGDP          Low income Overall 1311.00000   597.4053   288.4392 #> 5    PCGDP          Low income Between   28.00000   597.4053   243.8219 #> 6    PCGDP          Low income  Within   46.82143 12048.7780   154.1039 #> 7    PCGDP Lower middle income Overall 2246.00000  1574.2535   858.7183 #> 8    PCGDP Lower middle income Between   47.00000  1574.2535   676.3157 #>           Min        Max #> 1    932.0417 196061.417 #> 2   5413.4495 141200.380 #> 3 -33504.8721  76767.525 #> 4    164.3366   1864.793 #> 5    253.1886   1357.333 #> 6  11606.2382  12698.296 #> 7    144.9863   4818.192 #> 8    444.2899   2896.868 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 16 rows ]  # Output as nested list psrl <- qsu(wlddev, ~ income, ~ iso3c, cols = 9:10, array = FALSE) psrl #> $PCGDP #> $PCGDP$Overall #>                         N        Mean          SD       Min         Max #> High income          3179  30280.7283  23847.0483  932.0417  196061.417 #> Low income           1311    597.4053    288.4392  164.3366   1864.7925 #> Lower middle income  2246   1574.2535    858.7183  144.9863   4818.1922 #> Upper middle income  2734   4945.3258   2979.5609  132.0776  20532.9523 #>  #> $PCGDP$Between #>                       N        Mean          SD        Min         Max #> High income          71  30280.7283  20908.5323  5413.4495   141200.38 #> Low income           28    597.4053    243.8219   253.1886   1357.3326 #> Lower middle income  47   1574.2535    676.3157   444.2899   2896.8682 #> Upper middle income  60   4945.3258   2327.3834   1604.595  13344.5423 #>  #> $PCGDP$Within #>                            N       Mean          SD          Min         Max #> High income          44.7746  12048.778  11467.9987  -33504.8721  76767.5254 #> Low income           46.8214  12048.778    154.1039   11606.2382   12698.296 #> Lower middle income  47.7872  12048.778    529.1449   10377.7234  14603.1055 #> Upper middle income  45.5667  12048.778    1860.395    4846.3834  24883.1246 #>  #>  #> $LIFEEX #> $LIFEEX$Overall #>                         N     Mean      SD     Min      Max #> High income          3831  73.6246  5.6693  42.672  85.4171 #> Low income           1800  49.7301  9.0944  26.172    74.43 #> Lower middle income  2790  58.1481  9.3115  18.907   76.699 #> Upper middle income  3249  66.6466   7.537  36.535   80.279 #>  #> $LIFEEX$Between #>                       N     Mean      SD      Min      Max #> High income          73  73.6246  3.3499  64.0302  85.4171 #> Low income           30  49.7301  4.8321  40.9663   66.945 #> Lower middle income  47  58.1481  5.9945  45.7687  71.6078 #> Upper middle income  57  66.6466  4.9955   48.057  74.0504 #>  #> $LIFEEX$Within #>                            N     Mean      SD      Min      Max #> High income          52.4795  64.2963  4.5738  42.9381  78.1271 #> Low income                60  64.2963  7.7045  41.5678  84.4198 #> Lower middle income  59.3617  64.2963  7.1253  32.9068  83.9918 #> Upper middle income       57  64.2963  5.6437  41.4342  83.0122 #>  #>   # We can now use unlist2d to create a tidy data frame unlist2d(psrl, c(\"Variable\", \"Trans\"), row.names = \"Income\") #>   Variable   Trans              Income    N       Mean         SD       Min #> 1    PCGDP Overall         High income 3179 30280.7283 23847.0483  932.0417 #> 2    PCGDP Overall          Low income 1311   597.4053   288.4392  164.3366 #> 3    PCGDP Overall Lower middle income 2246  1574.2535   858.7183  144.9863 #> 4    PCGDP Overall Upper middle income 2734  4945.3258  2979.5609  132.0776 #> 5    PCGDP Between         High income   71 30280.7283 20908.5323 5413.4495 #> 6    PCGDP Between          Low income   28   597.4053   243.8219  253.1886 #> 7    PCGDP Between Lower middle income   47  1574.2535   676.3157  444.2899 #> 8    PCGDP Between Upper middle income   60  4945.3258  2327.3834 1604.5950 #>          Max #> 1 196061.417 #> 2   1864.793 #> 3   4818.192 #> 4  20532.952 #> 5 141200.380 #> 6   1357.333 #> 7   2896.868 #> 8  13344.542 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 16 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/qtab.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Weighted) Cross Tabulation — qtab","title":"Fast (Weighted) Cross Tabulation — qtab","text":"versatile computationally efficient replacement table. Notably, also supports tabulations frequency weights, computation statistic combinations variables.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qtab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Weighted) Cross Tabulation — qtab","text":"","code":"qtab(..., w = NULL, wFUN = NULL, wFUN.args = NULL,      dnn = \"auto\", sort = .op[[\"sort\"]], na.exclude = TRUE,      drop = FALSE, method = \"auto\")  qtable(...) # Long-form. Use set_collapse(mask = \"table\") to replace table()"},{"path":"https://sebkrantz.github.io/collapse/reference/qtab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Weighted) Cross Tabulation — qtab","text":"... atomic vectors factors spanning table dimensions, (optionally) tags dimension names, data frame / list . See Examples. w single vector aggregate table dimensions e.g. vector frequency weights. wFUN function used aggregate w table dimensions. default NULL computes sum non-missing weights via optimized internal algorithm. Fast Statistical Functions also receive vectorized execution. wFUN.args list (optional) arguments passed wFUN. See Examples. dnn names table dimensions. Either passed directly character vector list (internally unlist'ed), function applied ... list (e.g. names, vlabels), one following options: \"auto\" constructs names based ... arguments, calls names single list passed input. \"namlab\" \"auto\", also calls vlabels list appends names variable labels. dnn = NULL return table without dimension names. sort, na.exclude, drop, method arguments passed qF: sort = FALSE orders table dimensions first-appearance order items data (can efficient vectors factors already). Note factors option recast levels first-appearance order drop unused levels. na.exclude = FALSE includes NA's table (equivalent table's useNA = \"ifany\"). drop = TRUE removes unused factor levels (= zero frequency rows columns). method %% c(\"radix\", \"hash\") provides additional control algorithm used convert atomic vectors factors.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/qtab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Weighted) Cross Tabulation — qtab","text":"array class 'qtab' inherits 'table'. Thus 'table' methods apply .","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/qtab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Weighted) Cross Tabulation — qtab","text":"","code":"## Basic use qtab(iris$Species) #> iris$Species #>     setosa versicolor  virginica  #>         50         50         50  with(mtcars, qtab(vs, am)) #>    am #> vs   0  1 #>   0 12  6 #>   1  7  7 qtab(mtcars[.c(vs, am)]) #>    am #> vs   0  1 #>   0 12  6 #>   1  7  7  library(magrittr) iris %$% qtab(Sepal.Length > mean(Sepal.Length), Species) #>                                  Species #> Sepal.Length > mean(Sepal.Length) setosa versicolor virginica #>                             FALSE     50         24         6 #>                             TRUE       0         26        44 iris %$% qtab(AMSL = Sepal.Length > mean(Sepal.Length), Species) #>        Species #> AMSL    setosa versicolor virginica #>   FALSE     50         24         6 #>   TRUE       0         26        44  ## World after 2015 wlda15 <- wlddev |> fsubset(year >= 2015) |> collap(~ iso3c)  # Regions and income levels (country frequency) wlda15 %$% qtab(region, income) #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                 13          0                  13 #>   Europe & Central Asia               37          1                   4 #>   Latin America & Caribbean           17          1                   4 #>   Middle East & North Africa           8          2                   5 #>   North America                        3          0                   0 #>   South Asia                           0          2                   4 #>   Sub-Saharan Africa                   1         24                  17 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                         10 #>   Europe & Central Asia                       16 #>   Latin America & Caribbean                   20 #>   Middle East & North Africa                   6 #>   North America                                0 #>   South Asia                                   2 #>   Sub-Saharan Africa                           6 wlda15 %$% qtab(region, income, dnn = vlabels) #>                             Income Level #> Region                       High income Low income Lower middle income #>   East Asia & Pacific                 13          0                  13 #>   Europe & Central Asia               37          1                   4 #>   Latin America & Caribbean           17          1                   4 #>   Middle East & North Africa           8          2                   5 #>   North America                        3          0                   0 #>   South Asia                           0          2                   4 #>   Sub-Saharan Africa                   1         24                  17 #>                             Income Level #> Region                       Upper middle income #>   East Asia & Pacific                         10 #>   Europe & Central Asia                       16 #>   Latin America & Caribbean                   20 #>   Middle East & North Africa                   6 #>   North America                                0 #>   South Asia                                   2 #>   Sub-Saharan Africa                           6 wlda15 %$% qtab(region, income, dnn = \"namlab\") #>                             income: Income Level #> region: Region               High income Low income Lower middle income #>   East Asia & Pacific                 13          0                  13 #>   Europe & Central Asia               37          1                   4 #>   Latin America & Caribbean           17          1                   4 #>   Middle East & North Africa           8          2                   5 #>   North America                        3          0                   0 #>   South Asia                           0          2                   4 #>   Sub-Saharan Africa                   1         24                  17 #>                             income: Income Level #> region: Region               Upper middle income #>   East Asia & Pacific                         10 #>   Europe & Central Asia                       16 #>   Latin America & Caribbean                   20 #>   Middle East & North Africa                   6 #>   North America                                0 #>   South Asia                                   2 #>   Sub-Saharan Africa                           6  # Population (millions) wlda15 %$% qtab(region, income, w = POP) |> divide_by(1e6) #>                             income #> region                        High income   Low income Lower middle income #>   East Asia & Pacific         222.3763078    0.0000000         554.5787740 #>   Europe & Central Asia       499.6178162    8.8839460          86.1694676 #>   Latin America & Caribbean    32.1055806   10.9808262          33.3947544 #>   Middle East & North Africa   64.6289084   45.1314580         148.8518602 #>   North America               361.3653800    0.0000000           0.0000000 #>   South Asia                    0.0000000   63.9814276        1706.8229478 #>   Sub-Saharan Africa            0.0956652  530.2676994         450.7138318 #>                             income #> region                       Upper middle income #>   East Asia & Pacific               1486.9566064 #>   Europe & Central Asia              319.5918390 #>   Latin America & Caribbean          557.9670800 #>   Middle East & North Africa         182.6471952 #>   North America                        0.0000000 #>   South Asia                          21.9126958 #>   Sub-Saharan Africa                  66.1922298  # Life expectancy (years) wlda15 %$% qtab(region, income, w = LIFEEX, wFUN = fmean) #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific           81.12986                       69.32372 #>   Europe & Central Asia         80.39692   70.63140            71.44826 #>   Latin America & Caribbean     77.57964   63.26640            73.18650 #>   Middle East & North Africa    78.38796   68.61450            72.73452 #>   North America                 80.67948                                #>   South Asia                               67.13770            69.81250 #>   Sub-Saharan Africa            73.93805   60.63336            62.37481 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                   73.32000 #>   Europe & Central Asia                 74.39151 #>   Latin America & Caribbean             74.69349 #>   Middle East & North Africa            74.78043 #>   North America                                  #>   South Asia                            77.48130 #>   Sub-Saharan Africa                    65.54593  # Life expectancy (years), weighted by population wlda15 %$% qtab(region, income, w = LIFEEX, wFUN = fmean,                   wFUN.args = list(w = POP)) #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific           83.47390                       71.19076 #>   Europe & Central Asia         81.31296   70.63140            71.46540 #>   Latin America & Caribbean     78.99973   63.26640            73.01552 #>   Middle East & North Africa    76.84332   68.02666            73.12402 #>   North America                 78.97586                                #>   South Asia                               66.73455            69.15123 #>   Sub-Saharan Africa            73.93805   62.01679            58.97093 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                   76.43505 #>   Europe & Central Asia                 73.96995 #>   Latin America & Caribbean             75.44067 #>   Middle East & North Africa            74.93824 #>   North America                                  #>   South Asia                            76.68486 #>   Sub-Saharan Africa                    63.79719  # GDP per capita (constant 2010 US$): median wlda15 %$% qtab(region, income, w = PCGDP, wFUN = fmedian,                   wFUN.args = list(na.rm = TRUE)) #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific         37527.1689                      1863.8773 #>   Europe & Central Asia       44340.1356  1026.2292           2654.3445 #>   Latin America & Caribbean   16514.1173  1266.0910           2323.2438 #>   Middle East & North Africa  31003.0939   677.4104           3134.9231 #>   North America               53790.1303                                #>   South Asia                               677.2265           1558.9751 #>   Sub-Saharan Africa          14057.6452   635.8458           1657.2758 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                  5364.0199 #>   Europe & Central Asia                6552.2411 #>   Latin America & Caribbean            6824.2245 #>   Middle East & North Africa           5920.2114 #>   North America                                  #>   South Asia                           5883.8904 #>   Sub-Saharan Africa                   8578.7116  # GDP per capita (constant 2010 US$): median, weighted by population wlda15 %$% qtab(region, income, w = PCGDP, wFUN = fmedian,                   wFUN.args = list(w = POP)) #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific         48194.0408                      3038.6464 #>   Europe & Central Asia       42912.4250  1026.2292           3010.4318 #>   Latin America & Caribbean   14888.7707  1266.0910           2483.3969 #>   Middle East & North Africa  20945.0874   677.4104           2840.9586 #>   North America               53790.1303                                #>   South Asia                               570.9192           1970.4413 #>   Sub-Saharan Africa          14057.6452   543.6803           2430.1368 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                  7360.8953 #>   Europe & Central Asia               11623.6264 #>   Latin America & Caribbean           10231.2777 #>   Middle East & North Africa           6255.4776 #>   North America                                  #>   South Asia                           3846.9157 #>   Sub-Saharan Africa                   7457.1928  # Including OECD membership tab <- wlda15 %$% qtab(region, income, OECD) tab #> , , OECD = FALSE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                  9          0                  13 #>   Europe & Central Asia               11          1                   4 #>   Latin America & Caribbean           16          1                   4 #>   Middle East & North Africa           7          2                   5 #>   North America                        1          0                   0 #>   South Asia                           0          2                   4 #>   Sub-Saharan Africa                   1         24                  17 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                         10 #>   Europe & Central Asia                       15 #>   Latin America & Caribbean                   19 #>   Middle East & North Africa                   6 #>   North America                                0 #>   South Asia                                   2 #>   Sub-Saharan Africa                           6 #>  #> , , OECD = TRUE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                  4          0                   0 #>   Europe & Central Asia               26          0                   0 #>   Latin America & Caribbean            1          0                   0 #>   Middle East & North Africa           1          0                   0 #>   North America                        2          0                   0 #>   South Asia                           0          0                   0 #>   Sub-Saharan Africa                   0          0                   0 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                          0 #>   Europe & Central Asia                        1 #>   Latin America & Caribbean                    1 #>   Middle East & North Africa                   0 #>   North America                                0 #>   South Asia                                   0 #>   Sub-Saharan Africa                           0 #>   # Various 'table' methods tab |> addmargins() #> , , OECD = FALSE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                  9          0                  13 #>   Europe & Central Asia               11          1                   4 #>   Latin America & Caribbean           16          1                   4 #>   Middle East & North Africa           7          2                   5 #>   North America                        1          0                   0 #>   South Asia                           0          2                   4 #>   Sub-Saharan Africa                   1         24                  17 #>   Sum                                 45         30                  47 #>                             income #> region                       Upper middle income Sum #>   East Asia & Pacific                         10  32 #>   Europe & Central Asia                       15  31 #>   Latin America & Caribbean                   19  40 #>   Middle East & North Africa                   6  20 #>   North America                                0   1 #>   South Asia                                   2   8 #>   Sub-Saharan Africa                           6  48 #>   Sum                                         58 180 #>  #> , , OECD = TRUE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                  4          0                   0 #>   Europe & Central Asia               26          0                   0 #>   Latin America & Caribbean            1          0                   0 #>   Middle East & North Africa           1          0                   0 #>   North America                        2          0                   0 #>   South Asia                           0          0                   0 #>                             income #> region                       Upper middle income Sum #>   East Asia & Pacific                          0   4 #>   Europe & Central Asia                        1  27 #>   Latin America & Caribbean                    1   2 #>   Middle East & North Africa                   0   1 #>   North America                                0   2 #>   South Asia                                   0   0 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 slice ]  tab |> marginSums(margin = c(\"region\", \"income\")) #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                 13          0                  13 #>   Europe & Central Asia               37          1                   4 #>   Latin America & Caribbean           17          1                   4 #>   Middle East & North Africa           8          2                   5 #>   North America                        3          0                   0 #>   South Asia                           0          2                   4 #>   Sub-Saharan Africa                   1         24                  17 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                         10 #>   Europe & Central Asia                       16 #>   Latin America & Caribbean                   20 #>   Middle East & North Africa                   6 #>   North America                                0 #>   South Asia                                   2 #>   Sub-Saharan Africa                           6 tab |> proportions() #> , , OECD = FALSE #>  #>                             income #> region                       High income  Low income Lower middle income #>   East Asia & Pacific        0.041666667 0.000000000         0.060185185 #>   Europe & Central Asia      0.050925926 0.004629630         0.018518519 #>   Latin America & Caribbean  0.074074074 0.004629630         0.018518519 #>   Middle East & North Africa 0.032407407 0.009259259         0.023148148 #>   North America              0.004629630 0.000000000         0.000000000 #>   South Asia                 0.000000000 0.009259259         0.018518519 #>   Sub-Saharan Africa         0.004629630 0.111111111         0.078703704 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                0.046296296 #>   Europe & Central Asia              0.069444444 #>   Latin America & Caribbean          0.087962963 #>   Middle East & North Africa         0.027777778 #>   North America                      0.000000000 #>   South Asia                         0.009259259 #>   Sub-Saharan Africa                 0.027777778 #>  #> , , OECD = TRUE #>  #>                             income #> region                       High income  Low income Lower middle income #>   East Asia & Pacific        0.018518519 0.000000000         0.000000000 #>   Europe & Central Asia      0.120370370 0.000000000         0.000000000 #>   Latin America & Caribbean  0.004629630 0.000000000         0.000000000 #>   Middle East & North Africa 0.004629630 0.000000000         0.000000000 #>   North America              0.009259259 0.000000000         0.000000000 #>   South Asia                 0.000000000 0.000000000         0.000000000 #>   Sub-Saharan Africa         0.000000000 0.000000000         0.000000000 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                0.000000000 #>   Europe & Central Asia              0.004629630 #>   Latin America & Caribbean          0.004629630 #>   Middle East & North Africa         0.000000000 #>   North America                      0.000000000 #>   South Asia                         0.000000000 #>   Sub-Saharan Africa                 0.000000000 #>  tab |> proportions(margin = \"income\") #> , , OECD = FALSE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific         0.11392405 0.00000000          0.27659574 #>   Europe & Central Asia       0.13924051 0.03333333          0.08510638 #>   Latin America & Caribbean   0.20253165 0.03333333          0.08510638 #>   Middle East & North Africa  0.08860759 0.06666667          0.10638298 #>   North America               0.01265823 0.00000000          0.00000000 #>   South Asia                  0.00000000 0.06666667          0.08510638 #>   Sub-Saharan Africa          0.01265823 0.80000000          0.36170213 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                 0.16666667 #>   Europe & Central Asia               0.25000000 #>   Latin America & Caribbean           0.31666667 #>   Middle East & North Africa          0.10000000 #>   North America                       0.00000000 #>   South Asia                          0.03333333 #>   Sub-Saharan Africa                  0.10000000 #>  #> , , OECD = TRUE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific         0.05063291 0.00000000          0.00000000 #>   Europe & Central Asia       0.32911392 0.00000000          0.00000000 #>   Latin America & Caribbean   0.01265823 0.00000000          0.00000000 #>   Middle East & North Africa  0.01265823 0.00000000          0.00000000 #>   North America               0.02531646 0.00000000          0.00000000 #>   South Asia                  0.00000000 0.00000000          0.00000000 #>   Sub-Saharan Africa          0.00000000 0.00000000          0.00000000 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                 0.00000000 #>   Europe & Central Asia               0.01666667 #>   Latin America & Caribbean           0.01666667 #>   Middle East & North Africa          0.00000000 #>   North America                       0.00000000 #>   South Asia                          0.00000000 #>   Sub-Saharan Africa                  0.00000000 #>  as.data.frame(tab) |> head(10) #>                        region      income  OECD Freq #> 1         East Asia & Pacific High income FALSE    9 #> 2       Europe & Central Asia High income FALSE   11 #> 3   Latin America & Caribbean High income FALSE   16 #> 4  Middle East & North Africa High income FALSE    7 #> 5               North America High income FALSE    1 #> 6                  South Asia High income FALSE    0 #> 7          Sub-Saharan Africa High income FALSE    1 #> 8         East Asia & Pacific  Low income FALSE    0 #> 9       Europe & Central Asia  Low income FALSE    1 #> 10  Latin America & Caribbean  Low income FALSE    1 ftable(tab, row.vars = c(\"region\", \"OECD\")) #>                                  income High income Low income Lower middle income Upper middle income #> region                     OECD                                                                        #> East Asia & Pacific        FALSE                  9          0                  13                  10 #>                            TRUE                   4          0                   0                   0 #> Europe & Central Asia      FALSE                 11          1                   4                  15 #>                            TRUE                  26          0                   0                   1 #> Latin America & Caribbean  FALSE                 16          1                   4                  19 #>                            TRUE                   1          0                   0                   1 #> Middle East & North Africa FALSE                  7          2                   5                   6 #>                            TRUE                   1          0                   0                   0 #> North America              FALSE                  1          0                   0                   0 #>                            TRUE                   2          0                   0                   0 #> South Asia                 FALSE                  0          2                   4                   2 #>                            TRUE                   0          0                   0                   0 #> Sub-Saharan Africa         FALSE                  1         24                  17                   6 #>                            TRUE                   0          0                   0                   0  # Other options tab |> fsum(TRA = \"%\")    # Percentage table (on a matrix use fsum.default) #> , , OECD = FALSE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific          4.1666667  0.0000000           6.0185185 #>   Europe & Central Asia        5.0925926  0.4629630           1.8518519 #>   Latin America & Caribbean    7.4074074  0.4629630           1.8518519 #>   Middle East & North Africa   3.2407407  0.9259259           2.3148148 #>   North America                0.4629630  0.0000000           0.0000000 #>   South Asia                   0.0000000  0.9259259           1.8518519 #>   Sub-Saharan Africa           0.4629630 11.1111111           7.8703704 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                  4.6296296 #>   Europe & Central Asia                6.9444444 #>   Latin America & Caribbean            8.7962963 #>   Middle East & North Africa           2.7777778 #>   North America                        0.0000000 #>   South Asia                           0.9259259 #>   Sub-Saharan Africa                   2.7777778 #>  #> , , OECD = TRUE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific          1.8518519  0.0000000           0.0000000 #>   Europe & Central Asia       12.0370370  0.0000000           0.0000000 #>   Latin America & Caribbean    0.4629630  0.0000000           0.0000000 #>   Middle East & North Africa   0.4629630  0.0000000           0.0000000 #>   North America                0.9259259  0.0000000           0.0000000 #>   South Asia                   0.0000000  0.0000000           0.0000000 #>   Sub-Saharan Africa           0.0000000  0.0000000           0.0000000 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                  0.0000000 #>   Europe & Central Asia                0.4629630 #>   Latin America & Caribbean            0.4629630 #>   Middle East & North Africa           0.0000000 #>   North America                        0.0000000 #>   South Asia                           0.0000000 #>   Sub-Saharan Africa                   0.0000000 #>  tab %/=% (sum(tab)/100)    # Another way (division by reference, preserves integers) tab #> , , OECD = FALSE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                  4          0                   6 #>   Europe & Central Asia                5          0                   2 #>   Latin America & Caribbean            8          0                   2 #>   Middle East & North Africa           3          1                   2 #>   North America                        0          0                   0 #>   South Asia                           0          1                   2 #>   Sub-Saharan Africa                   0         12                   8 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                          5 #>   Europe & Central Asia                        7 #>   Latin America & Caribbean                    9 #>   Middle East & North Africa                   3 #>   North America                                0 #>   South Asia                                   1 #>   Sub-Saharan Africa                           3 #>  #> , , OECD = TRUE #>  #>                             income #> region                       High income Low income Lower middle income #>   East Asia & Pacific                  2          0                   0 #>   Europe & Central Asia               13          0                   0 #>   Latin America & Caribbean            0          0                   0 #>   Middle East & North Africa           0          0                   0 #>   North America                        1          0                   0 #>   South Asia                           0          0                   0 #>   Sub-Saharan Africa                   0          0                   0 #>                             income #> region                       Upper middle income #>   East Asia & Pacific                          0 #>   Europe & Central Asia                        0 #>   Latin America & Caribbean                    0 #>   Middle East & North Africa                   0 #>   North America                                0 #>   South Asia                                   0 #>   Sub-Saharan Africa                           0 #>   rm(tab, wlda15)"},{"path":"https://sebkrantz.github.io/collapse/reference/quick-conversion.html","id":null,"dir":"Reference","previous_headings":"","what":"Quick Data Conversion — quick-conversion","title":"Quick Data Conversion — quick-conversion","text":"Fast, flexible precise conversion common data objects, without method dispatch extensive checks: qDF, qDT qTBL convert vectors, matrices, higher-dimensional arrays suitable lists data frame, data.table tibble, respectively. qM converts vectors, higher-dimensional arrays, data frames suitable lists matrix. mctl mrtl column- row-wise convert matrix list, data frame data.table. used internally qDF/qDT/qTBL, dapply, , etc... qF converts atomic vectors factor (documented separate page). as_numeric_factor, as_integer_factor, as_character_factor convert factors, factor columns data frame / list, character numeric (converting levels).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/quick-conversion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quick Data Conversion — quick-conversion","text":"","code":"# Converting between matrices, data frames / tables / tibbles   qDF(X, row.names.col = FALSE, keep.attr = FALSE, class = \"data.frame\")  qDT(X, row.names.col = FALSE, keep.attr = FALSE, class = c(\"data.table\", \"data.frame\")) qTBL(X, row.names.col = FALSE, keep.attr = FALSE, class = c(\"tbl_df\",\"tbl\",\"data.frame\"))   qM(X, row.names.col = NULL , keep.attr = FALSE, class = NULL, sep = \".\")  # Programmer functions: matrix rows or columns to list / DF / DT - fully in C++  mctl(X, names = FALSE, return = \"list\") mrtl(X, names = FALSE, return = \"list\")  # Converting factors or factor columns    as_numeric_factor(X, keep.attr = TRUE)   as_integer_factor(X, keep.attr = TRUE) as_character_factor(X, keep.attr = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/reference/quick-conversion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quick Data Conversion — quick-conversion","text":"X vector, factor, matrix, higher-dimensional array, data frame list. mctl mrtl accept matrices, as_numeric_factor, as_integer_factor as_character_factor accept factors, data frames lists. row.names.col can used add column saving names row.names converting objects data frame using qDF/qDT/qTBL. TRUE add column \"row.names\", can supply name e.g. row.names.col = \"variable\". X named atomic vector, length 2 vector names can supplied, e.g., qDF(fmean(mtcars), c(\"car\", \"mean\")). qM, argument opposite meaning, can used select one columns data frame/list used create rownames matrix e.g. qM(iris, row.names.col = \"Species\"). case column(s) can specified using names, indices, logical vector selector function. See Examples. keep.attr logical. FALSE (default) yields hard / thorough object conversion: unnecessary attributes removed object yielding plain matrix / data.frame / data.table. FALSE yields soft / minimal object conversion: attributes 'names', 'row.names', 'dim', 'dimnames' 'levels' modified conversion. attributes preserved. See also class. class vector classes passed , converted object assigned classes. NULL passed, default classes assigned: qM assigns class, qDF class \"data.frame\", qDT class c(\"data.table\", \"data.frame\"). keep.attr = TRUE class = NULL object already inherits default classes, inherited classes preserved. See Details Example. sep character. Separator used interacting multiple variables selected row.names.col. names logical. list named using row/column names matrix? return integer string specifying return. options :","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/quick-conversion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quick Data Conversion — quick-conversion","text":"Object conversions using functions maximally efficient involve 3 consecutive steps: (1) Converting storage mode / dimensions / data object, (2) converting / modifying attributes (3) modifying class object: (1) determined choice function optional row.names.col argument. Higher-dimensional arrays converted expanding second dimension (adding columns, .matrix, .data.frame, .data.table). (2) determined keep.attr argument: keep.attr = TRUE seeks preserve attributes object. effect like copying attributes(converted) <- attributes(original), modifying \"dim\", \"dimnames\", \"names\", \"row.names\" \"levels\" attributes necessitated conversion task. keep.attr = FALSE converts / assigns / removes attributes drops others. (3) determined class argument: Setting class = \"myclass\" yield converted object class \"myclass\", / prior classes removed replacement. Setting class = NULL mean class NULL assigned (remove class attribute), rather default classes assigned: qM assigns class, qDF class \"data.frame\", qDT class c(\"data.table\", \"data.frame\"). point interaction keep.attr: keep.attr = TRUE class = NULL object converted already inherits respective default classes, inherited classes also preserved (qM(x, keep.attr = TRUE, class = NULL) class preserved .matrix(x) evaluates TRUE.) default keep.attr = FALSE ensures hard conversions unnecessary attributes dropped. Furthermore qDF/qDT/qTBL default classes explicitly assigned. ensure default methods apply, even user chooses preserve attributes. qM lenient default setup chosen enable full preservation time series matrices keep.attr = TRUE. user wants keep attributes attached matrix make sure default methods work properly, either one qM(x, keep.attr = TRUE, class = \"matrix\") unclass(qM(x, keep.attr = TRUE)) employed.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/quick-conversion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quick Data Conversion — quick-conversion","text":"qDF - returns data.frameqDT - returns data.tableqTBL - returns tibbleqM - returns matrixmctl, mrtl - return list, data frame data.table qF - returns factoras_numeric_factor - returns X factors converted numeric (double) variablesas_integer_factor - returns X factors converted integer variablesas_character_factor - returns X factors converted character variables","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/quick-conversion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quick Data Conversion — quick-conversion","text":"","code":"## Basic Examples mtcarsM <- qM(mtcars)                   # Matrix from data.frame mtcarsDT <- qDT(mtcarsM)                # data.table from matrix columns mtcarsTBL <- qTBL(mtcarsM)              # tibble from matrix columns head(mrtl(mtcarsM, TRUE, \"data.frame\")) # data.frame from matrix rows, etc.. #>     Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive Hornet Sportabout Valiant #> mpg        21            21       22.8           21.4              18.7    18.1 #> cyl         6             6        4.0            6.0               8.0     6.0 #>     Duster 360 Merc 240D Merc 230 Merc 280 Merc 280C Merc 450SE Merc 450SL #> mpg       14.3      24.4     22.8     19.2      17.8       16.4       17.3 #> cyl        8.0       4.0      4.0      6.0       6.0        8.0        8.0 #>     Merc 450SLC Cadillac Fleetwood Lincoln Continental Chrysler Imperial #> mpg        15.2               10.4                10.4              14.7 #> cyl         8.0                8.0                 8.0               8.0 #>     Fiat 128 Honda Civic Toyota Corolla Toyota Corona Dodge Challenger #> mpg     32.4        30.4           33.9          21.5             15.5 #> cyl      4.0         4.0            4.0           4.0              8.0 #>     AMC Javelin Camaro Z28 Pontiac Firebird Fiat X1-9 Porsche 914-2 #> mpg        15.2       13.3             19.2      27.3            26 #> cyl         8.0        8.0              8.0       4.0             4 #>     Lotus Europa Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E #> mpg         30.4           15.8         19.7            15       21.4 #> cyl          4.0            8.0          6.0             8        4.0 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] head(qDF(mtcarsM, \"cars\"))              # Adding a row.names column when converting from matrix #>                cars  mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(qDT(mtcars, \"cars\"))               # Saving row.names when converting data frame to data.table #>                 cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am #>               <char> <num> <num> <num> <num> <num> <num> <num> <num> <num> #> 1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1 #> 2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1 #> 3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1 #> 4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0 #>     gear  carb #>    <num> <num> #> 1:     4     4 #> 2:     4     4 #> 3:     4     1 #> 4:     3     1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] head(qM(iris, \"Species\"))               # Examples converting data to matrix, saving information #>        Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa          5.1         3.5          1.4         0.2 #> setosa          4.9         3.0          1.4         0.2 #> setosa          4.7         3.2          1.3         0.2 #> setosa          4.6         3.1          1.5         0.2 #> setosa          5.0         3.6          1.4         0.2 #> setosa          5.4         3.9          1.7         0.4 head(qM(GGDC10S, is.character))         # as rownames #>                               Year      AGR      MIN       MAN        PU #> BWA.SSA.Sub-saharan Africa.VA 1960       NA       NA        NA        NA #> BWA.SSA.Sub-saharan Africa.VA 1961       NA       NA        NA        NA #> BWA.SSA.Sub-saharan Africa.VA 1962       NA       NA        NA        NA #> BWA.SSA.Sub-saharan Africa.VA 1963       NA       NA        NA        NA #> BWA.SSA.Sub-saharan Africa.VA 1964 16.30154 3.494075 0.7365696 0.1043936 #>                                     CON      WRT      TRA     FIRE      GOV #> BWA.SSA.Sub-saharan Africa.VA        NA       NA       NA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA        NA       NA       NA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA        NA       NA       NA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA        NA       NA       NA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA 0.6600454 6.243732 1.658928 1.119194 4.822485 #>                                    OTH      SUM #> BWA.SSA.Sub-saharan Africa.VA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA       NA       NA #> BWA.SSA.Sub-saharan Africa.VA 2.341328 37.48229 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ] head(qM(gv(GGDC10S, -(2:3)), 1:3, sep = \"-\")) # plm-style rownames #>                  AGR      MIN       MAN        PU       CON      WRT      TRA #> BWA-VA-1960       NA       NA        NA        NA        NA       NA       NA #> BWA-VA-1961       NA       NA        NA        NA        NA       NA       NA #> BWA-VA-1962       NA       NA        NA        NA        NA       NA       NA #> BWA-VA-1963       NA       NA        NA        NA        NA       NA       NA #> BWA-VA-1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 1.658928 #> BWA-VA-1965 15.72700 2.495768 1.0181992 0.1350976 1.3462312 7.064825 1.939007 #>                 FIRE      GOV      OTH      SUM #> BWA-VA-1960       NA       NA       NA       NA #> BWA-VA-1961       NA       NA       NA       NA #> BWA-VA-1962       NA       NA       NA       NA #> BWA-VA-1963       NA       NA       NA       NA #> BWA-VA-1964 1.119194 4.822485 2.341328 37.48229 #> BWA-VA-1965 1.246789 5.695848 2.678338 39.34710  qDF(fmean(mtcars), c(\"cars\", \"mean\"))   # Data frame from named vector, with names #>    cars       mean #> 1   mpg  20.090625 #> 2   cyl   6.187500 #> 3  disp 230.721875 #> 4    hp 146.687500 #> 5  drat   3.596563 #> 6    wt   3.217250 #> 7  qsec  17.848750 #> 8    vs   0.437500 #> 9    am   0.406250 #> 10 gear   3.687500 #> 11 carb   2.812500  # mrtl() and mctl() are very useful for iteration over matrices # Think of a coordninates matrix e.g. from sf::st_coordinates() coord <- matrix(rnorm(10), ncol = 2, dimnames = list(NULL, c(\"X\", \"Y\"))) # Then we can for (d in mrtl(coord)) {   cat(\"lon =\", d[1], \", lat =\", d[2], fill = TRUE)   # do something complicated ... } #> lon = -1.34008 , lat = 0.4111546 #> lon = -0.9054664 , lat = 0.5640929 #> lon = -0.2121424 , lat = 0.06356228 #> lon = -0.778162 , lat = 0.5173632 #> lon = -0.1411845 , lat = 0.4395096 rm(coord)  ## Factors cylF <- qF(mtcars$cyl)                  # Factor from atomic vector cylF #>  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 #> Levels: 4 6 8  # Factor to numeric conversions identical(mtcars,  as_numeric_factor(dapply(mtcars, qF))) #> [1] TRUE"},{"path":"https://sebkrantz.github.io/collapse/reference/radixorder.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Radix-Based Ordering — radixorder","title":"Fast Radix-Based Ordering — radixorder","text":"slight modification order(..., method = \"radix\") programmer friendly , importantly, provides features ordered grouping data (similar data.table:::forderv descended).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/radixorder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Radix-Based Ordering — radixorder","text":"","code":"radixorder(..., na.last = TRUE, decreasing = FALSE, starts = FALSE,            group.sizes = FALSE, sort = TRUE)  radixorderv(x, na.last = TRUE, decreasing = FALSE, starts = FALSE,             group.sizes = FALSE, sort = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/reference/radixorder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Radix-Based Ordering — radixorder","text":"... comma-separated atomic vectors order. x atomic vector list atomic vectors data frame. na.last logical. controlling treatment NA's. TRUE, missing values data put last; FALSE, put first; NA, removed. decreasing logical. sort order increasing decreasing? Can vector length equal number arguments ... / x. starts logical. TRUE returns attribute 'starts' containing first element new group .e. row denoting start new group data sorted using computed ordering vector. See Examples. group.sizes logical. TRUE returns attribute 'group.sizes' containing sizes group order groups encountered data sorted using computed ordering vector. See Examples. sort logical. argument affects character vectors / columns passed. FALSE, ordered simply grouped order first appearance unique elements. provides slight performance gain grouping alphabetic ordering required. See also group.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/radixorder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Radix-Based Ordering — radixorder","text":"integer ordering vector attributes: Unless na.last = NA attribute \"sorted\" indicating whether input data already sorted attached. starts = TRUE, \"starts\" giving vector group starts ordered data, group.sizes = TRUE, \"group.sizes\" giving vector group sizes attached. either case attribute \"maxgrpn\" providing size largest group also attached.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/radixorder.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast Radix-Based Ordering — radixorder","text":"C code taken - slight modifications - base R source code, originally due data.table authors Matt Dowle Arun Srinivasan.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/radixorder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Radix-Based Ordering — radixorder","text":"","code":"radixorder(mtcars$mpg) #>  [1] 15 16 24  7 17 31 14 23 22 29 12 13 11  6  5 10 25 30  1  2  4 32 21  3  9 #> [26]  8 27 26 19 28 18 20 #> attr(,\"sorted\") #> [1] FALSE head(mtcars[radixorder(mtcars$mpg), ]) #>                      mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4 #> Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4 #> Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4 #> Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4 #> Maserati Bora       15.0   8  301 335 3.54 3.570 14.60  0  1    5    8 radixorder(mtcars$cyl, mtcars$vs) #>  [1] 27  3  8  9 18 19 20 21 26 28 32  1  2 30  4  6 10 11  5  7 12 13 14 15 16 #> [26] 17 22 23 24 25 29 31 #> attr(,\"sorted\") #> [1] FALSE  o <- radixorder(mtcars$cyl, mtcars$vs, starts = TRUE) st <- attr(o, \"starts\") head(mtcars[o, ]) #>                mpg cyl  disp hp drat    wt  qsec vs am gear carb #> Porsche 914-2 26.0   4 120.3 91 4.43 2.140 16.70  0  1    5    2 #> Datsun 710    22.8   4 108.0 93 3.85 2.320 18.61  1  1    4    1 #> Merc 240D     24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2 #> Merc 230      22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2 #> Fiat 128      32.4   4  78.7 66 4.08 2.200 19.47  1  1    4    1 #> Honda Civic   30.4   4  75.7 52 4.93 1.615 18.52  1  1    4    2 mtcars[o[st], c(\"cyl\", \"vs\")]  # Unique groups #>                   cyl vs #> Porsche 914-2       4  0 #> Datsun 710          4  1 #> Mazda RX4           6  0 #> Hornet 4 Drive      6  1 #> Hornet Sportabout   8  0  # Note that if attr(o, \"sorted\") == TRUE, then all(o[st] == st) radixorder(rep(1:3, each = 3), starts = TRUE) #> [1] 1 2 3 4 5 6 7 8 9 #> attr(,\"starts\") #> [1] 1 4 7 #> attr(,\"maxgrpn\") #> [1] 3 #> attr(,\"sorted\") #> [1] TRUE  # Group sizes radixorder(mtcars$cyl, mtcars$vs, group.sizes = TRUE) #>  [1] 27  3  8  9 18 19 20 21 26 28 32  1  2 30  4  6 10 11  5  7 12 13 14 15 16 #> [26] 17 22 23 24 25 29 31 #> attr(,\"group.sizes\") #> [1]  1 10  3  4 14 #> attr(,\"maxgrpn\") #> [1] 14 #> attr(,\"sorted\") #> [1] FALSE  # Both radixorder(mtcars$cyl, mtcars$vs, starts = TRUE, group.sizes = TRUE) #>  [1] 27  3  8  9 18 19 20 21 26 28 32  1  2 30  4  6 10 11  5  7 12 13 14 15 16 #> [26] 17 22 23 24 25 29 31 #> attr(,\"starts\") #> [1]  1  2 12 15 19 #> attr(,\"group.sizes\") #> [1]  1 10  3  4 14 #> attr(,\"maxgrpn\") #> [1] 14 #> attr(,\"sorted\") #> [1] FALSE"},{"path":"https://sebkrantz.github.io/collapse/reference/rapply2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursively Apply a Function to a List of Data Objects — rapply2d","title":"Recursively Apply a Function to a List of Data Objects — rapply2d","text":"rapply2d recursive version lapply three differences rapply: data frames (list-based objects specified classes) considered atomic, (sub-)lists FUN applied 'atomic' objects nested list result simplified / unlisted.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rapply2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursively Apply a Function to a List of Data Objects — rapply2d","text":"","code":"rapply2d(l, FUN, ..., classes = \"data.frame\")"},{"path":"https://sebkrantz.github.io/collapse/reference/rapply2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursively Apply a Function to a List of Data Objects — rapply2d","text":"l list. FUN function can applied 'atomic' elements l. ... additional elements passed FUN. classes character. Classes list-based objects inside l considered atomic.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rapply2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recursively Apply a Function to a List of Data Objects — rapply2d","text":"list structure l, FUN applied atomic elements list-based objects class included classes.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rapply2d.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Recursively Apply a Function to a List of Data Objects — rapply2d","text":"main reason rapply2d exists recursive function ---box applies function nested list data frames. purposes rapply, extension excellent rrapply function / package, provide advanced functionality greater performance.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/rapply2d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recursively Apply a Function to a List of Data Objects — rapply2d","text":"","code":"l <- list(mtcars, list(mtcars, as.matrix(mtcars))) rapply2d(l, fmean) #> [[1]] #>        mpg        cyl       disp         hp       drat         wt       qsec  #>  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750  #>         vs         am       gear       carb  #>   0.437500   0.406250   3.687500   2.812500  #>  #> [[2]] #> [[2]][[1]] #>        mpg        cyl       disp         hp       drat         wt       qsec  #>  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750  #>         vs         am       gear       carb  #>   0.437500   0.406250   3.687500   2.812500  #>  #> [[2]][[2]] #>        mpg        cyl       disp         hp       drat         wt       qsec  #>  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750  #>         vs         am       gear       carb  #>   0.437500   0.406250   3.687500   2.812500  #>  #>  unlist2d(rapply2d(l, fmean)) #>   .id.1 .id.2      mpg    cyl     disp       hp     drat      wt     qsec #> 1     1    NA 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #> 2     2     1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #> 3     2     2 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #>       vs      am   gear   carb #> 1 0.4375 0.40625 3.6875 2.8125 #> 2 0.4375 0.40625 3.6875 2.8125 #> 3 0.4375 0.40625 3.6875 2.8125"},{"path":"https://sebkrantz.github.io/collapse/reference/recode-replace.html","id":null,"dir":"Reference","previous_headings":"","what":"Recode and Replace Values in Matrix-Like Objects — recode-replace","title":"Recode and Replace Values in Matrix-Like Objects — recode-replace","text":"small suite functions efficiently perform common recoding replacing tasks matrix-like objects.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/recode-replace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recode and Replace Values in Matrix-Like Objects — recode-replace","text":"","code":"recode_num(X, ..., default = NULL, missing = NULL, set = FALSE)  recode_char(X, ..., default = NULL, missing = NULL, regex = FALSE,             ignore.case = FALSE, fixed = FALSE, set = FALSE)  replace_na(X, value = 0, cols = NULL, set = FALSE, type = \"const\")  replace_inf(X, value = NA, replace.nan = FALSE, set = FALSE)  replace_outliers(X, limits, value = NA,                  single.limit = c(\"sd\", \"mad\", \"min\", \"max\"),                  ignore.groups = FALSE, set = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/recode-replace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recode and Replace Values in Matrix-Like Objects — recode-replace","text":"X vector, matrix, array, data frame list atomic objects. replace_outliers internal methods grouped indexed data. ... comma-separated recode arguments form: value = replacement, `2` = 0, Secondary = \"SEC\" etc. recode_char regex = TRUE also supports regular expressions .e. `^S|D$` = \"STD\" etc. default optional argument specify scalar value replace non-matched elements . missing optional argument specify scalar value replace missing elements . Note increase efficiency done rest recoding .e. recoding performed data missing values filled! set logical. TRUE replacements reference (.e. -place modification data) returns result invisibly. type character. One \"const\", \"locf\" (last non-missing observation carried forward) \"focb\" (first non-missing observation carried back). latter two ignore value. regex logical. TRUE, recode-argument names (sequentially) passed grepl pattern search X. matches replaced. Note NA's also matched strings grepl. value single (scalar) value replace matching elements . replace_outliers setting value = \"clip\" replace outliers corresponding threshold values. See Examples. cols select columns replace missing values using function, column names, indices logical vector. replace.nan logical. TRUE replaces NaN/Inf/-Inf. FALSE (default) replaces Inf/-Inf. limits either vector two-numeric values c(minval, maxval) constituting two-sided outlier threshold, single numeric value: single.limit character, controls behavior length(limits) == 1: \"sd\"/\"mad\": limits interpreted (two-sided) outlier threshold terms (column) standard deviations/median absolute deviations. standard deviation equivalent X[abs(fscale(X)) > limits] <- value. Since fscale S3 generic methods 'grouped_df', 'pseries' 'pdata.frame', standardizing grouped objects passed (.e. outlier threshold measured within-group standard deviations) unless ignore.groups = TRUE. holds median absolute deviations. \"min\"/\"max\": limits interpreted (one-sided) minimum/maximum threshold. underlying code equivalent X[X <\/> limits] <- value. ignore.groups logical. length(limits) == 1 single.limit %% c(\"sd\", \"mad\") X 'grouped_df', 'pseries' 'pdata.frame', TRUE ignore grouped nature data calculate outlier thresholds entire dataset rather within group. ignore.case, fixed logical. Passed grepl applicable regex = TRUE.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/recode-replace.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recode and Replace Values in Matrix-Like Objects — recode-replace","text":"recode_num recode_char can used efficiently recode multiple numeric character values, respectively. syntax inspired dplyr::recode, functionality enhanced following respects: (1) passed data frame / list, appropriately typed columns recoded. (2) preserve attributes data object columns data frame / list, (3) recode_char also supports regular expression matching using grepl. replace_na efficiently replaces NA/NaN value (default 0). data can multi-typed, case appropriate columns can selected cols argument. numeric data versatile alternative provided data.table::nafill data.table::setnafill. replace_inf replaces Inf/-Inf (optionally NaN/Inf/-Inf) value (default NA). skips non-numeric columns data frame. replace_outliers replaces values falling outside 1- 2-sided numeric threshold outside certain number standard deviations median absolute deviation value (default NA). skips non-numeric columns data frame.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/recode-replace.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Recode and Replace Values in Matrix-Like Objects — recode-replace","text":"functions generic offer support factors date(-time) objects. see dplyr::recode_factor, forcats appropriate packages dealing classes. Simple replacing tasks vector can also effectively handled , setv / copyv. Fast vectorized switches offered package kit (functions iif, nif, vswitch, nswitch) well data.table::fcase data.table::fifelse. Using switches efficient recode_*, recode_* creates internal copy object enable cross-replacing. Function TRA, associated TRA ('transform') argument Fast Statistical Functions also option \"replace_na\", replace missing values statistic computed non-missing observations, e.g. fmedian(airquality, TRA = \"replace_na\") median imputation.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/recode-replace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recode and Replace Values in Matrix-Like Objects — recode-replace","text":"","code":"recode_char(c(\"a\",\"b\",\"c\"), a = \"b\", b = \"c\") #> [1] \"b\" \"c\" \"c\" recode_char(month.name, ber = NA, regex = TRUE) #>  [1] \"January\"  \"February\" \"March\"    \"April\"    \"May\"      \"June\"     #>  [7] \"July\"     \"August\"   NA         NA         NA         NA         mtcr <- recode_num(mtcars, `0` = 2, `4` = Inf, `1` = NaN) replace_inf(mtcr) #>                    mpg cyl disp  hp drat    wt  qsec  vs  am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46   2 NaN   NA   NA #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02   2 NaN   NA   NA #> Datsun 710        22.8  NA  108  93 3.85 2.320 18.61 NaN NaN   NA  NaN #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44 NaN   2    3  NaN #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02   2   2    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22 NaN   2    3  NaN #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] replace_inf(mtcr, replace.nan = TRUE) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  2 NA   NA   NA #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  2 NA   NA   NA #> Datsun 710        22.8  NA  108  93 3.85 2.320 18.61 NA NA   NA   NA #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44 NA  2    3   NA #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  2  2    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22 NA  2    3   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] replace_outliers(mtcars, c(2, 100))                 # Replace all values below 2 and above 100 w. NA #>                    mpg cyl disp hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6   NA NA 3.90 2.620 16.46 NA NA    4    4 #> Mazda RX4 Wag     21.0   6   NA NA 3.90 2.875 17.02 NA NA    4    4 #> Datsun 710        22.8   4   NA 93 3.85 2.320 18.61 NA NA    4   NA #> Hornet 4 Drive    21.4   6   NA NA 3.08 3.215 19.44 NA NA    3   NA #> Hornet Sportabout 18.7   8   NA NA 3.15 3.440 17.02 NA NA    3    2 #> Valiant           18.1   6   NA NA 2.76 3.460 20.22 NA NA    3   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] replace_outliers(mtcars, c(2, 100), value = \"clip\") # Clipping outliers to the thresholds #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  100 100 3.90 2.620 16.46  2  2    4    4 #> Mazda RX4 Wag     21.0   6  100 100 3.90 2.875 17.02  2  2    4    4 #> Datsun 710        22.8   4  100  93 3.85 2.320 18.61  2  2    4    2 #> Hornet 4 Drive    21.4   6  100 100 3.08 3.215 19.44  2  2    3    2 #> Hornet Sportabout 18.7   8  100 100 3.15 3.440 17.02  2  2    3    2 #> Valiant           18.1   6  100 100 2.76 3.460 20.22  2  2    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] replace_outliers(mtcars, 2, single.limit = \"min\")   # Replace all value smaller than 2 with NA #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46 NA NA    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02 NA NA    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61 NA NA    4   NA #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44 NA NA    3   NA #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02 NA NA    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22 NA NA    3   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] replace_outliers(mtcars, 100, single.limit = \"max\") # Replace all value larger than 100 with NA #>                    mpg cyl disp hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6   NA NA 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6   NA NA 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4   NA 93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6   NA NA 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8   NA NA 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6   NA NA 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ] replace_outliers(mtcars, 2)                         # Replace all values above or below 2 column- #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 26 rows ]                                                     # standard-deviations from the column-mean w. NA replace_outliers(fgroup_by(iris, Species), 2)       # Passing a grouped_df, pseries or pdata.frame #>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1           5.1         3.5          1.4         0.2  setosa #> 2           4.9         3.0          1.4         0.2  setosa #> 3           4.7         3.2          1.3         0.2  setosa #> 4           4.6         3.1          1.5         0.2  setosa #> 5           5.0         3.6          1.4         0.2  setosa #> 6           5.4         3.9          1.7         0.4  setosa #> 7           4.6         3.4          1.4         0.3  setosa #> 8           5.0         3.4          1.5         0.2  setosa #> 9           4.4         2.9          1.4         0.2  setosa #> 10          4.9         3.1          1.5         0.1  setosa #> 11          5.4         3.7          1.5         0.2  setosa #> 12          4.8         3.4          1.6         0.2  setosa #> 13          4.8         3.0          1.4         0.1  setosa #> 14           NA         3.0           NA         0.1  setosa #>  [ reached 'max' / getOption(\"max.print\") -- omitted 136 rows ] #>  #> Grouped by:  Species  [3 | 50 (0)]                                                      # allows to remove outliers according to                                                     # in-group standard-deviation. see ?fscale"},{"path":"https://sebkrantz.github.io/collapse/reference/rowbind.html","id":null,"dir":"Reference","previous_headings":"","what":"Row-Bind Lists / Data Frame-Like Objects — rowbind","title":"Row-Bind Lists / Data Frame-Like Objects — rowbind","text":"collapse's version data.table::rbindlist rbind.data.frame. core code copied data.table, deserves credit implementation. rowbind binds lists/data.frame's. flexible recursive version see unlist2d. combine lists column-wise see add_vars ftransform (replacement).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rowbind.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Row-Bind Lists / Data Frame-Like Objects — rowbind","text":"","code":"rowbind(..., idcol = NULL, row.names = FALSE,         use.names = TRUE, fill = FALSE, id.factor = \"auto\",         return = c(\"as.first\", \"data.frame\", \"data.table\", \"tibble\", \"list\"))"},{"path":"https://sebkrantz.github.io/collapse/reference/rowbind.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Row-Bind Lists / Data Frame-Like Objects — rowbind","text":"... single list list-like objects (data.frames) comma separated objects (internally assembled using list(...)). Names can supplied !.null(idcol). idcol character. name id-column generated identifying source rows final object. Using idcol = TRUE set name \".id\". input list names, form content id column, otherwise integers used. save memory, advised keep id.factor = TRUE. row.names TRUE extracts row names objects l adds output column named \"row.names\". Alternatively, column name .e. row.names = \"variable\" can supplied. use.names logical. TRUE binds matching column name, FALSE position. fill logical. TRUE fills missing columns NAs. TRUE, use.names set TRUE. id.factor TRUE !isFALSE(idcols), create id column factor instead character integer vector. also possible specify \"ordered\" generate ordered factor id. \"auto\" uses TRUE !.null(names(l)) l input list (factors much memory efficient character vectors). return integer string specifying return. 1 - \".first\" preserves attributes first element list, 2/3/4 - \"data.frame\"/\"data.table\"/\"tibble\" coerces specific objects, 5 - \"list\" returns (named) list.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rowbind.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Row-Bind Lists / Data Frame-Like Objects — rowbind","text":"long list data frame-like object formed combining rows / elements input objects. return argument controls exact format output.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/rowbind.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Row-Bind Lists / Data Frame-Like Objects — rowbind","text":"","code":"# These are the same rowbind(mtcars, mtcars) #>    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 58 rows ] rowbind(list(mtcars, mtcars)) #>    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 58 rows ]  # With id column rowbind(mtcars, mtcars, idcol = \"id\") #>   id  mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1  1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2  1 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3  1 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4  1 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5  1 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 59 rows ] rowbind(a = mtcars, b = mtcars, idcol = \"id\") #>   id  mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1  a 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2  a 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3  a 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4  a 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5  a 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 59 rows ]  # With saving row-names rowbind(mtcars, mtcars, row.names = \"cars\") #>                cars  mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 59 rows ] rowbind(a = mtcars, b = mtcars, idcol = \"id\", row.names = \"cars\") #>   id              cars  mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1  a         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2  a     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3  a        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4  a    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5  a Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 59 rows ]  # Filling up columns rowbind(mtcars, mtcars[2:8], fill = TRUE) #>    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 58 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/roworder.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Reordering of Data Frame Rows — roworder","title":"Fast Reordering of Data Frame Rows — roworder","text":"fast substitute dplyr::arrange, based radixorder(v) inspired data.table::setorder(v). returns sorted copy data frame, unless data already sorted case copy made. addition, rows can manually re-ordered. roworderv programmers version takes vectors/variables input. Use data.table::setorder(v) sort data frame without creating copy.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/roworder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Reordering of Data Frame Rows — roworder","text":"","code":"roworder(X, ..., na.last = TRUE, verbose = .op[[\"verbose\"]])  roworderv(X, cols = NULL, neworder = NULL, decreasing = FALSE,           na.last = TRUE, pos = \"front\", verbose = .op[[\"verbose\"]])"},{"path":"https://sebkrantz.github.io/collapse/reference/roworder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Reordering of Data Frame Rows — roworder","text":"X data frame list equal-length columns. ... comma-separated columns X sort e.g. var1, var2. Negatives .e. -var1, var2 can used sort decreasing order var1. Internally expressions turned strings startsWith(expr, \"-\") used detect , thus negate actual values (may well strings), apply functions columns inside roworder() induce different sorting behavior. cols select columns sort using function, column names, indices logical vector. default NULL sorts columns order occurrence (left right). na.last logical. TRUE, missing values sorting columns placed last; FALSE, placed first; NA removed (argument passed radixorder). decreasing logical. sort order increasing decreasing? Can also vector length equal number arguments cols (argument passed radixorder). neworder ordering vector, can < nrow(X). pos = \"front\" pos = \"end\", logical vector can also supplied. argument overwrites cols. pos integer character. Different arrangement options !.null(neworder) && length(neworder) < nrow(X).  verbose logical. 1L (default) prints message ordering grouped indexed frame, indicating efficient encouraging reordering data prior grouping/indexing step. Users can also set verbose = 2L also toggle message x already sorted, implying copy made call roworder(v) redundant.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/roworder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Reordering of Data Frame Rows — roworder","text":"copy X rows reordered. X already sorted, X simply returned.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/roworder.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Reordering of Data Frame Rows — roworder","text":"require copy data, use data.table::setorder (can also use piped call invisibly returns data). roworder(v) internal facilities deal grouped indexed data. however inefficient (since cases data reordered grouping/indexing), therefore issues message verbose > 0L.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/roworder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Reordering of Data Frame Rows — roworder","text":"","code":"head(roworder(airquality, Month, -Ozone)) #>   Ozone Solar.R Wind Temp Month Day #> 1   115     223  5.7   79     5  30 #> 2    45     252 14.9   81     5  29 #> 3    41     190  7.4   67     5   1 #> 4    37     279  7.4   76     5  31 #> 5    36     118  8.0   72     5   2 #> 6    34     307 12.0   66     5  17 head(roworder(airquality, Month, -Ozone, na.last = NA))  # Removes the missing values in Ozone #>   Ozone Solar.R Wind Temp Month Day #> 1   115     223  5.7   79     5  30 #> 2    45     252 14.9   81     5  29 #> 3    41     190  7.4   67     5   1 #> 4    37     279  7.4   76     5  31 #> 5    36     118  8.0   72     5   2 #> 6    34     307 12.0   66     5  17  ## Same in standard evaluation head(roworderv(airquality, c(\"Month\", \"Ozone\"), decreasing = c(FALSE, TRUE))) #>   Ozone Solar.R Wind Temp Month Day #> 1   115     223  5.7   79     5  30 #> 2    45     252 14.9   81     5  29 #> 3    41     190  7.4   67     5   1 #> 4    37     279  7.4   76     5  31 #> 5    36     118  8.0   72     5   2 #> 6    34     307 12.0   66     5  17 head(roworderv(airquality, c(\"Month\", \"Ozone\"), decreasing = c(FALSE, TRUE), na.last = NA)) #>   Ozone Solar.R Wind Temp Month Day #> 1   115     223  5.7   79     5  30 #> 2    45     252 14.9   81     5  29 #> 3    41     190  7.4   67     5   1 #> 4    37     279  7.4   76     5  31 #> 5    36     118  8.0   72     5   2 #> 6    34     307 12.0   66     5  17  ## Custom reordering head(roworderv(mtcars, neworder = 3:4))               # Bring rows 3 and 4 to the front #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 head(roworderv(mtcars, neworder = 3:4, pos = \"end\"))  # Bring them to the end #>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Duster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 head(roworderv(mtcars, neworder = mtcars$vs == 1))    # Bring rows with vs == 1 to the top #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 #> Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2 #> Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4"},{"path":"https://sebkrantz.github.io/collapse/reference/rsplit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast (Recursive) Splitting — rsplit","title":"Fast (Recursive) Splitting — rsplit","text":"rsplit (recursively) splits vector, matrix data frame subsets according combinations (multiple) vectors / factors returns (nested) list. flatten = TRUE, list flattened yielding result split. rsplit implemented wrapper around gsplit, significantly faster split.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rsplit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast (Recursive) Splitting — rsplit","text":"","code":"rsplit(x, ...)  # Default S3 method rsplit(x, fl, drop = TRUE, flatten = FALSE, use.names = TRUE, ...)  # S3 method for class 'matrix' rsplit(x, fl, drop = TRUE, flatten = FALSE, use.names = TRUE,        drop.dim = FALSE, ...)  # S3 method for class 'data.frame' rsplit(x, by, drop = TRUE, flatten = FALSE, cols = NULL,        keep.by = FALSE, simplify = TRUE, use.names = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/rsplit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast (Recursive) Splitting — rsplit","text":"x vector, matrix, data.frame list like object. fl GRP object, (list ) vector(s) / factor(s) (internally converted GRP object(s)) used split x. data.frame method: fl, also allows one- two-sided formulas .e. ~ group1 var1 + var2 ~ group1 + group2. See Examples. drop logical. TRUE removes unused levels combinations levels factors splitting; FALSE retains combinations yielding empty list elements output. flatten logical. fl list vectors / factors, TRUE calls GRP list, creating single grouping used splitting; FALSE yields recursive splitting. use.names logical. TRUE returns named list (like split); FALSE returns plain list. drop.dim logical. TRUE returns atomic vectors matrix-splits consisting one row. cols data.frame method: Select columns split using function, column names, indices logical vector. Note: cols ignored two-sided formula passed . keep.logical. formula passed , TRUE preserves splitting (right-hand-side) variables data frame. simplify data.frame method: Logical. TRUE calls rsplit.default single column split e.g. rsplit(data, col1 ~ group1) becomes rsplit(data$col1, data$group1). ... arguments passed GRP. Sensible choices sort = FALSE, decreasing = TRUE na.last = FALSE. Note options apply fl already (list ) factor(s).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/rsplit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast (Recursive) Splitting — rsplit","text":"(nested) list containing subsets x.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/rsplit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast (Recursive) Splitting — rsplit","text":"","code":"rsplit(mtcars$mpg, mtcars$cyl) #> $`4` #>  [1] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26.0 30.4 21.4 #>  #> $`6` #> [1] 21.0 21.0 21.4 18.1 19.2 17.8 19.7 #>  #> $`8` #>  [1] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 13.3 19.2 15.8 15.0 #>  rsplit(mtcars, mtcars$cyl) #> $`4` #>                 mpg cyl  disp hp drat    wt  qsec vs am gear carb #> Datsun 710     22.8   4 108.0 93 3.85 2.320 18.61  1  1    4    1 #> Merc 240D      24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2 #> Merc 230       22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2 #> Fiat 128       32.4   4  78.7 66 4.08 2.200 19.47  1  1    4    1 #> Honda Civic    30.4   4  75.7 52 4.93 1.615 18.52  1  1    4    2 #> Toyota Corolla 33.9   4  71.1 65 4.22 1.835 19.90  1  1    4    1 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ] #>  #> $`6` #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 #> Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] #>  #> $`8` #>                     mpg cyl  disp  hp drat   wt  qsec vs am gear carb #> Hornet Sportabout  18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2 #> Duster 360         14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4 #> Merc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3 #> Merc 450SL         17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3 #> Merc 450SLC        15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3 #> Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ] #>   rsplit(mtcars, mtcars[.c(cyl, vs, am)]) #> $`4` #> $`4`$`0` #> $`4`$`0`$`1` #>               mpg cyl  disp hp drat   wt qsec vs am gear carb #> Porsche 914-2  26   4 120.3 91 4.43 2.14 16.7  0  1    5    2 #>  #>  #> $`4`$`1` #> $`4`$`1`$`0` #>                mpg cyl  disp hp drat    wt  qsec vs am gear carb #> Merc 240D     24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2 #> Merc 230      22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2 #> Toyota Corona 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1 #>  #> $`4`$`1`$`1` #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 #> Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 #> Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 #> Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] #>  #>  #>  #> $`6` #> $`6`$`0` #> $`6`$`0`$`1` #>                mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Ferrari Dino  19.7   6  145 175 3.62 2.770 15.50  0  1    5    6 #>  #>  #> $`6`$`1` #> $`6`$`1`$`0` #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 #> Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #>  #>  #>  #> $`8` #> $`8`$`0` #> $`8`$`0`$`0` #>                     mpg cyl  disp  hp drat   wt  qsec vs am gear carb #> Hornet Sportabout  18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2 #> Duster 360         14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4 #> Merc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3 #> Merc 450SL         17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3 #> Merc 450SLC        15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3 #> Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ] #>  #> $`8`$`0`$`1` #>                 mpg cyl disp  hp drat   wt qsec vs am gear carb #> Ford Pantera L 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4 #> Maserati Bora  15.0   8  301 335 3.54 3.57 14.6  0  1    5    8 #>  #>  #>  rsplit(mtcars, ~ cyl + vs + am, keep.by = TRUE)  # Same thing #> $`4` #> $`4`$`0` #> $`4`$`0`$`1` #>               mpg cyl  disp hp drat   wt qsec vs am gear carb #> Porsche 914-2  26   4 120.3 91 4.43 2.14 16.7  0  1    5    2 #>  #>  #> $`4`$`1` #> $`4`$`1`$`0` #>                mpg cyl  disp hp drat    wt  qsec vs am gear carb #> Merc 240D     24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2 #> Merc 230      22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2 #> Toyota Corona 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1 #>  #> $`4`$`1`$`1` #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 #> Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 #> Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 #> Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] #>  #>  #>  #> $`6` #> $`6`$`0` #> $`6`$`0`$`1` #>                mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Ferrari Dino  19.7   6  145 175 3.62 2.770 15.50  0  1    5    6 #>  #>  #> $`6`$`1` #> $`6`$`1`$`0` #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 #> Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #>  #>  #>  #> $`8` #> $`8`$`0` #> $`8`$`0`$`0` #>                     mpg cyl  disp  hp drat   wt  qsec vs am gear carb #> Hornet Sportabout  18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2 #> Duster 360         14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4 #> Merc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3 #> Merc 450SL         17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3 #> Merc 450SLC        15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3 #> Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ] #>  #> $`8`$`0`$`1` #>                 mpg cyl disp  hp drat   wt qsec vs am gear carb #> Ford Pantera L 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4 #> Maserati Bora  15.0   8  301 335 3.54 3.57 14.6  0  1    5    8 #>  #>  #>  rsplit(mtcars, ~ cyl + vs + am) #> $`4` #> $`4`$`0` #> $`4`$`0`$`1` #>               mpg  disp hp drat   wt qsec gear carb #> Porsche 914-2  26 120.3 91 4.43 2.14 16.7    5    2 #>  #>  #> $`4`$`1` #> $`4`$`1`$`0` #>                mpg  disp hp drat    wt  qsec gear carb #> Merc 240D     24.4 146.7 62 3.69 3.190 20.00    4    2 #> Merc 230      22.8 140.8 95 3.92 3.150 22.90    4    2 #> Toyota Corona 21.5 120.1 97 3.70 2.465 20.01    3    1 #>  #> $`4`$`1`$`1` #>                 mpg  disp  hp drat    wt  qsec gear carb #> Datsun 710     22.8 108.0  93 3.85 2.320 18.61    4    1 #> Fiat 128       32.4  78.7  66 4.08 2.200 19.47    4    1 #> Honda Civic    30.4  75.7  52 4.93 1.615 18.52    4    2 #> Toyota Corolla 33.9  71.1  65 4.22 1.835 19.90    4    1 #> Fiat X1-9      27.3  79.0  66 4.08 1.935 18.90    4    1 #> Lotus Europa   30.4  95.1 113 3.77 1.513 16.90    5    2 #> Volvo 142E     21.4 121.0 109 4.11 2.780 18.60    4    2 #>  #>  #>  #> $`6` #> $`6`$`0` #> $`6`$`0`$`1` #>                mpg disp  hp drat    wt  qsec gear carb #> Mazda RX4     21.0  160 110 3.90 2.620 16.46    4    4 #> Mazda RX4 Wag 21.0  160 110 3.90 2.875 17.02    4    4 #> Ferrari Dino  19.7  145 175 3.62 2.770 15.50    5    6 #>  #>  #> $`6`$`1` #> $`6`$`1`$`0` #>                 mpg  disp  hp drat    wt  qsec gear carb #> Hornet 4 Drive 21.4 258.0 110 3.08 3.215 19.44    3    1 #> Valiant        18.1 225.0 105 2.76 3.460 20.22    3    1 #> Merc 280       19.2 167.6 123 3.92 3.440 18.30    4    4 #> Merc 280C      17.8 167.6 123 3.92 3.440 18.90    4    4 #>  #>  #>  #> $`8` #> $`8`$`0` #> $`8`$`0`$`0` #>                      mpg  disp  hp drat    wt  qsec gear carb #> Hornet Sportabout   18.7 360.0 175 3.15 3.440 17.02    3    2 #> Duster 360          14.3 360.0 245 3.21 3.570 15.84    3    4 #> Merc 450SE          16.4 275.8 180 3.07 4.070 17.40    3    3 #> Merc 450SL          17.3 275.8 180 3.07 3.730 17.60    3    3 #> Merc 450SLC         15.2 275.8 180 3.07 3.780 18.00    3    3 #> Cadillac Fleetwood  10.4 472.0 205 2.93 5.250 17.98    3    4 #> Lincoln Continental 10.4 460.0 215 3.00 5.424 17.82    3    4 #> Chrysler Imperial   14.7 440.0 230 3.23 5.345 17.42    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] #>  #> $`8`$`0`$`1` #>                 mpg disp  hp drat   wt qsec gear carb #> Ford Pantera L 15.8  351 264 4.22 3.17 14.5    5    4 #> Maserati Bora  15.0  301 335 3.54 3.57 14.6    5    8 #>  #>  #>   rsplit(mtcars, ~ cyl + vs + am, flatten = TRUE) #> $`4.0.1` #>               mpg  disp hp drat   wt qsec gear carb #> Porsche 914-2  26 120.3 91 4.43 2.14 16.7    5    2 #>  #> $`4.1.0` #>                mpg  disp hp drat    wt  qsec gear carb #> Merc 240D     24.4 146.7 62 3.69 3.190 20.00    4    2 #> Merc 230      22.8 140.8 95 3.92 3.150 22.90    4    2 #> Toyota Corona 21.5 120.1 97 3.70 2.465 20.01    3    1 #>  #> $`4.1.1` #>                 mpg  disp  hp drat    wt  qsec gear carb #> Datsun 710     22.8 108.0  93 3.85 2.320 18.61    4    1 #> Fiat 128       32.4  78.7  66 4.08 2.200 19.47    4    1 #> Honda Civic    30.4  75.7  52 4.93 1.615 18.52    4    2 #> Toyota Corolla 33.9  71.1  65 4.22 1.835 19.90    4    1 #> Fiat X1-9      27.3  79.0  66 4.08 1.935 18.90    4    1 #> Lotus Europa   30.4  95.1 113 3.77 1.513 16.90    5    2 #> Volvo 142E     21.4 121.0 109 4.11 2.780 18.60    4    2 #>  #> $`6.0.1` #>                mpg disp  hp drat    wt  qsec gear carb #> Mazda RX4     21.0  160 110 3.90 2.620 16.46    4    4 #> Mazda RX4 Wag 21.0  160 110 3.90 2.875 17.02    4    4 #> Ferrari Dino  19.7  145 175 3.62 2.770 15.50    5    6 #>  #> $`6.1.0` #>                 mpg  disp  hp drat    wt  qsec gear carb #> Hornet 4 Drive 21.4 258.0 110 3.08 3.215 19.44    3    1 #> Valiant        18.1 225.0 105 2.76 3.460 20.22    3    1 #> Merc 280       19.2 167.6 123 3.92 3.440 18.30    4    4 #> Merc 280C      17.8 167.6 123 3.92 3.440 18.90    4    4 #>  #> $`8.0.0` #>                      mpg  disp  hp drat    wt  qsec gear carb #> Hornet Sportabout   18.7 360.0 175 3.15 3.440 17.02    3    2 #> Duster 360          14.3 360.0 245 3.21 3.570 15.84    3    4 #> Merc 450SE          16.4 275.8 180 3.07 4.070 17.40    3    3 #> Merc 450SL          17.3 275.8 180 3.07 3.730 17.60    3    3 #> Merc 450SLC         15.2 275.8 180 3.07 3.780 18.00    3    3 #> Cadillac Fleetwood  10.4 472.0 205 2.93 5.250 17.98    3    4 #> Lincoln Continental 10.4 460.0 215 3.00 5.424 17.82    3    4 #> Chrysler Imperial   14.7 440.0 230 3.23 5.345 17.42    3    4 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ] #>  #> $`8.0.1` #>                 mpg disp  hp drat   wt qsec gear carb #> Ford Pantera L 15.8  351 264 4.22 3.17 14.5    5    4 #> Maserati Bora  15.0  301 335 3.54 3.57 14.6    5    8 #>   rsplit(mtcars, mpg ~ cyl) #> $`4` #>  [1] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26.0 30.4 21.4 #>  #> $`6` #> [1] 21.0 21.0 21.4 18.1 19.2 17.8 19.7 #>  #> $`8` #>  [1] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 13.3 19.2 15.8 15.0 #>  rsplit(mtcars, mpg ~ cyl, simplify = FALSE) #> $`4` #>                 mpg #> Datsun 710     22.8 #> Merc 240D      24.4 #> Merc 230       22.8 #> Fiat 128       32.4 #> Honda Civic    30.4 #> Toyota Corolla 33.9 #> Toyota Corona  21.5 #> Fiat X1-9      27.3 #> Porsche 914-2  26.0 #> Lotus Europa   30.4 #> Volvo 142E     21.4 #>  #> $`6` #>                 mpg #> Mazda RX4      21.0 #> Mazda RX4 Wag  21.0 #> Hornet 4 Drive 21.4 #> Valiant        18.1 #> Merc 280       19.2 #> Merc 280C      17.8 #> Ferrari Dino   19.7 #>  #> $`8` #>                      mpg #> Hornet Sportabout   18.7 #> Duster 360          14.3 #> Merc 450SE          16.4 #> Merc 450SL          17.3 #> Merc 450SLC         15.2 #> Cadillac Fleetwood  10.4 #> Lincoln Continental 10.4 #> Chrysler Imperial   14.7 #> Dodge Challenger    15.5 #> AMC Javelin         15.2 #> Camaro Z28          13.3 #> Pontiac Firebird    19.2 #> Ford Pantera L      15.8 #> Maserati Bora       15.0 #>  rsplit(mtcars, mpg + hp ~ cyl + vs + am) #> $`4` #> $`4`$`0` #> $`4`$`0`$`1` #>               mpg hp #> Porsche 914-2  26 91 #>  #>  #> $`4`$`1` #> $`4`$`1`$`0` #>                mpg hp #> Merc 240D     24.4 62 #> Merc 230      22.8 95 #> Toyota Corona 21.5 97 #>  #> $`4`$`1`$`1` #>                 mpg  hp #> Datsun 710     22.8  93 #> Fiat 128       32.4  66 #> Honda Civic    30.4  52 #> Toyota Corolla 33.9  65 #> Fiat X1-9      27.3  66 #> Lotus Europa   30.4 113 #> Volvo 142E     21.4 109 #>  #>  #>  #> $`6` #> $`6`$`0` #> $`6`$`0`$`1` #>                mpg  hp #> Mazda RX4     21.0 110 #> Mazda RX4 Wag 21.0 110 #> Ferrari Dino  19.7 175 #>  #>  #> $`6`$`1` #> $`6`$`1`$`0` #>                 mpg  hp #> Hornet 4 Drive 21.4 110 #> Valiant        18.1 105 #> Merc 280       19.2 123 #> Merc 280C      17.8 123 #>  #>  #>  #> $`8` #> $`8`$`0` #> $`8`$`0`$`0` #>                      mpg  hp #> Hornet Sportabout   18.7 175 #> Duster 360          14.3 245 #> Merc 450SE          16.4 180 #> Merc 450SL          17.3 180 #> Merc 450SLC         15.2 180 #> Cadillac Fleetwood  10.4 205 #> Lincoln Continental 10.4 215 #> Chrysler Imperial   14.7 230 #> Dodge Challenger    15.5 150 #> AMC Javelin         15.2 150 #> Camaro Z28          13.3 245 #> Pontiac Firebird    19.2 175 #>  #> $`8`$`0`$`1` #>                 mpg  hp #> Ford Pantera L 15.8 264 #> Maserati Bora  15.0 335 #>  #>  #>  rsplit(mtcars, mpg + hp ~ cyl + vs + am, keep.by = TRUE) #> $`4` #> $`4`$`0` #> $`4`$`0`$`1` #>               cyl vs am mpg hp #> Porsche 914-2   4  0  1  26 91 #>  #>  #> $`4`$`1` #> $`4`$`1`$`0` #>               cyl vs am  mpg hp #> Merc 240D       4  1  0 24.4 62 #> Merc 230        4  1  0 22.8 95 #> Toyota Corona   4  1  0 21.5 97 #>  #> $`4`$`1`$`1` #>                cyl vs am  mpg  hp #> Datsun 710       4  1  1 22.8  93 #> Fiat 128         4  1  1 32.4  66 #> Honda Civic      4  1  1 30.4  52 #> Toyota Corolla   4  1  1 33.9  65 #> Fiat X1-9        4  1  1 27.3  66 #> Lotus Europa     4  1  1 30.4 113 #> Volvo 142E       4  1  1 21.4 109 #>  #>  #>  #> $`6` #> $`6`$`0` #> $`6`$`0`$`1` #>               cyl vs am  mpg  hp #> Mazda RX4       6  0  1 21.0 110 #> Mazda RX4 Wag   6  0  1 21.0 110 #> Ferrari Dino    6  0  1 19.7 175 #>  #>  #> $`6`$`1` #> $`6`$`1`$`0` #>                cyl vs am  mpg  hp #> Hornet 4 Drive   6  1  0 21.4 110 #> Valiant          6  1  0 18.1 105 #> Merc 280         6  1  0 19.2 123 #> Merc 280C        6  1  0 17.8 123 #>  #>  #>  #> $`8` #> $`8`$`0` #> $`8`$`0`$`0` #>                     cyl vs am  mpg  hp #> Hornet Sportabout     8  0  0 18.7 175 #> Duster 360            8  0  0 14.3 245 #> Merc 450SE            8  0  0 16.4 180 #> Merc 450SL            8  0  0 17.3 180 #> Merc 450SLC           8  0  0 15.2 180 #> Cadillac Fleetwood    8  0  0 10.4 205 #> Lincoln Continental   8  0  0 10.4 215 #> Chrysler Imperial     8  0  0 14.7 230 #> Dodge Challenger      8  0  0 15.5 150 #> AMC Javelin           8  0  0 15.2 150 #> Camaro Z28            8  0  0 13.3 245 #> Pontiac Firebird      8  0  0 19.2 175 #>  #> $`8`$`0`$`1` #>                cyl vs am  mpg  hp #> Ford Pantera L   8  0  1 15.8 264 #> Maserati Bora    8  0  1 15.0 335 #>  #>  #>   # Split this sectoral data, first by Variable (Emloyment and Value Added), then by Country GGDCspl <- rsplit(GGDC10S, ~ Variable + Country, cols = 6:16) str(GGDCspl) #> List of 2 #>  $ EMP:List of 42 #>   ..$ ARG:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 1800 1835 1731 2030 1889 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 32.7 34.4 35.6 33.8 33.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 1603 1641 1690 1578 1722 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 39.3 42.4 49.2 52.2 57.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 314 353 311 292 330 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 890 880 932 904 914 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 425 428 462 455 469 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 204 204 219 210 214 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] 825 818 881 870 894 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 411 411 442 435 445 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 6544 6647 6752 6859 6967 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ BOL:'data.frame':\t61 obs. of  11 variables: #>   .. ..$ AGR : num [1:61] 1052 1019 987 976 946 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:61] 46.3 49.5 50.8 54.6 46.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:61] 117 117 115 122 141 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:61] 1.45 1.39 1.38 1.46 1.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:61] 26 37.8 48.2 34.3 33.5 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:61] 60.8 69.4 71.1 64.4 66.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:61] 23.1 22.9 27.7 28.8 32.5 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:61] 4.31 4.94 5.08 4.62 4.76 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:61] 119 113 114 121 121 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:61] 1450 1435 1421 1407 1393 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ BRA:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 12637 12886 13140 13400 13665 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 96.8 98.2 99.6 101 102.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 2254 2326 2399 2475 2554 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 174 178 182 186 190 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 709 731 754 777 801 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 1332 1394 1458 1525 1596 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 587 613 640 669 699 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 461 489 518 549 583 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] 690 732 777 824 875 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 694 737 782 830 881 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 19635 20183 20749 21337 21945 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ BWA:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA NA NA NA 152 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA NA NA NA 1.94 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA NA NA NA 2.42 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA NA NA NA 0.12 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA NA NA NA 2.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA NA NA NA 2.47 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA NA NA NA 2.31 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA NA NA NA 1.21 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA NA NA NA 4.51 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA NA NA NA 4.07 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA NA NA NA 174 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ CHL:'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] 679 696 749 702 630 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] 108 130 113 119 103 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] 418 379 348 351 558 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] 21.2 24.2 19.4 19.6 16.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] 97.6 97.9 97.1 86.3 84.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] 225 222 241 256 218 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] 93.4 101.6 108.2 119.4 119.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] 50.5 53.9 51 52.8 46.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] 480 491 492 537 491 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] 2172 2196 2219 2243 2267 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ CHN:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA 173170 177470 181510 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA 1610 1804 1979 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA 11653 13054 14325 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA 212 238 261 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA 1834 2055 2255 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA 4881 4936 4669 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA 3211 3247 3071 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA 1347 1362 1288 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA 6870 6946 6570 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA 2501 2529 2392 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA 207290 213640 218320 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ COL:'data.frame':\t61 obs. of  11 variables: #>   .. ..$ AGR : num [1:61] 2094 2097 2145 2095 2074 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:61] 56.6 61.6 58.4 58.7 57.5 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:61] 421 422 425 443 459 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:61] 10.6 10.6 10.4 10.7 10.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:61] 120 114 117 142 180 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:61] 180 191 201 226 253 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:61] 117 136 144 149 156 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:61] 170 173 175 190 206 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:61] 541 576 576 610 604 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:61] 3710 3781 3852 3925 4000 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ CRI:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 153 161 165 158 160 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 0.795 0.843 0.834 0.825 0.909 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 29.1 31 30.8 30.6 33.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 1.59 1.58 1.6 1.87 1.94 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 11.4 10.1 11 14.8 15.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 20.9 21 21.2 23.2 22.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 9.27 8.81 9.08 9.64 10.17 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 4.24 4.16 4.44 5.12 5.17 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] 25.8 24.9 25.1 29.3 30.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 13.1 12.7 13.1 15.1 15.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 269 276 282 289 296 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ DEW:'data.frame':\t61 obs. of  11 variables: #>   .. ..$ AGR : num [1:61] 4837 4658 4520 4394 4275 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:61] 620 640 675 691 685 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:61] 6262 6638 6791 7036 7327 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:61] 154 158 167 169 173 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:61] 1489 1588 1673 1788 1922 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:61] 2745 2943 3120 3331 3510 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:61] 1005 1029 1062 1086 1120 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:61] 587 647 718 776 832 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:61] 2149 2175 2257 2326 2403 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:61] 531 530 547 566 587 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:61] 20379 21007 21527 22164 22832 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ DNK:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] 516 509 502 495 484 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] 30.9 29.5 28.1 26.7 25.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] 476 487 514 522 503 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] 10.8 10.8 11.1 11.3 11.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] 128 133 138 135 140 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] 345 345 360 367 374 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] 128 130 131 132 126 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] 83.4 82.5 80.2 78.4 80.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] 155 157 159 176 182 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] 91.2 92.1 92.9 94.1 92.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] 1963 1975 2016 2038 2020 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ EGY:'data.frame':\t65 obs. of  11 variables: #>   .. ..$ AGR : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ESP:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA NA 4339 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA NA 111 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA NA 1532 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA NA 42.9 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA NA 523 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA NA 1104 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA NA 383 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA NA 224 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA NA 592 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA NA 957 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA NA 9808 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ETH:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA 8960 9140 9323 9510 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA 0.401 0.535 0.535 0.669 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA 119 125 129 147 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA 1.44 1.74 2.2 2.52 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA 16.5 18.1 18.7 19.5 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA 75.4 85.3 94.7 111.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA 14 15.4 16.6 19.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA 13.8 13.7 13.6 13.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA 75.1 80.7 88.6 97.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA 39.2 50.5 54 59.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA 9315 9531 9741 9981 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ FRA:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA NA 5004 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA NA 212 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA NA 4637 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA NA 111 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA NA 1494 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA NA 2459 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA NA 946 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA NA 1177 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA NA 3234 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA NA 367 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA NA 19643 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ GBR:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] 1372 1368 1352 1317 1289 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] 1651 1647 1606 1610 1647 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] 7255 7388 7573 7747 7632 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] 269 281 296 304 312 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] 1669 1655 1654 1670 1661 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] 3321 3418 3452 3507 3529 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] 3673 3669 3679 3601 3615 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] 547 561 555 561 568 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] 4006 3881 3906 4045 4127 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] 647 626 621 610 621 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] 24410 24494 24694 24970 25001 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ GHA:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 1555 1576 1599 1621 1644 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 47.4 46.5 44.7 45.8 38.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 280 281 277 290 262 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 13.95 12.86 11.6 11.15 8.88 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 87.2 89.1 89.1 95 87 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 365 378 383 414 407 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 66.7 69.7 71.2 77.5 72.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 7.03 7.47 7.76 8.59 8.18 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 96.7 105.7 112.8 128.5 125.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 44.2 46 46.8 50.8 47.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 2562 2613 2642 2742 2701 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ HKG:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ IDN:'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ IND:'data.frame':\t61 obs. of  11 variables: #>   .. ..$ AGR : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ITA:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA NA NA 9517 9284 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA NA NA 39.9 41.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA NA NA 4025 4035 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA NA NA 96.7 99.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA NA NA 1108 1264 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA NA NA 1818 1913 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA NA NA 607 619 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA NA NA 364 368 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA NA NA 1791 1838 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA NA NA 773 787 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA NA NA 20139 20249 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ JPN:'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA 17082 16570 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA 663 630 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA 6836 7074 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA 199 194 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA 1954 2038 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA 5952 6496 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA 1879 1830 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA 1509 1562 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA 3839 3974 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA 818 847 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA 40732 41216 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ KEN:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ KOR:'data.frame':\t61 obs. of  11 variables: #>   .. ..$ AGR : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MEX:'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] 4808 4781 4725 4748 4826 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] 96.8 94.7 101.6 100.1 94.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] 969 1002 1036 1060 1064 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] 24.9 25.3 25.9 27 26.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] 224 234 249 263 266 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] 682 708 727 751 737 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] 210 205 233 230 236 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] 86 89.1 94.8 93.6 101 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] 688 723 750 761 772 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] 421 443 459 465 472 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] 8210 8305 8401 8498 8596 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MOR:'data.frame':\t65 obs. of  11 variables: #>   .. ..$ AGR : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:65] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MUS:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA NA 67.2 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA NA 0.152 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA NA 26 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA NA 2.16 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA NA 18.8 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA NA 17.1 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA NA 11.2 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA NA 0.781 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA NA 15.8 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA NA 17.4 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA NA 177 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MWI:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MYS:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ NGA:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 13009 13182 13359 13537 13718 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 42 42.4 35.6 26.8 24.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 567 664 797 969 1079 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 21 22.9 22.4 23 23.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 252 240 212 202 199 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 2155 2189 2227 2601 2893 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 294 327 341 354 402 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 42 42.4 42.8 42.1 45.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 95 125 171 227 330 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 163 217 263 314 418 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 16640 17053 17470 18296 19133 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ NLD:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA 637 629 619 610 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA 61.5 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA 1034 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA 31.2 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA 356 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA 623 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA 293 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA 169 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA 708 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA 375 NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA 4288 629 619 610 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ PER:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ PHL:'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ SEN:'data.frame':\t51 obs. of  11 variables: #>   .. ..$ AGR : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:51] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ SGP:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ SWE:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA NA 735 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA NA 20.1 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA NA 971 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA NA 18.9 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA NA 273 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA NA 351 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA NA 263 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA NA 63.4 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA NA 451 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA NA 148 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA NA 3294 NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ THA:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ TWN:'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ TZA:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 4021 4119 4221 4324 4430 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 5.98 5.48 4.91 3.84 4.06 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 46.9 49.4 52.8 56.2 60.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 1.23 4.3 4.54 4.75 4.56 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 7.47 6.52 8.2 8.94 11.63 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 42.4 53.7 54.8 56.8 62 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 10.2 27.7 29.1 31.1 33 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 4.05 5.13 5.27 5.5 6.05 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 153 206 179 157 140 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 91.3 123.3 107 93.7 83.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 4383 4601 4666 4742 4835 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ USA:'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA NA NA 5703 5375 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA NA NA 980 995 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA NA NA 15641 16868 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA NA NA 463 473 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA NA NA 3676 3927 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA NA NA 12579 13168 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA NA NA 4898 5160 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA NA NA 4238 4456 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA NA NA 11826 13900 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA NA NA 2534 2604 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA NA NA 62539 66926 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ VEN:'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 695 729 733 727 678 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 47 49.5 49.3 45.6 47 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 164 160 174 194 219 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 6.07 6.85 7.56 8.96 9.78 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 97.1 113.2 119.5 106.7 121.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 144 163 166 181 209 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 54.6 55.2 54 65.6 72.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 50.5 55.8 55.4 58.8 66.1 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 309 287 313 339 360 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 1567 1619 1672 1727 1783 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ZAF:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 3375 3302 3231 3161 3091 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 615 641 629 635 648 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 644 668 697 731 795 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 28.3 30.2 30.9 33.5 35 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 276 269 272 283 320 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 751 774 797 821 846 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 205 216 227 242 253 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 119 125 132 138 146 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 323 337 355 354 362 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 586 607 622 639 654 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 6922 6969 6992 7038 7149 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ZMB:'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>  $ VA :List of 43 #>   ..$ ARG     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 5.89e-07 9.17e-07 9.96e-07 1.48e-06 1.40e-06 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 0 0 0 0 0 0 0 0 0 0 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 3.53e-06 4.77e-06 5.35e-06 5.37e-06 6.56e-06 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 0 0 0 0 0 0 0 0 0 0 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 5.59e-07 6.66e-07 7.02e-07 6.78e-07 8.21e-07 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 2.99e-06 4.43e-06 4.74e-06 4.97e-06 5.56e-06 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 8.81e-07 1.27e-06 1.50e-06 1.58e-06 1.76e-06 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 2.51e-07 3.12e-07 3.72e-07 4.37e-07 4.96e-07 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] 1.21e-06 1.68e-06 1.94e-06 2.19e-06 2.55e-06 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 2.92e-07 3.95e-07 5.05e-07 5.69e-07 6.64e-07 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 1.03e-05 1.44e-05 1.61e-05 1.73e-05 1.98e-05 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ BOL     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ BRA     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ BWA     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] NA NA NA NA 16.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] NA NA NA NA 3.49 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] NA NA NA NA 0.737 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] NA NA NA NA 0.104 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] NA NA NA NA 0.66 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] NA NA NA NA 6.24 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] NA NA NA NA 1.66 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] NA NA NA NA 1.12 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] NA NA NA NA 4.82 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] NA NA NA NA 2.34 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] NA NA NA NA 37.5 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ CHL     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 0.0149 0.0176 0.0251 0.0296 0.0449 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 0.0189 0.0269 0.0317 0.0424 0.0631 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 0.0129 0.0143 0.0189 0.0259 0.0752 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 0.00131 0.00185 0.00211 0.00287 0.00438 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 0.0114 0.0132 0.0174 0.0196 0.0322 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 0.0198 0.0247 0.0347 0.0485 0.0734 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 0.0137 0.0164 0.0247 0.0343 0.0578 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 0.0279 0.0347 0.0452 0.0611 0.086 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 0.0322 0.0385 0.0517 0.0723 0.1135 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 0.153 0.188 0.251 0.337 0.551 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ CHN     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA 34598 38139 39552 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA 1366 1855 2040 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA 11010 14947 16437 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA 681 924 1017 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA 2200 2900 2700 3100 5600 4600 6900 7700 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA 8470 12183 12690 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA 3080 3717 4036 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA 2247 2157 2157 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA 3049 4015 3710 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA 781 1028 950 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA 67482 81866 85288 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ COL     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 3305 3632 3859 4290 5003 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 223 286 350 370 394 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 1818 1995 2209 2819 3206 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 50.3 56.3 62.3 82.7 83.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 355 380 412 489 618 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 1600 1585 1820 2111 2490 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 589 714 783 856 988 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 754 869 923 960 1027 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 1365 1539 1749 1896 2158 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 10059 11057 12167 13873 15968 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ CRI     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 371 398 439 475 495 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 1.73 1.86 2.03 2.2 2.34 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 306 330 361 391 415 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 17.4 19 20.7 22.3 23.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 117 125 137 148 158 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 320 345 375 404 426 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 108 120 133 145 153 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 123 135 146 158 168 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] 106 117 131 145 164 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 50 55.1 61.8 68.5 77.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 1520 1646 1806 1960 2082 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ DEW     :'data.frame':\t61 obs. of  11 variables: #>   .. ..$ AGR : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:61] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ DNK     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ EGY     :'data.frame':\t64 obs. of  11 variables: #>   .. ..$ AGR : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:64] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ESP     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ETH     :'data.frame':\t53 obs. of  11 variables: #>   .. ..$ AGR : num [1:53] NA 4496 4514 4669 5129 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:53] NA 11.9 15.8 15.8 19.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:53] NA 110 121 127 150 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:53] NA 31.6 31.6 37.9 44.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:53] NA 284 313 323 339 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:53] NA 334 358 389 487 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:53] NA 62.4 66.8 72.1 85.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:53] NA 19 20.9 23.7 26.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:53] NA 91.8 113.5 117.6 129 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:53] NA 106 111 120 143 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:53] NA 5547 5665 5896 6552 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ FRA     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ GBR     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ GHA     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 0.0358 0.0382 0.0405 0.0452 0.0508 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 0.0051 0.00546 0.00579 0.00645 0.00724 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 0.0174 0.0187 0.0198 0.022 0.0248 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 0.00078 0.000834 0.000884 0.000985 0.001107 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 0.016 0.0171 0.0181 0.0202 0.0227 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 0.00906 0.00968 0.01027 0.01144 0.01286 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 0.0195 0.0209 0.0221 0.0247 0.0277 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 0.00651 0.00589 0.00634 0.00725 0.00792 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 0.0185 0.0198 0.021 0.0234 0.0263 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 0.00144 0.00154 0.00163 0.00182 0.00204 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 0.13 0.138 0.146 0.163 0.183 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ HKG     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ IDN     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ IND     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] 52242 53942 52545 57895 49249 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] 733 817 841 853 889 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] 10957 12158 11446 12902 13381 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] 252 309 320 343 378 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] 2581 2949 2734 2744 3052 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] 6540 6951 7063 7552 7685 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] 3185 3570 3529 3702 3874 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] 7177 7723 8156 8797 9423 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] 5008 5253 5384 5742 6100 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] 2999 3116 3232 3326 3354 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] 91673 96788 95250 103857 97384 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ITA     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ JPN     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA 1136846 1256178 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA 170793 180088 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA 1974868 2073194 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA 141430 163909 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA 292007 301608 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA 967553 1105979 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA 478903 556146 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA 434033 495936 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA 882683 1008780 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA 161783 184894 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA 6640899 7326712 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ KEN     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 4910 4714 5613 6043 6100 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 59.7 43.4 43.4 48.8 83 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 818 863 875 928 1083 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 299 299 352 384 414 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 447 441 385 277 294 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 549 561 570 604 669 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 626 650 687 758 804 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 284 308 316 335 388 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 841 912 950 973 1138 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 374 340 343 372 406 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 9208 9133 10134 10723 11378 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ KOR     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA 22697 26809 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA 724 888 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA 4012 7199 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA 204 297 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA 852 1441 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA 2442 4452 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA 2447 3581 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA 8701 11218 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA 5647 10541 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA 47726 66426 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MEX     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 10.9 13.1 13.9 14.6 18.1 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 3.2 4.18 4.61 3.96 4.75 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 9.24 11.9 13.18 13.48 16.19 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 0.282 0.358 0.429 0.48 0.567 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 1.34 2.03 2.63 1.95 2.57 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 13.1 18.3 20.4 19.2 23.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 4.25 5.22 6.13 6.34 6.86 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] 3.07 3.6 4.46 4.71 5.77 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] 2.31 2.72 3.15 3.47 4.22 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 0.653 0.769 0.891 0.984 1.195 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 48.3 62.2 69.8 69.2 84 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MOR     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MUS     :'data.frame':\t53 obs. of  11 variables: #>   .. ..$ AGR : num [1:53] 106 182 178 293 177 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:53] 22 22 22 22 22 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:53] 68.5 118.5 119.5 178.2 120.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:53] 29.9 22.2 22.6 21.7 22.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:53] 34.9 40.7 49.5 61.1 61.1 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:53] 96.4 106.5 106.5 113.7 125.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:53] 102 112 116 133 117 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:53] 32.6 32.6 35.1 37.6 37.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:53] 64.2 68.9 73.7 78.4 83.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:53] 11.2 11.7 12.5 13.1 13.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:53] 567 717 735 952 780 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MWI     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 81.1 84.9 88.3 94.6 87.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 0.585 0.63 0.652 0.922 0.854 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 8.73 9.4 9.73 13.76 12.75 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 1.33 1.99 1.66 1.99 1.99 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 7.93 7.13 7.13 7.53 10.7 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 22.2 21.8 21.8 23.4 23.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 12.1 13.2 14.2 14.2 13.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 0.732 0.619 0.628 0.86 1.204 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 17 18.7 21.3 23 50.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 2.94 3.1 3.25 3.33 17.57 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 155 161 169 183 220 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ MYS     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ NGA     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 1903 1955 2156 2247 2250 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 25.3 41.5 52.6 53.4 71.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 357 405 483 571 573 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 63.8 77.1 77.1 97.1 114.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 145 163 173 181 210 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 507 523 559 619 680 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 97.4 116 123.4 130.4 152.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 48.5 55.3 57.8 65 75.1 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 67.2 72.6 77.6 81.3 94.9 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 51.6 56.7 64.4 71.1 81.2 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 3266 3466 3824 4116 4303 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ NGA(alt):'data.frame':\t4 obs. of  11 variables: #>   .. ..$ AGR : num [1:4] 12988809 14421929 15918632 17625143 #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:4] 8454554 11140408 11382588 11631349 #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:4] 3578642 4085393 4744699 5476303 #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:4] 388270 569933 754282 961717 #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:4] 1570973 1819803 2142754 2502582 #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:4] 9156043 10608005 12170934 13732584 #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:4] 6655717 7508984 8573386 9773442 #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:4] 7095640 8179975 9848171 11650290 #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:4] 3385190 3862789 4422719 5056768 #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:4] 930958 1061360 1228371 1411950 #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:4] 54204795 63258579 71186535 79822128 #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ NLD     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ PER     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] 7.8 9.59 9.85 10.17 10.85 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] 2.45 3.35 3.55 3.74 4.72 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] 2.04 2.32 2.67 3.03 3.72 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] 0.281 0.344 0.389 0.407 0.441 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] 0.43 0.527 0.596 0.623 0.676 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] 1.76 2.15 2.5 2.69 2.82 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] 1.64 2.01 2.27 2.38 2.58 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] -7.26 -8.51 -13.76 -13.82 -14.94 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] 1.37 1.72 2.28 2.58 2.71 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] 10.5 13.5 10.3 11.8 13.6 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ PHL     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ SEN     :'data.frame':\t51 obs. of  11 variables: #>   .. ..$ AGR : num [1:51] 47013 49204 52369 47237 51412 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:51] 2033 2222 1843 1811 1958 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:51] 14586 15805 16212 18428 19915 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:51] 4658 5092 4224 4151 4486 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:51] 2515 2601 3086 3024 3174 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:51] 37244 41224 44208 47604 48727 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:51] 9903 10068 10524 10689 11561 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:51] 3157 2672 2710 3400 3342 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:51] 29462 29462 29462 35993 36363 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:51] 3627 3627 3627 4431 4476 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:51] 154198 161976 168266 176768 185413 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ SGP     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ SWE     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ THA     :'data.frame':\t62 obs. of  11 variables: #>   .. ..$ AGR : num [1:62] NA 12192 11161 12087 11062 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:62] NA 334 349 328 340 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:62] NA 3845 4358 4923 5008 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:62] NA 42.2 47.7 58.1 81.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:62] NA 541 784 887 898 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:62] NA 6065 6760 6955 7364 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:62] NA 810 1068 1481 1618 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:62] NA 52.3 57 62.9 99.3 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:62] NA 1765 2972 3094 3599 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:62] NA 529 592 688 698 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:62] NA 26176 28150 30564 30767 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ TWN     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA 3959 5529 7867 7027 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA 191 391 448 543 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA 1767 2148 2804 3848 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA 141 149 203 224 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA 449 620 870 1233 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA 1667 3035 4176 4264 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA 517 701 863 950 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA 720 926 1225 1360 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA 1567 1947 2377 3045 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA 355 442 520 590 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA 11334 15886 21354 23083 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ TZA     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 2696 2727 2969 3332 3394 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 334 353 327 282 385 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 1503 1912 2104 2131 2322 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 98.5 115 123.2 131.4 147.8 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 380 480 504 513 604 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 1881 1992 2142 2291 2571 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 700 692 756 756 805 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 510 540 581 622 698 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 1702 1978 2224 2239 2423 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 458 497 516 575 621 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 10263 11286 12246 12873 13970 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ USA     :'data.frame':\t65 obs. of  11 variables: #>   .. ..$ AGR : num [1:65] 19219 22268 18045 19217 22150 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:65] 5688 7922 6849 7901 8676 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:65] 59475 67616 65369 76046 89611 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:65] 4072 5160 5160 5904 6994 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:65] 7362 8341 9495 10481 12045 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:65] 52321 56739 56224 60339 66545 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:65] 18304 20651 20313 22658 25554 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:65] 35003 38985 42001 45917 50686 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:65] 38109 37691 39314 39649 48780 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:65] 4534 4915 5062 5421 5915 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:65] 244086 270289 267831 293534 336958 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ VEN     :'data.frame':\t63 obs. of  11 variables: #>   .. ..$ AGR : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:63] NA NA NA NA NA NA NA NA NA NA ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ZAF     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 559 608 622 678 646 687 774 950 863 907 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 618 638 674 702 757 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 1001 1088 1166 1336 1513 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 120 131 141 153 165 177 196 222 246 271 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 149 144 154 180 234 291 315 337 366 414 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 694 715 783 883 973 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 495 504 548 610 675 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 260 284 292 314 374 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 446 477 511 564 618 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 378 401 421 446 479 513 557 598 647 699 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 4720 4990 5312 5866 6434 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   ..$ ZMB     :'data.frame':\t52 obs. of  11 variables: #>   .. ..$ AGR : num [1:52] 54.7 58.8 56.2 62.2 64.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Agriculture \" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MIN : num [1:52] 410 371 359 368 421 ... #>   .. .. ..- attr(*, \"label\")= chr \"Mining\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ MAN : num [1:52] 6.79 8.51 8.59 9.81 11.53 ... #>   .. .. ..- attr(*, \"label\")= chr \"Manufacturing\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ PU  : num [1:52] 26.4 24.4 25.1 27 22.5 ... #>   .. .. ..- attr(*, \"label\")= chr \"Utilities\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ CON : num [1:52] 30.6 26.4 23.7 24.9 30 ... #>   .. .. ..- attr(*, \"label\")= chr \"Construction\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ WRT : num [1:52] 48.4 48.1 51.4 51.7 75.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Trade, restaurants and hotels\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ TRA : num [1:52] 58.9 55.5 55.5 56.1 58.4 ... #>   .. .. ..- attr(*, \"label\")= chr \"Transport, storage and communication\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ FIRE: num [1:52] 5.46 5.21 5.59 5.94 5.58 ... #>   .. .. ..- attr(*, \"label\")= chr \"Finance, insurance, real estate and business services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ GOV : num [1:52] 47.5 52.6 57.8 60.3 68 ... #>   .. .. ..- attr(*, \"label\")= chr \"Government services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ OTH : num [1:52] 5.09 5.18 5.27 5.75 5.87 ... #>   .. .. ..- attr(*, \"label\")= chr \"Community, social and personal services\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\" #>   .. ..$ SUM : num [1:52] 693 656 648 672 763 ... #>   .. .. ..- attr(*, \"label\")= chr \"Summation of sector GDP\" #>   .. .. ..- attr(*, \"format.stata\")= chr \"%10.0g\"  # The nested list can be reassembled using unlist2d() head(unlist2d(GGDCspl, idcols = .c(Variable, Country))) #>   Variable Country      AGR      MIN      MAN       PU      CON      WRT #> 1      EMP     ARG 1799.565 32.71936 1603.249 39.26323 314.1059 889.9666 #> 2      EMP     ARG 1835.181 34.37387 1640.927 42.43318 353.4173 879.7295 #> 3      EMP     ARG 1730.611 35.55357 1690.117 49.16048 311.3910 932.2140 #> 4      EMP     ARG 2029.762 33.83809 1578.124 52.20825 291.6067 903.6310 #> 5      EMP     ARG 1889.316 33.34185 1721.997 57.66520 330.1971 913.8400 #>        TRA     FIRE      GOV      OTH      SUM #> 1 425.3517 203.8384 824.9212 410.8922 6543.872 #> 2 427.9257 204.0642 817.7359 411.4769 6647.265 #> 3 461.5084 218.9973 881.0104 441.7270 6752.291 #> 4 455.3023 209.7324 870.0849 434.6854 6858.976 #> 5 468.6230 213.7772 893.7040 444.8849 6967.347 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] rm(GGDCspl)  # Another example with mtcars (not as clean because of row.names) nl <- rsplit(mtcars, mpg + hp ~ cyl + vs + am) str(nl) #> List of 3 #>  $ 4:List of 2 #>   ..$ 0:List of 1 #>   .. ..$ 1:'data.frame':\t1 obs. of  2 variables: #>   .. .. ..$ mpg: num 26 #>   .. .. ..$ hp : num 91 #>   ..$ 1:List of 2 #>   .. ..$ 0:'data.frame':\t3 obs. of  2 variables: #>   .. .. ..$ mpg: num [1:3] 24.4 22.8 21.5 #>   .. .. ..$ hp : num [1:3] 62 95 97 #>   .. ..$ 1:'data.frame':\t7 obs. of  2 variables: #>   .. .. ..$ mpg: num [1:7] 22.8 32.4 30.4 33.9 27.3 30.4 21.4 #>   .. .. ..$ hp : num [1:7] 93 66 52 65 66 113 109 #>  $ 6:List of 2 #>   ..$ 0:List of 1 #>   .. ..$ 1:'data.frame':\t3 obs. of  2 variables: #>   .. .. ..$ mpg: num [1:3] 21 21 19.7 #>   .. .. ..$ hp : num [1:3] 110 110 175 #>   ..$ 1:List of 1 #>   .. ..$ 0:'data.frame':\t4 obs. of  2 variables: #>   .. .. ..$ mpg: num [1:4] 21.4 18.1 19.2 17.8 #>   .. .. ..$ hp : num [1:4] 110 105 123 123 #>  $ 8:List of 1 #>   ..$ 0:List of 2 #>   .. ..$ 0:'data.frame':\t12 obs. of  2 variables: #>   .. .. ..$ mpg: num [1:12] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 ... #>   .. .. ..$ hp : num [1:12] 175 245 180 180 180 205 215 230 150 150 ... #>   .. ..$ 1:'data.frame':\t2 obs. of  2 variables: #>   .. .. ..$ mpg: num [1:2] 15.8 15 #>   .. .. ..$ hp : num [1:2] 264 335 unlist2d(nl, idcols = .c(cyl, vs, am), row.names = \"car\") #>    cyl vs am            car  mpg  hp #> 1    4  0  1  Porsche 914-2 26.0  91 #> 2    4  1  0      Merc 240D 24.4  62 #> 3    4  1  0       Merc 230 22.8  95 #> 4    4  1  0  Toyota Corona 21.5  97 #> 5    4  1  1     Datsun 710 22.8  93 #> 6    4  1  1       Fiat 128 32.4  66 #> 7    4  1  1    Honda Civic 30.4  52 #> 8    4  1  1 Toyota Corolla 33.9  65 #> 9    4  1  1      Fiat X1-9 27.3  66 #> 10   4  1  1   Lotus Europa 30.4 113 #> 11   4  1  1     Volvo 142E 21.4 109 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 21 rows ] rm(nl)"},{"path":"https://sebkrantz.github.io/collapse/reference/select_replace_vars.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","title":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","text":"Efficiently select replace (add) subset columns () data frame. can done data type, using expressions, column names, indices, logical vectors, selector functions regular expressions matching column names.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/select_replace_vars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","text":"","code":"## Select and replace variables, analgous to dplyr::select but significantly faster fselect(.x, ..., return = \"data\") fselect(x, ...) <- valuefselect(x, ...) <- value slt(.x, ..., return = \"data\")   # Shorthand for fselect slt(x, ...) <- value            # Shorthand for fselect<-  ## Select and replace columns by names, indices, logical vectors, ## regular expressions or using functions to identify columns  get_vars(x, vars, return = \"data\", regex = FALSE, rename = FALSE, ...)       gv(x, vars, return = \"data\", ...)   # Shorthand for get_vars      gvr(x, vars, return = \"data\", ...)   # Shorthand for get_vars(..., regex = TRUE)  get_vars(x, vars, regex = FALSE, ...) <- valueget_vars(x, vars, regex = FALSE, ...) <- value gv(x, vars, ...) <- value           # Shorthand for get_vars<- gvr(x, vars, ...) <- value           # Shorthand for get_vars<-(..., regex = TRUE)  ## Add columns at any position within a data.frame  add_vars(x, ..., pos = \"end\") add_vars(x, pos = \"end\") <- valueadd_vars(x, pos = \"end\") <- value       av(x, ..., pos = \"end\")             # Shorthand for add_vars av(x, pos = \"end\") <- value         # Shorthand for add_vars<-  ## Select and replace columns by data type  num_vars(x, return = \"data\") num_vars(x) <- valuenum_vars(x) <- value       nv(x, return = \"data\")       # Shorthand for num_vars nv(x) <- value               # Shorthand for num_vars<- cat_vars(x, return = \"data\")       # Categorical variables, see is_categorical cat_vars(x) <- valuecat_vars(x) <- value char_vars(x, return = \"data\") char_vars(x) <- valuechar_vars(x) <- value fact_vars(x, return = \"data\") fact_vars(x) <- valuefact_vars(x) <- value logi_vars(x, return = \"data\") logi_vars(x) <- valuelogi_vars(x) <- value date_vars(x, return = \"data\")      # See is_date date_vars(x) <- valuedate_vars(x) <- value"},{"path":"https://sebkrantz.github.io/collapse/reference/select_replace_vars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","text":"x, .x data frame list. value data frame list columns whose dimensions exactly match extracted subset x. 1 variable subset x, value can also atomic vector matrix, provided NROW(value) == nrow(x). vars vector column names, indices (can negative), suitable logical vector, vector regular expressions matching column names (regex = TRUE). also possible pass function returning TRUE FALSE applied columns x. return integer string specifying selector function return. options : Note: replacement functions replace data, however column names replaced together data (available). regex logical. TRUE regular expression search column names x using (vector ) regular expression(s) passed vars. Matching done using grep. rename logical. vars named vector column names indices, rename = TRUE use (non missing) names rename columns. pos position columns added data frame. \"end\" (default) append data frame end (right) side. \"front\" add columns front (left). Alternatively one can pass vector positions (matching length(value) value list). case columns shifted around new ones maintaining order. ... fselect: column names expressions e.g. fselect(mtcars, newname = mpg, hp, carb:vs). get_vars: arguments passed grep, regex = TRUE. add_vars: multiple lists/data frames vectors (given names e.g. name = vector). single argument passed may also (unnamed) vector matrix.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/select_replace_vars.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","text":"get_vars(<-) around 2x faster `[.data.frame` 8x faster `[<-.data.frame`, common operation data[cols] <- someFUN(data[cols]) can made 10x efficient (abstracting computations performed someFUN) using get_vars(data, cols) <- someFUN(get_vars(data, cols)) shorthand gv(data, cols) <- someFUN(gv(data, cols)). Similarly type-wise operations like data[sapply(data, .numeric)] data[sapply(data, .numeric)] <- value facilitated efficient using num_vars(data) num_vars(data) <- value shortcuts nv nv<- etc. fselect provides efficient alternative dplyr::select, allowing selection variables based expressions evaluated within data frame, see Examples. 100x faster dplyr::select also simple provide special methods (except 'sf' 'data.table' handled internally) . Finally, add_vars(data1, data2, data3, ...) lot faster cbind(data1, data2, data3, ...), preserves attributes data1 (.e. like adding columns data1). replacement function add_vars(data) <- someFUN(get_vars(data, cols)) efficiently appends data computed columns. pos argument allows adding columns positions end (right) data frame, see Examples. Note add_vars check duplicated column names NULL columns, evaluate expressions data environment, replicate length 1 inputs like cbind. provided ftransform. functions introduced perform operations class-independent. basically work like : (1) save attributes x, (2) unclass x, (3) subset, replace append x list, (4) modify \"names\" component attributes x accordingly (5) efficiently attach attributes result step (3). Thus can freely applied data.table's, grouped tibbles, panel data frames classes return object exactly class attributes.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/select_replace_vars.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","text":"many cases functions check length first column, one reasons fast. lists unequal-length columns offered replacements yields malformed data frame (also print warning console .e. notice ).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/select_replace_vars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Select, Replace or Add Data Frame Columns — fselect-get_vars-add_vars","text":"","code":"## Wold Development Data head(fselect(wlddev, Country = country, Year = year, ODA)) # Fast dplyr-like selecting #>       Country Year       ODA #> 1 Afghanistan 1960 116769997 #> 2 Afghanistan 1961 232080002 #> 3 Afghanistan 1962 112839996 #> 4 Afghanistan 1963 237720001 #> 5 Afghanistan 1964 295920013 #> 6 Afghanistan 1965 341839996 head(fselect(wlddev, -country, -year, -PCGDP)) #>   iso3c       date decade     region     income  OECD LIFEEX GINI       ODA #> 1   AFG 1961-01-01   1960 South Asia Low income FALSE 32.446   NA 116769997 #> 2   AFG 1962-01-01   1960 South Asia Low income FALSE 32.962   NA 232080002 #> 3   AFG 1963-01-01   1960 South Asia Low income FALSE 33.471   NA 112839996 #> 4   AFG 1964-01-01   1960 South Asia Low income FALSE 33.971   NA 237720001 #> 5   AFG 1965-01-01   1960 South Asia Low income FALSE 34.463   NA 295920013 #> 6   AFG 1966-01-01   1960 South Asia Low income FALSE 34.948   NA 341839996 #>       POP #> 1 8996973 #> 2 9169410 #> 3 9351441 #> 4 9543205 #> 5 9744781 #> 6 9956320 head(fselect(wlddev, country, year, PCGDP:ODA)) #>       country year PCGDP LIFEEX GINI       ODA #> 1 Afghanistan 1960    NA 32.446   NA 116769997 #> 2 Afghanistan 1961    NA 32.962   NA 232080002 #> 3 Afghanistan 1962    NA 33.471   NA 112839996 #> 4 Afghanistan 1963    NA 33.971   NA 237720001 #> 5 Afghanistan 1964    NA 34.463   NA 295920013 #> 6 Afghanistan 1965    NA 34.948   NA 341839996 head(fselect(wlddev, -(PCGDP:ODA))) #>       country iso3c       date year decade     region     income  OECD     POP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE 8996973 #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE 9169410 #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE 9351441 #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE 9543205 #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE 9744781 #> 6 Afghanistan   AFG 1966-01-01 1965   1960 South Asia Low income FALSE 9956320 fselect(wlddev, country, year, PCGDP:ODA) <- NULL          # Efficient deleting head(wlddev) #>   iso3c       date decade     region     income  OECD     POP #> 1   AFG 1961-01-01   1960 South Asia Low income FALSE 8996973 #> 2   AFG 1962-01-01   1960 South Asia Low income FALSE 9169410 #> 3   AFG 1963-01-01   1960 South Asia Low income FALSE 9351441 #> 4   AFG 1964-01-01   1960 South Asia Low income FALSE 9543205 #> 5   AFG 1965-01-01   1960 South Asia Low income FALSE 9744781 #> 6   AFG 1966-01-01   1960 South Asia Low income FALSE 9956320 rm(wlddev)  head(num_vars(wlddev))                                     # Select numeric variables #>   year decade PCGDP LIFEEX GINI       ODA     POP #> 1 1960   1960    NA 32.446   NA 116769997 8996973 #> 2 1961   1960    NA 32.962   NA 232080002 9169410 #> 3 1962   1960    NA 33.471   NA 112839996 9351441 #> 4 1963   1960    NA 33.971   NA 237720001 9543205 #> 5 1964   1960    NA 34.463   NA 295920013 9744781 #> 6 1965   1960    NA 34.948   NA 341839996 9956320 head(cat_vars(wlddev))                                     # Select categorical (non-numeric) vars #>       country iso3c       date     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 South Asia Low income FALSE #> 2 Afghanistan   AFG 1962-01-01 South Asia Low income FALSE #> 3 Afghanistan   AFG 1963-01-01 South Asia Low income FALSE #> 4 Afghanistan   AFG 1964-01-01 South Asia Low income FALSE #> 5 Afghanistan   AFG 1965-01-01 South Asia Low income FALSE #> 6 Afghanistan   AFG 1966-01-01 South Asia Low income FALSE head(get_vars(wlddev, is_categorical))                     # Same thing #>       country iso3c       date     region     income  OECD #> 1 Afghanistan   AFG 1961-01-01 South Asia Low income FALSE #> 2 Afghanistan   AFG 1962-01-01 South Asia Low income FALSE #> 3 Afghanistan   AFG 1963-01-01 South Asia Low income FALSE #> 4 Afghanistan   AFG 1964-01-01 South Asia Low income FALSE #> 5 Afghanistan   AFG 1965-01-01 South Asia Low income FALSE #> 6 Afghanistan   AFG 1966-01-01 South Asia Low income FALSE  num_vars(wlddev) <- num_vars(wlddev)                       # Replace Numeric Variables by themselves get_vars(wlddev,is.numeric) <- get_vars(wlddev,is.numeric) # Same thing  head(get_vars(wlddev, 9:12))                               # Select columns 9 through 12, 2x faster #>   PCGDP LIFEEX GINI       ODA #> 1    NA 32.446   NA 116769997 #> 2    NA 32.962   NA 232080002 #> 3    NA 33.471   NA 112839996 #> 4    NA 33.971   NA 237720001 #> 5    NA 34.463   NA 295920013 #> 6    NA 34.948   NA 341839996 head(get_vars(wlddev, -(9:12)))                            # All except columns 9 through 12 #>       country iso3c       date year decade     region     income  OECD     POP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE 8996973 #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE 9169410 #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE 9351441 #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE 9543205 #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE 9744781 #> 6 Afghanistan   AFG 1966-01-01 1965   1960 South Asia Low income FALSE 9956320 head(get_vars(wlddev, c(\"PCGDP\",\"LIFEEX\",\"GINI\",\"ODA\")))   # Select using column names #>   PCGDP LIFEEX GINI       ODA #> 1    NA 32.446   NA 116769997 #> 2    NA 32.962   NA 232080002 #> 3    NA 33.471   NA 112839996 #> 4    NA 33.971   NA 237720001 #> 5    NA 34.463   NA 295920013 #> 6    NA 34.948   NA 341839996 head(get_vars(wlddev, \"[[:upper:]]\", regex = TRUE))        # Same thing: match upper-case var. names #>    OECD PCGDP LIFEEX GINI       ODA     POP #> 1 FALSE    NA 32.446   NA 116769997 8996973 #> 2 FALSE    NA 32.962   NA 232080002 9169410 #> 3 FALSE    NA 33.471   NA 112839996 9351441 #> 4 FALSE    NA 33.971   NA 237720001 9543205 #> 5 FALSE    NA 34.463   NA 295920013 9744781 #> 6 FALSE    NA 34.948   NA 341839996 9956320 head(gvr(wlddev, \"[[:upper:]]\"))                           # Same thing #>    OECD PCGDP LIFEEX GINI       ODA     POP #> 1 FALSE    NA 32.446   NA 116769997 8996973 #> 2 FALSE    NA 32.962   NA 232080002 9169410 #> 3 FALSE    NA 33.471   NA 112839996 9351441 #> 4 FALSE    NA 33.971   NA 237720001 9543205 #> 5 FALSE    NA 34.463   NA 295920013 9744781 #> 6 FALSE    NA 34.948   NA 341839996 9956320  get_vars(wlddev, 9:12) <- get_vars(wlddev, 9:12)           # 9x faster wlddev[9:12] <- wlddev[9:12] add_vars(wlddev) <- STD(gv(wlddev,9:12), wlddev$iso3c)     # Add Standardized columns 9 through 12 head(wlddev)                                               # gv and av are shortcuts #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA #> 1 32.446   NA 116769997 8996973        NA  -1.653181       NA -0.6498451 #> 2 32.962   NA 232080002 9169410        NA  -1.602256       NA -0.5951801 #> 3 33.471   NA 112839996 9351441        NA  -1.552023       NA -0.6517082 #> 4 33.971   NA 237720001 9543205        NA  -1.502678       NA -0.5925063 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]  get_vars(wlddev, 14:17) <- NULL                            # Efficient Deleting added columns again av(wlddev, \"front\") <- STD(gv(wlddev,9:12), wlddev$iso3c)  # Again adding in Front head(wlddev) #>   STD.PCGDP STD.LIFEEX STD.GINI    STD.ODA     country iso3c       date year #> 1        NA  -1.653181       NA -0.6498451 Afghanistan   AFG 1961-01-01 1960 #> 2        NA  -1.602256       NA -0.5951801 Afghanistan   AFG 1962-01-01 1961 #> 3        NA  -1.552023       NA -0.6517082 Afghanistan   AFG 1963-01-01 1962 #> 4        NA  -1.502678       NA -0.5925063 Afghanistan   AFG 1964-01-01 1963 #>   decade     region     income  OECD PCGDP LIFEEX GINI       ODA     POP #> 1   1960 South Asia Low income FALSE    NA 32.446   NA 116769997 8996973 #> 2   1960 South Asia Low income FALSE    NA 32.962   NA 232080002 9169410 #> 3   1960 South Asia Low income FALSE    NA 33.471   NA 112839996 9351441 #> 4   1960 South Asia Low income FALSE    NA 33.971   NA 237720001 9543205 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] get_vars(wlddev, 1:4) <- NULL                              # Deleting av(wlddev,c(10,12,14,16)) <- W(wlddev,~iso3c, cols = 9:12, # Adding next to original variables                                keep.by = FALSE) head(wlddev) #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #>   W.PCGDP LIFEEX  W.LIFEEX GINI W.GINI       ODA       W.ODA     POP #> 1      NA 32.446 -16.75117   NA     NA 116769997 -1370778502 8996973 #> 2      NA 32.962 -16.23517   NA     NA 232080002 -1255468497 9169410 #> 3      NA 33.471 -15.72617   NA     NA 112839996 -1374708502 9351441 #> 4      NA 33.971 -15.22617   NA     NA 237720001 -1249828497 9543205 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] get_vars(wlddev, c(10,12,14,16)) <- NULL                   # Deleting  head(add_vars(wlddev, new = STD(wlddev$PCGDP)))                  # Can also add columns like this #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #> 4 Afghanistan   AFG 1964-01-01 1963   1960 South Asia Low income FALSE    NA #> 5 Afghanistan   AFG 1965-01-01 1964   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP new #> 1 32.446   NA 116769997 8996973  NA #> 2 32.962   NA 232080002 9169410  NA #> 3 33.471   NA 112839996 9351441  NA #> 4 33.971   NA 237720001 9543205  NA #> 5 34.463   NA 295920013 9744781  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] head(add_vars(wlddev, STD(nv(wlddev)), new = W(wlddev$PCGDP)))   # etc... #>       country iso3c       date year decade     region     income  OECD PCGDP #> 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA #> 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA #> 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA #>   LIFEEX GINI       ODA     POP  STD.year STD.decade STD.PCGDP STD.LIFEEX #> 1 32.446   NA 116769997 8996973 -1.703821  -1.460378        NA  -2.775283 #> 2 32.962   NA 232080002 9169410 -1.647027  -1.460378        NA  -2.730321 #> 3 33.471   NA 112839996 9351441 -1.590233  -1.460378        NA  -2.685969 #>   STD.GINI    STD.ODA    STD.POP new #> 1       NA -0.3890241 -0.1493233  NA #> 2       NA -0.2562874 -0.1476348  NA #> 3       NA -0.3935480 -0.1458523  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]  head(add_vars(mtcars, mtcars, mpg = mtcars$mpg, mtcars), 2)      # add_vars does not check names! #>               mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg cyl disp #> Mazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4  21   6  160 #> Mazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4  21   6  160 #>                hp drat    wt  qsec vs am gear carb mpg mpg cyl disp  hp drat #> Mazda RX4     110  3.9 2.620 16.46  0  1    4    4  21  21   6  160 110  3.9 #> Mazda RX4 Wag 110  3.9 2.875 17.02  0  1    4    4  21  21   6  160 110  3.9 #>                  wt  qsec vs am gear carb #> Mazda RX4     2.620 16.46  0  1    4    4 #> Mazda RX4 Wag 2.875 17.02  0  1    4    4"},{"path":"https://sebkrantz.github.io/collapse/reference/seqid.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Group-Id from Integer Sequences — seqid","title":"Generate Group-Id from Integer Sequences — seqid","text":"seqid can used group sequences integers vector, e.g. seqid(c(1:3, 5:7)) becomes c(rep(1,3), rep(2,3)). also supports increments > 1, unordered sequences, missing values sequence. applications facilitate identification , grouped operations , (irregular) time series panels.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/seqid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Group-Id from Integer Sequences — seqid","text":"","code":"seqid(x, o = NULL, del = 1L, start = 1L, na.skip = FALSE,       skip.seq = FALSE, check.o = TRUE)"},{"path":"https://sebkrantz.github.io/collapse/reference/seqid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Group-Id from Integer Sequences — seqid","text":"x factor integer vector. Numeric vectors converted integer .e. rounded downwards. o (optional) integer ordering vector specifying order pass x. del integer. integer deliminating two consecutive points sequence. del = 1 lets seqid track sequences form c(1,2,3,..), del = 2 tracks sequences c(1,3,5,..) etc. start integer. starting value resulting sequence id. Default starting 1. na.skip logical. TRUE skips missing values sequence. default behavior skipping seqid(c(1, NA, 2)) regarded one sequence coded c(1, NA, 1). skip.seq logical. na.skip = TRUE, changes behavior missing values viewed part sequence, .e. seqid(c(1, NA, 3)) regarded one sequence coded c(1, NA, 1). check.o logical. Programmers option: FALSE prevents checking element o range [1, length(x)], checks length o. gives extra speed, terminate R element o large small.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/seqid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Group-Id from Integer Sequences — seqid","text":"seqid created primarily workaround deal problems computing lagged values, differences growth rates irregularly spaced time series panels collapse version 1.5.0 (#26). Now flag, fdiff fgrowth natively support irregular data workaround superfluous, except iterated differencing yet supported irregular data. theory workaround express irregular time series panel series regular panel series group-id created time-periods within group consecutive. seqid makes easy: irregular panel gaps repeated values time variable, appropriate id variable can generated using settransform(data, newid = seqid(time, radixorder(id, time))). Lags can computed using L(data, 1, ~newid, ~time) etc. general, regularly spaced panel identity given identical(groupid(id, order(id, time)), seqid(time, order(id, time))) hold. opposite operation creating new time-variable consecutive group, see data.table::rowid.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/seqid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Group-Id from Integer Sequences — seqid","text":"integer vector class 'qG'. See qG.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/seqid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Group-Id from Integer Sequences — seqid","text":"","code":"## This creates an irregularly spaced panel, with a gap in time for id = 2 data <- data.frame(id = rep(1:3, each = 4),                    time = c(1:4, 1:2, 4:5, 1:4),                    value = rnorm(12)) data #>    id time        value #> 1   1    1 -1.540203133 #> 2   1    2 -2.131035606 #> 3   1    3  0.377222256 #> 4   1    4  1.693972475 #> 5   2    1  0.512538179 #> 6   2    2 -1.414468126 #> 7   2    4 -0.007331919 #> 8   2    5 -1.392200673 #> 9   3    1 -1.007205635 #> 10  3    2 -1.300076492 #> 11  3    3  0.749182687 #> 12  3    4  1.687936283  ## This gave a gaps in time error previous to collapse 1.5.0 L(data, 1, value ~ id, ~time) #>    id time     L1.value #> 1   1    1           NA #> 2   1    2 -1.540203133 #> 3   1    3 -2.131035606 #> 4   1    4  0.377222256 #> 5   2    1           NA #> 6   2    2  0.512538179 #> 7   2    4           NA #> 8   2    5 -0.007331919 #> 9   3    1           NA #> 10  3    2 -1.007205635 #> 11  3    3 -1.300076492 #> 12  3    4  0.749182687  ## Generating new id variable (here seqid(time) would suffice as data is sorted) settransform(data, newid = seqid(time, order(id, time))) data #>    id time        value newid #> 1   1    1 -1.540203133     1 #> 2   1    2 -2.131035606     1 #> 3   1    3  0.377222256     1 #> 4   1    4  1.693972475     1 #> 5   2    1  0.512538179     2 #> 6   2    2 -1.414468126     2 #> 7   2    4 -0.007331919     3 #> 8   2    5 -1.392200673     3 #> 9   3    1 -1.007205635     4 #> 10  3    2 -1.300076492     4 #> 11  3    3  0.749182687     4 #> 12  3    4  1.687936283     4  ## Lag the panel this way L(data, 1, value ~ newid, ~time) #>    newid time     L1.value #> 1      1    1           NA #> 2      1    2 -1.540203133 #> 3      1    3 -2.131035606 #> 4      1    4  0.377222256 #> 5      2    1           NA #> 6      2    2  0.512538179 #> 7      3    4           NA #> 8      3    5 -0.007331919 #> 9      4    1           NA #> 10     4    2 -1.007205635 #> 11     4    3 -1.300076492 #> 12     4    4  0.749182687  ## A different possibility: Creating a consecutive time variable settransform(data, newtime = data.table::rowid(id)) data #>    id time        value newid newtime #> 1   1    1 -1.540203133     1       1 #> 2   1    2 -2.131035606     1       2 #> 3   1    3  0.377222256     1       3 #> 4   1    4  1.693972475     1       4 #> 5   2    1  0.512538179     2       1 #> 6   2    2 -1.414468126     2       2 #> 7   2    4 -0.007331919     3       3 #> 8   2    5 -1.392200673     3       4 #> 9   3    1 -1.007205635     4       1 #> 10  3    2 -1.300076492     4       2 #> 11  3    3  0.749182687     4       3 #> 12  3    4  1.687936283     4       4 L(data, 1, value ~ id, ~newtime) #>    id newtime     L1.value #> 1   1       1           NA #> 2   1       2 -1.540203133 #> 3   1       3 -2.131035606 #> 4   1       4  0.377222256 #> 5   2       1           NA #> 6   2       2  0.512538179 #> 7   2       3 -1.414468126 #> 8   2       4 -0.007331919 #> 9   3       1           NA #> 10  3       2 -1.007205635 #> 11  3       3 -1.300076492 #> 12  3       4  0.749182687  ## With sorted data, the time variable can also just be omitted.. L(data, 1, value ~ id) #>    id     L1.value #> 1   1           NA #> 2   1 -1.540203133 #> 3   1 -2.131035606 #> 4   1  0.377222256 #> 5   2           NA #> 6   2  0.512538179 #> 7   2 -1.414468126 #> 8   2 -0.007331919 #> 9   3           NA #> 10  3 -1.007205635 #> 11  3 -1.300076492 #> 12  3  0.749182687"},{"path":"https://sebkrantz.github.io/collapse/reference/small-helpers.html","id":null,"dir":"Reference","previous_headings":"","what":"Small (Helper) Functions  — small-helpers","title":"Small (Helper) Functions  — small-helpers","text":"Convenience functions collapse package help deal object attributes variable names labels, object checking, metaprogramming, improve workflow.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/small-helpers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Small (Helper) Functions  — small-helpers","text":"","code":".c(...)                       # Non-standard concatenation i.e. .c(a, b) == c(\"a\", \"b\") nam %=% values                # Multiple-assignment e.g. .c(x, y) %=% c(1, 2), massign(nam, values,          # can also assign to different environment.         envir = parent.frame()) vlabels(X, attrn = \"label\",   # Get labels of variables in X, in attr(X[[i]], attrn)         use.names = TRUE) vlabels(X, attrn = \"label\") <- value    # Set labels of variables in X (by reference) setLabels(X, value = NULL,    # Set labels of variables in X (by reference) and return X           attrn = \"label\", cols = NULL) vclasses(X, use.names = TRUE) # Get classes of variables in X namlab(X, class = FALSE,      # Return data frame of names and labels,   attrn = \"label\", N = FALSE, # and (optionally) classes, number of observations   Ndistinct = FALSE)          # and number of non-missing distinct values add_stub(X, stub, pre = TRUE, # Add a stub (i.e. prefix or postfix) to column names          cols = NULL) rm_stub(X, stub, pre = TRUE,  # Remove stub from column names, also supports general         regex = FALSE,        # regex matching and removing of characters         cols = NULL, ...) all_identical(...)            # Check exact equality of multiple objects or list-elements all_obj_equal(...)            # Check near equality of multiple objects or list-elements all_funs(expr)                # Find all functions called in an R language expression setRownames(object,           # Set rownames of object and return object     nm = if(is.atomic(object)) seq_row(object) else NULL) setColnames(object, nm)       # Set colnames of object and return object setDimnames(object, dn,       # Set dimension names of object and return object             which = NULL) unattrib(object)              # Remove all attributes from object setAttrib(object, a)          # Replace all attributes with list of attributes 'a' setattrib(object, a)          # Same thing by reference, returning object invisibly copyAttrib(to, from)          # Copy all attributes from object 'from' to object 'to' copyMostAttrib(to, from)      # Copy most attributes from object 'from' to object 'to' is_categorical(x)             # The opposite of is.numeric is_date(x)                    # Check if object is of class \"Date\", \"POSIXlt\" or \"POSIXct\""},{"path":"https://sebkrantz.github.io/collapse/reference/small-helpers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Small (Helper) Functions  — small-helpers","text":"X matrix data frame (functions also support vectors arrays although less common). x (atomic) vector. expr expression type \"language\" e.g. quote(x / sum(x)). object, , suitable R object. suitable list attributes. attrn character. Name attribute store labels retrieve labels . N, Ndistinct logical. Options display number observations number distinct non-missing values. value whichv alloc: single value vector type. vlabels<- setLabels: matching character vector list variable labels. use.names logical. Preserve names X list. cols integer. (optional) indices columns apply operation . Note small functions needs integer, whereas functions package argument flexible. class logical. Also show classes variables X column? stub single character stub, .e. \"log.\", default pre-applied variables column names X. pre logical. FALSE post-apply stub. regex logical. Match pattern anywhere names using regular expression remove gsub. nm suitable vector row- column-names. dn suitable vector list names dimension(s). integer. NULL, dn list fully specifying dimension names object. Alternatively, vector list names dimensions can supplied. See Examples. nam character. vector object names. values matching atomic vector list objects. envir environment assign . ... .c: Comma-separated expressions. all_identical / all_obj_equal: Either multiple comma-separated objects single list objects elements checked exact / numeric equality. rm_stub: arguments passed gsub.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/small-helpers.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Small (Helper) Functions  — small-helpers","text":"all_funs opposite .vars, return functions called rather variables expression. See Examples. copyAttrib copyMostAttrib take shallow copy attribute list, .e. duplicate memory attributes . also, along setAttrib, take shallow copy lists passed argument, lists modified reference. Atomic arguments however modified reference. function setattrib, added v1.8.9, modifies object reference .e. shallow copies taken. copyMostAttrib copies attributes except \"names\", \"dim\" \"dimnames\" (like corresponding C-API function), copies \"row.names\" attribute data frames known valid. Thus suitable choice objects type equal dimensions.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/small-helpers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Small (Helper) Functions  — small-helpers","text":"","code":"## Non-standard concatenation .c(a, b, \"c d\", e == f) #> [1] \"a\"      \"b\"      \"c d\"    \"e == f\"  ## Multiple assignment .c(a, b) %=% list(1, 2) .c(T, N) %=% dim(EuStockMarkets) names(iris) %=% iris list2env(iris)          # Same thing #> <environment: 0x136b4c6d8> rm(list = c(\"a\", \"b\", \"T\", \"N\", names(iris)))  ## Variable labels namlab(wlddev) #>    Variable #> 1   country #> 2     iso3c #> 3      date #> 4      year #> 5    decade #> 6    region #> 7    income #> 8      OECD #> 9     PCGDP #> 10   LIFEEX #> 11     GINI #> 12      ODA #> 13      POP #>                                                                                Label #> 1                                                                       Country Name #> 2                                                                       Country Code #> 3                                                         Date Recorded (Fictitious) #> 4                                                                               Year #> 5                                                                             Decade #> 6                                                                             Region #> 7                                                                       Income Level #> 8                                                            Is OECD Member Country? #> 9                                                 GDP per capita (constant 2010 US$) #> 10                                           Life expectancy at birth, total (years) #> 11                                                  Gini index (World Bank estimate) #> 12 Net official development assistance and official aid received (constant 2018 US$) #> 13                                                                 Population, total namlab(wlddev, class = TRUE, N = TRUE, Ndistinct = TRUE) #>    Variable     Class     N Ndist #> 1   country character 13176   216 #> 2     iso3c    factor 13176   216 #> 3      date      Date 13176    61 #> 4      year   integer 13176    61 #> 5    decade   integer 13176     7 #> 6    region    factor 13176     7 #> 7    income    factor 13176     4 #> 8      OECD   logical 13176     2 #> 9     PCGDP   numeric  9470  9470 #> 10   LIFEEX   numeric 11670 10548 #> 11     GINI   numeric  1744   368 #> 12      ODA   numeric  8608  7832 #> 13      POP   numeric 12919 12877 #>                                                                                Label #> 1                                                                       Country Name #> 2                                                                       Country Code #> 3                                                         Date Recorded (Fictitious) #> 4                                                                               Year #> 5                                                                             Decade #> 6                                                                             Region #> 7                                                                       Income Level #> 8                                                            Is OECD Member Country? #> 9                                                 GDP per capita (constant 2010 US$) #> 10                                           Life expectancy at birth, total (years) #> 11                                                  Gini index (World Bank estimate) #> 12 Net official development assistance and official aid received (constant 2018 US$) #> 13                                                                 Population, total vlabels(wlddev) #>                                                                             country  #>                                                                      \"Country Name\"  #>                                                                               iso3c  #>                                                                      \"Country Code\"  #>                                                                                date  #>                                                        \"Date Recorded (Fictitious)\"  #>                                                                                year  #>                                                                              \"Year\"  #>                                                                              decade  #>                                                                            \"Decade\"  #>                                                                              region  #>                                                                            \"Region\"  #>                                                                              income  #>                                                                      \"Income Level\"  #>                                                                                OECD  #>                                                           \"Is OECD Member Country?\"  #>                                                                               PCGDP  #>                                                \"GDP per capita (constant 2010 US$)\"  #>                                                                              LIFEEX  #>                                           \"Life expectancy at birth, total (years)\"  #>                                                                                GINI  #>                                                  \"Gini index (World Bank estimate)\"  #>                                                                                 ODA  #> \"Net official development assistance and official aid received (constant 2018 US$)\"  #>                                                                                 POP  #>                                                                 \"Population, total\"  vlabels(wlddev) <- vlabels(wlddev)  ## Stub-renaming log_mtc <- add_stub(log(mtcars), \"log.\") head(log_mtc) #>                    log.mpg  log.cyl log.disp   log.hp log.drat    log.wt #> Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 #> Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 #> Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 #> Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 #> Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 #> Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 #>                   log.qsec log.vs log.am log.gear  log.carb #> Mazda RX4         2.800933   -Inf      0 1.386294 1.3862944 #> Mazda RX4 Wag     2.834389   -Inf      0 1.386294 1.3862944 #> Datsun 710        2.923699      0      0 1.386294 0.0000000 #> Hornet 4 Drive    2.967333      0   -Inf 1.098612 0.0000000 #> Hornet Sportabout 2.834389   -Inf   -Inf 1.098612 0.6931472 #> Valiant           3.006672      0   -Inf 1.098612 0.0000000 head(rm_stub(log_mtc, \"log.\")) #>                        mpg      cyl     disp       hp     drat        wt #> Mazda RX4         3.044522 1.791759 5.075174 4.700480 1.360977 0.9631743 #> Mazda RX4 Wag     3.044522 1.791759 5.075174 4.700480 1.360977 1.0560527 #> Datsun 710        3.126761 1.386294 4.682131 4.532599 1.348073 0.8415672 #> Hornet 4 Drive    3.063391 1.791759 5.552960 4.700480 1.124930 1.1678274 #> Hornet Sportabout 2.928524 2.079442 5.886104 5.164786 1.147402 1.2354715 #> Valiant           2.895912 1.791759 5.416100 4.653960 1.015231 1.2412686 #>                       qsec   vs   am     gear      carb #> Mazda RX4         2.800933 -Inf    0 1.386294 1.3862944 #> Mazda RX4 Wag     2.834389 -Inf    0 1.386294 1.3862944 #> Datsun 710        2.923699    0    0 1.386294 0.0000000 #> Hornet 4 Drive    2.967333    0 -Inf 1.098612 0.0000000 #> Hornet Sportabout 2.834389 -Inf -Inf 1.098612 0.6931472 #> Valiant           3.006672    0 -Inf 1.098612 0.0000000 rm(log_mtc)  ## Setting dimension names of an object head(setRownames(mtcars)) #>    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 ar <- array(1:9, c(3,3,3)) setRownames(ar) #> , , 1 #>  #>   [,1] [,2] [,3] #> 1    1    4    7 #> 2    2    5    8 #> 3    3    6    9 #>  #> , , 2 #>  #>   [,1] [,2] [,3] #> 1    1    4    7 #> 2    2    5    8 #> 3    3    6    9 #>  #> , , 3 #>  #>   [,1] [,2] [,3] #> 1    1    4    7 #> 2    2    5    8 #> 3    3    6    9 #>  setColnames(ar, c(\"a\",\"b\",\"c\")) #> , , 1 #>  #>      a b c #> [1,] 1 4 7 #> [2,] 2 5 8 #> [3,] 3 6 9 #>  #> , , 2 #>  #>      a b c #> [1,] 1 4 7 #> [2,] 2 5 8 #> [3,] 3 6 9 #>  #> , , 3 #>  #>      a b c #> [1,] 1 4 7 #> [2,] 2 5 8 #> [3,] 3 6 9 #>  setDimnames(ar, c(\"a\",\"b\",\"c\"), which = 3) #> , , a #>  #>      [,1] [,2] [,3] #> [1,]    1    4    7 #> [2,]    2    5    8 #> [3,]    3    6    9 #>  #> , , b #>  #>      [,1] [,2] [,3] #> [1,]    1    4    7 #> [2,]    2    5    8 #> [3,]    3    6    9 #>  #> , , c #>  #>      [,1] [,2] [,3] #> [1,]    1    4    7 #> [2,]    2    5    8 #> [3,]    3    6    9 #>  setDimnames(ar, list(c(\"d\",\"e\",\"f\"), c(\"a\",\"b\",\"c\")), which = 2:3) #> , , a #>  #>      d e f #> [1,] 1 4 7 #> [2,] 2 5 8 #> [3,] 3 6 9 #>  #> , , b #>  #>      d e f #> [1,] 1 4 7 #> [2,] 2 5 8 #> [3,] 3 6 9 #>  #> , , c #>  #>      d e f #> [1,] 1 4 7 #> [2,] 2 5 8 #> [3,] 3 6 9 #>  setDimnames(ar, list(c(\"g\",\"h\",\"i\"), c(\"d\",\"e\",\"f\"), c(\"a\",\"b\",\"c\"))) #> , , a #>  #>   d e f #> g 1 4 7 #> h 2 5 8 #> i 3 6 9 #>  #> , , b #>  #>   d e f #> g 1 4 7 #> h 2 5 8 #> i 3 6 9 #>  #> , , c #>  #>   d e f #> g 1 4 7 #> h 2 5 8 #> i 3 6 9 #>   ## Checking exact equality of multiple objects all_identical(iris, iris, iris, iris) #> [1] TRUE l <- replicate(100, fmean(num_vars(iris), iris$Species), simplify = FALSE) all_identical(l) #> [1] TRUE rm(l)  ## Function names from expressions ex = quote(sum(x) + mean(y) / z) all.names(ex) #> [1] \"+\"    \"sum\"  \"x\"    \"/\"    \"mean\" \"y\"    \"z\"    all.vars(ex) #> [1] \"x\" \"y\" \"z\" all_funs(ex) #> [1] \"+\"    \"sum\"  \"/\"    \"mean\" rm(ex)"},{"path":"https://sebkrantz.github.io/collapse/reference/summary-statistics.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Statistics — summary-statistics","title":"Summary Statistics — summary-statistics","text":"collapse provides following functions efficiently summarize examine data: qsu, shorthand quick-summary, extremely fast summary command inspired (xt)summarize command STATA statistical software. computes set 7 statistics (nobs, mean, sd, min, max, skewness kurtosis) using numerically stable one-pass method. Statistics can computed weighted, groups, also within-entities (multilevel / panel data). qtab, shorthand quick-table, faster versatile alternative table. Notably, also supports tabulations frequency weights, well computing statistic combinations variables. 'qtab's inherit 'table' class, allowing seamless application 'table' methods. descr computes concise detailed description data frame, including (sorted) frequency tables categorical variables various statistics quantiles numeric variables. inspired Hmisc::describe, 10x faster. pwcor, pwcov pwnobs compute (weighted) pairwise correlations, covariances observation counts matrices data frames. Pairwise correlations covariances can computed together observation counts p-values. elaborate print method displays statistics single correlation table. varying efficiently checks presence variation data (optionally) within groups (panel-identifiers). variable variant least 2 distinct non-missing data points.","code":""},{"path":[]},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/t_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Efficient List Transpose — t_list","title":"Efficient List Transpose — t_list","text":"t_list turns list lists inside-. performance quite efficient regardless size list.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/t_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Efficient List Transpose — t_list","text":"","code":"t_list(l)"},{"path":"https://sebkrantz.github.io/collapse/reference/t_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Efficient List Transpose — t_list","text":"l list lists. Elements inside sublists can heterogeneous, including lists.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/t_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Efficient List Transpose — t_list","text":"l transposed second layer list becomes top layer top layer second layer. See Examples.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/t_list.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Efficient List Transpose — t_list","text":"transpose data frame / list atomic vectors see data.table::transpose().","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/t_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Efficient List Transpose — t_list","text":"","code":"# Homogenous list of lists l <- list(a = list(c = 1, d = 2), b = list(c = 3, d = 4)) str(l) #> List of 2 #>  $ a:List of 2 #>   ..$ c: num 1 #>   ..$ d: num 2 #>  $ b:List of 2 #>   ..$ c: num 3 #>   ..$ d: num 4 str(t_list(l)) #> List of 2 #>  $ c:List of 2 #>   ..$ a: num 1 #>   ..$ b: num 3 #>  $ d:List of 2 #>   ..$ a: num 2 #>   ..$ b: num 4  # Heterogenous case l2 <- list(a = list(c = 1, d = letters), b = list(c = 3:10, d = list(4, e = 5))) attr(l2, \"bla\") <- \"abc\"  # Attributes other than names are preserved str(l2) #> List of 2 #>  $ a:List of 2 #>   ..$ c: num 1 #>   ..$ d: chr [1:26] \"a\" \"b\" \"c\" \"d\" ... #>  $ b:List of 2 #>   ..$ c: int [1:8] 3 4 5 6 7 8 9 10 #>   ..$ d:List of 2 #>   .. ..$  : num 4 #>   .. ..$ e: num 5 #>  - attr(*, \"bla\")= chr \"abc\" str(t_list(l2)) #> List of 2 #>  $ c:List of 2 #>   ..$ a: num 1 #>   ..$ b: int [1:8] 3 4 5 6 7 8 9 10 #>  $ d:List of 2 #>   ..$ a: chr [1:26] \"a\" \"b\" \"c\" \"d\" ... #>   ..$ b:List of 2 #>   .. ..$  : num 4 #>   .. ..$ e: num 5 #>  - attr(*, \"bla\")= chr \"abc\"  rm(l, l2)"},{"path":"https://sebkrantz.github.io/collapse/reference/time-series-panel-series.html","id":null,"dir":"Reference","previous_headings":"","what":"Time Series and Panel Series — time-series-panel-series","title":"Time Series and Panel Series — time-series-panel-series","text":"collapse provides flexible powerful set functions classes work time-dependent data: findex_by/iby creates 'indexed_frame': flexible structure can imposed upon data-frame like object facilitates indexed (time-aware) computations time series panel data. Indexed frames composed 'indexed_series', can also created vector matrix-based objects using reindex function. functions findex/ix, unindex, is_irregular to_plm help operate classes, check irregularity, ensure plm compatibility. Methods defined various time series, data transformation data manipulation functions collapse. timeid efficiently converts numeric time sequences, 'Date' 'POSIXct' vectors, time-factor / integer id, unit-step represents greatest common divisor underlying sequence. flag, lag- lead- operators L F S3 generics efficiently compute sequences lags leads regular irregular / unbalanced time series panel data. Similarly, fdiff, fgrowth, operators D, Dlog G S3 generics efficiently compute sequences suitably lagged / leaded iterated differences, log-differences growth rates. fdiff/D/Dlog can also compute quasi-differences form \\(x_t - \\rho x_{t-1}\\). fcumsum S3 generic efficiently compute cumulative sums time series panel data. contrast cumsum, can handle missing values supports grouped indexed / ordered computations. psmat S3 generic efficiently convert panel-vectors / 'indexed_series' data frames / 'indexed_frame's panel series matrices 3D arrays, respectively (time, individuals variables receive different dimensions, allowing fast indexation, visualization, computations). psacf, pspacf psccf S3 generics compute estimates auto-, partial auto- cross- correlation covariance functions panel-vectors / 'indexed_series', multivariate versions data frames / 'indexed_frame's.","code":""},{"path":[]},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/timeid.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Integer-Id From Time/Date Sequences — timeid","title":"Generate Integer-Id From Time/Date Sequences — timeid","text":"timeid groups time vectors way preserves temporal structure. generate integer id unit steps represent greatest common divisor original sequence e.g c(4, 6, 10) -> c(1, 2, 4) c(0.25, 0.75, 1) -> c(1, 3, 4).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/timeid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Integer-Id From Time/Date Sequences — timeid","text":"","code":"timeid(x, factor = FALSE, ordered = factor, extra = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/timeid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Integer-Id From Time/Date Sequences — timeid","text":"x numeric time object Date, POSIXct integer double vector representing time. factor logical. TRUE returns (ordered) factor levels corresponding full sequence (without irregular gaps) time. useful inclusion index might computationally expensive long sequences, see Details. FALSE returns simpler object class 'qG'. ordered logical. TRUE adds class 'ordered'. extra logical. TRUE attaches set 4 diagnostic items attributes result: \"unique_ints\": unique(unattrib(timeid(x))) - unique integer time steps first-appearance order. can useful check size gaps sequence. \"sort_unique_x\": sort(unique(x)). \"range_x\": range(x). \"step_x\": vgcd(sort(unique(diff(sort(unique(x)))))) - greatest common divisor. Note returning attributes incur additional computations.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/timeid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Integer-Id From Time/Date Sequences — timeid","text":"Let range_x step_x like-named attributes returned extra = TRUE, , factor = TRUE, complete sequence levels generated seq(range_x[1], range_x[2], = step_x) |> copyMostAttrib(x) |> .character(). factor = FALSE, number timesteps recorded \"N.groups\" attribute computed (range_x[2]-range_x[1])/step_x + 1, equal number factor levels. cases underlying integer id preserves gaps time. Large gaps (strong irregularity) can result many unused factor levels, generation can become expensive. Using factor = FALSE (default) thus efficient.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/timeid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Integer-Id From Time/Date Sequences — timeid","text":"factor 'qG' object, optionally additional attributes attached.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/timeid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Integer-Id From Time/Date Sequences — timeid","text":"","code":"oldopts <- options(max.print = 30)  # A normal use case timeid(wlddev$decade) #>  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13146 entries ] #> attr(,\"N.groups\") #> [1] 7 #> attr(,\"class\") #> [1] \"qG\"          \"na.included\" timeid(wlddev$decade, factor = TRUE) #>  [1] 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 1970 1970 1970 1970 1970 #> [16] 1970 1970 1970 1970 1970 1980 1980 1980 1980 1980 1980 1980 1980 1980 1980 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13146 entries ] #> Levels: 1960 < 1970 < 1980 < 1990 < 2000 < 2010 < 2020 timeid(wlddev$decade, extra = TRUE) #>  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13146 entries ] #> attr(,\"N.groups\") #> [1] 7 #> attr(,\"class\") #> [1] \"qG\"          \"na.included\" #> attr(,\"unique_ints\") #> [1] 1 2 3 4 5 6 7 #> attr(,\"sort_unique_x\") #> [1] 1960 1970 1980 1990 2000 2010 2020 #> attr(,\"sort_unique_x\")attr(,\"label\") #> [1] \"Decade\" #> attr(,\"range_x\") #> [1] 1960 2020 #> attr(,\"range_x\")attr(,\"label\") #> [1] \"Decade\" #> attr(,\"step_x\") #> [1] 10  # Here a large number of levels is generated, which is expensive timeid(wlddev$date, factor = TRUE) #>  [1] 1961-01-01 1962-01-01 1963-01-01 1964-01-01 1965-01-01 1966-01-01 #>  [7] 1967-01-01 1968-01-01 1969-01-01 1970-01-01 1971-01-01 1972-01-01 #> [13] 1973-01-01 1974-01-01 1975-01-01 1976-01-01 1977-01-01 1978-01-01 #> [19] 1979-01-01 1980-01-01 1981-01-01 1982-01-01 1983-01-01 1984-01-01 #> [25] 1985-01-01 1986-01-01 1987-01-01 1988-01-01 1989-01-01 1990-01-01 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13146 entries ] #> 21916 Levels: 1961-01-01 < 1961-01-02 < 1961-01-03 < 1961-01-04 < ... < 2021-01-01 tid <- timeid(wlddev$date, extra = TRUE) # Much faster str(tid) #>  'qG' int [1:13176] 1 366 731 1096 1462 1827 2192 2557 2923 3288 ... #>  - attr(*, \"N.groups\")= int 21916 #>  - attr(*, \"unique_ints\")= int [1:61] 1 366 731 1096 1462 1827 2192 2557 2923 3288 ... #>  - attr(*, \"sort_unique_x\")= Date[1:61], format: \"1961-01-01\" \"1962-01-01\" ... #>  - attr(*, \"range_x\")= Date[1:2], format: \"1961-01-01\" \"2021-01-01\" #>  - attr(*, \"step_x\")= num 1  # The reason for step = 1 are leap years with 366 days every 4 years diff(attr(tid, \"unique\")) #>  [1] 365 365 365 366 365 365 365 366 365 365 365 366 365 365 365 366 365 365 365 #> [20] 366 365 365 365 366 365 365 365 366 365 365 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 30 entries ]  # So in this case simple factor generation gives a better result qF(wlddev$date, ordered = TRUE, na.exclude = FALSE) #>  [1] 1961-01-01 1962-01-01 1963-01-01 1964-01-01 1965-01-01 1966-01-01 #>  [7] 1967-01-01 1968-01-01 1969-01-01 1970-01-01 1971-01-01 1972-01-01 #> [13] 1973-01-01 1974-01-01 1975-01-01 1976-01-01 1977-01-01 1978-01-01 #> [19] 1979-01-01 1980-01-01 1981-01-01 1982-01-01 1983-01-01 1984-01-01 #> [25] 1985-01-01 1986-01-01 1987-01-01 1988-01-01 1989-01-01 1990-01-01 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13146 entries ] #> attr(,\"label\") #> [1] Date Recorded (Fictitious) #> 61 Levels: 1961-01-01 < 1962-01-01 < 1963-01-01 < 1964-01-01 < ... < 2021-01-01  # The best way to deal with this data would be to convert it # to zoo::yearmon and then use timeid: timeid(zoo::as.yearmon(wlddev$date), factor = TRUE, extra = TRUE) #>  [1] Jan 1961 Jan 1962 Jan 1963 Jan 1964 Jan 1965 Jan 1966 Jan 1967 Jan 1968 #>  [9] Jan 1969 Jan 1970 Jan 1971 Jan 1972 Jan 1973 Jan 1974 Jan 1975 Jan 1976 #> [17] Jan 1977 Jan 1978 Jan 1979 Jan 1980 Jan 1981 Jan 1982 Jan 1983 Jan 1984 #> [25] Jan 1985 Jan 1986 Jan 1987 Jan 1988 Jan 1989 Jan 1990 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13146 entries ] #> attr(,\"unique_ints\") #>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #> [26] 26 27 28 29 30 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 31 entries ] #> attr(,\"sort_unique_x\") #>  [1] Jan 1961 Jan 1962 Jan 1963 Jan 1964 Jan 1965 Jan 1966 Jan 1967 Jan 1968 #>  [9] Jan 1969 Jan 1970 Jan 1971 Jan 1972 Jan 1973 Jan 1974 Jan 1975 Jan 1976 #> [17] Jan 1977 Jan 1978 Jan 1979 Jan 1980 Jan 1981 Jan 1982 Jan 1983 Jan 1984 #> [25] Jan 1985 Jan 1986 Jan 1987 Jan 1988 Jan 1989 Jan 1990 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 31 entries ] #> attr(,\"range_x\") #> [1] Jan 1961 Jan 2021 #> attr(,\"step_x\") #> [1] 1 #> 61 Levels: Jan 1961 < Jan 1962 < Jan 1963 < Jan 1964 < ... < Jan 2021  options(oldopts) rm(oldopts, tid)"},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"unlist2d efficiently unlists lists regular R objects (objects built atomic elements) creates data frame representation list recursive flattening intelligent row-binding operations. full 2-dimensional generalization unlist, best understood recursive generalization .call(rbind, ...). powerful tool create tidy data frame representation (nested) lists vectors, data frames, matrices, arrays heterogeneous objects. simple row-wise combining lists/data.frame's use non-recursive rowbind function.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"","code":"unlist2d(l, idcols = \".id\", row.names = FALSE, recursive = TRUE,          id.factor = FALSE, DT = FALSE)"},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"l unlistable list (atomic elements final nodes, see is_unlistable). idcols character stub vector names id-columns automatically added - one level nesting l. default stub \".id\", columns form \".id.1\", \".id.2\", etc... . idcols = TRUE, stub also set \".id\". idcols = FALSE, id-columns omitted. content id columns list names, (missing) integers list elements. Missing elements asymmetric nested structures filled NA. See Examples. row.names TRUE extracts row names objects l (available) adds output column named \"row.names\". Alternatively, column name .e. row.names = \"variable\" can supplied. plain matrices l, integer row names generated. recursive logical. FALSE, process lowest (deepest) level l. See Details. id.factor TRUE !isFALSE(idcols), create id columns factors instead character integer vectors. Alternatively possible specify id.factor = \"ordered\" generate ordered factor id's. strongly recommended binding lists larger data frames, factors much memory efficient character vectors also speed subsequent grouping operations columns. DT logical. TRUE returns data.table, data.frame.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"data frame representation created unlist2d built follows: Recurse lowest level list-tree, data frames exempted treated final (atomic) elements. Identify objects, vectors, matrices arrays convert data frame (case atomic vectors element becomes column). Row-bind data frames using data.table's rbindlist function. Columns matched name. number columns differ, fill empty spaces NA's. !isFALSE(idcols), create id-columns left, filled object names indices ((sub-)list unnamed). !isFALSE(row.names), store rownames objects (available) separate column. Move next higher level list-tree repeat: Convert atomic objects data frame row-bind matching columns filling unmatched ones NA's. Create another id-column level nesting passed . list-tree asymmetric, fill empty spaces lower-level id columns NA's. result iterative procedure single data frame containing left side id-columns level nesting (higher lower level), followed column containing rownames objects (!isFALSE(row.names)), followed data columns, matched level recursion. Optimal results obtained symmetric lists arrays, matrices data frames, unlist2d efficiently binds beautiful data frame ready plotting analysis. See examples .","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"data frame (DT = TRUE) data.table.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"lists data frames unlist2d works just like data.table::rbindlist(l, use.names = TRUE, fill = TRUE, idcol = \".id\") however lists lists unlist2d produce output data.table::rbindlist unlist2d recursive function. can use rowbind faithful alternative data.table::rbindlist. function rrapply::rrapply(l, = \"melt\"|\"bind\") fast alternative (written fully C) nested lists atomic elements.","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/unlist2d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recursive Row-Binding / Unlisting in 2D - to Data Frame — unlist2d","text":"","code":"## Basic Examples: l <- list(mtcars, list(mtcars, mtcars)) tail(unlist2d(l)) #>    .id.1 .id.2  mpg cyl  disp  hp drat    wt qsec vs am gear carb #> 91     2     2 26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2 #> 92     2     2 30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2 #> 93     2     2 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4 #> 94     2     2 19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6 #> 95     2     2 15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] unlist2d(rapply2d(l, fmean)) #>   .id.1 .id.2      mpg    cyl     disp       hp     drat      wt     qsec #> 1     1    NA 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #> 2     2     1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #> 3     2     2 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #>       vs      am   gear   carb #> 1 0.4375 0.40625 3.6875 2.8125 #> 2 0.4375 0.40625 3.6875 2.8125 #> 3 0.4375 0.40625 3.6875 2.8125 l = list(a = qM(mtcars[1:8]),          b = list(c = mtcars[4:11], d = list(e = mtcars[2:10], f = mtcars))) tail(unlist2d(l, row.names = TRUE)) #>     .id.1 .id.2 .id.3      row.names  mpg cyl  disp  hp drat    wt qsec vs am #> 123     b     d     f  Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.7  0  1 #> 124     b     d     f   Lotus Europa 30.4   4  95.1 113 3.77 1.513 16.9  1  1 #> 125     b     d     f Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1 #> 126     b     d     f   Ferrari Dino 19.7   6 145.0 175 3.62 2.770 15.5  0  1 #>     gear carb #> 123    5    2 #> 124    5    2 #> 125    5    4 #> 126    5    6 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] unlist2d(rapply2d(l, fmean)) #>   .id.1 .id.2 .id.3      mpg    cyl     disp       hp     drat      wt     qsec #> 1     a  <NA>  <NA> 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #> 2     b     c  <NA>       NA     NA       NA 146.6875 3.596563 3.21725 17.84875 #> 3     b     d     e       NA 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #> 4     b     d     f 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 #>       vs      am   gear   carb #> 1 0.4375      NA     NA     NA #> 2 0.4375 0.40625 3.6875 2.8125 #> 3 0.4375 0.40625 3.6875     NA #> 4 0.4375 0.40625 3.6875 2.8125 unlist2d(rapply2d(l, fmean), recursive = FALSE) #> $a #>        mpg        cyl       disp         hp       drat         wt       qsec  #>  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750  #>         vs  #>   0.437500  #>  #> $b #> $b$c #>         hp       drat         wt       qsec         vs         am       gear  #> 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250   3.687500  #>       carb  #>   2.812500  #>  #> $b$d #>   .id    cyl     disp       hp     drat      wt     qsec     vs      am   gear #> 1   e 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 #> 2   f 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 #>        mpg   carb #> 1       NA     NA #> 2 20.09062 2.8125 #>  #>   ## Groningen Growth and Development Center 10-Sector Database head(GGDC10S) # See ?GGDC10S #>   Country Regioncode             Region Variable Year AGR MIN MAN PU CON WRT #> 1     BWA        SSA Sub-saharan Africa       VA 1960  NA  NA  NA NA  NA  NA #> 2     BWA        SSA Sub-saharan Africa       VA 1961  NA  NA  NA NA  NA  NA #> 3     BWA        SSA Sub-saharan Africa       VA 1962  NA  NA  NA NA  NA  NA #> 4     BWA        SSA Sub-saharan Africa       VA 1963  NA  NA  NA NA  NA  NA #>   TRA FIRE GOV OTH SUM #> 1  NA   NA  NA  NA  NA #> 2  NA   NA  NA  NA  NA #> 3  NA   NA  NA  NA  NA #> 4  NA   NA  NA  NA  NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ] namlab(GGDC10S, class = TRUE) #>      Variable     Class                                                 Label #> 1     Country character                                               Country #> 2  Regioncode character                                           Region code #> 3      Region character                                                Region #> 4    Variable character                                              Variable #> 5        Year   numeric                                                  Year #> 6         AGR   numeric                                          Agriculture  #> 7         MIN   numeric                                                Mining #> 8         MAN   numeric                                         Manufacturing #> 9          PU   numeric                                             Utilities #> 10        CON   numeric                                          Construction #> 11        WRT   numeric                         Trade, restaurants and hotels #> 12        TRA   numeric                  Transport, storage and communication #> 13       FIRE   numeric Finance, insurance, real estate and business services #> 14        GOV   numeric                                   Government services #> 15        OTH   numeric               Community, social and personal services #> 16        SUM   numeric                               Summation of sector GDP  # Panel-Summarize this data by Variable (Emloyment and Value Added) l <- qsu(GGDC10S, by = ~ Variable,             # Output as list (instead of 4D array)          pid = ~ Variable + Country,          cols = 6:16, array = FALSE) str(l, give.attr = FALSE)                      # A list of 2-levels with matrices of statistics #> List of 11 #>  $ AGR :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2225 2139 16746 5137561 55645 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 16746 5137561 54119 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.30e+01 4.97e+01 2.53e+06 2.53e+06 1.29e+04 ... #>  $ MIN :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2216 2139 360 3802687 1295 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 360 3802687 1155 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.28e+01 4.97e+01 1.87e+06 1.87e+06 5.86e+02 ... #>  $ MAN :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2216 2139 5204 11270966 13925 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 5204 11270966 11862 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.28e+01 4.97e+01 5.54e+06 5.54e+06 7.29e+03 ... #>  $ PU  :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2215 2139 153 683127 365 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 153 683127 294 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 52.7 49.7 335679.5 335679.5 216.3 ... #>  $ CON :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2216 2139 1794 3666191 5114 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 1794 3666191 3712 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.28e+01 4.97e+01 1.80e+06 1.80e+06 3.52e+03 ... #>  $ WRT :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2216 2139 4368 6903432 8617 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 4368 6903432 6929 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.28e+01 4.97e+01 3.39e+06 3.39e+06 5.12e+03 ... #>  $ TRA :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2216 2139 1442 2998080 3289 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 1442 2998080 2738 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.28e+01 4.97e+01 1.47e+06 1.47e+06 1.82e+03 ... #>  $ FIRE:List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2216 2139 1331 3372504 3114 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 1331 3372504 2598 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.28e+01 4.97e+01 1.66e+06 1.66e+06 1.72e+03 ... #>  $ GOV :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 1780 1702 4197 3498683 7278 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 34 35 4197 3498683 6577 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.24e+01 4.86e+01 1.71e+06 1.71e+06 3.12e+03 ... #>  $ OTH :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2109 2139 2268 3343192 8022 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 40 43 2268 3343192 5268 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.27e+01 4.97e+01 1.68e+06 1.68e+06 6.05e+03 ... #>  $ SUM :List of 3 #>   ..$ Overall: 'qsu' num [1:2, 1:5] 2225 2139 36847 43961639 96319 ... #>   ..$ Between: 'qsu' num [1:2, 1:5] 42 43 36847 43961639 89206 ... #>   ..$ Within : 'qsu' num [1:2, 1:5] 5.30e+01 4.97e+01 2.16e+07 2.16e+07 3.63e+04 ... head(unlist2d(l))                              # Default output, missing the variables (row-names) #>   .id.1   .id.2          N       Mean          SD           Min          Max #> 1   AGR Overall 2225.00000   16746.43    55644.84  5.240734e+00     390980.0 #> 2   AGR Overall 2139.00000 5137560.88 52913681.79  5.887857e-07 1191877784.8 #> 3   AGR Between   42.00000   16746.43    54118.72  1.357596e+01     287744.2 #> 4   AGR Between   43.00000 5137560.88 27760188.51  1.674095e+02  189627688.7 #> 5   AGR  Within   52.97619 2526696.50    12942.66  2.394221e+06    2629932.3 #> 6   AGR  Within   49.74419 2526696.50 45046971.64 -1.869215e+08 1004776792.6 head(unlist2d(l, row.names = TRUE))            # Here we go, but this is still not very nice #>   .id.1   .id.2 row.names          N       Mean          SD           Min #> 1   AGR Overall       EMP 2225.00000   16746.43    55644.84  5.240734e+00 #> 2   AGR Overall        VA 2139.00000 5137560.88 52913681.79  5.887857e-07 #> 3   AGR Between       EMP   42.00000   16746.43    54118.72  1.357596e+01 #> 4   AGR Between        VA   43.00000 5137560.88 27760188.51  1.674095e+02 #> 5   AGR  Within       EMP   52.97619 2526696.50    12942.66  2.394221e+06 #> 6   AGR  Within        VA   49.74419 2526696.50 45046971.64 -1.869215e+08 #>            Max #> 1     390980.0 #> 2 1191877784.8 #> 3     287744.2 #> 4  189627688.7 #> 5    2629932.3 #> 6 1004776792.6 head(unlist2d(l, idcols = c(\"Sector\",\"Trans\"), # Now this is looking pretty good               row.names = \"Variable\")) #>   Sector   Trans Variable          N       Mean          SD           Min #> 1    AGR Overall      EMP 2225.00000   16746.43    55644.84  5.240734e+00 #> 2    AGR Overall       VA 2139.00000 5137560.88 52913681.79  5.887857e-07 #> 3    AGR Between      EMP   42.00000   16746.43    54118.72  1.357596e+01 #> 4    AGR Between       VA   43.00000 5137560.88 27760188.51  1.674095e+02 #> 5    AGR  Within      EMP   52.97619 2526696.50    12942.66  2.394221e+06 #> 6    AGR  Within       VA   49.74419 2526696.50 45046971.64 -1.869215e+08 #>            Max #> 1     390980.0 #> 2 1191877784.8 #> 3     287744.2 #> 4  189627688.7 #> 5    2629932.3 #> 6 1004776792.6  dat <- unlist2d(l, c(\"Sector\",\"Trans\"),        # Id-columns can also be generated as factors                 \"Variable\", id.factor = TRUE) str(dat) #> 'data.frame':\t66 obs. of  8 variables: #>  $ Sector  : Factor w/ 11 levels \"AGR\",\"MIN\",\"MAN\",..: 1 1 1 1 1 1 2 2 2 2 ... #>  $ Trans   : Factor w/ 3 levels \"Overall\",\"Between\",..: 1 1 2 2 3 3 1 1 2 2 ... #>  $ Variable: chr  \"EMP\" \"VA\" \"EMP\" \"VA\" ... #>  $ N       : num  2225 2139 42 43 53 ... #>  $ Mean    : num  16746 5137561 16746 5137561 2526697 ... #>  $ SD      : num  55645 52913682 54119 27760189 12943 ... #>  $ Min     : num  5.24 5.89e-07 1.36e+01 1.67e+02 2.39e+06 ... #>  $ Max     : num  3.91e+05 1.19e+09 2.88e+05 1.90e+08 2.63e+06 ...  # Split this sectoral data, first by Variable (Emloyment and Value Added), then by Country sdat <- rsplit(GGDC10S, ~ Variable + Country, cols = 6:16)  # Compute pairwise correlations between sectors and recombine: dat <- unlist2d(rapply2d(sdat, pwcor),                 idcols = c(\"Variable\",\"Country\"),                 row.names = \"Sector\") head(dat) #>   Variable Country Sector         AGR        MIN         MAN         PU #> 1      EMP     ARG    AGR  1.00000000 -0.5432238 -0.06195285 -0.6039527 #> 2      EMP     ARG    MIN -0.54322382  1.0000000  0.27420132  0.5591395 #> 3      EMP     ARG    MAN -0.06195285  0.2742013  1.00000000  0.4387383 #> 4      EMP     ARG     PU -0.60395268  0.5591395  0.43873834  1.0000000 #> 5      EMP     ARG    CON -0.85244262  0.7670132  0.32534168  0.6119454 #>          CON         WRT         TRA        FIRE         GOV         OTH #> 1 -0.8524426 -0.88582197 -0.65710602 -0.77508301 -0.85797631 -0.86895334 #> 2  0.7670132  0.75872350  0.77160809  0.81346988  0.75874992  0.73533837 #> 3  0.3253417  0.05909901 -0.07476592 -0.08183532 -0.05793235 -0.03827729 #> 4  0.6119454  0.57784570  0.57586671  0.50156150  0.49865348  0.52565945 #> 5  1.0000000  0.87105470  0.65409838  0.79003514  0.81912978  0.82502726 #>           SUM #> 1 -0.84715229 #> 2  0.81170356 #> 3  0.05960032 #> 4  0.56998662 #> 5  0.86608973 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ] plot(hclust(as.dist(1-pwcor(dat[-(1:3)]))))    # Using corrs. as distance metric to cluster sectors   # List of panel-series matrices psml <- psmat(fsubset(GGDC10S, Variable == \"VA\"), ~Country, ~Year, cols = 6:16, array = FALSE)  # Recombining with unlist2d() (effectively like reshapig the data) head(unlist2d(psml, idcols = \"Sector\", row.names = \"Country\")) #>   Sector Country 1947 1948 1949         1950         1951         1952 #> 1    AGR     ARG   NA   NA   NA 5.887857e-07 9.165327e-07 9.964412e-07 #>           1953         1954         1955         1956         1957         1958 #> 1 1.482589e-06 1.396692e-06 1.574796e-06 2.001253e-06 2.644127e-06 3.741081e-06 #>           1959         1960         1961         1962         1963         1964 #> 1 8.395798e-06 1.028314e-05 9.868339e-06 1.342763e-05 1.902081e-05 2.947121e-05 #>           1965         1966         1967         1968         1969        1970 #> 1 3.549257e-05 3.856346e-05 4.832476e-05 5.232561e-05 6.292321e-05 7.61702e-05 #>           1971         1972         1973         1974        1975        1976 #> 1 0.0001145865 0.0002086401 0.0004172802 0.0004868269 0.001043201 0.005841923 #>         1977       1978      1979      1980      1981     1982     1983 #> 1 0.01613484 0.03672066 0.1052937 0.1730322 0.4325805 1.816838 8.219029 #>       1984     1985     1986     1987    1988     1989     1990     1991 #> 1 57.10062 350.3902 673.7874 1633.943 8626.52 269.8437 4843.777 10511.53 #>       1992     1993     1994     1995     1996     1997     1998     1999 #> 1 11757.53 12148.85 13084.82 13808.48 15269.96 15293.02 15702.92 12638.96 #>       2000     2001     2002     2003     2004     2005     2006     2007 #> 1 13300.48 12275.65 31903.94 38824.56 43148.47 46330.59 50759.94 70101.51 #>      2008     2009     2010     2011 2012 2013 #> 1 93178.7 79362.85 132365.8 178745.3   NA   NA #>  [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]  rm(l, dat, sdat, psml)"},{"path":"https://sebkrantz.github.io/collapse/reference/varying.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Check of Variation in Data — varying","title":"Fast Check of Variation in Data — varying","text":"varying generic function (column-wise) checks variation values x, (optionally) within groups g (e.g. panel-identifier).","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/varying.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Check of Variation in Data — varying","text":"","code":"varying(x, ...)  # Default S3 method varying(x, g = NULL, any_group = TRUE, use.g.names = TRUE, ...)  # S3 method for class 'matrix' varying(x, g = NULL, any_group = TRUE, use.g.names = TRUE, drop = TRUE, ...)  # S3 method for class 'data.frame' varying(x, by = NULL, cols = NULL, any_group = TRUE, use.g.names = TRUE, drop = TRUE, ...)  # Methods for indexed data / compatibility with plm:  # S3 method for class 'pseries' varying(x, effect = 1L, any_group = TRUE, use.g.names = TRUE, ...)  # S3 method for class 'pdata.frame' varying(x, effect = 1L, cols = NULL, any_group = TRUE, use.g.names = TRUE,         drop = TRUE, ...)  # Methods for grouped data frame / compatibility with dplyr:  # S3 method for class 'grouped_df' varying(x, any_group = TRUE, use.g.names = FALSE, drop = TRUE,         keep.group_vars = TRUE, ...)  # Methods for grouped data frame / compatibility with sf:  # S3 method for class 'sf' varying(x, by = NULL, cols = NULL, any_group = TRUE, use.g.names = TRUE, drop = TRUE, ...)"},{"path":"https://sebkrantz.github.io/collapse/reference/varying.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Check of Variation in Data — varying","text":"x vector, matrix, data frame, 'indexed_series' ('pseries'), 'indexed_frame' ('pdata.frame') grouped data frame ('grouped_df'). Data must numeric. g factor, GRP object, atomic vector (internally converted factor) list vectors / factors (internally converted GRP object) used group x. g, also allows one- two-sided formulas .e. ~ group1 + group2 var1 + var2 ~ group1 + group2. See Examples any_group logical. !.null(g), FALSE check report variation groups, whereas default TRUE checks variation within group. See Examples. cols select columns using column names, indices function (e.g. .numeric). Two-sided formulas passed overwrite cols. use.g.names logical. Make group-names add result names (default method) row-names (matrix data frame methods). row-names generated data.table's. drop matrix data.frame methods: Logical. TRUE drops dimensions returns atomic vector result 1-dimensional. effect plm methods: Select panel identifier variation data examined. 1L takes first variable index, 2L second etc.. Index variables can also called name. one index variable can supplied, interacted. keep.group_vars grouped_df method: Logical. FALSE removes grouping variables computation. ... arguments passed methods.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/varying.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Check of Variation in Data — varying","text":"Without groups passed g, varying simply checks variation columns x returns TRUE column case FALSE otherwise. set data points defined varying contains least 2 distinct non-missing values (non-0 standard deviation can computed numeric data). varying checks variation numeric non-numeric data. groups supplied g (alternatively grouped_df x), varying can operate one 2 modes: any_group = TRUE (default), varying checks column variation groups defined g, returns TRUE within-variation detected FALSE otherwise. Thus one logical value returned column computation column terminated soon variation within group found. any_group = FALSE, varying runs entire data checking group variation returns, column x, logical vector reporting variation check groups. group contains missing values, NA returned group. sf method simply ignores geometry column.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/varying.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Check of Variation in Data — varying","text":"logical vector (!.null(g) any_group = FALSE), matrix data frame logical vectors indicating whether data vary (dimension supplied g).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/varying.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Check of Variation in Data — varying","text":"","code":"## Checks overall variation in all columns varying(wlddev) #> country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX  #>    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE    TRUE  #>    GINI     ODA     POP  #>    TRUE    TRUE    TRUE   ## Checks whether data are time-variant i.e. vary within country varying(wlddev, ~ country) #>  iso3c   date   year decade region income   OECD  PCGDP LIFEEX   GINI    ODA  #>  FALSE   TRUE   TRUE   TRUE  FALSE  FALSE  FALSE   TRUE   TRUE   TRUE   TRUE  #>    POP  #>   TRUE   ## Same as above but done for each country individually, countries without data are coded NA head(varying(wlddev, ~ country, any_group = FALSE)) #>                iso3c date year decade region income  OECD PCGDP LIFEEX GINI #> Afghanistan    FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE   NA #> Albania        FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE TRUE #> Algeria        FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE   TRUE TRUE #> American Samoa FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE     NA   NA #> Andorra        FALSE TRUE TRUE   TRUE  FALSE  FALSE FALSE  TRUE     NA   NA #>                 ODA  POP #> Afghanistan    TRUE TRUE #> Albania        TRUE TRUE #> Algeria        TRUE TRUE #> American Samoa   NA TRUE #> Andorra          NA TRUE #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]"},{"path":"https://sebkrantz.github.io/collapse/reference/wlddev.html","id":null,"dir":"Reference","previous_headings":"","what":"World Development Dataset — wlddev","title":"World Development Dataset — wlddev","text":"dataset contains 5 indicators World Bank's World Development Indicators (WDI) database: (1) GDP per capita, (2) Life expectancy birth, (3) GINI index, (4) Net ODA official aid received (5) Population. panel data balanced covers 216 present historic countries 1960-2020 (World Bank aggregates regional entities excluded). Apart indicators data contains number identifiers (character country name, factor ISO3 country code, World Bank region income level, numeric year decade) 2 generated variables: logical variable indicating whether country OECD member, fictitious variable stating date data recorded. variables added common data-types represented dataset, making ideal test-dataset certain collapse functions.","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/wlddev.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"World Development Dataset — wlddev","text":"","code":"data(\"wlddev\")"},{"path":"https://sebkrantz.github.io/collapse/reference/wlddev.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"World Development Dataset — wlddev","text":"data frame 13176 observations following 13 variables. variables labeled e.g. 'label' attribute. country chr Country Name iso3c fct Country Code date date Date Recorded (Fictitious) year int Year decade int Decade region fct World Bank Region income fct World Bank Income Level OECD log OECD Member Country? PCGDP num GDP per capita (constant 2010 US$) LIFEEX num Life expectancy birth, total (years) GINI num GINI index (World Bank estimate) ODA num Net official development assistance official aid received (constant 2018 US$) POP num Population, total","code":""},{"path":"https://sebkrantz.github.io/collapse/reference/wlddev.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"World Development Dataset — wlddev","text":"https://data.worldbank.org/, accessed via WDI package. codes series c(\"NY.GDP.PCAP.KD\", \"SP.DYN.LE00.\", \"SI.POV.GINI\", \"DT.ODA.ALLD.KD\", \"SP.POP.TOTL\").","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/reference/wlddev.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"World Development Dataset — wlddev","text":"","code":"data(wlddev)  # Panel-summarizing the 5 series qsu(wlddev, pid = ~iso3c, cols = 9:13, vlabels = TRUE) #> , , PCGDP: GDP per capita (constant 2010 US$) #>  #>              N/T        Mean          SD          Min         Max #> Overall     9470   12048.778  19077.6416     132.0776  196061.417 #> Between      206  12962.6054  20189.9007     253.1886   141200.38 #> Within   45.9709   12048.778   6723.6808  -33504.8721  76767.5254 #>  #> , , LIFEEX: Life expectancy at birth, total (years) #>  #>              N/T     Mean       SD      Min      Max #> Overall    11670  64.2963  11.4764   18.907  85.4171 #> Between      207  64.9537   9.8936  40.9663  85.4171 #> Within   56.3768  64.2963   6.0842  32.9068  84.4198 #>  #> , , GINI: Gini index (World Bank estimate) #>  #>              N/T     Mean      SD      Min      Max #> Overall     1744  38.5341  9.2006     20.7     65.8 #> Between      167  39.4233  8.1356  24.8667  61.7143 #> Within   10.4431  38.5341  2.9277  25.3917  55.3591 #>  #> , , ODA: Net official development assistance and official aid received (constant 2018 US$) #>  #>              N/T        Mean          SD              Min             Max #> Overall     8608  454'720131  868'712654      -997'679993  2.56715605e+10 #> Between      178  439'168412  569'049959       468717.916  3.62337432e+09 #> Within   48.3596  454'720131  650'709624  -2.44379420e+09  2.45610972e+10 #>  #> , , POP: Population, total #>  #>              N/T         Mean           SD          Min             Max #> Overall    12919  24'245971.6   102'120674         2833  1.39771500e+09 #> Between      216    24'178573  98'616506.7    8343.3333  1.08786967e+09 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 1 row ]   # By Region qsu(wlddev, by = ~region, cols = 9:13, vlabels = TRUE) #> , , PCGDP: GDP per capita (constant 2010 US$) #>  #>                                N        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , LIFEEX: Life expectancy at birth, total (years) #>  #>                                N     Mean       SD      Min      Max #> East Asia & Pacific         1807  65.9445  10.1633   18.907   85.078 #> Europe & Central Asia       3046  72.1625   5.7602   45.369  85.4171 #> Latin America & Caribbean   2107  68.3486   7.3768   41.762  82.1902 #> Middle East & North Africa  1226  66.2508   9.8306   29.919  82.8049 #> North America                144  76.2867   3.5734  68.8978  82.0488 #> South Asia                   480  57.5585  11.3004   32.446   78.921 #> Sub-Saharan Africa          2860   51.581   8.6876   26.172  74.5146 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 3 slices ]   # Panel-summary by region qsu(wlddev, by = ~region, pid = ~iso3c, cols = 9:13, vlabels = TRUE) #> , , Overall, PCGDP: GDP per capita (constant 2010 US$) #>  #>                              N/T        Mean          SD         Min #> East Asia & Pacific         1467  10513.2441  14383.5507    132.0776 #> Europe & Central Asia       2243  25992.9618  26435.1316    366.9354 #> Latin America & Caribbean   1976   7628.4477   8818.5055   1005.4085 #> Middle East & North Africa   842  13878.4213  18419.7912    578.5996 #> North America                180    48699.76  24196.2855  16405.9053 #> South Asia                   382   1235.9256   1611.2232    265.9625 #> Sub-Saharan Africa          2380   1840.0259   2596.0104    164.3366 #>                                    Max #> East Asia & Pacific         71992.1517 #> Europe & Central Asia       196061.417 #> Latin America & Caribbean   88391.3331 #> Middle East & North Africa  116232.753 #> North America               113236.091 #> South Asia                    8476.564 #> Sub-Saharan Africa          20532.9523 #>  #> , , Between, PCGDP: GDP per capita (constant 2010 US$) #>  #>                             N/T        Mean          SD         Min         Max #> East Asia & Pacific          34  10513.2441   12771.742    444.2899  39722.0077 #> Europe & Central Asia        56  25992.9618   24051.035    809.4753   141200.38 #> Latin America & Caribbean    38   7628.4477   8470.9708   1357.3326  77403.7443 #> Middle East & North Africa   20  13878.4213  17251.6962   1069.6596  64878.4021 #> North America                 3    48699.76  18604.4369  35260.4708  74934.5874 #> South Asia                    8   1235.9256   1488.3669      413.68   6621.5002 #> Sub-Saharan Africa           47   1840.0259   2234.3254    253.1886   9922.0052 #>  #>  [ reached 'max' / getOption(\"max.print\") -- omitted 13 slices ]   # Pairwise correlations: Ovarall print(pwcor(get_vars(wlddev, 9:13), N = TRUE, P = TRUE), show = \"lower.tri\") #>               PCGDP        LIFEEX         GINI          ODA           POP #> PCGDP    1   (9470)                                                       #> LIFEEX  .57* (9022)   1   (11670)                                         #> GINI   -.44* (1735)  -.35* (1742)   1   (1744)                            #> ODA    -.16* (7128)  -.02  (8142) -.20* (1109)   1   (8608)               #> POP    -.06* (9470)  .03* (11659)  .04  (1744)  .31* (8597)   1   (12919)  # Pairwise correlations: Between Countries print(pwcor(fmean(get_vars(wlddev, 9:13), wlddev$iso3c), N = TRUE, P = TRUE), show = \"lower.tri\") #>              PCGDP      LIFEEX        GINI         ODA         POP #> PCGDP    1   (206)                                                 #> LIFEEX  .60* (199)   1   (207)                                     #> GINI   -.42* (165) -.40* (165)   1   (167)                         #> ODA    -.25* (172) -.21* (172) -.19* (145)   1   (178)             #> POP    -.07  (206) -.02  (207) -.04  (167)  .50* (178)   1   (216)  # Pairwise correlations: Within Countries print(pwcor(fwithin(get_vars(wlddev, 9:13), wlddev$iso3c), N = TRUE, P = TRUE), show = \"lower.tri\") #>               PCGDP        LIFEEX         GINI          ODA           POP #> PCGDP    1   (9470)                                                       #> LIFEEX  .31* (9022)   1   (11670)                                         #> GINI   -.01  (1735)  -.16* (1742)   1   (1744)                            #> ODA    -.01  (7128)   .17* (8142) -.08* (1109)   1   (8608)               #> POP     .06* (9470)  .29* (11659)  .01  (1744) -.11* (8597)   1   (12919)"},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-211","dir":"Changelog","previous_headings":"","what":"collapse 2.1.1","title":"collapse 2.1.1","text":"CRAN release: 2025-04-14 alloc(list(1), 2) now gives list(1, 1) instead list(list(1), list(1)), can still generated alloc(list(1), 2, simplify = FALSE). change also affects ftransform()/fmutate(), making, e.g., fmutate(data, y = list(1)) consistent dplyr::mutate(data, y = list(1)). Thanks @MattAFiedler (#753). fslice() now works sf data frames.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-210","dir":"Changelog","previous_headings":"","what":"collapse 2.1.0","title":"collapse 2.1.0","text":"CRAN release: 2025-03-10 collapse 2.1.0, released March 2025, introduces fast slicing function, improved weighted quantile algorithm, convenience features, removes legacy functions package.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"potentially-breaking-changes-2-1-0","dir":"Changelog","previous_headings":"","what":"Potentially breaking changes","title":"collapse 2.1.0","text":"Functions pwNobs, .factor_GRP, .factor_qG, .GRP, .qG, .unlistable, .categorical, .Date, .numeric_factor, .character_factor, Date_vars, renamed v1.6.0 either replacing ‘.’ ’_’ using lower-case letters, depreciated since , now finally removed package. num_vars() (thus also cat_vars() collap()) changed simpler C-definition numeric data types -line .numeric(): is_numeric_C <- function(x) typeof(x) %% c(\"integer\", \"double\") && !inherits(x, c(\"factor\", \"Date\", \"POSIXct\", \"yearmon\", \"yearqtr\")). previous definition : is_numeric_C_old <- function(x) typeof(x) %% c(\"integer\", \"double\") && (!.object(x) || inherits(x, c(\"ts\", \"units\", \"integer64\"))). Thus, definition changed including certain classes excluding important classes. Thanks @maouw flagging (#727).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-2-1-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 2.1.0","text":"Fixed issues using collapse tidyverse together, particularly regarding tidyverse methods ‘grouped_df’ - thanks @NicChr (#645). consistent handling zero-length inputs - now also returned fmean() fmedian()/fnth() instead returning NA (#628).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-2-1-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 2.1.0","text":"Added function fslice(): fast alternative dplyr::slice_[head|tail|min|max] also works matrices. Thanks @alinacherkas proposal initial implementation (#725). Added function groupv() programmers version group(), rather, groupv() now identical former group(), group() now supports multiple vectors input e.g. group(v1, v2). done convenience consistency radixorder[v](). backwards compatibility, group() also supports single list input. join() new argument require allowing user generate messages errors join operation successful enough: psmat() now fill argument fill empty slots matrix/array elements (default NULL/NA).","code":"join(df1, df2, require = list(x = 0.8, fail = \"warning\")) #> Warning: Matched 75.0% of records in table df1 (x), but 80.0% is required #> left join: df1[id1, id2] 3/4 (75%) <1:1st> df2[id1, id2] 3/4 (75%) #>   id1 id2 name age salary      dept #> 1   1   a John  35  60000        IT #> 2   1   b Jane  28     NA      <NA> #> 3   2   b  Bob  42  55000 Marketing #> 4   3   c Carl  50  70000     Sales"},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-2-1-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 2.1.0","text":"weighted quantile algorithm fquantile()/fnth() improved theoretically sound method following excellent notes Matthew Kay. now also supports quantile type 4, skip zero weights anymore, new algorithm makes difficult skip ‘fly’. Note existing collapse algorithm already good properties bug fix v2.0.17, new algorithm exact also faster. collapse arXiv article updated significantly enhanced. excellent resource get overview package.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"notes-2-1-0","dir":"Changelog","previous_headings":"","what":"Notes","title":"collapse 2.1.0","text":"CRAN, collapse R dependency changed >= 4.1.0 able use base pipe examples without generating NOTE R CMD check (another absolutely unnecessary restriction). package depends R >= 3.5.0 DESCRIPTION file GitHub/R-universe continue reflect .","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2019","dir":"Changelog","previous_headings":"","what":"collapse 2.0.19","title":"collapse 2.0.19","text":"CRAN release: 2025-01-09 fmatch(factor(NA), NA) now gives 1 instead NA. Thanks @NicChr (#675). New developer focused vignette developing collapse. Fixed minor CRAN issues (#676, #702). Fixed bug integer64 types rowbind(). Thanks @arthurgailes reporting @aitap providing fix (#697). collapse now also Bluesky account https://bsky.app/profile/rcollapse.bsky.social.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2018","dir":"Changelog","previous_headings":"","what":"collapse 2.0.18","title":"collapse 2.0.18","text":"CRAN release: 2024-11-23 Cases pivot(..., = \"longer\") values columns now longer give error. Thanks @alvarocombo flagging (#663). Fixed bug qF(c(4L, 1L, NA), sort = FALSE): hash function failure due coding bug. Thanks @mayer79 flagging (#666). x already qG object right properties, calling qG(x) now copy x anymore. Thanks @mayer79 (https://github.com/mayer79/effectplots/issues/11).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2017","dir":"Changelog","previous_headings":"","what":"collapse 2.0.17","title":"collapse 2.0.17","text":"CRAN release: 2024-11-03 GRP.default(), \"group.starts\" attribute always returned, even one group every observation group. Thanks @JamesThompsonC (#631). Fixed bug pivot() na.rm = TRUE = \"wider\"|\"recast\" multiple value columns different missingness patterns. case na_omit(values) applied default settings original (long) value columns, implying potential loss information. fix applies na_omit(values, prop = 1), .e., removes completely missing rows. qDF()/qDT()/qTBL() now allow length-2 vector names row.names.col X named atomic vector, e.g., qDF(fmean(mtcars), c(\"cars\", \"mean\")) gives pivot(fmean(mtcars, drop = FALSE), names = list(\"car\", \"mean\")). Added subsection using internal (ad-hoc) grouping collapse tidyverse users vignette. qsu() now adds WeightSum column giving sum (non-zero missing) weights w argument used. Thanks @mayer79 suggesting (#650). panel data (pid) ‘’ sum weights also simply number groups, ‘Within’ sum weights ‘Overall’ sum weights divided number groups. Fixed inaccuracy fquantile()/fnth() weights: per documentation target sum sumwp = (sum(w) - min(w)) * p, however, practice, weight minimum element x used instead minimum weight. Since smallest element sample usually small weight unnoticed long , thanks @Jahnic-kb now reported fixed (#659). Fixed bug recode_char() regex = TRUE default argument used. Thanks @alinacherkas reporting fixing (#654).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2016","dir":"Changelog","previous_headings":"","what":"collapse 2.0.16","title":"collapse 2.0.16","text":"CRAN release: 2024-08-21 Fixes installation bug Linux systems (conflicting types) (#613). collapse now enforces string encoding fmatch() / join(), caused problems strings matched different encodings (#566, #579, #618). avoid noticeable performance implications, checks done heuristically, .e., first, 25th, 50th 75th percentile last string character vector checked, UTF8, entire vector internally coerced UTF8 strings matching process. general, character vectors R can contain strings different encodings, case regular data. performance reasons, collapse assumes character vectors uniform terms string encoding. Heterogeneous strings coerced using tools like stringi::stri_trans_general(x, \"latin-ascii\"). Fixes bug using qualified names fast statistical functions inside across() (#621, thanks @alinacherkas). collapse now depends R >= 3.4.0 due enforcement STRICT_R_HEADERS = 1 R v4.5.0. particular R API functions renamed Calloc -> R_Calloc Free -> R_Free.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2015","dir":"Changelog","previous_headings":"","what":"collapse 2.0.15","title":"collapse 2.0.15","text":"CRAN release: 2024-07-08 changes C-side move package closer C API compliance (demanded R-Core). One notable change gsplit() longer supports S4 objects (SET_S4_OBJECT part API asS4() expensive tight loops). think single example necessary split S4 object, applications please file issue. pivot() new arguments FUN = \"last\" FUN.args = NULL, allowing wide recast pivots aggregation (default last value ). FUN currently supports single function returning scalar value. Fast Statistical Functions receive vectorized execution. FUN.args can used supply list function arguments, including data-length arguments weights. also couple internal functions callable using function strings: \"first\", \"last\", \"count\", \"sum\", \"mean\", \"min\", \"max\". built reshaping C-code thus extremely fast. Thanks @AdrianAntico request (#582). join() now provides enhanced verbosity, indicating average order join two tables, e.g. collap(), multiple functions passed FUN catFUN return = \"long\", \"Function\" column now generated factor variable instead character (efficient).","code":"join(data.frame(id = c(1, 2, 2, 4)), data.frame(id = c(rep(1,4), 2:3))) #> left join: x[id] 3/4 (75%) <1.5:1st> y[id] 2/6 (33.3%) #>   id #> 1  1 #> 2  2 #> 3  2 #> 4  4 join(data.frame(id = c(1, 2, 2, 4)), data.frame(id = c(rep(1,4), 2:3)), multiple = TRUE) #> left join: x[id] 3/4 (75%) <1.5:2.5> y[id] 5/6 (83.3%) #>   id #> 1  1 #> 2  1 #> 3  1 #> 4  1 #> 5  2 #> 6  2 #> 7  4"},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2014","dir":"Changelog","previous_headings":"","what":"collapse 2.0.14","title":"collapse 2.0.14","text":"CRAN release: 2024-05-24 Updated ‘collapse sf’ vignette reflect recent support units objects, added examples. Fixed bug join() full join silently became left join matches tables (#574). Thanks @D3SL reporting. Added function group_by_vars(): standard evaluation version fgroup_by() slimmer safer programming, e.g. data |> group_by_vars(ind1) |> collapg(custom = list(fmean = ind2, fsum = ind3)). , using magrittr: Added function as_integer_factor() turn factors/factor columns integer vectors. as_numeric_factor() already exists, memory inefficient factors levels can integers. join() now internally checks rows joined datasets match exactly. check, using identical(m, seq_row(y)), inexpensive, , TRUE, saves full subset deep copy y. Thus join() now inherits intelligence already present functions like fsubset(), roworder() funique() - key efficient data manipulation simply less. join(), attr = TRUE, count option fmatch() always invoked, attribute attached always form, regardless verbose validate settings. roworder[v]() optional setting verbose = 2L indicate x already sorted, making call roworder[v]() redundant.","code":"library(magrittr) set_collapse(mask = \"manip\") # for fgroup_vars -> group_vars  data %>%    group_by_vars(ind1) %>% {   add_vars(     group_vars(., \"unique\"),     get_vars(., ind2) %>% fmean(keep.g = FALSE) %>% add_stub(\"mean_\"),     get_vars(., ind3) %>% fsum(keep.g = FALSE) %>% add_stub(\"sum_\")   )  }"},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2013","dir":"Changelog","previous_headings":"","what":"collapse 2.0.13","title":"collapse 2.0.13","text":"CRAN release: 2024-04-13 collapse now explicitly supports xts/zoo units objects concurrently removes additional check .default method statistical functions called matrix method .matrix(x) && !inherits(x, \"matrix\"). smart solution account fact xts objects matrix-based don’t inherit \"matrix\" class, thus wrongly calling default method. case units, , recent intensive engagement spatial data convinced changed. one, previous heuristic solution, possible call default method units matrix, e.g., fmean.default(st_distance(points_sf)) called fmean.matrix() yielded vector. case. Secondly, aggregation e.g. fmean(st_distance(points_sf)) fmean(st_distance(points_sf), g = group_vec) yielded plain numeric object lost units class (line general attribute handling principles). Therefore, now decided remove heuristic check within default methods, explicitly support zoo units objects. Fast Statistical Functions, methods FUN.zoo <- function(x, ...) (.matrix(x)) FUN.matrix(x, ...) else FUN.default(x, ...) FUN.units <- function(x, ...) (.matrix(x)) copyMostAttrib(FUN.matrix(x, ...), x) else FUN.default(x, ...). behavior xts/zoo remains , behavior units enhanced, now class preserved aggregations (.default method preserves attributes except ts), possible manually invoke .default method units matrix obtain aggregate statistic. change may impact computations matrix based classes don’t inherit \"matrix\" (mts inherit \"matrix\", aware affected classes, user code like m <- matrix(rnorm(25), 5); class(m) <- \"bla\"; fmean(m) now yield scalar instead vector. code must adjusted either class(m) <- c(\"bla\", \"matrix\") fmean.matrix(m)). Overall, change makes collapse behave standard predictable way, enhances support units objects central sf ecosystem. fquantile() now also preserves attributes input, line quantile().","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2012","dir":"Changelog","previous_headings":"","what":"collapse 2.0.12","title":"collapse 2.0.12","text":"CRAN release: 2024-04-01 Fixes issues signed int overflows inside hash functions possible protect bugs flagged RCHK.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2011","dir":"Changelog","previous_headings":"","what":"collapse 2.0.11","title":"collapse 2.0.11","text":"CRAN release: 2024-03-21 article collapse submitted Journal Statistical Software. preprint available arXiv. Removed magrittr documentation examples (using base pipe). Improved plot.GRP little bit - request JSS editors.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-2010","dir":"Changelog","previous_headings":"","what":"collapse 2.0.10","title":"collapse 2.0.10","text":"CRAN release: 2024-02-17 Fixed bug fmatch() matching integer vectors factors. also affected join(). Improved cross-platform compatibility OpenMP flags. Thanks @kalibera. Added stub = TRUE argument grouped_df methods Fast Statistical Functions supporting weights, able remove alter prefixes given aggregated weights columns keep.w = TRUE. Globally, users can set st_collapse(stub = FALSE) disable prefixing statistical functions operators.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-209","dir":"Changelog","previous_headings":"","what":"collapse 2.0.9","title":"collapse 2.0.9","text":"CRAN release: 2024-01-11 Added functions na_locf() na_focb() fast basic C implementations procedures (optionally reference). replace_na() now also type argument supports options \"locf\" \"focb\" (default \"const\"), similar data.table::nafill. implementation also supports character data list-columns (NULL/empty elements). Thanks @BenoitLondon suggesting (#489). note na_locf() exists packages (imputeTS) implemented R additional options. Users utilize flexible namespace .e. set_collapse(remove = \"na_locf\") deal . Fixed bug weighted quantile estimation (fquantile()) lead wrong/--range estimates cases. Thanks @zander-prinsloo reporting (#523). Improved right join join column names x instead y preserved. consistent joins join columns x y different names. fluent safe interplay ‘mask’ ‘remove’ options set_collapse(): now seamlessly possible switch combination ‘mask’ ‘remove’ combination without need setting NULL first.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-208","dir":"Changelog","previous_headings":"","what":"collapse 2.0.8","title":"collapse 2.0.8","text":"pivot(..., values = [multiple columns], labels = \"new_labels_column\", = \"wieder\"), columns selected values already variable labels, concatenated new labels provided \"new_labels_col\" using \" - \" separator (similar names separator \"_\"). whichv() operators %==%, %!=% now properly account missing double values, e.g. c(NA_real_, 1) %==% c(NA_real_, 1) yields c(1, 2) rather 2. Thanks @eutwt flagging (#518). setv(X, v, R), type R greater X e.g. setv(1:10, 1:3, 9.5), warning issued conversion R lower type (real integer case) may incur loss information. Thanks @tony-aw suggesting (#498). frange() option finite = FALSE, like base::range. Thanks @MLopez-Ibanez suggesting (#511). varying.pdata.frame(..., any_group = FALSE) now unindexes result (case).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-207","dir":"Changelog","previous_headings":"","what":"collapse 2.0.7","title":"collapse 2.0.7","text":"CRAN release: 2023-12-07 Fixed bug full join verbose = 0. Thanks @zander-prinsloo reporting. Added argument multiple = FALSE join(). Setting multiple = TRUE performs multiple-matching join row x matched matching rows y. default FALSE just takes first matching row y. Improved recode/replace functions. Notably, replace_outliers() now supports option value = \"clip\" replace outliers respective upper/lower bounds, also option single.limit = \"mad\" removes outliers exceeding certain number median absolute deviations. Furthermore, functions now set argument fully applies transformations reference. Functions replace_NA replace_Inf renamed replace_na replace_inf make namespace bit consistent. earlier versions remain available.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-206","dir":"Changelog","previous_headings":"","what":"collapse 2.0.6","title":"collapse 2.0.6","text":"CRAN release: 2023-11-12 Fixed serious bug qsu() higher order weighted statistics erroneous, .e. whenever qsu(x, ..., w = weights, higher = TRUE) invoked, ‘SD’, ‘Skew’ ‘Kurt’ columns wrong (higher = FALSE weighted ‘SD’ correct). reason appears straightforward generalization Welford’s Online Algorithm higher-order weighted statistics. detected earlier algorithm tested unit weights. fix involved replacing Welford’s Algorithm higher-order weighted case 2-pass method, additionally uses long doubles higher-order terms. Thanks @randrescastaneda reporting. Fixed unexpected behavior t_list() names ‘V1’, ‘V2’, etc. assigned unnamed inner lists. now preserves missing names. Thanks @orgadish flagging .","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-205","dir":"Changelog","previous_headings":"","what":"collapse 2.0.5","title":"collapse 2.0.5","text":"join, y expression e.g. join(x = mtcars, y = subset(mtcars, mpg > 20)), name extracted just set \"y\". , name y captured .character(substitute(y))[1] = \"subset\" case. improvement mainly display purposes, also affect code duplicate columns datasets suffix provided join call: , y-columns renamed using (non-sensible) \"_subset\" suffix, now using \"_y\" suffix. Note concerns cases y expression rather single object. Small performance improvements %[!]% operators: %!% now uses .na(fmatch(x, table)) rather fmatch(x, table, 0L) == 0L, %%, exported using set_collapse(mask = \"%%\"|\"special\"|\"\") .logical(fmatch(x, table, 0L)) instead fmatch(x, table, 0L) > 0L. latter faster comparison operators >, == integers additionally need check NA’s (= smallest integer C).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-204","dir":"Changelog","previous_headings":"","what":"collapse 2.0.4","title":"collapse 2.0.4","text":"fnth()/fquantile(), slight change weighted quantile algorithm. outlined documentation, algorithm gives weighted versions continuous quantile methods (type 7-9) R replacing sample quantities weighted counterparts. E.g., default quantile type 7, continuous (lower) target element (n - 1) * p. weighted algorithm, became (sum(w) - mean(w)) * p compared cumulative sum ordered (x) weights, preserve equivalence algorithms cases weights equal. However, upon second thought, use mean(w) really reflect standard interpretation weights frequencies. reasoned using min(w) instead mean(w) better reflects interpretation, minimum (non-zero) weight reflects size smallest sampled unit. weighted quantile type 7 target now (sum(w) - min(w)) * p, also methods adjusted accordingly (note zero weight observations ignored algorithm). Note change package: issue vctrs users can encounter using collapse together tidyverse (especially ggplot2), collapse internally optimizes computations factors giving additional \"na.included\" class known contain missing values. example pivot(mtcars) gives \"variable\" factor class c(\"factor\", \"na.included\"), grouping \"variable\" subsequent operations faster. Unfortunately, pivot(mtcars) |> ggplot(aes(y = value)) + geom_histogram() + facet_wrap( ~ variable) currently gives error produced vctrs, vctrs implement standard S3 method dispatch thus ignore \"na.included\" class. turns way deal swap order classes .e. c(\"na.included\", \"factor\"), import vctrs, implement vec_ptype2 vec_cast methods \"na.included\" objects. never happen, collapse remain independent tidyverse. two ways can deal : first way remove \"na.included\" class ggplot2 e.g. facet_wrap( ~ set_class(variable, \"factor\")) facet_wrap( ~ factor(variable)) work. second option define function vec_ptype2.factor.factor <- function(x, y, ...) x global environment, avoids vctrs performing extra checks factor objects.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-203","dir":"Changelog","previous_headings":"","what":"collapse 2.0.3","title":"collapse 2.0.3","text":"CRAN release: 2023-10-17 Fixed signed integer overflow inside hash function detected CRAN checks (changing unsigned int). Updated cheatsheet (see README.md).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-202","dir":"Changelog","previous_headings":"","what":"collapse 2.0.2","title":"collapse 2.0.2","text":"CRAN release: 2023-10-14 Added global option ‘stub’ (default TRUE) set_collapse. passed stub(s) arguments statistical operators, B, W, STD, HDW, HDW, L, D, Dlog, G (.OPERATOR_FUN). default operators add prefix/stub transformed matrix data.frame columns. Setting set_collapse(stub = FALSE) now allows switch behavior columns prepended prefix (default). roworder[v]() now also supports grouped data frames, prints message indicating inefficient (also indexed data). additional argument verbose can set 0 avoid messages.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-201","dir":"Changelog","previous_headings":"","what":"collapse 2.0.1","title":"collapse 2.0.1","text":"%% set_collapse(mask = \"%%\") warn anymore overidentification used data frames (.e. using overid = 2 fmatch()). Fixed several typos documentation.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-200","dir":"Changelog","previous_headings":"","what":"collapse 2.0.0","title":"collapse 2.0.0","text":"collapse 2.0, released Mid-October 2023, introduces fast table joins data reshaping capabilities alongside convenience functions, enhances packages global configurability, including interactive namespace control.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"potentially-breaking-changes-2-0-0","dir":"Changelog","previous_headings":"","what":"Potentially breaking changes","title":"collapse 2.0.0","text":"grouped setting, .data used inside fsummarise() fmutate(), .cols = NULL, .data contain columns except grouping columns (-line .SD syntax data.table). , .data contained columns. selection .cols still refers columns, thus still possible select columns using e.g. grouped_data %>% fsummarise(some_expression_involving(.data), .cols = seq_col(.)).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"other-changes-2-0-0","dir":"Changelog","previous_headings":"","what":"Other changes","title":"collapse 2.0.0","text":"qsu(), argument vlabels renamed labels. vlabels continue work.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-2-0-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 2.0.0","text":"Fixed bug integer methods fsum(), fmean() fprod() returned NA single integer followed NA’s e.g fsum(c(1L, NA, NA)) erroneously gave NA. caused C-level shortcut returned NA first element vector reached (moving back front) without encountering non-NA-values. bug consisted content first element evaluated case. Note bug occur real numbers, also grouped execution. Thanks @blset reporting (#432).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-2-0-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 2.0.0","text":"Added join(): class-agnostic, vectorized, (default) verbose joins R, modeled polars API. Two different join algorithms implemented: hash-join (default, sort = FALSE) sort-merge-join (sort = TRUE). Added pivot(): fast easy data reshaping! supports longer, wider recast pivoting, including handling variable labels, uniform parsimonious API. perform data aggregation, default check data uniquely identified supplied ids. Underidentification ‘wide’ ‘recast’ pivots results last value taken within group. Users can toggle duplicates check setting check.dups = TRUE. Added rowbind(): fast class-agnostic alternative rbind.data.frame() data.table::rbindlist(). Added fmatch(): fast match() function vectors data frames/lists. workhorse function join(), also benefits ckmatch(), %!%, new operators %iin% %!iin% (see ). also possible set_collapse(mask = \"%%\") replace base::\"%%\" using fmatch(). Thanks fmatch(), operators also support data frames/lists vectors, compared row-wise. Added operators %iin% %!iin%: directly return indices, .e. %[!]iin% equivalent (x %[!]% table). useful especially subsetting directly supplying indices efficient e.g. x[x %[!]iin% table] faster x[x %[!]% table]. Similarly fsubset(wlddev, iso3c %iin% c(\"DEU\", \"ITA\", \"FRA\")) fast. Added vec(): efficiently turn matrices data frames / lists single atomic vector. aware multiple implementations packages, mostly inefficient. atomic objects, vec() simply removes attributes without copying object, lists directly calls C_pivot_longer.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-2-0-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 2.0.0","text":"set_collapse() now supports options ‘mask’ ‘remove’, giving collapse flexible namespace broadest sense can changed point within active session: ‘mask’ supports base R dplyr functions can masked faster collapse versions. E.g. library(collapse); set_collapse(mask = \"unique\") (, equivalently, set_collapse(mask = \"funique\")) create unique <- funique collapse namespace, export unique() namespace, detach attach namespace R can find . re-attaching also ensures collapse comes right global environment, implying ’s functions take priority libraries. Users can use fastverse::fastverse_conflicts() check functions masked using set_collapse(mask = ...). option can changed time. Using set_collapse(mask = NULL) removes masked functions namespace, can also called simply ensure collapse top search path. ‘remove’ allows removing arbitrary functions collapse namespace. E.g. set_collapse(remove = \"D\") remove difference operator D(), also exists stats calculate symbolic algorithmic derivatives (convenient example necessary since collapse::D S3 generic call stats::D() R calls, expressions names). safe modifies objects exported namespace (truly remove objects namespace). option can also changed time. set_collapse(remove = NULL) restore exported namespace. options exist number convenient keywords bulk-mask / remove functions. example set_collapse(mask = \"manip\", remove = \"shorthand\") mask data manipulation functions mutate <- fmutate remove function shorthands mtt (.e. abbreviations frequently used functions collapse supplies faster coding / prototyping). set_collapse() also supports options ‘digits’, ‘verbose’ ‘stable.algo’, enhancing global configurability collapse. qM() now also row.names.col argument second position allowing generation rownames converting data frame-like objects matrix e.g. qM(iris, \"Species\") qM(GGDC10S, 1:5) (interaction id’s). as_factor_GRP() finteraction() now argument sep = \".\" denoting separator used compound factor labels. alloc() now additional argument simplify = TRUE. FALSE always returns list output. frename() supports new = old (pandas, used far) old = new (dplyr) style renaming conventions. across() supports negative indices, also grouped settings: select variables apart grouping variables. TRA() allows shorthands \"NA\" \"replace_NA\" \"fill\" \"replace_fill\". group() experienced minor speedup >= 2 vectors first two vectors now hashed jointly. fquantile() names = TRUE adds 1 digit comma percent-names, e.g. fquantile(airmiles, probs = 0.001) generates appropriate names (0% previous version).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-196","dir":"Changelog","previous_headings":"","what":"collapse 1.9.6","title":"collapse 1.9.6","text":"CRAN release: 2023-05-28 New vignette collapse’s Handling R Objects: provides overview collapse’s (internal) class-agnostic R programming framework. print.descr() groups option perc = TRUE (default) also shows percentages group frequencies variable. funique(mtcars[NULL, ], sort = TRUE) gave error (data frame zero rows). Thanks @NicChr (#406). Added SIMD vectorization fsubset(). vlengths() now also works strings, hence much faster version lengths() nchar(). Also atomic vectors behavior like lengths(), e.g. vlengths(rnorm(10)) gives rep(1L, 10). collap[v/g](), ... argument now placed custom argument instead last argument, order better guard unwanted partial argument matching. particular, previously n argument passed fnth partially matched na.last. Thanks @ummel alerting (#421).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-195","dir":"Changelog","previous_headings":"","what":"collapse 1.9.5","title":"collapse 1.9.5","text":"CRAN release: 2023-04-07 Using DATAPTR_RO point R lists use ALTLISTS R-devel. Replacing != loop controls SIMD loops < ensure compatibility platforms. Thanks @albertus82 (#399).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-194","dir":"Changelog","previous_headings":"","what":"collapse 1.9.4","title":"collapse 1.9.4","text":"CRAN release: 2023-03-31 Improvements get_elem()/has_elem(): Option invert = TRUE implemented robustly, function passed get_elem()/has_elem() now applied elements list, including elements list-like. enables use inherits find list-like objects inside broader list structure e.g. get_elem(l, inherits, = \"lm\") fetches linear model objects inside l. Fixed small bug descr() introduced v1.9.0, producing error data frame contained numeric columns - internal function defined case. Also, POSIXct columns handled better print - preserving time zone (thanks @cdignam-chwy #392). fmean() fsum() g = NULL, well TRA(), setop(), related operators %r+%, %+=% etc., setv() fdist() now utilize Single Instruction Multiple Data (SIMD) vectorization default (OpenMP enabled), enabling potentially fast computing speeds. Whether instructions utilized compilation depends system. general, want max collapse system, consider compiling source CFLAGS += -O3 -march=native -fopenmp CXXFLAGS += -O3 -march=native .R/Makevars.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-193","dir":"Changelog","previous_headings":"","what":"collapse 1.9.3","title":"collapse 1.9.3","text":"CRAN release: 2023-02-27 Added functions fduplicated() any_duplicated(), vectors lists / data frames. Thanks @NicChr (#373) sort option added set_collapse() able set unordered grouping default. E.g. setting set_collapse(sort = FALSE) affect collap(), (), GRP(), fgroup_by(), qF(), qG(), finteraction(), qtab() internal use functions ad-hoc grouping fast statistical functions. uses sort, example funique() default sort = FALSE, affected global default setting. Fixed small bug group() / funique() resulting unnecessary memory allocation error rare cases. Thanks @NicChr (#381).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-192","dir":"Changelog","previous_headings":"","what":"collapse 1.9.2","title":"collapse 1.9.2","text":"CRAN release: 2023-01-25 fix Address Sanitizer issue required CRAN (eliminating unused bounds access end loop). qsu() finally grouped_df method. Added options option(\"collapse_nthreads\") option(\"collapse_na.rm\"), allow load collapse different defaults e.g. .Rprofile .fastverse configuration file. collapse loaded, options take effect, users need use set_collapse() change .op[[\"nthreads\"]] .op[[\"na.rm\"]] interactively. Exported method plot.psmat() (can useful plot time series matrices).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-191","dir":"Changelog","previous_headings":"","what":"collapse 1.9.1","title":"collapse 1.9.1","text":"Fixed minor C/C++ issues flagged CRAN’s detailed checks. Added functions set_collapse() get_collapse(), allowing globally set defaults nthreads na.rm arguments functions package. E.g. set_collapse(nthreads = 4, na.rm = FALSE) suitable setting larger data without missing values. implemented using internal environment name .op, defaults received using e.g. .op[[\"nthreads\"]], computational cost nanoseconds (8-10x faster getOption(\"nthreads\") take 1 microsecond). .op accessible user, function get_collapse() can used retrieve settings. Exempt functions .quantile, new function .range (alias frange), go directly C maximum performance repeated executions, affected global settings. Function descr(), internally calls bunch statistical functions, also affected settings. improvements thread safety fsum() fmean() grouped computations across data frame columns. OpenMP enabled functions collapse can now considered thread safe .e. pass full battery tests multithreaded mode.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-190","dir":"Changelog","previous_headings":"","what":"collapse 1.9.0","title":"collapse 1.9.0","text":"CRAN release: 2023-01-15 collapse 1.9.0 released mid January 2023, provides improvements performance versatility many areas, well greater statistical capabilities, notably efficient (grouped, weighted) estimation sample quantiles.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-9-0","dir":"Changelog","previous_headings":"","what":"Changes to functionality","title":"collapse 1.9.0","text":"functions renamed collapse 1.6.0 now depreciated, removed end 2023. functions already giving messages since v1.6.0. See help(\"collapse-renamed\"). lead operator F() exported anymore package namespace, avoid clashes base::F flagged multiple people. operator still part package can accessed using collapse:::F. also added option \"collapse_export_F\", setting options(collapse_export_F = TRUE) loading package exports operator . Thanks @matthewross07 (#100), @edrubin (#194), @arthurgailes (#347). Function fnth() new default ties = \"q7\", gives result quantile(..., type = 7) (R’s default). details .","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-9-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.9.0","text":"fmode() gave wrong results singleton groups (groups size 1) unsorted data. optimized fmode() singleton groups directly return corresponding element, access element (internal) ordering vector, first element/row entire vector/data taken. mistake occurred fndistinct singleton groups NA, counted 1 instead 0 na.rm = TRUE default (provided first element vector/data NA). mistake occur data sorted groups, data pointer already pointed first element group. (apologies bug, took half year discover , using collapse daily basis, escaped 700 unit tests well). Function groupid(x, na.skip = TRUE) returned uninitialized first elements first values x NA. Thanks reporting @Henrik-P (#335). Fixed bug .names argument across(). Passing naming function .names = function(c, f) paste0(c, \"-\", f) now works intended .e. function applied combinations columns (c) functions (f) using outer(). Previously just internally evaluated .names(cols, funs), work multiple cols multiple funs. also now possibility set .names = \"flip\", names columns f_c instead c_f. fnrow() rewritten C also supports data frames 0 columns. Similarly seq_row(). Thanks @NicChr (#344).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-9-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.9.0","text":"Added functions fcount() fcountv(): versatile blazing fast alternative dplyr::count. also works vectors, matrices, well grouped indexed data. Added function fquantile(): Fast (weighted) continuous quantile estimation (methods 5-9 following Hyndman Fan (1996)), implemented fully C based quickselect radixsort algorithms, also supports ordering vector optional input speed process. 2x faster stats::quantile larger vectors, also especially fast smaller data, R overhead stats::quantile becomes burdensome. maximum performance repeated executions, programmers version .quantile() different defaults also provided. Added function fdist(): fast versatile replacement stats::dist. computes full euclidean distance matrix around 4x faster stats::dist serial mode, additional gains possible multithreading along distance matrix columns (decreasing thread loads matrix lower triangular). also supports computing distance matrix single row-vector, simply two vectors. E.g. fdist(mat, mat[1, ]) sqrt(colSums((t(mat) - mat[1, ])^2))), 20x faster serial mode, fdist(x, y) sqrt(sum((x-y)^2)), 3x faster serial mode. cases (sub-column level) multithreading available. Note fdist skip missing values .e. NA’s result NA distances. also internal implementation integers data frames. inputs coerced numeric matrices. Added function GRPid() easily fetch group id grouping object, especially inside grouped fmutate() calls. addition warranted especially new improved fnth.default() method allows orderings supplied performance improvements. See commends fnth() example provided . fsummarize() added synonym fsummarise. Thanks @arthurgailes PR. C API: collapse exports around 40 C functions provide functionality either convenient rather complicated implement scratch. exported functions can found bottom src/ExportSymbols.c. API include Fast Statistical Functions, thought closely related collapse works internally much use C programmer (e.g. expect grouping objects certain kinds integer vectors). free request export additional functions, including C++ functions.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-9-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.9.0","text":"fnth() fmedian() rewritten C, significant gains performance versatility. Notably, fnth() now supports (grouped, weighted) continuous quantile estimation like fquantile() (fmedian(), wrapper around fnth(), can also estimate various quantile based weighted medians). new default fnth() ties = \"q7\", gives result (f)quantile(..., type = 7) (R’s default). OpenMP multithreading across groups also much effective weighted unweighted case. Finally, fnth.default gained additional argument o pass ordering vector, can dramatically speed repeated invocations function dame data: now supports data-length arguments passed e.g. (mtcars, mtcars$cyl, fquantile, w = mtcars$wt), making effectively generic grouped mapply function well. Furthermore, grouped_df method now also expands grouping columns output length > 1. collap(), internally uses non-Fast Statistical Functions, now also supports arbitrary arguments passed functions split groups. Thus users can also apply custom weighted functions collap(). Furthermore, parsing FUN, catFUN wFUN arguments improved brought -line parsing .fns across(). main benefit Fast Statistical Functions now also detected optimizations carried passed list providing new name e.g. collap(data, ~ id, list(mean = fmean)) now optimized! Thanks @ttrodrigz (#358) requesting . descr(), virtue fquantile improvements , supports full-blown grouped weighted descriptions data. implemented additional w arguments. function also turned S3 generic, default ‘grouped_df’ method. ‘descr’ methods .data.frame print also feature various improvements, new compact argument print.descr, allowing compact printout. Users also notice improved performance, mainly due fquantile: M1 descr(wlddev) now 2x faster summary(wlddev), 41x faster Hmisc::describe(wlddev). Thanks @statzhero request (#355). radixorder 25% faster characters doubles. also benefits grouping performance. Note group() may still substantially faster unsorted data, performance critical try sort = FALSE argument functions like fgroup_by compare. list processing functions noticeably faster, checking data types elements list now also done C, made improvements collapse’s version rbindlist() (used unlist2d(), various places). fsummarise fmutate gained ability evaluate arbitrary expressions result lists / data frames without need use across(). example: mtcars |> fgroup_by(cyl, vs, ) |> fsummarise(mctl(cor(cbind(mpg, wt, carb)), names = TRUE)) mtcars |> fgroup_by(cyl) |> fsummarise(mctl(lmtest::coeftest(lm(mpg ~ wt + carb)), names = TRUE)). also possibility compute expressions using .data e.g. mtcars |> fgroup_by(cyl) |> fsummarise(mctl(lmtest::coeftest(lm(mpg ~ wt + carb, .data)), names = TRUE)) yields thing, less efficient whole dataset (including ‘cyl’) split groups. greater efficiency convenience, can pre-select columns using global .cols argument, e.g. mtcars |> fgroup_by(cyl, vs, ) |> fsummarise(mctl(cor(.data), names = TRUE), .cols = .c(mpg, wt, carb)) gives . Three Notes : grouped vectorizations fast statistical functions .e. entire expression evaluated group. (Let know applications vectorization possible beneficial. can’t think .) elements result list need length, , fmutate, length data (group). .data used, entire expression (expr) turned function .data (function(.data) expr), means columns available accessed .data e.g. .data$col1. fsummarise supports computations mixed result lengths e.g. mtcars |> fgroup_by(cyl) |> fsummarise(N = GRPN(), mean_mpg = fmean(mpg), quantile_mpg = fquantile(mpg)), long computations result either length 1 length k vectors, k maximum result length (e.g. fquantile default settings k = 5). List extraction function get_elem() now option invert = TRUE (default FALSE) remove matching elements (nested) list. Also functionality argument keep.class = TRUE implemented better way, default keep.class = FALSE toggles classes (non-matched) list-like objects inside list removed. num_vars() become bit smarter: columns class ‘ts’ ‘units’ now also recognized numeric. general, users aware num_vars() regard R methods defined .numeric(), implemented C simply checks whether objects type integer double, class. addition two exceptions now guards two common cases num_vars() may give undesirable outcomes. Note num_vars() also called collap() distinguish numeric (FUN) non-numeric (catFUN) columns. Improvements setv() copyv(), making robust borderline cases: integer(0) passed v nothing (instead error), also possible pass single real index vind1 = TRUE .e. passing 1 instead 1L produce error. alloc() now works types objects .e. can replicate object. input non-atomic, atomic length > 1 NULL, output list objects, e.g. alloc(NULL, 10) gives length 10 list NULL objects, alloc(mtcars, 10) gives list mtcars datasets. Note latter case datasets deep-copied, additional memory consumed. missing_cases() na_omit() gained argument prop = 0, indicating proportion values missing case considered missing/omitted. default value 0 indicates least 1 value must missing. course setting prop = 1 indicates values must missing. data frames/lists checking done efficiently C. matrices currently still implemented using rowSums(.na(X)) >= max(.integer(prop * ncol(X)), 1L), performance less optimal. missing_cases() extra argument count = FALSE. Setting count = TRUE returns case-wise missing value count (cols). Functions frename() setrename() additional argument .nse = TRUE, conforming default non-standard evaluation tagged vector expressions e.g. frename(mtcars, mpg = newname) frename(mtcars, mpg = \"newname\"). Setting .nse = FALSE allows newname variable holding name e.g. newname = \"othername\"; frename(mtcars, mpg = newname, .nse = FALSE). Another use argument (named) character vector can now passed function rename (subset ) columns e.g. cvec = letters[1:3]; frename(mtcars, cvec, cols = 4:6, .nse = FALSE) (works even .nse = TRUE), names(cvec) = c(\"cyl\", \"vs\", \"\"); frename(mtcars, cvec, .nse = FALSE). Furthermore, setrename() now also returns renamed data invisibly, relabel() setrelabel() also gained similar flexibility allow (named) lists vectors variable labels passed. Note function NSE capabilities, work essentially like frename(..., .nse = FALSE). Function add_vars() became bit flexible also allows single vectors added tags e.g. add_vars(mtcars, log_mpg = log(mtcars$mpg), STD(mtcars)), similar cbind. However add_vars() continues replicate length 1 inputs. Safer multithreading: OpenMP multithreading parts R API minimized, reducing errors occurred especially multithreading across data frame columns. Also number threads supplied user OpenMP enabled functions ensured exceed either omp_get_num_procs(), omp_get_thread_limit(), omp_get_max_threads().","code":"# Estimating multiple weighted-grouped quantiles on mpg: pre-computing an ordering provides extra speed.  mtcars %>% fgroup_by(cyl, vs, am) %>%     fmutate(o = radixorder(GRPid(), mpg)) %>% # On grouped data, need to account for GRPid()     fsummarise(mpg_Q1 = fnth(mpg, 0.25, o = o, w = wt),                mpg_median = fmedian(mpg, o = o, w = wt),                mpg_Q3 = fnth(mpg, 0.75, o = o, w = wt)) # Note that without weights this is not always faster. Quickselect can be very efficient, so it depends  # on the data, the number of groups, whether they are sorted (which speeds up radixorder), etc..."},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-189","dir":"Changelog","previous_headings":"","what":"collapse 1.8.9","title":"collapse 1.8.9","text":"CRAN release: 2022-10-07 Fixed warnings rchk newer C compilers (LLVM clang 10+). .pseries / .indexed_series methods also change implicit class vector (attached \"pseries\"), data type changed. e.g. calling function like fgrowth integer pseries changed data type double, “integer” class still attached “pseries”. Fixed bad testing SE inputs fgroup_by() findex_by(). See #320. Added rsplit.matrix method. descr() now default also reports 10% 90% quantiles numeric variables (line STATA’s detailed summary statistics), can also applied ‘pseries’ / ‘indexed_series’. Furthermore, descr() now argument stepwise descr(big_data, stepwise = TRUE) yields computation summary statistics variable--variable basis (finished ‘descr’ object returned invisibly). printed result thus identical print(descr(big_data), stepwise = TRUE), difference latter first entire computation whereas former computes statistics demand. Function ss() new argument check = TRUE. Setting check = FALSE allows subsetting data frames / lists positive integers without checking whether integers positive -range. programmers. Function get_vars() new argument rename allowing select-renaming columns standard evaluation programming, e.g. get_vars(mtcars, c(newname = \"cyl\", \"vs\", \"\"), rename = TRUE). default rename = FALSE, warrant full backwards compatibility. See #327. Added helper function setattrib(), set new attribute list object reference + invisible return. different existing function setAttrib() (note capital ), takes shallow copy list-like objects returns result.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-188","dir":"Changelog","previous_headings":"","what":"collapse 1.8.8","title":"collapse 1.8.8","text":"CRAN release: 2022-08-15 flm fFtest now internal generic added formula method e.g. flm(mpg ~ hp + carb, mtcars, weights = wt) fFtest(mpg ~ hp + carb | vs + , mtcars, weights = wt) addition programming interface. Thanks Grant McDermott suggesting. Added method .data.frame.qsu, efficiently turn default array outputs qsu() tidy data frames. Major improvements setv copyv, generalizing scope operations can performed common cases. means even simple base R operations X[v] <- R can now done significantly faster using setv(X, v, R). n qtab can now added options(\"collapse_mask\") e.g. options(collapse_mask = c(\"manip\", \"helper\", \"n\", \"qtab\")). export function n() get (group) count fsummarise fmutate (can also always done using GRPN() n() familiar dplyr users), mask table() qtab(), principally fast drop-replacement, different arguments. Added C-level helper function all_funs, fetches functions called expression, similar setdiff(.names(x), .vars(x)) better takes account syntax. example let x = quote(sum(sum)) .e. summing column named sum. .names(x) = c(\"sum\", \"sum\") .vars(x) = \"sum\" difference character(0), whereas all_funs(x) returns \"sum\". function makes collapse smarter parsing expressions fsummarise fmutate deciding ones vectorize.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-187","dir":"Changelog","previous_headings":"","what":"collapse 1.8.7","title":"collapse 1.8.7","text":"Fixed bug fscale.pdata.frame default C++ method called instead list method (.e. method didn’t work ). Fixed 2 minor rchk issues (remaining ones spurious). fsum additional argument fill = TRUE (default FALSE) initializes result vector 0 instead NA na.rm = TRUE, fsum(NA, fill = TRUE) gives 0 like base::sum(NA, na.rm = TRUE). Slight performance increase fmean groups na.rm = TRUE (default). Significant performance improvement using base R expressions involving multiple functions one column e.g. mid_col = (min(col) + max(col)) / 2 lorentz_col = cumsum(sort(col)) / sum(col) etc. inside fsummarise fmutate. Instead evaluating expressions data subset one column group, now turned function e.g. function(x) cumsum(sort(x)) / sum(x) applied single vector split groups. fsummarise now also adds groupings transformation functions operators, allows full vectorization complex tasks involving transformations subsequently aggregated. prime example grouped bivariate linear model fitting, can now done using mtcars |> fgroup_by(cyl) |> fsummarise(slope = fsum(W(mpg), hp) / fsum(W(mpg)^2)). 1.8.7 necessary mutate step first e.g. mtcars |> fgroup_by(cyl) |> fmutate(dm_mpg = W(mpg)) |> fsummarise(slope = fsum(dm_mpg, hp) / fsum(dm_mpg^2)), fsummarise add groupings transformation functions like fwithin/W. Thanks Brodie Gaslam making aware . Argument return.groups GRP.default now also available fgroup_by, allowing grouped data frames without materializing unique grouping columns. allows efficient mutate-operations e.g. mtcars |> fgroup_by(cyl, return.groups = FALSE) |> fmutate(across(hp:carb, fscale)). Similarly aggregation dropping grouping columns mtcars |> fgroup_by(cyl, return.groups = FALSE) |> fmean() equivalent faster mtcars |> fgroup_by(cyl) |> fmean(keep.group_vars = FALSE).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-186","dir":"Changelog","previous_headings":"","what":"collapse 1.8.6","title":"collapse 1.8.6","text":"CRAN release: 2022-06-14 inline functions TRA.c needed declared ‘static’ local scope (#275) timeid.Rd now uses zoo package conditionally limits size printout","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-185","dir":"Changelog","previous_headings":"","what":"collapse 1.8.5","title":"collapse 1.8.5","text":"CRAN release: 2022-06-13 Installation linux distributions failed omp.h included Rinternals.h signed integer overflows running tests caused UBSAN warnings. (happened inside hash function overflows problem. changed unsigned int avoid UBSAN warning.) Ensured package passes R CMD Check without suggested packages","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-184","dir":"Changelog","previous_headings":"","what":"collapse 1.8.4","title":"collapse 1.8.4","text":"CRAN release: 2022-06-08 Makevars text substitution hack CRAN accept package combines C, C++ OpenMP. Thanks also @MichaelChirico pointing right direction.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-183","dir":"Changelog","previous_headings":"","what":"collapse 1.8.3","title":"collapse 1.8.3","text":"Significant speed improvement qF/qG (factor-generation) character vectors 100,000 obs many levels sort = TRUE (default). details see method argument ?qF. Optimizations fmode fndistinct singleton groups.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-182","dir":"Changelog","previous_headings":"","what":"collapse 1.8.2","title":"collapse 1.8.2","text":"Fixed rchk issues found Thomas Kalibera CRAN. faster funique.default method. group now also internally optimizes ‘qG’ objects.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-181","dir":"Changelog","previous_headings":"","what":"collapse 1.8.1","title":"collapse 1.8.1","text":"Added function fnunique (yet another alternative data.table::uniqueN, kit::uniqLen dplyr::n_distinct, principally simple wrapper attr(group(x), \"N.groups\")). present fnunique generally outperforms others data frames. finteraction additional argument factor = TRUE. Setting factor = FALSE returns ‘qG’ object, efficient just integer id factor object required. Operators (see .OPERATOR_FUN) improved bit id-variables selected .data.frame (, w t arguments) .pdata.frame methods (variables index) computed upon even numeric (since default cols = .numeric). general, cols function used select columns certain data type, id variables excluded computation even data type. still possible compute id variables explicitly selecting using names indices passed cols, including lhs formula passed . efforts facilitate adding group-count fsummarise fmutate: options(collapse_mask = \"\") loading package, additional function n() exported works just like dplyr:::n(). fetch group sizes directly grouping object grouped data frame .e. data |> gby(id) |> GRPN() data %>% gby(id) %>% ftransform(N = GRPN(.)) (note dot). compute group sizes fly, example fsubset(data, GRPN(id) > 10L) fsubset(data, GRPN(list(id1, id2)) > 10L) GRPN(data, = ~ id1 + id2).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-180","dir":"Changelog","previous_headings":"","what":"collapse 1.8.0","title":"collapse 1.8.0","text":"collapse 1.8.0, released mid May 2022, brings enhanced support indexed computations time series panel data introducing flexible ‘indexed_frame’ ‘indexed_series’ classes surrounding infrastructure, sets modest start OpenMP multithreading well data transformation reference statistical functions, enhances packages descriptive statistics toolset.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-8-0","dir":"Changelog","previous_headings":"","what":"Changes to functionality","title":"collapse 1.8.0","text":"Functions Recode, replace_non_finite, depreciated since collapse v1.1.0 .regular, depreciated since collapse v1.5.1 clashing important function zoo package, now removed. Fast Statistical Functions operating numeric data (fmean, fmedian, fsum, fmin, fmax, …) now preserve attributes cases. Previously functions preserve attributes simple computations using default method, preserved attributes grouped computations !.object(x) (see NEWS section collapse 1.4.0). meant fmin fmax preserve attributes Date POSIXct objects, none functions preserved ‘units’ objects (used lot sf package). Now, attributes preserved !inherits(x, \"ts\"), new default functions generally keep attributes, except ‘ts’ objects obviously causes unwanted error (note ‘xts’ others handled matrix data.frame method principles apply, see NEWS 1.4.0). exception functions fnobs fndistinct previous default kept. Time Series Functions flag, fdiff, fgrowth psacf/pspacf/psccf (operators L/F/D/Dlog/G) now internally process time objects passed t argument (.object(t) && .numeric(unclass(t))) via new function called timeid turns integer vectors based greatest common divisor (GCD) (see ). Previously objects converted factor. can change behavior code e.g. ‘Date’ variable representing monthly data may regular converted factor, now irregular regarded daily data (GCD 1) different day counts months. Users fix code either calling qG time variable (grouping / factor-conversion) using appropriate classes e.g. zoo::yearmon. Note plain numeric vectors !.object(t) still used directly indexation without passing timeid (can still applied manually desired). now argument reorder = TRUE, casts elements original order NROW(result) == NROW(x) (like fmutate). Previously result just order groups, regardless length output. obtain former outcome users need set reorder = FALSE. options(\"collapse_DT_alloccol\") removed, default now fixed 100. reason data.table automatically expands range overallocated columns required (option really necessary), calling R options C slows C code can cause problems parallel code.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-8-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.8.0","text":"Fixed bug fcumsum caused segfault grouped operations larger data, due flawed internal memory allocation. Thanks @Gulde91 reporting #237. Fixed bug across caused two function(x) statements passed list e.g. mtcars |> fsummarise(acr(mpg, list(ssdd = function(x) sd(x), mu = function(x) mean(x)))). Thanks @trang1618 reporting #233. Fixed issue across() logical vectors used select column grouped data e.g. mtcars %>% gby(vs, ) %>% smr(acr(startsWith(names(.), \"c\"), fmean)) now works without error. qsu gives proper output length 1 vectors e.g. qsu(1). collapse depends R > 3.3.0, due use newer C-level macros introduced . earlier indication R > 2.1.0 based R-level code misleading. Thanks @ben-schwen reporting #236. try maintain dependency long possible, without restrained development R’s C API ALTREP system particular, collapse might utilize future.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-8-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.8.0","text":"Introduction ‘indexed_frame’,‘indexed_series’ ‘index_df’ classes: fast flexible indexed time series panel data classes inherit plm’s ‘pdata.frame’, ‘pseries’ ‘pindex’ classes. classes take full advantage collapse’s computational infrastructure, class-agnostic .e. can superimposed upon data frame vector/matrix like object maintaining functionality object, support time series panel data, natively handle irregularity, supports ad-hoc computations inside arbitrary data masking functions model formulas. infrastructure comprises additional functions methods, modification existing functions ‘pdata.frame’ / ‘pseries’ methods. New functions: findex_by/iby, findex/ix, unindex, reindex, is_irregular, to_plm. New methods: [.indexed_series, [.indexed_frame, [<-.indexed_frame, $.indexed_frame, $<-.indexed_frame, [[.indexed_frame, [[<-.indexed_frame, [.index_df, fsubset.pseries, fsubset.pdata.frame, funique.pseries, funique.pdata.frame, roworder(v) (internal) na_omit (internal), print.indexed_series, print.indexed_frame, print.index_df, Math.indexed_series, Ops.indexed_series. Modification ‘pseries’ ‘pdata.frame’ methods functions flag/L/F, fdiff/D/Dlog, fgrowth/G, fcumsum, psmat, psacf/pspacf/psccf, fscale/STD, fbetween/B, fwithin/W, fhdbetween/HDB, fhdwithin/HDW, qsu varying take advantage ‘indexed_frame’ ‘indexed_series’ continuing work ‘pdata.frame’ ‘pseries’. information details see help(\"indexing\"). Added function timeid: Generation integer-id/time-factor time date sequences represented integer double vectors (‘Date’, ‘POSIXct’, ‘ts’, ‘yearmon’, ‘yearquarter’ plain integers / doubles) numerically quite robust greatest common divisor method (see ). function used internally findex_by, reindex also evaluation t argument functions like flag/fdiff/fgrowth whenever .object(t) && .numeric(unclass(t)) (see also note ). Programming helper function vgcd efficiently compute greatest common divisor vector positive integer double values (ideally unique sorted well, timeid uses vgcd(sort(unique(diff(sort(unique(na_rm(x)))))))). Precision doubles 6 digits. Programming helper function frange: significantly faster alternative base::range, calls min max. Note frange inherits collapse’s global na.rm = TRUE default. Added function qtab/qtable: versatile computationally efficient alternative base::table. Notably, also supports tabulations frequency weights, computation statistic combinations variables. Objects class ‘qtab’ inherits ‘table’. Thus ‘table’ methods apply . TRA rewritten C, now additional argument set = TRUE toggles data transformation reference. function setTRA added shortcut additionally returns result invisibly. Since TRA usually accessed internally like-named argument Fast Statistical Functions, passing set = TRUE functions yields internal call setTRA. example fmedian(num_vars(iris), g = iris$Species, TRA = \"-\", set = TRUE) subtracts species-wise median numeric variables iris dataset, modifying data place returning result invisibly. Similarly argument can added workflows iris |> fgroup_by(Species) |> fmutate(across(1:2, fmedian, set = TRUE)) mtcars |> ftransform(mpg = mpg %+=% hp, wt = fsd(wt, cyl, TRA = \"replace_fill\", set = TRUE)). Note chains must ended invisible() printout wanted. Exported helper function greorder, companion gsplit reorder output fmutate (now also ): let g ‘GRP’ object (something coercible vector) x vector, greorder orders data y = unlist(gsplit(x, g)) identical(greorder(y, g), x).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-8-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.8.0","text":"fmean, fprod, fmode fndistinct rewritten C, providing performance improvements, particularly fmode fndistinct, improvements integers fmean fprod. OpenMP multithreading fsum, fmean, fmedian, fnth, fmode fndistinct, implemented via additional nthreads argument. default use 1 thread, internally calls serial version code fsum fmean (thus change default behavior). plan slowly roll statistical functions introduce options set alternative global defaults. Multi-threading internally works different different functions, see nthreads argument documentation particular function. Unfortunately currently guarantee thread safety, parallelization complex loops entails tricky bugs limited time sort . please report bugs, happen experience OpenMP please consider examining code making suggestions. TRA additional option \"replace_NA\", e.g. wlddev |> fgroup_by(iso3c) |> fmutate(across(PCGDP:POP, fmedian, TRA = \"replace_NA\")) performs median value imputation missing values. Similarly matrix X <- matrix(na_insert(rnorm(1e7)), ncol = 100), fmedian(X, TRA = \"replace_NA\", set = TRUE) (column-wise median imputation reference). Fast Statistical Functions support zero group sizes (e.g. grouping factor unused levels always produce output length nlevels(x) 0 NA elements unused levels). Previously produced error message counting/ordinal functions fmode, fndistinct, fnth fmedian. ‘GRP’ objects now also contain ‘group.starts’ item 8’th slot gives first positions unique groups, returned alongside groups whenever return.groups = TRUE. now benefits ffirst invoked na.rm = FALSE, e.g. wlddev %>% fgroup_by(country) %>% ffirst(na.rm = FALSE) now just efficient funique(wlddev, cols = \"country\"). Note additional computing cost incurred preserving ‘group.starts’ information. Conversion methods GRP.factor, GRP.qG, GRP.pseries, GRP.pdata.frame GRP.grouped_df now also efficiently check grouping vectors sorted (information stored “ordered” element ‘GRP’ objects). leads performance improvements gsplit / greorder dependent functions rsplit factors sorted. descr() received performance improvements (2x categorical data), additional argument sort.table, allowing frequency tables categorical variables sorted frequency (\"freq\") table values (\"value\"). new default (\"freq\"), presents tables decreasing order frequency. method [.descr added allowing ‘descr’ objects subset like list. print method also enhanced, default now prints 14 values highest frequency groups remaining values single ... %s Others category. Furthermore, missing values column, percentage values missing now printed behind Statistics. Additional arguments reverse stepwise allow printing reverse order /one variable time. whichv (operators %==%, %!=%) now also support comparisons equal-length arguments e.g. 1:3 %==% 1:3. Note used compare 2 factors. Added code .onLoad function checks existence .fastverse configuration file containing setting _opt_collapse_mask: found code makes sure option takes effect package loaded. means inside projects using fastverse options(\"collapse_mask\") replace base R / dplyr functions, collapse loaded without masking applied, making secure utilize feature. information function masking see help(\"collapse-options\") .fastverse configuration files see fastverse vignette. Added hidden .list methods fhdwithin/HDW fhdbetween/HDB. .FAST_FUN just wrapper data frame method meant used unclassed data frames. ss() supports unnamed lists / data frames. t w arguments ‘grouped_df’ methods (NSE) formula input allowed, supports ad-hoc transformations. E.g. wlddev %>% gby(iso3c) %>% flag(t = qG(date)) L(wlddev, 1, ~ iso3c, ~qG(date)), similarly qsu(wlddev, w = ~ log(POP)), wlddev %>% gby(iso3c) %>% collapg(w = log(POP)) wlddev %>% gby(iso3c) %>% nv() %>% fmean(w = log(POP)). Small improvements group() algorithm, avoiding cases hash function performed badly, particularly integers. Function GRPnames now sep argument choose separator \".\".","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-176","dir":"Changelog","previous_headings":"","what":"collapse 1.7.6","title":"collapse 1.7.6","text":"CRAN release: 2022-02-11 Corrected C-level bug gsplit lead R crash instances (gsplit used internally fsummarise, fmutate, collap perform computations base R (non-optimized) functions). Ensured .grouped_df always (default) returns grouping columns aggregations .e. iris |> gby(Species) |> nv() |> (sum) now gives iris |> gby(Species) |> nv() |> fsum(). . added first argument functions fselect, fsubset, colorder fgroup_by, .e. fselect(x, ...) -> fselect(.x, ...). reason time added option select-rename columns e.g. fselect(mtcars, cylinders = cyl), offered functions created. presents problems columns renamed x, e.g. fselect(mtcars, x = cyl) failed, see #221. Renaming first argument .x somewhat guards situations. think change worthwhile implement, makes package robust going forward, usually first argument functions never invoked explicitly. really hope breaks nobody’s code. Added function GRPN make easy add column group sizes e.g. mtcars %>% fgroup_by(cyl,vs,) %>% ftransform(Sizes = GRPN(.)) mtcars %>% ftransform(Sizes = GRPN(list(cyl, vs, ))) GRPN(mtcars, = ~cyl+vs+). Added [.pwcor [.pwcov, able subset correlation/covariance matrices without loosing print formatting.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-175","dir":"Changelog","previous_headings":"","what":"collapse 1.7.5","title":"collapse 1.7.5","text":"CRAN release: 2022-02-03 Also ensuring tidyverse examples \\donttest{} building without dplyr testing file avoid issues static code analysis CRAN. 20-50% Speed improvement gsplit (therefore fsummarise, fmutate, collap invoked base R functions) grouping GRP(..., sort = TRUE, return.order = TRUE). enable default, default argument return.order GRP set sort, retains ordering vector (needed optimization). Retaining ordering vector uses memory can possibly adversely affect computations big data, big data sort = FALSE usually gives faster results anyway, can also always set return.order = FALSE (also fgroup_by, collap), default gives best worlds. ancient depreciated argument sort.row (replaced sort 2020) now removed collap. Also arguments return.order method added collap providing full control grouping happens internally.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-174","dir":"Changelog","previous_headings":"","what":"collapse 1.7.4","title":"collapse 1.7.4","text":"Tests needed adjusted upcoming release dplyr 1.0.8 involves API change mutate. fmutate take changes .e. fmutate(..., .keep = \"none\") continue work like dplyr::transmute. Furthermore, tests involving dplyr run CRAN, also follow along future dplyr API changes. C-API macro installTrChar (used new massign function) replaced installChar maintain backwards compatibility R versions prior 3.6.0. Thanks @tedmoorman #213. Minor improvements group(), providing increased performance doubles also increased performance second grouping variable integer, turned slow instances.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-173","dir":"Changelog","previous_headings":"","what":"collapse 1.7.3","title":"collapse 1.7.3","text":"CRAN release: 2022-01-26 Removed tests involving weights package (available R-devel CRAN checks). fgroup_by flexible, supporting computing columns e.g. fgroup_by(GGDC10S, Variable, Decade = floor(Year / 10) * 10) various programming options e.g. fgroup_by(GGDC10S, 1:3), fgroup_by(GGDC10S, c(\"Variable\", \"Country\")), fgroup_by(GGDC10S, .character). can also use column sequences e.g. fgroup_by(GGDC10S, Country:Variable, Year), mixed computing columns. Compute expressions may also include : function. memory efficient attribute handling C/C++ (using C-API macro SHALLOW_DUPLICATE_ATTRIB instead DUPLICATE_ATTRIB) places.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-172","dir":"Changelog","previous_headings":"","what":"collapse 1.7.2","title":"collapse 1.7.2","text":"CRAN release: 2022-01-22 Ensured base pipe |> used tests examples, avoid errors CRAN checks older versions R. Also adjusted psacf / pspacf / psccf take advantage faster grouping group.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-171","dir":"Changelog","previous_headings":"","what":"collapse 1.7.1","title":"collapse 1.7.1","text":"Fixed minor C/C++ issues flagged CRAN checks. Added option ties = \"last\" fmode. Added argument stable.algo qsu. Setting stable.algo = FALSE toggles faster calculation standard deviation, yielding 2x speedup large datasets. Fast Statistical Functions now internally use group grouping data g TRA arguments used, yielding efficiency gains unsorted data. Ensured fmutate fsummarise can called collapse attached.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-170","dir":"Changelog","previous_headings":"","what":"collapse 1.7.0","title":"collapse 1.7.0","text":"CRAN release: 2022-01-14 collapse 1.7.0, released mid January 2022, brings major improvements computational backend package, data manipulation capabilities, whole set new functions enable flexible memory efficient R programming - significantly enhancing language . vast majority codes, updating 1.7 cause problems.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-7-0","dir":"Changelog","previous_headings":"","what":"Changes to functionality","title":"collapse 1.7.0","text":"num_vars now implemented C, yielding massive performance increase checking columns using vapply(x, .numeric, logical(1)). selects columns (.double(x) || .integer(x)) && !.object(x). provides results common classes found data frames (e.g. factors date columns numeric), however possible users define methods .numeric objects, respected num_vars anymore. prominent example base R’s ‘ts’ objects .e. .numeric(AirPassengers) returns TRUE, .object(AirPassengers) also TRUE yields FALSE, implying - happened work data frames ‘ts’ columns - num_vars now select anymore. Please make aware important classes found data frames .numeric returns TRUE. num_vars also used internally collap might affect aggregations. flag, fdiff fgrowth, plain numeric vector passed t argument .double(t) && !.object(t), coerced integer using .integer(t) directly used time variable, rather applying ordered grouping first. avoid inefficiency grouping, owes fact data imported R various packages, time (year) variables coded double although integer (also don’t know cases time needs indexed non-date variable decimal places). Note algorithm internally handles irregularity time variable problem. break code, kindly raise issue GitHub. function setrename now truly renames objects reference (without creating shallow copy). true vlabels<- (rewritten C) new function setrelabel. Thus additional care needs taken (use inside functions etc.) renaming take global effects unless shallow copy data created prior operation inside function. doubt, better use frename creates shallow copy. improvements function, terms performance security. Performance enhanced new C function gsplit, providing split-apply-combine computing speeds competitive dplyr much broader range R objects. Regarding Security: result computation length original data, names / rownames grouping columns (grouped data) added result object known valid, .e. data originally sorted grouping columns (information recorded GRP.default(..., sort = TRUE), called internally non-factor/GRP/qG objects). reorder data split-apply-combine step (unlike dplyr::mutate); data simply recombined order groups. , general, used compute summary statistics (unless data sorted grouping). added security makes explicit. Added method length.GRP giving length grouping object. break code calling length grouping object (just returned length list). Functions renamed collapse 1.6.0 now print message telling use updated names. functions old names stay around 1-3 years. passing argument order instead sort function GRP (early version collapse), now disabled.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-7-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.7.0","text":"Fixed bug functions using Welfords Online Algorithm (fvar, fsd, fscale qsu) calculate variances, occurring initial final zero weights caused running sum weights algorithm zero, yielding division zero NA output although value expected. functions now skip zero weights alongside missing weights, also implies can pass logical vector weights argument efficiently calculate statistics subset data (e.g. using qsu).","code":""},{"path":[]},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"basic-computational-infrastructure-1-7-0","dir":"Changelog","previous_headings":"Additions","what":"Basic Computational Infrastructure","title":"collapse 1.7.0","text":"Function group added, providing low-level interface new unordered grouping algorithm based hashing C optimized R’s data structures. algorithm heavily inspired great kit package Morgan Jacob, now feeds package multiple central functions (including GRP / fgroup_by, funique qF) invoked argument sort = FALSE. also used internal groupings performed data transformation functions fwithin (factor ‘GRP’ object provided g argument). speed algorithm promising (often superior radixorder), used places still. welcome feedback performance different datasets. Function gsplit provides efficient alternative split based grouping objects. used new backend rsplit (also supports data frame) well , collap, fsummarise fmutate - efficient grouped operations functions external package. Added multiple functions facilitate memory efficient programming (written C). include elementary mathematical operations reference (setop, %+=%, %-=%, %*=%, %/=%), supporting computations involving integers doubles vectors, matrices data frames (including row-wise operations via setop) copies . Furthermore set functions check single value vector without generating logical vectors: whichv, whichNA (operators %==% %!=% return indices significantly faster ==, especially inside functions like fsubset), anyv allv (allNA already added ). Finally, functions setv copyv speed operations involving replacement value (x[x == 5] <- 6) sequence values equally sized object (x[x == 5] <- y[x == 5], x[ind] <- y[ind] ind pre-computed vectors indices) vectors data frames without generating logical vectors materializing vector subsets. Function vlengths added efficient alternative lengths (without method dispatch, simply coded C). Function massign provides multivariate version assign (written C, supporting basic vector types). addition operator %=% added efficient multiple assignment operator. (called %=% %<-% facilitate translation Matlab Python codes R, zeallot package already provides multiple-assignment operators (%<-% %->%), significantly versatile, orders magnitude slower %=%)","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"high-level-features-1-7-0","dir":"Changelog","previous_headings":"Additions","what":"High-Level Features","title":"collapse 1.7.0","text":"Fully fledged fmutate function provides functionality analogous dplyr::mutate (sequential evaluation arguments, including arbitrary tagged expressions across statements). fmutate optimized work together packages Fast Statistical Data Transformation Functions, yielding fast, vectorized execution, also benefits gsplit operations. across() function implemented use inside fsummarise fmutate. also optimized Fast Statistical Data Transformation Functions, performs well functions . additional arguments .apply = FALSE apply functions entire subset data instead individual columns, thus allows nesting tibbles estimating models correlation matrices groups etc.. across() also supports arbitrary number additional arguments split evaluated groups necessary. Multiple across() statements can combined tagged vector expressions single call fsummarise fmutate. Thus computational framework pretty general similar data.table, although less efficient big datasets. Added functions relabel setrelabel make interactive dealing variable labels bit easier. Note functions operate reference. (vlabels<- implemented C. Taking shallow copy data frame useless case variable labels attributes columns, frame). difference two setrelabel returns result invisibly. function shortcuts rnm mtt added frename fmutate. across can also abbreviated using acr. Added two options can invoked loading package change namespace: options(collapse_mask = c(...)) can set export copies selected () functions package start f removing leading f e.g. fsubset -> subset (fsubset subset exported). allows masking base R dplyr functions (even basic functions sum, mean, unique etc. desired) collapse’s fast functions, facilitating optimization existing codes allowing work collapse using natural namespace. package internally insulated changes, course might major effects existing codes. Also options(collapse_F_to_FALSE = FALSE) can invoked get rid lead operator F, masks base::F (issue raised people like use T/F instead TRUE/FALSE). Read help page ?collapse-options information.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-7-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.7.0","text":"Package loads faster (don’t fetch functions C/C++ heavy packages .onLoad anymore, implied unnecessary loading lot DLLs). fsummarise now also fully featured supporting evaluation arbitrary expressions across() statements. Note mixing Fast Statistical Functions functions single expression can yield unintended outcomes, read ?fsummarise. funique benefits group default sort = FALSE, configuration, providing extra speed unique values first-appearance order default data frame method, data types. Function ss supports empty j. printout fgroup_by also shows minimum maximum group size unbalanced groupings. ftransformv/settransformv fcomputev, vars argument also evaluated inside data frame environment, allowing NSE specifications using column names e.g. ftransformv(data, c(col1, col2:coln), FUN). qF option sort = FALSE now generates factors levels first-appearance order (instead random order assigned hash function), can also called existing factor recast levels first-appearance order. also faster sort = FALSE (thanks group). finteraction argument sort = FALSE also take advantage group. rsplit improved performance gsplit, additional argument use.names, can used return unnamed list. Speedup vtypes functions num_vars, cat_vars, char_vars, logi_vars fact_vars. Note num_vars behaves slightly differently discussed . vlabels(<-) / setLabels rewritten C, giving ~20x speed improvement. Note now operate reference. vlabels, vclasses vtypes use.names argument. default TRUE (). colorder can rename columns fly also new mode pos = \"\" place selected columns first selected one, e.g.: colorder(mtcars, cyl, vs_new = vs, , pos = \"\"). pos = \"\" option also added roworderv. add_stub rm_stub additional cols argument apply stub certain columns e.g. add_stub(mtcars, \"new_\", cols = 6:9). namlab additional arguments N Ndistinct, allowing display number observations distinct values next variable names, labels classes, get nice quick overview variables large dataset. copyMostAttrib copies \"row.names\" attribute known valid. na_rm can now used efficiently remove empty NULL elements list. flag, fdiff fgrowth produce less messages (.e. message don’t use time variable grouped operations, messages computations highly irregular panel data data length exceeds 10 million obs.). print methods pwcor pwcov now return argument, allowing users obtain formatted correlation matrix, exporting purposes. replace_NA, recode_num recode_char improved performance additional argument set take advantage setv change () data reference. replace_NA, feature mature setting set = TRUE modify selected columns place return data invisibly. recode_num recode_char part transformations done reference, thus users still assign data preserve changes. future, improved set = TRUE toggles transformations done reference.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-165","dir":"Changelog","previous_headings":"","what":"collapse 1.6.5","title":"collapse 1.6.5","text":"CRAN release: 2021-07-24 Use VECTOR_PTR C API now gives error R-devel even USE_RINTERNALS defined. Thus patch gets rid remaining usage macro avoid errors CRAN checks using development version R. print method qsu now uses apostrophe (’) designate million digits, instead comma (,). avoid confusion decimal point, typical use (,) thousands (don’t like).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-164","dir":"Changelog","previous_headings":"","what":"collapse 1.6.4","title":"collapse 1.6.4","text":"CRAN release: 2021-07-13 Checks gcc11 compiler flagged additional issue pointer pointing element -1 C array (done purpose index R integer vector).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-163","dir":"Changelog","previous_headings":"","what":"collapse 1.6.3","title":"collapse 1.6.3","text":"CRAN checks flagged valgrind issue comparing uninitialized value something.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-162","dir":"Changelog","previous_headings":"","what":"collapse 1.6.2","title":"collapse 1.6.2","text":"CRAN release: 2021-07-04 CRAN maintainers asked remove line Makevars file intended reduce size Rcpp object files (since version 1.4). installed size package may now larger.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-161","dir":"Changelog","previous_headings":"","what":"collapse 1.6.1","title":"collapse 1.6.1","text":"patch 1.6.0 fixes issues flagged CRAN adds handy extras.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-6-1","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.6.1","text":"Puts examples using new base pipe |> inside \\donttest{} don’t fail CRAN tests older R versions. Fixes LTO issue caused small mistake header file (implications user detected CRAN checks).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-6-1","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.6.1","text":"Added function fcomputev, allows selecting columns transforming function one go. keep argument can used add columns selection transformed. Added function setLabels wrapper around vlabels<- facilitate setting variable labels inside pipes. Function rm_stub now argument regex = TRUE triggers call gsub allows general removing character sequences column names fly.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-6-1","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.6.1","text":"vlabels<- setLabels now support list variable labels attributes (.e. value internally subset using [[, [). Thus now general functions attach vector list attributes columns list / data frame.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-160","dir":"Changelog","previous_headings":"","what":"collapse 1.6.0","title":"collapse 1.6.0","text":"CRAN release: 2021-06-28 collapse 1.6.0, released end June 2021, presents significant improvements user-friendliness, compatibility programmability package, well function additions.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-6-0","dir":"Changelog","previous_headings":"","what":"Changes to Functionality","title":"collapse 1.6.0","text":"ffirst, flast, fnobs, fsum, fmin fmax rewritten C. former three now also support list columns (NULL empty list elements considered missing values na.rm = TRUE), extremely fast grouped aggregation na.rm = FALSE. latter three also support return integers, significant performance gains, even compared base R. Code using functions expecting error list-columns expecting double output even input integer adjusted. collapse now directly supports sf data frames functions like fselect, fsubset, num_vars, qsu, descr, varying, funique, roworder, rsplit, fcompute etc., take along geometry column even explicitly selected (mirroring dplyr methods sf data frames). mostly done internally C-level, functions remain simple fast. Existing code explicitly selects geometry column unaffected change, code form sf_data %>% num_vars %>% qDF %>% ..., columns excluding geometry selected object later converted data frame, needs rewritten sf_data %>% qDF %>% num_vars %>% .... short vignette added describing integration collapse sf. ’ve received several requests increased namespace consistency. collapse functions named consistent base R, dplyr data.table, resulting names like .Date, fgroup_by settransformv. makes sense, ’ve convinced bit consistency advantageous. Towards end decided eliminate ‘.’ notation base R remove unexpected capitalizations function names giving people impression using camel-case. following functions renamed: fNobs -> fnobs, fNdistinct -> fndistinct, pwNobs -> pwnobs, fHDwithin -> fhdwithin, fHDbetween -> fhdbetween, .factor_GRP -> as_factor_GRP, .factor_qG -> as_factor_qG, .GRP -> is_GRP, .qG -> is_qG, .unlistable -> is_unlistable, .categorical -> is_categorical, .Date -> is_date, .numeric_factor -> as_numeric_factor, .character_factor -> as_character_factor, Date_vars -> date_vars. done careful manner, others stick around long (end 2022), generics fNobs, fNdistinct, fHDbetween fHDwithin kept package indeterminate period, core methods exported beyond 2022. start warning renamed functions 2022. future undogmatically stick function naming style lowercase function names underslashes words need split. function names kept. say something : quick-conversion functions qDF qDT, qM, qF, qG consistent -line data.table (setDT etc.), similarly operators L, F, D, Dlog, G, B, W, HDB, HDW. ’ll keep GRP, TRA, lack better names, parsimony central package. camel case kept helper functions setDimnames etc. work like stats setNames modify argument reference (like settransform setrename various data.table functions). Functions copyAttrib copyMostAttrib exports like-named functions C API thus kept . Finally, want keep fFtest way F-distribution widely recognized capital F. ’ve updated wlddev dataset latest data World Bank, also added variable giving total population (may useful e.g. population-weighted aggregations across regions). extra column invalidate codes used demonstrate something (adjust examples, tests code vignettes).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-6-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.6.0","text":"Added function fcumsum (written C), permitting flexible (grouped, ordered) cumulative summations matrix-like objects (integer double typed) extra methods grouped data frames panel series data frames. Apart internal grouping, ordering argument allowing cumulative sums different order data appear, fcumsum 2 options deal missing values. default (na.rm = TRUE) skip (preserve) missing values, whereas setting fill = TRUE allows missing values populated previous value cumulative sum (starting 0). Added function alloc efficiently generate vectors initialized value (faster rep_len). Added function pad efficiently pad vectors / matrices / data.frames value (default NA). function mainly created make easy expand results coming statistical model fitted data missing values original length. example let data <- na_insert(mtcars); mod <- lm(mpg ~ cyl, data), can settransform(data, resid = pad(resid(mod), mod$na.action)), pad(model.matrix(mod), mod$na.action) pad(model.frame(mod), mod$na.action) receive matrices data frames model data matching rows data. pad general function also work mixed-type data. also possible pass vector indices matching rows data pad, case pad fill gaps indices value/row data.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-6-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.6.0","text":"Full data.table support, including reference semantics (set*, :=)!! complex C-level programming behind data.table’s operations reference. Notably, additional (hidden) column pointers allocated able add columns without taking shallow copy data.table, \".internal.selfref\" attribute containing external pointer used check shallow copy made using base R commands like <-. done avoid even shallow copy data.table manipulations using := (opinion worth even large tables shallow copied base R (>=3.1.0) within microseconds complicates development immensely). Previously, collapse treated data.table’s like data frame, using shallow copies manipulations preserving attributes (thus ignoring data.table works internally). produced warning whenever wanted use data.table reference semantics (set*, :=) passing data.table collapse function collap, fselect, fsubset, fgroup_by etc. v1.6.0, adopted essential C code data.table overallocation generate \".internal.selfref\" attribute, thus seamless workflows combining collapse data.table now possible. comes cost 2-3 microseconds per function, shallow copy data.table add extra column pointers \".internal.selfref\" attribute telling data.table table copied (seems way now). integration encompasses data manipulation functions collapse, Fast Statistical Functions . Thus can agDT <- DT %>% fselect(id, col1:coln) %>% collap(~id, fsum); agDT[, newcol := 1], need add qDT function like fsum want use reference semantics without incurring warning: agDT <- DT %>% fselect(id, col1:coln) %>% fgroup_by(id) %>% fsum %>% qDT; agDT[, newcol := 1]. collapse appears first package attempts account data.table’s internal working without importing data.table, qDT now fastest way create fully functional data.table R object. global option \"collapse_DT_alloccol\" added regulate many columns collapse overallocates creating data.table’s. default 100, lower data.table default 1024. done increase efficiency additional shallow copies, may changed user. Programming enabled fselect fgroup_by (can now pass vectors containing column names indices). Note instead fselect use get_vars standard eval programming. fselect fsubset support -place renaming, e.g. fselect(data, newname = var1, var3:varN), fsubset(data, vark > varp, newname = var1, var3:varN). collap supports renaming columns custom argument, e.g. collap(data, ~ id, custom = list(fmean = c(newname = \"var1\", \"var2\"), fmode = c(newname = 3), flast = is_date)). Performance improvements: fsubset / ss return data perform simple column subset without deep copying data rows selected logical expression. fselect get_vars, num_vars etc. slightly faster data frame subsetting done fully C. ftransform / fcompute use alloc instead base::rep replicate scalar value slightly efficient. fcompute now keep argument, preserve several existing columns computing columns data frame. replace_NA now cols argument, can replace_NA(data, cols = .numeric), replace NA’s numeric columns. note big numeric data data.table::setnafill efficient solution. fhdbetween fhdwithin effect argument plm methods, allowing centering selected identifiers. default still center panel identifiers.  plot method panel series matrices arrays plot.psmat improved slightly. now supports custom colours drawing grid. settransform settransformv can now called without attaching package e.g. collapse::settransform(data, ...). errored collapse loaded simply wrappers around data <- ftransform(data, ...). ’d like note discussion avoiding shallow copies <- (e.g. via :=) appear yield noticeable performance gains. data.table faster big data mostly parallelism sometimes algorithms, generally memory efficiency. Functions setAttrib, copyAttrib copyMostAttrib make shallow copy lists, atomic vectors (amounts full copy inefficient). Thus atomic objects now modified -place. Small improvements: Calling qF(x, ordered = FALSE) ordered factor remove ordered class, operators L, F, D, Dlog, G, B, W, HDB, HDW functions like pwcor now work unnamed matrices data frames.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-153","dir":"Changelog","previous_headings":"","what":"collapse 1.5.3","title":"collapse 1.5.3","text":"CRAN release: 2021-03-07 test occasionally fails Mac removed, unit testing now removed CRAN. collapse close 10,000 unit tests covering central pieces code. Half tests depend generated data, reasons always test two occasionally fail operating system (usually Windows), requiring submit patch. constructive either development use package, therefore tests now removed CRAN. still run codecov.io, every new release thoroughly tested Windows.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-152","dir":"Changelog","previous_headings":"","what":"collapse 1.5.2","title":"collapse 1.5.2","text":"CRAN release: 2021-03-02","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-5-2","dir":"Changelog","previous_headings":"","what":"Changes to Functionality","title":"collapse 1.5.2","text":"first argument ftransform renamed .data X. done enable user transform columns named “X”. reason first argument frename renamed .x x (.data make explicit .x can R object “names” attribute). possible depreciate X x without time undoing benefits argument renaming, thus change immediate code breaking rare cases first argument explicitly set. function .regular check whether R object atomic list-like depreciated removed end year. done avoid namespace clash zoo package (#127).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-5-2","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.5.2","text":"unlist2d produced subsetting error empty list present list-tree. now fixed, empty NULL elements list-tree simply ignored (#99).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-5-2","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.5.2","text":"function fsummarize added facilitate translating dplyr / data.table code collapse. Like collap, fast used Fast Statistical Functions. function t_list made available efficiently transpose lists lists.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-5-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.5.2","text":"C files compiled -O3 Windows, gives boost around 20% grouping mechanism applied character data.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-151","dir":"Changelog","previous_headings":"","what":"collapse 1.5.1","title":"collapse 1.5.1","text":"CRAN release: 2021-01-12 small patch 1.5.0 : Fixes numeric precision issue grouping doubles (e.g. qF(wlddev$LIFEEX) gave error, now works). Fixes minor issue fhdwithin applied pseries fill = FALSE.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-150","dir":"Changelog","previous_headings":"","what":"collapse 1.5.0","title":"collapse 1.5.0","text":"CRAN release: 2021-01-04 collapse 1.5.0, released early January 2021, presents important refinements additional functionality.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"back-to-cran-1-5-0","dir":"Changelog","previous_headings":"","what":"Back to CRAN","title":"collapse 1.5.0","text":"apologize inconveniences caused temporal archival collapse December 19, 2020. archival caused archival important lfe package 4th December. collapse depended lfe higher-dimensional centering, providing fhdbetween / fhdwithin functions generalized linear projecting / partialling . remedy damage caused removal lfe, rewrite fhdbetween / fhdwithin take advantage demeaning algorithm provided fixest, quite different mechanics. Beforehand, made significant changes fixest::demean make integration happen. CRAN deadline 18th December, realized late make . request CRAN extension declined, collapse got archived 19th. learned experience, collapse now sufficiently insulated taken CRAN even suggested packages removed CRAN.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-5-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.5.0","text":"Segfaults several Fast Statistical Functions passed numeric(0) fixed (thanks @eshom @acylam, #101). default behavior collapse functions return numeric(0) , except fnobs, fndistinct return 0L, fvar, fsd return NA_real_.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-5-0","dir":"Changelog","previous_headings":"","what":"Changes to Functionality","title":"collapse 1.5.0","text":"Functions fhdwithin / HDW fhdbetween / HDB reworked, delivering higher performance greater functionality: higher-dimensional centering heterogeneous slopes, demean function fixest package imported (conditional availability package). linear prediction partialling functionality now built around flm also allows weights different fitting methods. collap, default behavior give.names = \"auto\" altered used together custom argument. function name always added column names. Now added column aggregated two different functions. apologize breaks code dependent new names, behavior just better reflects common use (applying one function per column), well STATA’s collapse. list processing functions like get_elem, has_elem etc. default argument DF..list changed TRUE FALSE. means nested lists contains data frame’s, data frame’s searched matching elements. default also reflects common usage functions (extracting entire data frame’s computed quantities nested lists rather searching / subsetting lists data frame’s). change also delivers considerable performance gain. Vignettes outsourced website. nearly halves size source package, induce users appreciate built-documentation. website also makes much convenient reading navigation book-style vignettes.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-5-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.5.0","text":"Added set 10 operators %rr%, %r+%, %r-%, %r*%, %r/%, %cr%, %c+%, %c-%, %c*%, %c/% facilitate speed row- column-wise arithmetic operations involving vector matrix / data frame / list. example X %r*% v efficiently multiplies every row X v. Note advanced functionality already provided TRA(), dapply() Fast Statistical Functions, operators intuitive convenient use matrix matrix-style code, piped expressions. Added function missing_cases (opposite complete.cases faster data frame’s / lists). Added function allNA atomic vectors. New vignette using collapse together data.table, available online.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-5-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.5.0","text":"Time series functions operators flag / L / F, fdiff / D / Dlog fgrowth / G now natively support irregular time series panels, feature ‘complete approach’ .e. values shifted around taking full account underlying time-dimension! Functions pwcor pwcov can now compute weighted correlations pairwise complete observations, supported C-code (conditionally) imported weights package. fFtest now also supports weights. collap now provides easy workaround aggregate columns using weights others without. user may simply append names Fast Statistical Functions _uw disable weights. Example: collapse::collap(mtcars, ~ cyl, custom = list(fmean_uw = 3:4, fmean = 8:10), w = ~ wt) aggregates columns 3 4 using simple mean columns 8 10 using weighted mean. parallelism collap using parallel::mclapply reworked operate column-level, function level . still available Windows though. default number cores set mc.cores = 2L, now gives error windows parallel = TRUE. function recode_char now additional options ignore.case fixed (passed grepl), enhanced recoding character data based regular expressions. rapply2d now classes argument permitting flexible use. na_rm internal functions rewritten C. na_rm now 2x faster x[!.na(x)] missing values 10x faster without missing values.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-142","dir":"Changelog","previous_headings":"","what":"collapse 1.4.2","title":"collapse 1.4.2","text":"CRAN release: 2020-11-10 improvement [.GRP_df method enabling use data.table methods (:=) grouped data.table created fgroup_by. documentation updates Kevin Tappe.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-141","dir":"Changelog","previous_headings":"","what":"collapse 1.4.1","title":"collapse 1.4.1","text":"CRAN release: 2020-11-09 collapse 1.4.1 small patch 1.4.0 : fixes clang-UBSAN rchk issues 1.4.0 (minor bugs compiled code resulting, case, trying coerce NaN value integer, failing protect shallow copy variable). Adds method [.GRP_df allows robust subsetting grouped objects created fgroup_by (thanks Patrice Kiener flagging ).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-140","dir":"Changelog","previous_headings":"","what":"collapse 1.4.0","title":"collapse 1.4.0","text":"CRAN release: 2020-11-01 collapse 1.4.0, released early November 2020, presents important refinements, particularly domain attribute handling, well additional functionality. changes make collapse smarter, broadly compatible secure, break existing code.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-4-0","dir":"Changelog","previous_headings":"","what":"Changes to Functionality","title":"collapse 1.4.0","text":"Deep Matrix Dispatch / Extended Time Series Support: default methods statistical transformation functions dispatch matrix method .matrix(x) && !inherits(x, \"matrix\") evaluates TRUE. specification avoids invoking default method classed matrix-based objects (multivariate time series xts / zoo class) inheriting ‘matrix’ class, still allowing user manually call default method matrices (objects implicit explicit ‘matrix’ class). change implies collapse’s generic statistical functions now well suited transform xts / zoo many time series matrix-based classes. Fully Non-Destructive Piped Workflow: fgroup_by(x, ...) now adds class grouped_df, classes table_df, tbl, grouped_df, preserves classes x. implies workflows x %>% fgroup_by(...) %>% fmean etc. yields object xAG class attributes x, tibble . collapse aims broadly compatible, class-agnostic attribute preserving possible. Thorough Controlled Object Conversions: Quick conversion functions qDF, qDT qM now additional arguments keep.attr class providing precise user control object conversions terms classes attributes assigned / maintained. default (keep.attr = FALSE) yields hard conversions removing essential attributes object. E.g. qM(EuStockMarkets) just returned EuStockMarkets (.matrix(EuStockMarkets) TRUE) whereas now time series class ‘tsp’ attribute removed. qM(EuStockMarkets, keep.attr = TRUE) returns EuStockMarkets . Smarter Attribute Handling: Drawing guidance given R Internals manual, following standards optimal non-destructive attribute handling formalized communicated user: default matrix methods Fast Statistical Functions preserve attributes input grouped aggregations (‘names’, ‘dim’ ‘dimnames’ suitably modified). inputs classed objects (e.g. factors, time series, checked .object), class attributes dropped. Simple (non-grouped) aggregations vectors matrices preserve attributes, unless drop = FALSE matrix method. exemption made default methods functions ffirst, flast fmode, always preserve attributes (input well factor date variable). data frame methods unaltered: attributes data frame columns data frame preserved unless computation result column scalar (computing groups) drop = TRUE (default). Transformations functions like flag, fwithin, fscale etc. also unaltered: attributes input preserved output (regardless whether input vector, matrix, data.frame related classed object). holds transformation options modifying input (“-”, “-+”, “/”, “+”, “*”, “%%”, “-%%”) using TRA() function TRA = \"...\" argument Fast Statistical Functions. TRA ‘replace’ ‘replace_fill’ options, data type STATS preserved, x. provides better results particularly functions like fnobs fndistinct. E.g. previously fnobs(letters, TRA = \"replace\") returned observation counts coerced character, letters character. Now result integer typed. attribute handling means attributes x preserved unless x classed object data types x STATS match. exemption rule made x factor integer (non-factor) replacement offered STATS. case attributes x copied exempting ‘class’ ‘levels’ attribute, e.g. fnobs(iris$Species, TRA = \"replace\") gives integer vector, (malformed) factor. unlikely event STATS classed object, attributes STATS preserved attributes x discarded.   Reduced Dependency Burden: dependency lfe package made optional. Functions fhdwithin / fhdbetween can perform higher-dimensional centering lfe available. Linear prediction centering single factor (among list covariates) still possible without installing lfe. change means collapse now depends base R Rcpp supported R version 2.10.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-4-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.4.0","text":"Added function rsplit efficient (recursive) splitting vectors data frames. Added function fdroplevels fast missing level removal + added argument drop qF GRP.factor, default drop = FALSE. addition fdroplevels also enhances speed fFtest function. fgrowth supports annualizing / compounding growth rates added power argument. function flm added bare bones (weighted) linear regression fitting using different efficient methods: 4 base R (.lm.fit, solve, qr, chol), using fastLm RcppArmadillo (installed), fastLm RcppEigen (installed). Added function qTBL quickly convert R objects tibble. helpers setAttrib, copyAttrib copyMostAttrib exported fast attribute handling R (similar attributes<-(), functions return shallow copy first argument set attributes replaced, perform checks attribute validity like attributes<-(). can yield large performance gains big objects). helper cinv added wrapping expression chol2inv(chol(x)) (efficient inverse symmetric, positive definite matrix via Choleski factorization). shortcut gby now available abbreviate frequently used fgroup_by function. print method grouped data frames class added.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-4-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.4.0","text":"Faster internal methods factors funique, fmode fndistinct. grouped_df methods flag, fdiff, fgrowth now also support multiple time variables identify panel e.g. data %>% fgroup_by(region, person_id) %>% flag(1:2, list(month, day)). security features fsubset.data.frame / ss, ss now internal generic also supports subsetting matrices. functions (like na_omit), passing double values (e.g. 1 instead integer 1L) negative indices cols argument produced error unexpected behavior. now fixed functions. Fixed bug helper function all_obj_equal occurring objects equal. performance improvements increased use pointers C API functions.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-132","dir":"Changelog","previous_headings":"","what":"collapse 1.3.2","title":"collapse 1.3.2","text":"CRAN release: 2020-09-13 collapse 1.3.2, released mid September 2020: Fixed small bug fndistinct grouped distinct value counts logical vectors. Additional security ftransform, now efficiently checks names data replacement arguments uniqueness, also allows computing transforming list-columns. Added function ftransformv facilitate transforming selected columns function - efficient replacement dplyr::mutate_if dplyr::mutate_at. frename now allows additional arguments passed renaming function.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-131","dir":"Changelog","previous_headings":"","what":"collapse 1.3.1","title":"collapse 1.3.1","text":"CRAN release: 2020-08-27 collapse 1.3.1, released end August 2020, patch v1.3.0 takes care unit test failures certain operating systems (mostly numeric precision issues). provides changes code functionality.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-130","dir":"Changelog","previous_headings":"","what":"collapse 1.3.0","title":"collapse 1.3.0","text":"CRAN release: 2020-08-11 collapse 1.3.0, released mid August 2020:","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-3-0","dir":"Changelog","previous_headings":"","what":"Changes to Functionality","title":"collapse 1.3.0","text":"dapply now drop unnecessary attributes return = \"matrix\" return = \"data.frame\" explicitly requested (default return = \"\" still seeks preserve input data structure). unlist2d now saves integer rownames row.names = TRUE list matrices without rownames passed, id.factor = TRUE generates normal factor ordered factor. however possible write id.factor = \"ordered\" get ordered factor id. fdiff argument logdiff renamed log, taking logs now done R (reduces size C++ code generate many NaN’s). logdiff may still used, may deactivated future. Also matrix data.frame methods flag, fdiff fgrowth, columns stub-renamed one lag/difference/growth rate computed.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-3-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.3.0","text":"Added fnth fast (grouped, weighted) n’th element/quantile computations. Added roworder(v) colorder(v) fast row column reordering. Added frename setrename fast flexible renaming (reference). Added function fungroup, replacement dplyr::ungroup, intended use fgroup_by. fmedian now supports weights, computing decently fast (grouped) weighted median based radix ordering. fmode now option compute min max mode, default still simply first mode. fwithin now supports quasi-demeaning (added argument theta) can thus used manually estimate random-effects models. funique now generic default vector data.frame method, providing fast unique values rows data. default changed sort = FALSE. shortcut gvr created get_vars(..., regex = TRUE). helper .c introduced non-standard concatenation (.e. .c(, b) == c(\"\", \"b\")).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-3-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.3.0","text":"fmode fndistinct become bit faster. fgroup_by now preserves data.table’s. ftransform now also supports data.frame replacement argument, automatically replaces matching columns adds unmatched ones. Also ftransform<- created formal replacement method feature. collap columns selected cols argument returned order selected keep.col.order = FALSE. Argument sort.row depreciated, replace argument sort. addition decreasing na.last arguments added handed GRP.default. radixorder ‘sorted’ attribute now always attached. stats::D masked collapse attached, now preserved methods D.expression D.call. GRP option call = FALSE omit call match.call -> minor performance improvement. Several small performance improvements rewriting internal helper functions C reworking R code. Performance improvements helper functions, setRownames / setColnames, na_insert etc. Increased scope testing statistical functions. functionality package now secured 7700 unit tests covering central bits pieces.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-121","dir":"Changelog","previous_headings":"","what":"collapse 1.2.1","title":"collapse 1.2.1","text":"CRAN release: 2020-05-26 collapse 1.2.1, released end May 2020: Minor fixes 1.2.0 issues prevented correct installation Mac OS X vignette rebuilding error solaris. fmode.grouped_df groups weights now saves sum weights instead max (makes sense max applies elements unique).","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-120","dir":"Changelog","previous_headings":"","what":"collapse 1.2.0","title":"collapse 1.2.0","text":"CRAN release: 2020-05-19 collapse 1.2.0, released mid May 2020:","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"changes-to-functionality-1-2-0","dir":"Changelog","previous_headings":"","what":"Changes to Functionality","title":"collapse 1.2.0","text":"grouped_df methods fast statistical functions now always attach grouping variables output aggregations, unless argument keep.group_vars = FALSE. (formerly grouping variables attached also present data. Code hinged feature adjusted) qF ordered argument default changed ordered = FALSE, NA level added na.exclude = FALSE. Thus qF now behaves exactly like .factor. Recode depreciated favor recode_num recode_char, removed soon. Similarly replace_non_finite renamed replace_Inf. mrtl mctl argument ret renamed return now takes descriptive character arguments (previous version direct C++ export unsafe, code written functions adjusted). GRP argument order depreciated favor argument decreasing. order can still used removed point.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"bug-fixes-1-2-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"collapse 1.2.0","text":"Fixed bug flag unused factor levels caused group size error.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"additions-1-2-0","dir":"Changelog","previous_headings":"","what":"Additions","title":"collapse 1.2.0","text":"Added suite functions fast data manipulation: fselect selects variables data frame equivalent much faster dplyr::select. fsubset much faster version base::subset subset vectors, matrices data.frames. function ss also added faster alternative [.data.frame. ftransform much faster update base::transform, transform data frames adding, modifying deleting columns. function settransform reference. fcompute equivalent ftransform returns new data frame containing columns computed existing one. na_omit much faster enhanced version base::na.omit. replace_NA efficiently replaces missing values multi-type data. Added function fgroup_by much faster version dplyr::group_by based collapse grouping. attaches ‘GRP’ object data frame, works collapse’s fast functions. allows dplyr like manipulations fully collapse based thus significantly faster, .e. data %>% fgroup_by(g1,g2) %>% fselect(cola,colb) %>% fmean. Note data %>% dplyr::group_by(g1,g2) %>% dplyr::select(cola,colb) %>% fmean still works, case dplyr ‘group’ object converted ‘GRP’ . However data %>% fgroup_by(g1,g2) %>% dplyr::summarize(...) work. Added function varying efficiently check variation multi-type data dimension within groups. Added function radixorder, base::order(..., method = \"radix\") accessible built-grouping features. Added functions seqid groupid generalized run-length type id variable generation grouping time variables. seqid particular strongly facilitates lagging / differencing irregularly spaced panels using flag, fdiff etc. fdiff now supports quasi-differences .e. x_t - \\rho x_{t-1} quasi-log differences .e. log(x_t) - \\rho log(x_{t-1}). arbitrary \\rho can supplied. Added Dlog operator faster access log-differences.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"improvements-1-2-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"collapse 1.2.0","text":"Faster grouping GRP faster factor generation added radix method + automatic dispatch hash radix method. qF now ~ 5x faster .factor character around 30x faster numeric data. Also qG enhanced. slight speed tweaks . collap now provides control weighted aggregations additional arguments w, keep.w wFUN aggregate weights well. defaults keep.w = TRUE wFUN = fsum. specialty collap remains keep.keep.w also work external objects passed, code form collap(data, , FUN, catFUN, w = data$weights) now aggregated weights vector first column. qsu now also allows weights passed formula .e. qsu(data, = ~ group, pid = ~ panelid, w = ~ weights). fgrowth scale argument, default scale = 100 provides growth rates percentage terms (), may now changed. statistical transformation functions now hidden list method, can applied unclassed list-objects well. error however provided grouped operations unequal-length columns.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-110","dir":"Changelog","previous_headings":"","what":"collapse 1.1.0","title":"collapse 1.1.0","text":"CRAN release: 2020-04-01 collapse 1.1.0 released early April 2020: Fixed remaining gcc10, LTO valgrind issues C/C++ code, added tests (now ~ 5300 tests ensuring collapse statistical functions perform expected). Fixed issue supplying unnamed list GRP(), .e. GRP(list(v1, v2)) give error. Unnamed lists now automatically named ‘Group.1’, ‘Group.2’, etc… Fixed issue aggregating single id collap() (.e. collap(data, ~ id1)), id coded factor aggregated data.frame. variables including id’s now retain class attributes aggregated data. Added weights (w) argument fsum fprod. Added argument mean = 0 fwithin / W. allows simple grouped centering arbitrary mean, 0 default. grouped centering mean = \"overall.mean\" can specified, center data overall mean data. logical argument add.global.mean = TRUE used toggle collapse 1.0.0 therefore depreciated. Added arguments mean = 0 (default) sd = 1 (default) fscale / STD. arguments now allow (group) scale center data arbitrary mean standard deviation. Setting mean = FALSE just scale data preserving mean(s). Special options grouped scaling mean = \"overall.mean\" (fwithin / W), sd = \"within.sd\", scale data standard deviation group equal within- standard deviation (= standard deviation computed group-centered data). Thus group scaling panel-dataset mean = \"overall.mean\" sd = \"within.sd\" harmonizes data across groups terms mean variance. fast algorithm variance calculation toggled stable.algo = FALSE removed fscale. Welford’s numerically stable algorithm used default fast enough practical purposes. fast algorithm still available fvar fsd. Added modulus (%%) subtract modulus (-%%) operations TRA(). Added function finteraction, fast interactions, as_character_factor coerce factor, factors list, character (analogous as_numeric_factor). Also exported function ckmatch, matching error message showing non-matched elements.","code":""},{"path":"https://sebkrantz.github.io/collapse/news/index.html","id":"collapse-100-and-earlier","dir":"Changelog","previous_headings":"","what":"collapse 1.0.0 and earlier","title":"collapse 1.0.0 and earlier","text":"CRAN release: 2020-03-19 First version package featuring functions collap qsu based code shared Sebastian Krantz R-devel, February 2019. Major rework package using Rcpp data.table internals, introduction fast statistical functions operators expansion scope package broad set data transformation exploration tasks. Several iterations enhancing speed R code. Seamless integration collapse dplyr, plm data.table. CRAN release collapse 1.0.0 19th March 2020.","code":""}]
